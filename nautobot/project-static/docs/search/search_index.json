{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Overview \u00b6 Welcome to the Nautobot Core Documentation! Use the top navigation bar to read about: Installing Nautobot , Configuring Nautobot , Getting Started with the Web Interface , REST and GraphQL APIs, Plugins , additional features and much more can be found under the Documentation section. Learn about how to Develop and Contribute to Nautobot , Set up your Development Environment , and more under the Development Guide section. Nautobot has a thriving ecosystem of Apps (also known as Plugins), developed as separate projects, for which you can find links to documentation under the Nautobot Apps section. What is Nautobot? \u00b6 Nautobot is an open source Network Source of Truth and Network Automation Platform. Nautobot was initially developed as a fork of NetBox (v2.10.4), which was originally created by Jeremy Stretch at DigitalOcean and by the NetBox open source community. Key Use Cases \u00b6 Nautobot enables three (3) key use cases. Flexible Source of Truth for Networking - Nautobot core data models are used to define the intended state of network infrastructure enabling it as a Source of Truth. While a baseline set of models are provided (such as IP networks and addresses, devices and racks, circuits and cable, etc.) it is Nautobot's goal to offer maximum data model flexibility. This is enabled through features such as user-defined relationships, custom fields on any model, and data validation that permits users to codify everything from naming standards to having automated tests run before data can be populated into Nautobot. Extensible Data Platform for Automation - Nautobot has a rich feature set to seamlessly integrate with network automation solutions. Nautobot offers GraphQL and native Git integration along with REST APIs and webhooks. Git integration dynamically loads YAML data files as Nautobot config contexts. Nautobot also has an evolving plugin system that enables users to create custom models, APIs, and UI elements. The plugin system is also used to unify and aggregate disparate data sources creating a Single Source of Truth to streamline data management for network automation. Platform for Network Automation Apps - The Nautobot plugin system enables users to create Network Automation Apps. Apps can be as lightweight or robust as needed based on user needs. Using Nautobot for creating custom applications saves up to 70% development time by re-using features such as authentication, permissions, webhooks, GraphQL, change logging, etc. all while having access to the data already stored in Nautobot. Some production ready applications include: Golden Configuration Device Lifecycle Firewall Models SSoT ChatOps Circuit Maintenance Capacity Metrics Device Onboarding Design Philosophy \u00b6 The following tenets drive the direction of Nautobot. Replicate the Real World \u00b6 Careful consideration has been given to the data model to ensure that it can accurately reflect a real-world network. For instance, IP addresses are assigned not to devices, but to specific interfaces attached to a device, and an interface may have multiple IP addresses assigned to it. Serve as a Source of Truth (SoT) \u00b6 Nautobot intends to represent the desired state of a network versus its operational state. While plugins and apps can be built and integrated with Nautobot to extend its capabilities to various aspects of the operational state, the core platform's focus is on the desired state. Serve as a Network Automation Platform \u00b6 Nautobot intends to be a vehicle to deliver high-value network automation apps. Using the extensible plugin system, users have the choice and freedom to create the integrations that make sense for them. Ensure Maximum Flexibility & Extensibility \u00b6 While Nautobot intends to replicate the real world and offer opinionated models to get started defining the intended state of the network, it is understood that organizations and networks have unique design considerations that may need to be addressed in the SoT or Network Automation Platform. Nautobot strives to enable flexibility and extensibility to power and automate all types of networks. Keep it Simple \u00b6 When given a choice between a relatively simple 80% solution and a much more complex complete solution, the former will typically be favored. This ensures a lean codebase with a low learning curve. Application Stack \u00b6 Nautobot is built on the Django Python Web framework and requires either a PostgreSQL or MySQL database backend. It runs as a WSGI service behind your choice of HTTP server. Function Component HTTP service NGINX WSGI service uWSGI or Gunicorn Application Django/Python Database PostgreSQL 9.6+ or MySQL 8.0+ Cache Redis/django-cacheops Task queuing Redis/Celery/django-rq Live device access NAPALM Added in version 1.1.0 MySQL support was added. The following diagram displays how data travels through Nautobot's application stack. Supported Python Versions \u00b6 Nautobot supports Python 3.7, 3.8, 3.9, and 3.10. Added in version 1.3.0 Python 3.10 support was added. Removed in version 1.3.0 Python 3.6 support was removed. Getting Started \u00b6 See the installation guide for help getting Nautobot up and running quickly.","title":"Overview"},{"location":"index.html#overview","text":"Welcome to the Nautobot Core Documentation! Use the top navigation bar to read about: Installing Nautobot , Configuring Nautobot , Getting Started with the Web Interface , REST and GraphQL APIs, Plugins , additional features and much more can be found under the Documentation section. Learn about how to Develop and Contribute to Nautobot , Set up your Development Environment , and more under the Development Guide section. Nautobot has a thriving ecosystem of Apps (also known as Plugins), developed as separate projects, for which you can find links to documentation under the Nautobot Apps section.","title":"Overview"},{"location":"index.html#what-is-nautobot","text":"Nautobot is an open source Network Source of Truth and Network Automation Platform. Nautobot was initially developed as a fork of NetBox (v2.10.4), which was originally created by Jeremy Stretch at DigitalOcean and by the NetBox open source community.","title":"What is Nautobot?"},{"location":"index.html#key-use-cases","text":"Nautobot enables three (3) key use cases. Flexible Source of Truth for Networking - Nautobot core data models are used to define the intended state of network infrastructure enabling it as a Source of Truth. While a baseline set of models are provided (such as IP networks and addresses, devices and racks, circuits and cable, etc.) it is Nautobot's goal to offer maximum data model flexibility. This is enabled through features such as user-defined relationships, custom fields on any model, and data validation that permits users to codify everything from naming standards to having automated tests run before data can be populated into Nautobot. Extensible Data Platform for Automation - Nautobot has a rich feature set to seamlessly integrate with network automation solutions. Nautobot offers GraphQL and native Git integration along with REST APIs and webhooks. Git integration dynamically loads YAML data files as Nautobot config contexts. Nautobot also has an evolving plugin system that enables users to create custom models, APIs, and UI elements. The plugin system is also used to unify and aggregate disparate data sources creating a Single Source of Truth to streamline data management for network automation. Platform for Network Automation Apps - The Nautobot plugin system enables users to create Network Automation Apps. Apps can be as lightweight or robust as needed based on user needs. Using Nautobot for creating custom applications saves up to 70% development time by re-using features such as authentication, permissions, webhooks, GraphQL, change logging, etc. all while having access to the data already stored in Nautobot. Some production ready applications include: Golden Configuration Device Lifecycle Firewall Models SSoT ChatOps Circuit Maintenance Capacity Metrics Device Onboarding","title":"Key Use Cases"},{"location":"index.html#design-philosophy","text":"The following tenets drive the direction of Nautobot.","title":"Design Philosophy"},{"location":"index.html#replicate-the-real-world","text":"Careful consideration has been given to the data model to ensure that it can accurately reflect a real-world network. For instance, IP addresses are assigned not to devices, but to specific interfaces attached to a device, and an interface may have multiple IP addresses assigned to it.","title":"Replicate the Real World"},{"location":"index.html#serve-as-a-source-of-truth-sot","text":"Nautobot intends to represent the desired state of a network versus its operational state. While plugins and apps can be built and integrated with Nautobot to extend its capabilities to various aspects of the operational state, the core platform's focus is on the desired state.","title":"Serve as a Source of Truth (SoT)"},{"location":"index.html#serve-as-a-network-automation-platform","text":"Nautobot intends to be a vehicle to deliver high-value network automation apps. Using the extensible plugin system, users have the choice and freedom to create the integrations that make sense for them.","title":"Serve as a Network Automation Platform"},{"location":"index.html#ensure-maximum-flexibility-extensibility","text":"While Nautobot intends to replicate the real world and offer opinionated models to get started defining the intended state of the network, it is understood that organizations and networks have unique design considerations that may need to be addressed in the SoT or Network Automation Platform. Nautobot strives to enable flexibility and extensibility to power and automate all types of networks.","title":"Ensure Maximum Flexibility &amp; Extensibility"},{"location":"index.html#keep-it-simple","text":"When given a choice between a relatively simple 80% solution and a much more complex complete solution, the former will typically be favored. This ensures a lean codebase with a low learning curve.","title":"Keep it Simple"},{"location":"index.html#application-stack","text":"Nautobot is built on the Django Python Web framework and requires either a PostgreSQL or MySQL database backend. It runs as a WSGI service behind your choice of HTTP server. Function Component HTTP service NGINX WSGI service uWSGI or Gunicorn Application Django/Python Database PostgreSQL 9.6+ or MySQL 8.0+ Cache Redis/django-cacheops Task queuing Redis/Celery/django-rq Live device access NAPALM Added in version 1.1.0 MySQL support was added. The following diagram displays how data travels through Nautobot's application stack.","title":"Application Stack"},{"location":"index.html#supported-python-versions","text":"Nautobot supports Python 3.7, 3.8, 3.9, and 3.10. Added in version 1.3.0 Python 3.10 support was added. Removed in version 1.3.0 Python 3.6 support was removed.","title":"Supported Python Versions"},{"location":"index.html#getting-started","text":"See the installation guide for help getting Nautobot up and running quickly.","title":"Getting Started"},{"location":"additional-features/caching.html","text":"Caching \u00b6 A fundamental trade-off in dynamic websites like Nautobot is that, well, they\u2019re dynamic. Each time a user requests a page, the Web server makes all sorts of calculations \u2013 from database queries to template rendering to business logic \u2013 to create the page that your site\u2019s visitor sees. This is a lot more expensive, from a processing-overhead perspective, than your standard read-a-file-off-the-filesystem server arrangement. That\u2019s where caching comes in. To cache something is to save the result of an expensive calculation so that you don\u2019t have to perform the calculation next time. Nautobot makes extensive use of caching; this is not a simple topic but it's a useful one for a Nautobot administrator to understand, so read on if you please. How it Works \u00b6 Nautobot supports database query caching using django-cacheops and Redis. When a query is made, the results are cached in Redis for a short period of time, as defined by the CACHEOPS_DEFAULTS parameter (15 minutes by default). Within that time, all recurrences of that specific query will return the pre-fetched results from the cache. Caching can be completely disabled by toggling CACHEOPS_ENABLED to False (it is True by default). If a change is made to any of the objects returned by the cached query within that time, or if the timeout expires, the cached results are automatically invalidated and the next request for those results will be sent to the database. Caching is a complex topic and there are some important details to clarify with how caching is implemented and configured in Nautobot. Caching in Django \u00b6 Django includes with its own cache framework that works for common cases, but does not work well for the wide array of use-cases within Nautobot. For that reason, Django's built-in caching is not used for the caching of web UI views, API results, and underlying database queries. Instead, we use django-cacheops . Please see below for more on this. CACHES and django-redis \u00b6 The CACHES setting is used to, among other things, configure Django's built-in caching. You'll observe that, even though we aren't using Django's built-in caching, we still have this as a required setting . Here's why: Nautobot uses the django-redis Django plugin which allows it to use Redis as a backend for caching and session storage. This is used to provide a concurrent write lock for preventing race conditions when allocating IP address objects, and also to define centralized Redis connection settings that will be used by RQ. django-redis also uses the CACHES setting, in its case to simplify the configuration for establishing concurrent write locks, and also for referencing the correct Redis connection information when defining RQ task queues using the RQ_QUEUES setting. Again: CACHES is not used for Django's built-in caching at this time, but it is still a required setting for django-redis to function properly. Django Cacheops \u00b6 Cacheops (aka django-cacheops ) is a Django plugin that does some very advanced caching, but does not leverage the built-in cache framework . Instead it uses a technique called \"monkey patching\" . By monkey patching, a library can inject its own functionality into the core code behind the scenes. This technique allows Cacheops to do more advanced caching operations that are not provided by the Django built-in cache framework without requiring Nautobot to also include some elaborate code of its own. This is accomplished by intercepting calls to the underlying queryset methods that get and set cached results in Redis. For this purpose, Cacheops has its own CACHEOPS_* settings required to configure it that are not related to the CACHES setting. For more information on the required settings needed to configure Cacheops, please see the Caching section of the required settings documentation . The optional settings include: CACHEOPS_DEFAULTS : To define the cache timeout value (Defaults to 15 minutes) CACHEOPS_ENABLED : To turn on/off caching (Defaults to True ) Invalidating Cached Data \u00b6 Although caching is performed automatically and rarely requires administrative intervention, Nautobot provides the invalidate management command to force invalidation of cached results. This command can reference a specific object my its type and UUID: $ nautobot-server invalidate dcim.Device.84ae706d-c189-4d13-a898-9737648e34b3 Alternatively, it can also delete all cached results for an object type: $ nautobot-server invalidate dcim.Device Finally, calling it with the all argument will force invalidation of the entire cache database: $ nautobot-server invalidate all High Availability Caching \u00b6 Redis provides two different methods to achieve high availability: The first is Redis Sentinel and the second is the newer Redis Clustering feature. Unfortunately, due to an known issue with django-cacheops (last updated November 2021) Nautobot is unable to support Redis Clustering at this time. Therefore, Nautobot only supports Redis Sentinel for high availability. Using Redis Sentinel \u00b6 The installation/configuration of the Redis Sentinel cluster itself is outside the scope of this document, this section is intended to provide the steps necessary to configure Nautobot to connect to a Sentinel cluster. We need to configure django-redis , django-cacheops , and celery to use Sentinel. Each library is configured differently, so please pay close attention to the details. django-redis Sentinel Configuration \u00b6 Notable settings: SENTINELS : List of tuples or tuple of tuples with each inner tuple containing the name or IP address of the Redis server and port for each sentinel instance to connect to LOCATION : Similar to a redis URL, however , the hostname in the URL is the master/service name in redis sentinel SENTINEL_KWARGS : Options which will be passed directly to Redis Sentinel PASSWORD : The redis password (if set), the SENTINEL_KWARGS[\"password\"] setting is the password for Sentinel Example: DJANGO_REDIS_CONNECTION_FACTORY = \"django_redis.pool.SentinelConnectionFactory\" CACHES = { \"default\" : { \"BACKEND\" : \"django_redis.cache.RedisCache\" , \"LOCATION\" : \"redis://nautobot/0\" , # in this context 'nautobot' is the redis master/service name \"OPTIONS\" : { \"CLIENT_CLASS\" : \"django_redis.client.SentinelClient\" , \"CONNECTION_POOL_CLASS\" : \"redis.sentinel.SentinelConnectionPool\" , \"PASSWORD\" : \"\" , \"SENTINEL_KWARGS\" : { \"password\" : \"\" , # likely the same password from above }, \"SENTINELS\" : [ ( \"mysentinel.redis.example.com\" , 26379 ), ( \"othersentinel.redis.example.com\" , 26379 ), ( \"thirdsentinel.redis.example.com\" , 26379 ) ], }, }, } Note It is permissible to use Sentinel for only one database and not the other, see RQ_QUEUES for details. For more details on configuring django-redis with Redis Sentinel, please see the documentation for Django Redis . django-cacheops Sentinel Configuration \u00b6 Notable settings: locations : List of tuples or tuple of tuples with each inner tuple containing the name or IP address of the Redis server and port for each sentinel instance to connect to service_name : the master/service name in redis sentinel Additional parameters may be specified in the CACHEOPS_SENTINEL dictionary which are passed directly to Sentinel Note locations for django-cacheops has a different meaning than the LOCATION value for django-redis Warning CACHEOPS_REDIS and CACHEOPS_SENTINEL are mutually exclusive and will result in an error if both are set. Example: CACHEOPS_REDIS = False CACHEOPS_SENTINEL = { \"db\" : 1 , \"locations\" : [ ( \"mysentinel.redis.example.com\" , 26379 ), ( \"othersentinel.redis.example.com\" , 26379 ), ( \"thirdsentinel.redis.example.com\" , 26379 ) ], \"service_name\" : \"nautobot\" , \"socket_timeout\" : 10 , \"sentinel_kwargs\" : { \"password\" : \"\" }, \"password\" : \"\" , # Everything else is passed to `Sentinel()` } For more details on how to configure Cacheops to use Redis Sentinel see the documentation for Cacheops setup . celery Sentinel Configuration \u00b6 Note Celery is not directly related caching but it does utilize Redis, therefore in more advanced deployments if Redis Sentinel is required for caching, Celery must also be configured to use Redis Sentinel to high availability. Celery Sentinel configuration is controlled by four settings within your nautobot_config.py : CELERY_BROKER_URL CELERY_BROKER_TRANSPORT_OPTIONS CELERY_RESULT_BACKEND CELERY_RESULT_BACKEND_TRANSPORT_OPTIONS By default Nautobot configures the celery broker and results backend with the same settings, so this pattern is mirrored here. redis_password = \"\" sentinel_password = \"\" CELERY_BROKER_URL = ( f \"sentinel://: { redis_password } @mysentinel.redis.example.com:26379;\" f \"sentinel://: { redis_password } @othersentinel.redis.example.com:26379;\" # The final entry must not have the `;` delimiter f \"sentinel://: { redis_password } @thirdsentinel.redis.example.com:26379\" ) CELERY_BROKER_TRANSPORT_OPTIONS = { \"master_name\" : \"nautobot\" , \"sentinel_kwargs\" : { \"password\" : sentinel_password }, } CELERY_RESULT_BACKEND = CELERY_BROKER_URL CELERY_RESULT_BACKEND_TRANSPORT_OPTIONS = CELERY_BROKER_TRANSPORT_OPTIONS Please see the official Celery documentation for more information on how to configure Celery to use Redis Sentinel . Please also see the Nautobot documentation on required settings for Celery for additional information.","title":"Caching"},{"location":"additional-features/caching.html#caching","text":"A fundamental trade-off in dynamic websites like Nautobot is that, well, they\u2019re dynamic. Each time a user requests a page, the Web server makes all sorts of calculations \u2013 from database queries to template rendering to business logic \u2013 to create the page that your site\u2019s visitor sees. This is a lot more expensive, from a processing-overhead perspective, than your standard read-a-file-off-the-filesystem server arrangement. That\u2019s where caching comes in. To cache something is to save the result of an expensive calculation so that you don\u2019t have to perform the calculation next time. Nautobot makes extensive use of caching; this is not a simple topic but it's a useful one for a Nautobot administrator to understand, so read on if you please.","title":"Caching"},{"location":"additional-features/caching.html#how-it-works","text":"Nautobot supports database query caching using django-cacheops and Redis. When a query is made, the results are cached in Redis for a short period of time, as defined by the CACHEOPS_DEFAULTS parameter (15 minutes by default). Within that time, all recurrences of that specific query will return the pre-fetched results from the cache. Caching can be completely disabled by toggling CACHEOPS_ENABLED to False (it is True by default). If a change is made to any of the objects returned by the cached query within that time, or if the timeout expires, the cached results are automatically invalidated and the next request for those results will be sent to the database. Caching is a complex topic and there are some important details to clarify with how caching is implemented and configured in Nautobot.","title":"How it Works"},{"location":"additional-features/caching.html#caching-in-django","text":"Django includes with its own cache framework that works for common cases, but does not work well for the wide array of use-cases within Nautobot. For that reason, Django's built-in caching is not used for the caching of web UI views, API results, and underlying database queries. Instead, we use django-cacheops . Please see below for more on this.","title":"Caching in Django"},{"location":"additional-features/caching.html#caches-and-django-redis","text":"The CACHES setting is used to, among other things, configure Django's built-in caching. You'll observe that, even though we aren't using Django's built-in caching, we still have this as a required setting . Here's why: Nautobot uses the django-redis Django plugin which allows it to use Redis as a backend for caching and session storage. This is used to provide a concurrent write lock for preventing race conditions when allocating IP address objects, and also to define centralized Redis connection settings that will be used by RQ. django-redis also uses the CACHES setting, in its case to simplify the configuration for establishing concurrent write locks, and also for referencing the correct Redis connection information when defining RQ task queues using the RQ_QUEUES setting. Again: CACHES is not used for Django's built-in caching at this time, but it is still a required setting for django-redis to function properly.","title":"CACHES and django-redis"},{"location":"additional-features/caching.html#django-cacheops","text":"Cacheops (aka django-cacheops ) is a Django plugin that does some very advanced caching, but does not leverage the built-in cache framework . Instead it uses a technique called \"monkey patching\" . By monkey patching, a library can inject its own functionality into the core code behind the scenes. This technique allows Cacheops to do more advanced caching operations that are not provided by the Django built-in cache framework without requiring Nautobot to also include some elaborate code of its own. This is accomplished by intercepting calls to the underlying queryset methods that get and set cached results in Redis. For this purpose, Cacheops has its own CACHEOPS_* settings required to configure it that are not related to the CACHES setting. For more information on the required settings needed to configure Cacheops, please see the Caching section of the required settings documentation . The optional settings include: CACHEOPS_DEFAULTS : To define the cache timeout value (Defaults to 15 minutes) CACHEOPS_ENABLED : To turn on/off caching (Defaults to True )","title":"Django Cacheops"},{"location":"additional-features/caching.html#invalidating-cached-data","text":"Although caching is performed automatically and rarely requires administrative intervention, Nautobot provides the invalidate management command to force invalidation of cached results. This command can reference a specific object my its type and UUID: $ nautobot-server invalidate dcim.Device.84ae706d-c189-4d13-a898-9737648e34b3 Alternatively, it can also delete all cached results for an object type: $ nautobot-server invalidate dcim.Device Finally, calling it with the all argument will force invalidation of the entire cache database: $ nautobot-server invalidate all","title":"Invalidating Cached Data"},{"location":"additional-features/caching.html#high-availability-caching","text":"Redis provides two different methods to achieve high availability: The first is Redis Sentinel and the second is the newer Redis Clustering feature. Unfortunately, due to an known issue with django-cacheops (last updated November 2021) Nautobot is unable to support Redis Clustering at this time. Therefore, Nautobot only supports Redis Sentinel for high availability.","title":"High Availability Caching"},{"location":"additional-features/caching.html#using-redis-sentinel","text":"The installation/configuration of the Redis Sentinel cluster itself is outside the scope of this document, this section is intended to provide the steps necessary to configure Nautobot to connect to a Sentinel cluster. We need to configure django-redis , django-cacheops , and celery to use Sentinel. Each library is configured differently, so please pay close attention to the details.","title":"Using Redis Sentinel"},{"location":"additional-features/caching.html#django-redis-sentinel-configuration","text":"Notable settings: SENTINELS : List of tuples or tuple of tuples with each inner tuple containing the name or IP address of the Redis server and port for each sentinel instance to connect to LOCATION : Similar to a redis URL, however , the hostname in the URL is the master/service name in redis sentinel SENTINEL_KWARGS : Options which will be passed directly to Redis Sentinel PASSWORD : The redis password (if set), the SENTINEL_KWARGS[\"password\"] setting is the password for Sentinel Example: DJANGO_REDIS_CONNECTION_FACTORY = \"django_redis.pool.SentinelConnectionFactory\" CACHES = { \"default\" : { \"BACKEND\" : \"django_redis.cache.RedisCache\" , \"LOCATION\" : \"redis://nautobot/0\" , # in this context 'nautobot' is the redis master/service name \"OPTIONS\" : { \"CLIENT_CLASS\" : \"django_redis.client.SentinelClient\" , \"CONNECTION_POOL_CLASS\" : \"redis.sentinel.SentinelConnectionPool\" , \"PASSWORD\" : \"\" , \"SENTINEL_KWARGS\" : { \"password\" : \"\" , # likely the same password from above }, \"SENTINELS\" : [ ( \"mysentinel.redis.example.com\" , 26379 ), ( \"othersentinel.redis.example.com\" , 26379 ), ( \"thirdsentinel.redis.example.com\" , 26379 ) ], }, }, } Note It is permissible to use Sentinel for only one database and not the other, see RQ_QUEUES for details. For more details on configuring django-redis with Redis Sentinel, please see the documentation for Django Redis .","title":"django-redis Sentinel Configuration"},{"location":"additional-features/caching.html#django-cacheops-sentinel-configuration","text":"Notable settings: locations : List of tuples or tuple of tuples with each inner tuple containing the name or IP address of the Redis server and port for each sentinel instance to connect to service_name : the master/service name in redis sentinel Additional parameters may be specified in the CACHEOPS_SENTINEL dictionary which are passed directly to Sentinel Note locations for django-cacheops has a different meaning than the LOCATION value for django-redis Warning CACHEOPS_REDIS and CACHEOPS_SENTINEL are mutually exclusive and will result in an error if both are set. Example: CACHEOPS_REDIS = False CACHEOPS_SENTINEL = { \"db\" : 1 , \"locations\" : [ ( \"mysentinel.redis.example.com\" , 26379 ), ( \"othersentinel.redis.example.com\" , 26379 ), ( \"thirdsentinel.redis.example.com\" , 26379 ) ], \"service_name\" : \"nautobot\" , \"socket_timeout\" : 10 , \"sentinel_kwargs\" : { \"password\" : \"\" }, \"password\" : \"\" , # Everything else is passed to `Sentinel()` } For more details on how to configure Cacheops to use Redis Sentinel see the documentation for Cacheops setup .","title":"django-cacheops Sentinel Configuration"},{"location":"additional-features/caching.html#celery-sentinel-configuration","text":"Note Celery is not directly related caching but it does utilize Redis, therefore in more advanced deployments if Redis Sentinel is required for caching, Celery must also be configured to use Redis Sentinel to high availability. Celery Sentinel configuration is controlled by four settings within your nautobot_config.py : CELERY_BROKER_URL CELERY_BROKER_TRANSPORT_OPTIONS CELERY_RESULT_BACKEND CELERY_RESULT_BACKEND_TRANSPORT_OPTIONS By default Nautobot configures the celery broker and results backend with the same settings, so this pattern is mirrored here. redis_password = \"\" sentinel_password = \"\" CELERY_BROKER_URL = ( f \"sentinel://: { redis_password } @mysentinel.redis.example.com:26379;\" f \"sentinel://: { redis_password } @othersentinel.redis.example.com:26379;\" # The final entry must not have the `;` delimiter f \"sentinel://: { redis_password } @thirdsentinel.redis.example.com:26379\" ) CELERY_BROKER_TRANSPORT_OPTIONS = { \"master_name\" : \"nautobot\" , \"sentinel_kwargs\" : { \"password\" : sentinel_password }, } CELERY_RESULT_BACKEND = CELERY_BROKER_URL CELERY_RESULT_BACKEND_TRANSPORT_OPTIONS = CELERY_BROKER_TRANSPORT_OPTIONS Please see the official Celery documentation for more information on how to configure Celery to use Redis Sentinel . Please also see the Nautobot documentation on required settings for Celery for additional information.","title":"celery Sentinel Configuration"},{"location":"additional-features/change-logging.html","text":"Change Logging \u00b6 Every time an object in Nautobot is created, updated, or deleted, a serialized copy of that object is saved to the database, along with meta data including the current time and the user associated with the change. These records form a persistent record of changes both for each individual object as well as Nautobot as a whole. The global change log can be viewed by navigating to Extensibility > Logging > Change Log. A serialized representation of the instance being modified is included in JSON format. This is similar to how objects are conveyed within the REST API, but does not include any nested representations. For instance, the tenant field of a site will record only the tenant's ID, not a representation of the tenant. When a request is made, a UUID is generated and attached to any change records resulting from that request. For example, editing three objects in bulk will create a separate change record for each (three in total), and each of those objects will be associated with the same UUID. This makes it easy to identify all the change records resulting from a particular request. Change records are exposed in the API via the read-only endpoint /api/extras/object-changes/ . They may also be exported via the web UI in CSV format. Change records can also be accessed via the read-only GraphQL endpoint /api/graphql/ . An example query to fetch change logs by action: { query: object_changes(action: \"created\") { action user_name object_repr } }","title":"Change Logging"},{"location":"additional-features/change-logging.html#change-logging","text":"Every time an object in Nautobot is created, updated, or deleted, a serialized copy of that object is saved to the database, along with meta data including the current time and the user associated with the change. These records form a persistent record of changes both for each individual object as well as Nautobot as a whole. The global change log can be viewed by navigating to Extensibility > Logging > Change Log. A serialized representation of the instance being modified is included in JSON format. This is similar to how objects are conveyed within the REST API, but does not include any nested representations. For instance, the tenant field of a site will record only the tenant's ID, not a representation of the tenant. When a request is made, a UUID is generated and attached to any change records resulting from that request. For example, editing three objects in bulk will create a separate change record for each (three in total), and each of those objects will be associated with the same UUID. This makes it easy to identify all the change records resulting from a particular request. Change records are exposed in the API via the read-only endpoint /api/extras/object-changes/ . They may also be exported via the web UI in CSV format. Change records can also be accessed via the read-only GraphQL endpoint /api/graphql/ . An example query to fetch change logs by action: { query: object_changes(action: \"created\") { action user_name object_repr } }","title":"Change Logging"},{"location":"additional-features/config-contexts.html","text":"Configuration Contexts & Schemas \u00b6 Configuration Contexts \u00b6 Sometimes it is desirable to associate additional data with a group of devices or virtual machines to aid in automated configuration. For example, you might want to associate a set of syslog servers for all devices within a particular region. Context data enables the association of extra user-defined data with devices and virtual machines grouped by one or more of the following assignments: Region Site Role Device type Platform Cluster group Cluster Tenant group Tenant Tag Context data not specifically assigned to one or more of the above groups is by default associated with all devices and virtual machines. Configuration contexts may be managed within Nautobot via the UI and/or API; they may also be managed externally to Nautobot in a Git repository if desired. Hierarchical Rendering \u00b6 Context data is arranged hierarchically, so that data with a higher weight can be entered to override lower-weight data. Multiple instances of data are automatically merged by Nautobot to present a single dictionary for each object. For example, suppose we want to specify a set of syslog and NTP servers for all devices within a region. We could create a config context instance with a weight of 1000 assigned to the region, with the following JSON data: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ], \"syslog-servers\" : [ \"172.16.9.100\" , \"172.16.9.101\" ] } But suppose there's a problem at one particular site within this region preventing traffic from reaching the regional syslog server. Devices there need to use a local syslog server instead of the two defined above. We'll create a second config context assigned only to that site with a weight of 2000 and the following data: { \"syslog-servers\" : [ \"192.168.43.107\" ] } When the context data for a device at this site is rendered, the second, higher-weight data overwrite the first, resulting in the following: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ], \"syslog-servers\" : [ \"192.168.43.107\" ] } Data from the higher-weight context overwrites conflicting data from the lower-weight context, while the non-conflicting portion of the lower-weight context (the list of NTP servers) is preserved. Local Context Data \u00b6 Devices and virtual machines may also have a local config context defined. This local context will always take precedence over any separate config context objects which apply to the device/VM. This is useful in situations where we need to call out a specific deviation in the data for a particular object. Warning If you find that you're routinely defining local context data for many individual devices or virtual machines, custom fields may offer a more effective solution. Config Context Schemas \u00b6 Added in version 1.1.0 While config contexts allow for arbitrary data structures to be stored within Nautobot, at scale it is desirable to apply validation constraints to that data to ensure its consistency and to avoid data entry errors. To service this need, Nautobot supports optionally backing config contexts with JSON Schemas for validation. These schema are managed via the config context schema model and are optionally linked to config context instances, in addition to devices and virtual machines for the purpose of validating their local context data. A JSON Schema is capable of validating the structure, format, and type of your data, and acts as a form of documentation useful in a number of automation use cases. A config context is linked to a single schema object and thus they are meant to model individual units of the overall context. In this way, they validate each config context object, not the fully rendered context as viewed on a particular device or virtual machine. When a config context schema is employed on a config or local context, the data therein is validated when the object in question is saved. Should validation against the schema fail, a relevant error message is returned to the user and they are prevented from saving the data until the validation issue has been resolved. Here is an example JSON Schema which can be used to validate an NTP server config context: { \"type\" : \"object\" , \"properties\" : { \"ntp-servers\" : { \"type\" : \"array\" , \"minItems\" : 2 , \"maxItems\" : 2 , \"items\" : { \"type\" : \"string\" , \"format\" : \"ipv4\" } } }, \"additionalProperties\" : false } This schema would allow a config context with this data to pass: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ] } However it would not allow any of these examples to be saved: { \"ntp-servers\" : [ \"172.16.10.22\" ] } { \"ntp\" : \"172.16.10.22,172.16.10.22\" } { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" , \"5.5.4\" ] } For more information on JSON Schemas and specifically type formats for specialized objects like IP addresses, hostnames, and more see the JSON Schema docs . Note Config Context Schemas currently support the JSON Schema draft 7 specification.","title":"Context Data"},{"location":"additional-features/config-contexts.html#configuration-contexts-schemas","text":"","title":"Configuration Contexts &amp; Schemas"},{"location":"additional-features/config-contexts.html#configuration-contexts","text":"Sometimes it is desirable to associate additional data with a group of devices or virtual machines to aid in automated configuration. For example, you might want to associate a set of syslog servers for all devices within a particular region. Context data enables the association of extra user-defined data with devices and virtual machines grouped by one or more of the following assignments: Region Site Role Device type Platform Cluster group Cluster Tenant group Tenant Tag Context data not specifically assigned to one or more of the above groups is by default associated with all devices and virtual machines. Configuration contexts may be managed within Nautobot via the UI and/or API; they may also be managed externally to Nautobot in a Git repository if desired.","title":"Configuration Contexts"},{"location":"additional-features/config-contexts.html#hierarchical-rendering","text":"Context data is arranged hierarchically, so that data with a higher weight can be entered to override lower-weight data. Multiple instances of data are automatically merged by Nautobot to present a single dictionary for each object. For example, suppose we want to specify a set of syslog and NTP servers for all devices within a region. We could create a config context instance with a weight of 1000 assigned to the region, with the following JSON data: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ], \"syslog-servers\" : [ \"172.16.9.100\" , \"172.16.9.101\" ] } But suppose there's a problem at one particular site within this region preventing traffic from reaching the regional syslog server. Devices there need to use a local syslog server instead of the two defined above. We'll create a second config context assigned only to that site with a weight of 2000 and the following data: { \"syslog-servers\" : [ \"192.168.43.107\" ] } When the context data for a device at this site is rendered, the second, higher-weight data overwrite the first, resulting in the following: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ], \"syslog-servers\" : [ \"192.168.43.107\" ] } Data from the higher-weight context overwrites conflicting data from the lower-weight context, while the non-conflicting portion of the lower-weight context (the list of NTP servers) is preserved.","title":"Hierarchical Rendering"},{"location":"additional-features/config-contexts.html#local-context-data","text":"Devices and virtual machines may also have a local config context defined. This local context will always take precedence over any separate config context objects which apply to the device/VM. This is useful in situations where we need to call out a specific deviation in the data for a particular object. Warning If you find that you're routinely defining local context data for many individual devices or virtual machines, custom fields may offer a more effective solution.","title":"Local Context Data"},{"location":"additional-features/config-contexts.html#config-context-schemas","text":"Added in version 1.1.0 While config contexts allow for arbitrary data structures to be stored within Nautobot, at scale it is desirable to apply validation constraints to that data to ensure its consistency and to avoid data entry errors. To service this need, Nautobot supports optionally backing config contexts with JSON Schemas for validation. These schema are managed via the config context schema model and are optionally linked to config context instances, in addition to devices and virtual machines for the purpose of validating their local context data. A JSON Schema is capable of validating the structure, format, and type of your data, and acts as a form of documentation useful in a number of automation use cases. A config context is linked to a single schema object and thus they are meant to model individual units of the overall context. In this way, they validate each config context object, not the fully rendered context as viewed on a particular device or virtual machine. When a config context schema is employed on a config or local context, the data therein is validated when the object in question is saved. Should validation against the schema fail, a relevant error message is returned to the user and they are prevented from saving the data until the validation issue has been resolved. Here is an example JSON Schema which can be used to validate an NTP server config context: { \"type\" : \"object\" , \"properties\" : { \"ntp-servers\" : { \"type\" : \"array\" , \"minItems\" : 2 , \"maxItems\" : 2 , \"items\" : { \"type\" : \"string\" , \"format\" : \"ipv4\" } } }, \"additionalProperties\" : false } This schema would allow a config context with this data to pass: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ] } However it would not allow any of these examples to be saved: { \"ntp-servers\" : [ \"172.16.10.22\" ] } { \"ntp\" : \"172.16.10.22,172.16.10.22\" } { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" , \"5.5.4\" ] } For more information on JSON Schemas and specifically type formats for specialized objects like IP addresses, hostnames, and more see the JSON Schema docs . Note Config Context Schemas currently support the JSON Schema draft 7 specification.","title":"Config Context Schemas"},{"location":"additional-features/graphql.html","text":"GraphQL \u00b6 Nautobot supports a Read-Only GraphQL interface that can be used to query most information present in the database. The GraphQL interface is available at the endpoint graphql/ for a human to explore and GraphQL can be queried as an API via the endpoint api/graphql/ . Currently the support for GraphQL is limited to query , other operation type like mutations and subscriptions are not supported. Additionally, GraphQL variables are supported. The GraphQL implementation is leveraging the graphene-django library and supports the standard GraphQL language . How to use the GraphQL interface \u00b6 The GraphQL interface can be used to query multiple tables at once in a single request. In GraphQL, only the information requested will be returned which can be contrasted to REST APIs. In the example below, this query will return the name of all interfaces attached to the device nyc-sw01 along with all ip_addresses attached to those interfaces. query { devices(name: \"nyc-sw01\") { name interfaces { name ip_addresses { address } } } } Result { \"data\" : { \"devices\" : [ { \"name\" : \"nyc-sw01\" , \"interfaces\" : [ { \"name\" : \"xe-0/0/0\" , \"ip_addresses\" : [ { \"address\" : \"10.52.0.1/30\" } ] }, { \"name\" : \"xe-0/0/1\" , \"ip_addresses\" : [] } ] } ] } } It is possible to explore the Graph and create some queries in a human friendly UI at the endpoint graphql/ . This interface (called graphqli ) provides a great playground to build new queries as it provides full autocompletion and type validation. Querying the GraphQL interface over the rest API \u00b6 It is possible to query the GraphQL interface via the rest API as well, the endpoint is available at api/graphql/ and supports the same Token based authentication as all other Nautobot APIs. A GraphQL Query must be encapsulated in a JSON payload with the query key and sent with a POST request. Optionally it is possible to provide a list of variables in the same payload as presented below. { \"query\" : \"query ($id: Int!) { device(id: $id) { name }}\" , \"variables\" : { \"id\" : 3 } } Working with Custom Fields \u00b6 GraphQL custom fields data data is provided in two formats, a \"greedy\" and a \"prefixed\" format. The greedy format provides all custom field data associated with this record under a single \"custom_field_data\" key. This is helpful in situations where custom fields are likely to be added at a later date, the data will simply be added to the same root key and immediately accessible without the need to adjust the query. query { sites { name custom_field_data } } Result { \"data\" : { \"sites\" : [ { \"name\" : \"nyc-site-01\" , \"custom_field_data\" : { \"site_type\" : \"large\" } }, { \"name\" : \"nyc-site-02\" , \"custom_field_data\" : { \"site_type\" : \"small\" } } ] } } Additionally, by default, all custom fields in GraphQL will be prefixed with cf . A custom field name site_type will appear in GraphQL as cf_site_type as an example. The prefix can be changed or remove the prefix by setting the value of GRAPHQL_CUSTOM_FIELD_PREFIX . query { sites { name cf_site_type } } Result { \"data\" : { \"sites\" : [ { \"name\" : \"nyc-site-01\" , \"cf_site_type\" : \"large\" }, { \"name\" : \"nyc-site-02\" , \"cf_site_type\" : \"small\" } ] } } Working with Relationships \u00b6 Defined relationships are available in GraphQL as well. In most cases, the associated objects for a given relationship will be available under the key rel_<relationship_slug> . The one exception is for relationships between objects of the same type that are not defined as symmetric; for these relationships it's important to be able to distinguish between the two \"sides\" of the relationship, and so the associated objects will be available under rel_<relationship_slug>_source and/or rel_<relationship_slug>_destination as appropriate. query { ip_addresses { address rel_peer_address { address } rel_parent_child_source { address } rel_parent_child_destination { address } } } Result { \"data\" : { \"ip_addresses\" : [ { \"address\" : \"10.1.1.1/24\" , \"rel_peer_address\" : { \"address\" : \"10.1.1.2/24\" }, \"rel_parent_child_source\" : null , \"rel_parent_child_destination\" : [ { \"address\" : \"10.1.1.1/30\" }, { \"address\" : \"10.1.1.1/32\" } ] }, { \"address\" : \"10.1.1.1/30\" , \"rel_peer_address\" : null , \"rel_parent_child_source\" : { \"address\" : \"10.1.1.1/24\" }, \"rel_parent_child_destination\" : [] }, { \"address\" : \"10.1.1.1/32\" , \"rel_peer_address\" : null , \"rel_parent_child_source\" : { \"address\" : \"10.1.1.1/24\" }, \"rel_parent_child_destination\" : [] }, { \"address\" : \"10.1.1.2/24\" , \"rel_peer_address\" : { \"address\" : \"10.1.1.1/24\" }, \"rel_parent_child_source\" : null , \"rel_parent_child_destination\" : [] } ] } } Saved Queries \u00b6 Added in version 1.1.0 Queries can now be stored inside of Nautobot, allowing the user to easily rerun previously defined queries. Inside of Extensibility -> Data Management -> GraphQL Queries , there are views to create and manage GraphQL queries. Saved queries can be executed from the detailed query view or via a REST API request. The queries can also be populated from the detailed query view into GraphiQL by using the \"Open in GraphiQL\" button. Additionally, in the GraphiQL UI, there is now a menu item, \"Queries\", which can be used to populate GraphiQL with any previously saved query. To execute a stored query via the REST API, a POST request can be sent to /api/extras/graphql-queries/[slug]/run/ . Any GraphQL variables required by the query can be passed in as JSON data within the request body.","title":"Overview"},{"location":"additional-features/graphql.html#graphql","text":"Nautobot supports a Read-Only GraphQL interface that can be used to query most information present in the database. The GraphQL interface is available at the endpoint graphql/ for a human to explore and GraphQL can be queried as an API via the endpoint api/graphql/ . Currently the support for GraphQL is limited to query , other operation type like mutations and subscriptions are not supported. Additionally, GraphQL variables are supported. The GraphQL implementation is leveraging the graphene-django library and supports the standard GraphQL language .","title":"GraphQL"},{"location":"additional-features/graphql.html#how-to-use-the-graphql-interface","text":"The GraphQL interface can be used to query multiple tables at once in a single request. In GraphQL, only the information requested will be returned which can be contrasted to REST APIs. In the example below, this query will return the name of all interfaces attached to the device nyc-sw01 along with all ip_addresses attached to those interfaces. query { devices(name: \"nyc-sw01\") { name interfaces { name ip_addresses { address } } } } Result { \"data\" : { \"devices\" : [ { \"name\" : \"nyc-sw01\" , \"interfaces\" : [ { \"name\" : \"xe-0/0/0\" , \"ip_addresses\" : [ { \"address\" : \"10.52.0.1/30\" } ] }, { \"name\" : \"xe-0/0/1\" , \"ip_addresses\" : [] } ] } ] } } It is possible to explore the Graph and create some queries in a human friendly UI at the endpoint graphql/ . This interface (called graphqli ) provides a great playground to build new queries as it provides full autocompletion and type validation.","title":"How to use the GraphQL interface"},{"location":"additional-features/graphql.html#querying-the-graphql-interface-over-the-rest-api","text":"It is possible to query the GraphQL interface via the rest API as well, the endpoint is available at api/graphql/ and supports the same Token based authentication as all other Nautobot APIs. A GraphQL Query must be encapsulated in a JSON payload with the query key and sent with a POST request. Optionally it is possible to provide a list of variables in the same payload as presented below. { \"query\" : \"query ($id: Int!) { device(id: $id) { name }}\" , \"variables\" : { \"id\" : 3 } }","title":"Querying the GraphQL interface over the rest API"},{"location":"additional-features/graphql.html#working-with-custom-fields","text":"GraphQL custom fields data data is provided in two formats, a \"greedy\" and a \"prefixed\" format. The greedy format provides all custom field data associated with this record under a single \"custom_field_data\" key. This is helpful in situations where custom fields are likely to be added at a later date, the data will simply be added to the same root key and immediately accessible without the need to adjust the query. query { sites { name custom_field_data } } Result { \"data\" : { \"sites\" : [ { \"name\" : \"nyc-site-01\" , \"custom_field_data\" : { \"site_type\" : \"large\" } }, { \"name\" : \"nyc-site-02\" , \"custom_field_data\" : { \"site_type\" : \"small\" } } ] } } Additionally, by default, all custom fields in GraphQL will be prefixed with cf . A custom field name site_type will appear in GraphQL as cf_site_type as an example. The prefix can be changed or remove the prefix by setting the value of GRAPHQL_CUSTOM_FIELD_PREFIX . query { sites { name cf_site_type } } Result { \"data\" : { \"sites\" : [ { \"name\" : \"nyc-site-01\" , \"cf_site_type\" : \"large\" }, { \"name\" : \"nyc-site-02\" , \"cf_site_type\" : \"small\" } ] } }","title":"Working with Custom Fields"},{"location":"additional-features/graphql.html#working-with-relationships","text":"Defined relationships are available in GraphQL as well. In most cases, the associated objects for a given relationship will be available under the key rel_<relationship_slug> . The one exception is for relationships between objects of the same type that are not defined as symmetric; for these relationships it's important to be able to distinguish between the two \"sides\" of the relationship, and so the associated objects will be available under rel_<relationship_slug>_source and/or rel_<relationship_slug>_destination as appropriate. query { ip_addresses { address rel_peer_address { address } rel_parent_child_source { address } rel_parent_child_destination { address } } } Result { \"data\" : { \"ip_addresses\" : [ { \"address\" : \"10.1.1.1/24\" , \"rel_peer_address\" : { \"address\" : \"10.1.1.2/24\" }, \"rel_parent_child_source\" : null , \"rel_parent_child_destination\" : [ { \"address\" : \"10.1.1.1/30\" }, { \"address\" : \"10.1.1.1/32\" } ] }, { \"address\" : \"10.1.1.1/30\" , \"rel_peer_address\" : null , \"rel_parent_child_source\" : { \"address\" : \"10.1.1.1/24\" }, \"rel_parent_child_destination\" : [] }, { \"address\" : \"10.1.1.1/32\" , \"rel_peer_address\" : null , \"rel_parent_child_source\" : { \"address\" : \"10.1.1.1/24\" }, \"rel_parent_child_destination\" : [] }, { \"address\" : \"10.1.1.2/24\" , \"rel_peer_address\" : { \"address\" : \"10.1.1.1/24\" }, \"rel_parent_child_source\" : null , \"rel_parent_child_destination\" : [] } ] } }","title":"Working with Relationships"},{"location":"additional-features/graphql.html#saved-queries","text":"Added in version 1.1.0 Queries can now be stored inside of Nautobot, allowing the user to easily rerun previously defined queries. Inside of Extensibility -> Data Management -> GraphQL Queries , there are views to create and manage GraphQL queries. Saved queries can be executed from the detailed query view or via a REST API request. The queries can also be populated from the detailed query view into GraphiQL by using the \"Open in GraphiQL\" button. Additionally, in the GraphiQL UI, there is now a menu item, \"Queries\", which can be used to populate GraphiQL with any previously saved query. To execute a stored query via the REST API, a POST request can be sent to /api/extras/graphql-queries/[slug]/run/ . Any GraphQL variables required by the query can be passed in as JSON data within the request body.","title":"Saved Queries"},{"location":"additional-features/healthcheck.html","text":"Healthcheck Endpoint \u00b6 Nautobot includes a health check endpoint /health which utilizes the django-health-check project and some custom health checks (database connection and cache availability). This endpoint is designed for use by an optional load balancer placed in front of Nautobot to determine the health of the Nautobot application server. By default the health check enables checks for the following: Database Backend Caching Backend Storage Backend In addition to exposing a health check URL the nautobot-server utility also provides a health_check management command which provides the same information as the web interface. Additional health checks are available as part of the django-health-check project and can be added to the EXTRA_INSTALLED_APPS configuration variable as desired. The Nautobot server is healthy if the HTTP response is 200 from a GET request to /health , a web UI is also available at the same endpoint for human consumption.","title":"Health Check"},{"location":"additional-features/healthcheck.html#healthcheck-endpoint","text":"Nautobot includes a health check endpoint /health which utilizes the django-health-check project and some custom health checks (database connection and cache availability). This endpoint is designed for use by an optional load balancer placed in front of Nautobot to determine the health of the Nautobot application server. By default the health check enables checks for the following: Database Backend Caching Backend Storage Backend In addition to exposing a health check URL the nautobot-server utility also provides a health_check management command which provides the same information as the web interface. Additional health checks are available as part of the django-health-check project and can be added to the EXTRA_INSTALLED_APPS configuration variable as desired. The Nautobot server is healthy if the HTTP response is 200 from a GET request to /health , a web UI is also available at the same endpoint for human consumption.","title":"Healthcheck Endpoint"},{"location":"additional-features/job-scheduling-and-approvals.html","text":"Job Scheduling and Approvals \u00b6 Added in version 1.2.0 Oftentimes jobs will need to be run at a later date or periodically, or require approval from someone before they can be started. To this end, Nautobot offers facilities for scheduling and approving jobs. Job Scheduling \u00b6 Jobs can be scheduled to be run immediately, at some point in the future, or at an interval. Jobs can be scheduled through the UI or the API. Scheduling via the UI \u00b6 The Job Scheduling views can be accessed via the navigation at Jobs > Jobs , selecting a Job as appropriate. The UI allows you to select a scheduling type. Further fields will be displayed as appropriate for that schedule type. If Recurring custom is chosen, you can schedule the recurrence in the Crontab field in crontab syntax. If the job requires no approval, it will then be added to the queue of scheduled jobs or run immediately. Otherwise, the job will be added to the approval queue where it can be approved by other users. Scheduling via the API \u00b6 Jobs can also be scheduled via the REST API. The endpoint used for this is the regular job endpoint; specifying the optional schedule parameter will act just as scheduling in the UI. curl -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/extras/jobs/$JOB_ID/run/ \\ --data '{\"schedule\": {\"name\": \"test\", \"interval\": \"future\", \"start_time\": \"2030-01-01T01:00:00.000Z\"}}' For custom interval, a crontab parameter must be added. start_time becomes optional when interval is set to custom . --data '{\"schedule\": {\"name\": \"test\", \"interval\": \"custom\", \"start_time\": \"2030-01-01T01:00:00.000Z\", \"crontab\": \"*/15 * * * *\"}}' Job Approvals \u00b6 Jobs that have approval_required set to True on their Meta object require another user to approve a scheduled job. Scheduled jobs can be approved or denied via the UI and API by any user that has the extras.approve_job permission for the job in question, as well as the appropriate extras.change_scheduledjob and/or extras.delete_scheduledjob permissions. Changed in version 1.3.0 The extras.approve_job permission is now required for job approvers. Note Jobs that are past their scheduled run date can still be approved, but the approver will be asked to confirm the operation. Approval via the UI \u00b6 The queue of jobs that need approval can be found under Jobs > Job Approval Queue . This view lists all currently requested jobs that need approval before they are run. To approve a job, select it and click the button to approve. Please note that you will be asked for confirmation if a job is being approved that is past its scheduled date and time. If the approver is unsure what a job would do, a dry run can also be started via that same view. Approval via the API \u00b6 Approvals can also be given via the REST API. The endpoints to approve, deny, and dry run a scheduled job are found on the scheduled job endpoint under approve , deny , and dry-run , respectively. curl -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/extras/scheduled-jobs/$JOB_ID/approve?force=true The approval endpoint additionally provides a force query parameter that needs to be set if a job is past its scheduled datetime. This mimics the confirmation dialog in the UI.","title":"Job Scheduling and Approvals"},{"location":"additional-features/job-scheduling-and-approvals.html#job-scheduling-and-approvals","text":"Added in version 1.2.0 Oftentimes jobs will need to be run at a later date or periodically, or require approval from someone before they can be started. To this end, Nautobot offers facilities for scheduling and approving jobs.","title":"Job Scheduling and Approvals"},{"location":"additional-features/job-scheduling-and-approvals.html#job-scheduling","text":"Jobs can be scheduled to be run immediately, at some point in the future, or at an interval. Jobs can be scheduled through the UI or the API.","title":"Job Scheduling"},{"location":"additional-features/job-scheduling-and-approvals.html#scheduling-via-the-ui","text":"The Job Scheduling views can be accessed via the navigation at Jobs > Jobs , selecting a Job as appropriate. The UI allows you to select a scheduling type. Further fields will be displayed as appropriate for that schedule type. If Recurring custom is chosen, you can schedule the recurrence in the Crontab field in crontab syntax. If the job requires no approval, it will then be added to the queue of scheduled jobs or run immediately. Otherwise, the job will be added to the approval queue where it can be approved by other users.","title":"Scheduling via the UI"},{"location":"additional-features/job-scheduling-and-approvals.html#scheduling-via-the-api","text":"Jobs can also be scheduled via the REST API. The endpoint used for this is the regular job endpoint; specifying the optional schedule parameter will act just as scheduling in the UI. curl -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/extras/jobs/$JOB_ID/run/ \\ --data '{\"schedule\": {\"name\": \"test\", \"interval\": \"future\", \"start_time\": \"2030-01-01T01:00:00.000Z\"}}' For custom interval, a crontab parameter must be added. start_time becomes optional when interval is set to custom . --data '{\"schedule\": {\"name\": \"test\", \"interval\": \"custom\", \"start_time\": \"2030-01-01T01:00:00.000Z\", \"crontab\": \"*/15 * * * *\"}}'","title":"Scheduling via the API"},{"location":"additional-features/job-scheduling-and-approvals.html#job-approvals","text":"Jobs that have approval_required set to True on their Meta object require another user to approve a scheduled job. Scheduled jobs can be approved or denied via the UI and API by any user that has the extras.approve_job permission for the job in question, as well as the appropriate extras.change_scheduledjob and/or extras.delete_scheduledjob permissions. Changed in version 1.3.0 The extras.approve_job permission is now required for job approvers. Note Jobs that are past their scheduled run date can still be approved, but the approver will be asked to confirm the operation.","title":"Job Approvals"},{"location":"additional-features/job-scheduling-and-approvals.html#approval-via-the-ui","text":"The queue of jobs that need approval can be found under Jobs > Job Approval Queue . This view lists all currently requested jobs that need approval before they are run. To approve a job, select it and click the button to approve. Please note that you will be asked for confirmation if a job is being approved that is past its scheduled date and time. If the approver is unsure what a job would do, a dry run can also be started via that same view.","title":"Approval via the UI"},{"location":"additional-features/job-scheduling-and-approvals.html#approval-via-the-api","text":"Approvals can also be given via the REST API. The endpoints to approve, deny, and dry run a scheduled job are found on the scheduled job endpoint under approve , deny , and dry-run , respectively. curl -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/extras/scheduled-jobs/$JOB_ID/approve?force=true The approval endpoint additionally provides a force query parameter that needs to be set if a job is past its scheduled datetime. This mimics the confirmation dialog in the UI.","title":"Approval via the API"},{"location":"additional-features/jobs.html","text":"Jobs \u00b6 Jobs are a way for users to execute custom logic on demand from within the Nautobot UI. Jobs can interact directly with Nautobot data to accomplish various data creation, modification, and validation tasks, such as: Automatically populate new devices and cables in preparation for a new site deployment Create a range of new reserved prefixes or IP addresses Fetch data from an external source and import it to Nautobot Check and report whether all top-of-rack switches have a console connection Check and report whether every router has a loopback interface with an assigned IP address Check and report whether all IP addresses have a parent prefix ...and so on. Jobs are Python code and exist outside of the official Nautobot code base, so they can be updated and changed without interfering with the core Nautobot installation. And because they're completely customizable, there's practically no limit to what a job can accomplish. Note Jobs unify and supersede the functionality previously provided in NetBox by \"custom scripts\" and \"reports\". Jobs are backwards-compatible for now with the Script and Report class APIs, but you are urged to move to the new Job class API described below. Jobs may be optionally marked as read-only which equates to the Report functionally, but in all cases, user input is supported via job variables . Writing Jobs \u00b6 Jobs may be installed in one of three ways: Manually installed as files in the JOBS_ROOT path (which defaults to $NAUTOBOT_ROOT/jobs/ ). The JOBS_ROOT directory must contain a file named __init__.py . Do not delete this file. Each file created within this path is considered a separate module; there is no support for cross-file dependencies (such as a file acting as a common \"library\" module of functions shared between jobs) for files installed in this way. Imported from an external Git repository . The repository's jobs/ directory must contain a file named __init__.py . Each Job file in the repository is considered a separate module; there is no support for cross-file dependencies (such as a file acting as a common \"library\" module of functions shared between jobs) for files installed in this way. Packaged as part of a plugin . Jobs installed this way are part of the plugin module and can import code from elsewhere in the plugin or even have dependencies on other packages, if needed, via the standard Python packaging mechanisms. In any case, each module holds one or more Jobs (Python classes), each of which serves a specific purpose. The logic of each job can be split into a number of distinct methods, each of which performs a discrete portion of the overall job logic. For example, we can create a module named devices.py to hold all of our jobs which pertain to devices in Nautobot. Within that module, we might define several jobs. Each job is defined as a Python class inheriting from extras.jobs.Job , which provides the base functionality needed to accept user input and log activity. Warning Make sure you are not inheriting extras.jobs.models.Job instead, otherwise Django will think you want to define a new database model. from nautobot.extras.jobs import Job class CreateDevices ( Job ): ... class DeviceConnectionsReport ( Job ): ... class DeviceIPsReport ( Job ): ... Each job class will implement some or all of the following components: Module and class attributes, providing for default behavior, documentation and discoverability a set of variables for user input via the Nautobot UI (if your job requires any user inputs) a run() method, which is executed first and receives the user input values, if any any number of test_*() methods, which will be invoked next in order of declaration. Log messages generated by the job will be grouped together by the test method they were invoked from. a post_run() method, which is executed last and can be used to handle any necessary cleanup or final events (such as sending an email or triggering a webhook). The status of the overall job is available at this time as self.failed and the JobResult data object is available as self.result . You can implement the entire job within the run() function, but for more complex jobs, you may want to provide more granularity in the output and logging of activity. For this purpose, you can implement portions of the logic as test_*() methods (i.e., methods whose name begins with test_* ) and/or a post_run() method. Log messages generated by the job logging APIs (more below on this topic) will be grouped together according to their base method ( run , test_a , test_b , ..., post_run ) which can aid in understanding the operation of the job. Note Your job can of course define additional Python methods to compartmentalize and reuse logic as required; however the run , test_* , and post_run methods are the only ones that will be automatically invoked by Nautobot. It's important to understand that jobs execute on the server asynchronously as background tasks; they log messages and report their status to the database by updating JobResult records and creating JobLogEntry records. Note When actively developing a Job utilizing a development environment it's important to understand that the \"automatically reload when code changes are detected\" debugging functionality provided by nautobot-server runserver does not automatically restart the Celery worker process when code changes are made; therefore, it is required to restart the worker after each update to your Job source code or else it will continue to run the version of the Job code that was present when it first started. Additionally, as of Nautobot 1.3, the Job database records corresponding to installed Jobs are not automatically refreshed when the development server auto-restarts. If you make changes to any of the class and module metadata attributes described in the following sections, the database will be refreshed to reflect these changes only after running nautobot-server migrate or nautobot-server post_upgrade (recommended) or if you manually edit a Job database record to force it to be refreshed. Module Metadata Attributes \u00b6 name (Grouping) \u00b6 You can define a global constant called name within a job module (the Python file which contains one or more job classes) to set the default grouping under which jobs in this module will be displayed in the Nautobot UI. If this value is not defined, the module's file name will be used. This \"grouping\" value may also be defined or overridden when editing Job records in the database. Note In some UI elements and API endpoints, the module file name is displayed in addition to or in place of this attribute, so even if defining this attribute, you should still choose an appropriately explanatory file name as well. Class Metadata Attributes \u00b6 Job-specific attributes may be defined under a class named Meta within each job class you implement. All of these are optional, but encouraged. name \u00b6 This is the human-friendly name of your job, as will be displayed in the Nautobot UI. If not set, the class name will be used. Note In some UI elements and API endpoints, the class name is displayed in addition to or in place of this attribute, so even if defining this attribute, you should still choose an appropriately explanatory class name as well. description \u00b6 An optional human-friendly description of what this job does. This can accept either plain text or Markdown-formatted text. It can also be multiple lines: class ExampleJob ( Job ): class Meta : description = \"\"\" This job does a number of interesting things. 1. It hacks the Gibson 2. It immanentizes the eschaton 3. It's a floor wax *and* a dessert topping \"\"\" If you code a multi-line description, the first line only will be used in the description column of the jobs list, while the full description will be rendered in the job detail view, submission, approval, and results pages. approval_required \u00b6 Default: False A boolean that will mark this job as requiring approval from another user to be run. For more details on approvals, please refer to the section on scheduling and approvals . commit_default \u00b6 Default: True The checkbox to commit database changes when executing a job is checked by default in the Nautobot UI. You can set commit_default to False under the Meta class if you want this option to instead be unchecked by default. class MyJob ( Job ): class Meta : commit_default = False field_order \u00b6 Default: [] A list of strings (field names) representing the order your job variables should be rendered as form fields in the job submission UI. If not defined, the variables will be listed in order of their definition in the code. If variables are defined on a parent class and no field order is defined, the parent class variables will appear before the subclass variables. has_sensitive_variables \u00b6 Added in version 1.3.10 Default: True Unless set to False, it prevents the job's input parameters from being saved to the database. This defaults to True so as to protect against inadvertent database exposure of input parameters that may include sensitive data such as passwords or other user credentials. Review whether each job's inputs contain any such variables before setting this to False; if a job does contain sensitive inputs, if possible you should consider whether the job could be re-implemented using Nautobot's Secrets feature as a way to ensure that the sensitive data is not directly provided as a job variable at all. Important notes about jobs with sensitive variables: Such jobs cannot be scheduled to run in the future or on a recurring schedule (as scheduled jobs must by necessity store their variables in the database for future reference). Jobs with sensitive variables cannot be marked as requiring approval (as jobs pending approval must store their variables in the database until approved). hidden \u00b6 Default: False A Boolean that if set to True prevents the job from being displayed by default in the list of Jobs in the Nautobot UI. Since the jobs execution framework is designed to be generic, there may be several technical jobs defined by users which interact with or are invoked by external systems. In such cases, these jobs are not meant to be executed by a human and likely do not make sense to expose to end users for execution, and thus having them exposed in the UI at all is extraneous. Important notes about hidden jobs: This is merely hiding them by default from the web interface. It is NOT a security feature. In the Jobs list view it is possible to filter to \"Hidden: (no selection)\" or even \"Hidden: Yes\" to list the hidden jobs. All Job UI and REST API endpoints still exist for hidden jobs and can be accessed by any user who is aware of their existence. Hidden jobs can still be executed through the UI or the REST API given the appropriate URL. Results for hidden jobs will still appear in the Job Results list after they are run. read_only \u00b6 Added in version 1.1.0 Default: False A boolean that designates whether the job is able to make changes to data in the database. The value defaults to False but when set to True , any data modifications executed from the job's code will be automatically aborted at the end of the job. The job input form is also modified to remove the commit checkbox as it is irrelevant for read-only jobs. When a job is marked as read-only, log messages that are normally automatically emitted about the DB transaction state are not included because no changes to data are allowed. Note that user input may still be optionally collected with read-only jobs via job variables, as described below. soft_time_limit \u00b6 Added in version 1.3.0 An int or float value, in seconds, which can be used to override the default soft time limit for a job task to complete. The celery.exceptions.SoftTimeLimitExceeded exception will be raised when this soft time limit is exceeded. The job task can catch this to clean up before the hard time limit (10 minutes by default) is reached: from celery.exceptions import SoftTimeLimitExceeded from nautobot.extras.jobs import Job class ExampleJobWithSoftTimeLimit ( Job ): class Meta : name = \"Soft Time Limit\" description = \"Set a soft time limit of 10 seconds`\" soft_time_limit = 10 def run ( self , data , commit ): try : # code which might take longer than 10 seconds to run job_code () except SoftTimeLimitExceeded : # any clean up code cleanup_in_a_hurry () template_name \u00b6 Added in version 1.4.0 A path relative to the job source code containing a Django template which provides additional code to customize the Job's submission form. This template should extend the existing job template, extras/job.html , otherwise the base form and functionality may not be available. A template can provide additional JavaScript, CSS, or even display HTML. A good starting template would be: {% extends 'extras/job.html' %} {% block extra_styles %} {{ block.super }} <!-- Add additional CSS here. --> {% endblock %} {% block content %} {{ block.super }} <!-- Add additional HTML here. --> {% endblock content %} {% block javascript %} {{ block.super }} <!-- Add additional JavaScript here. --> {% endblock javascript %} For another example checkout the template used in example plugin in the GitHub repo. time_limit \u00b6 Added in version 1.3.0 An int or float value, in seconds, which can be used to override the default hard time limit (10 minutes by default) for a job task to complete. Unlike the soft_time_limit above, no exceptions are raised when a time_limit is exceeded. The task will just terminate silently: from nautobot.extras.jobs import Job class ExampleJobWithHardTimeLimit ( Job ): class Meta : name = \"Hard Time Limit\" description = \"Set a hard time limit of 10 seconds`\" time_limit = 10 def run ( self , data , commit ): # code which might take longer than 10 seconds to run # this code will fail silently if the time_limit is exceeded job_code () Note If the time_limit is set to a value less than or equal to the soft_time_limit , a warning log is generated to inform the user that this job will fail silently after the time_limit as the soft_time_limit will never be reached. Variables \u00b6 Variables allow your job to accept user input via the Nautobot UI, but they are optional; if your job does not require any user input, there is no need to define any variables. Conversely, if you are making use of user input in your job, you must also implement the run() method, as it is the only entry point to your job that has visibility into the variable values provided by the user. from nautobot.extras.jobs import Job , StringVar , IntegerVar , ObjectVar class CreateDevices ( Job ): var1 = StringVar ( ... ) var2 = IntegerVar ( ... ) var3 = ObjectVar ( ... ) def run ( self , data , commit ): ... The remainder of this section documents the various supported variable types and how to make use of them. Default Variable Options \u00b6 All job variables support the following default options: default - The field's default value description - A brief user-friendly description of the field label - The field name to be displayed in the rendered form required - Indicates whether the field is mandatory (all fields are required by default) widget - The class of form widget to use (see the Django documentation ) StringVar \u00b6 Stores a string of characters (i.e. text). Options include: min_length - Minimum number of characters max_length - Maximum number of characters regex - A regular expression against which the provided value must match Note that min_length and max_length can be set to the same number to effect a fixed-length field. TextVar \u00b6 Arbitrary text of any length. Renders as a multi-line text input field. IntegerVar \u00b6 Stores a numeric integer. Options include: min_value - Minimum value max_value - Maximum value BooleanVar \u00b6 A true/false flag. This field has no options beyond the defaults listed above. ChoiceVar \u00b6 A set of choices from which the user can select one. choices - A list of (value, label) tuples representing the available choices. For example: CHOICES = ( ( 'n' , 'North' ), ( 's' , 'South' ), ( 'e' , 'East' ), ( 'w' , 'West' ) ) direction = ChoiceVar ( choices = CHOICES ) In the example above, selecting the choice labeled \"North\" will submit the value n . MultiChoiceVar \u00b6 Similar to ChoiceVar , but allows for the selection of multiple choices. ObjectVar \u00b6 A particular object within Nautobot. Each ObjectVar must specify a particular model, and allows the user to select one of the available instances. ObjectVar accepts several arguments, listed below. model - The model class display_field - The name of the REST API object field to display in the selection list (default: 'display' ) query_params - A dictionary of REST API query parameters to use when retrieving available options (optional) null_option - A label representing a \"null\" or empty choice (optional) The display_field argument is useful in cases where using the display API field is not desired for referencing the object. For example, when displaying a list of IP Addresses, you might want to use the dns_name field: device_type = ObjectVar ( model = IPAddress , display_field = \"dns_name\" , ) To limit the selections available within the list, additional query parameters can be passed as the query_params dictionary. For example, to show only devices with an \"active\" status: device = ObjectVar ( model = Device , query_params = { 'status' : 'active' } ) Multiple values can be specified by assigning a list to the dictionary key. It is also possible to reference the value of other fields in the form by prepending a dollar sign ( $ ) to the variable's name. The keys you can use in this dictionary are the same ones that are available in the REST API - as an example it is also possible to filter the Site ObjectVar for its tenant_group_id . region = ObjectVar ( model = Region ) tenant_group = ObjectVar ( model = TenantGroup ) site = ObjectVar ( model = Site , query_params = { 'region_id' : '$region' , 'tenant_group_id' : '$tenant_group' } ) MultiObjectVar \u00b6 Similar to ObjectVar , but allows for the selection of multiple objects. FileVar \u00b6 An uploaded file. Note that uploaded files are present in memory only for the duration of the job's execution: They will not be automatically saved for future use. The job is responsible for writing file contents to disk where necessary. IPAddressVar \u00b6 An IPv4 or IPv6 address, without a mask. Returns a netaddr.IPAddress object. IPAddressWithMaskVar \u00b6 An IPv4 or IPv6 address with a mask. Returns a netaddr.IPNetwork object which includes the mask. IPNetworkVar \u00b6 An IPv4 or IPv6 network with a mask. Returns a netaddr.IPNetwork object. Two attributes are available to validate the provided mask: min_prefix_length - Minimum length of the mask max_prefix_length - Maximum length of the mask The run() Method \u00b6 The run() method, if you choose to implement it, should accept two arguments: data - A dictionary which will contain all of the variable data passed in by the user (via the web UI or REST API) commit - A boolean indicating whether database changes should be committed. If this is False , even if your Job attempts to make database changes, they will be automatically rolled back when the Job completes. from nautobot.extras.jobs import Job , StringVar , IntegerVar , ObjectVar class CreateDevices ( Job ): var1 = StringVar ( ... ) var2 = IntegerVar ( ... ) var3 = ObjectVar ( ... ) def run ( self , data , commit ): ... Again, defining user variables is totally optional; you may create a job with just a run() method if no user input is needed, in which case data will be an empty dictionary. Note The test_*() and post_run() methods do not accept any arguments; if you need to access user data or the commit flag, your run() method is responsible for storing these values in the job instance, such as: def run(self, data, commit): self.data = data self.commit = commit Warning When writing Jobs that create and manipulate data it is recommended to make use of the validated_save() convenience method which exists on all core models. This method saves the instance data but first enforces model validation logic. Simply calling save() on the model instance does not enforce validation automatically and may lead to bad data. See the development best practices . Warning The Django ORM provides methods to create/edit many objects at once, namely bulk_create() and update() . These are best avoided in most cases as they bypass a model's built-in validation and can easily lead to database corruption if not used carefully. The test_*() Methods \u00b6 If your job class defines any number of methods whose names begin with test_ , these will be automatically invoked after the run() method (if any) completes. These methods must take no arguments (other than self ). Log messages generated by any of these methods will be automatically grouped together by the test method they were invoked from, which can be helpful for readability. The post_run() Method \u00b6 If your job class implements a post_run() method (which must take no arguments other than self ), this method will be automatically invoked after the run() and test_*() methods (if any). It will be called even if one of the other methods raises an exception, so this method can be used to handle any necessary cleanup or final events (such as sending an email or triggering a webhook). The status of the overall job is available at this time as self.failed and the associated JobResult data field is available as self.results . Logging \u00b6 The following instance methods are available to log results from an executing job to be stored into JobLogEntry records associated with the current JobResult : self.log(message) self.log_debug(message) self.log_success(obj=None, message=None) self.log_info(obj=None, message=None) self.log_warning(obj=None, message=None) self.log_failure(obj=None, message=None) Messages recorded with log() or log_debug() will appear in a job's results but are never associated with a particular object; the other log_* functions may be invoked with or without a provided object to associate the message with. It is advised to log a message for each object that is evaluated so that the results will reflect how many objects are being manipulated or reported on. Markdown rendering is supported for log messages. Changed in version 1.3.4 As a security measure, the message passed to any of these methods will be passed through the nautobot.utilities.logging.sanitize() function in an attempt to strip out information such as usernames/passwords that should not be saved to the logs. This is of course best-effort only, and Job authors should take pains to ensure that such information is not passed to the logging APIs in the first place. The set of redaction rules used by the sanitize() function can be configured as settings.SANITIZER_PATTERNS . Note Using self.log_failure() , in addition to recording a log message, will flag the overall job as failed, but it will not stop the execution of the job, nor will it result in an automatic rollback of any database changes made by the job. To end a job early, you can use a Python raise or return as appropriate. Raising nautobot.utilities.exceptions.AbortTransaction will ensure that any database changes are rolled back as part of the process of ending the job. Accessing Request Data \u00b6 Details of the current HTTP request (the one being made to execute the job) are available as the instance attribute self.request . This can be used to infer, for example, the user executing the job and their client IP address: username = self . request . user . username ip_address = self . request . META . get ( 'HTTP_X_FORWARDED_FOR' ) or \\ self . request . META . get ( 'REMOTE_ADDR' ) self . log_info ( f \"Running as user { username } (IP: { ip_address } )...\" ) For a complete list of available request parameters, please see the Django documentation . Reading Data from Files \u00b6 The Job class provides two convenience methods for reading data from files: load_yaml load_json These two methods will load data in YAML or JSON format, respectively, from files within the local path (i.e. JOBS_ROOT/ ). Managing Jobs \u00b6 As of Nautobot 1.3, each Job class installed in Nautobot is represented by a corresponding Job data record in the Nautobot database. These data records are refreshed when the nautobot-server migrate or nautobot-server post_upgrade command is run, or (for Jobs from a Git repository) when a Git repository is enabled or re-synced in Nautobot. These data records make it possible for an administrative user (or other user with appropriate access privileges) to exert a level of administrative control over the Jobs created and updated by Job authors. Enabling Jobs for Running \u00b6 When a new Job record is created for a newly discovered Job class, it defaults to enabled = False , which prevents the Job from being run by any user. This is intended to provide a level of security and oversight regarding the installation of new Jobs into Nautobot. Important One exception to this default is when upgrading from a Nautobot release before 1.3 to Nautobot 1.3.0 or later. In this case, at the time of the upgrade, any Job class that shows evidence of having been run or scheduled under the older Nautobot version (that is, there is at least one JobResult and/or ScheduledJob record that references this Job class) will result in the creation of a Job database record with enabled = True . The reasoning for this feature is the assertion that because the Job has been run or scheduled previously, it has presumably already undergone appropriate review at that time, and so it should remain possible to run it as it was possible before the upgrade. An administrator or user with extras.change_job permission can edit the Job to change it to enabled = True , permitting running of the Job, when they have completed any appropriate review of the new Job to ensure that it meets their standards. Similarly, an obsolete or no-longer-used Job can be prevented from inadvertent execution by changing it back to enabled = False . Overriding Metadata \u00b6 An administrator or user with extras.change_job permission can also edit a Job database record to optionally override any or all of the following metadata attributes defined by the Job module or class: grouping name description approval_required commit_default hidden read_only soft_time_limit time_limit This is done by setting the corresponding \"override\" flag ( grouping_override , name_override , etc.) to True then providing a new value for the attribute in question. An overridden attribute will remain set to its overridden value even if the underlying Job class definition changes and nautobot-server <migrate|post_upgrade> gets run again. Conversely, clearing the \"override\" flag for an attribute and saving the database record will revert the attribute to the underlying value defined within the Job class source code. Deleting Jobs \u00b6 When a previously installed Job class is removed, after running nautobot-server <migrate|post_upgrade> or refreshing the providing Git repository, the Job database record will not be automatically deleted, but will be flagged as installed = False and can no longer be run or scheduled. An administrator or user with extras.delete_job permissions may delete such a Job database record if desired, but be aware that doing so will result in any existing JobResult or ScheduledJob records that originated from this Job losing their association to the Job; this association will not be automatically restored even if the Job is later reinstalled or reintroduced. Running Jobs \u00b6 Note To run any job, a user must be assigned the extras.run_job permission. This is achieved by assigning the user (or group) a permission on the extras > job object and specifying the run action in the admin UI as shown below. Similarly, to approve a job request by another user , a user must be assigned the extras.approve_job permission via the same process. Job approvers also need the extras.change_scheduledjob and/or extras.delete_scheduledjob permissions as job approvals are implemented via the ScheduledJob data model. Jobs and class_path \u00b6 It is a key concept to understand the 3 class_path elements: grouping_name : which can be one of local , git , or plugins - depending on where the Job has been defined. module_name : which is the Python path to the job definition file, for a plugin-provided job, this might be something like my_plugin_name.jobs.my_job_filename or nautobot_golden_config.jobs and is the importable Python path name (which would not include the .py extension, as per Python syntax standards). JobClassName : which is the name of the class inheriting from nautobot.extras.jobs.Job contained in the above file. The class_path is often represented as a string in the format of <grouping_name>/<module_name>/<JobClassName> , such as local/example/MyJobWithNoVars or plugins/nautobot_golden_config.jobs/BackupJob . Understanding the definitions of these elements will be important in running jobs programmatically. Changed in version 1.3.0 With the addition of Job database models, it is now generally possible and preferable to refer to a job by its UUID primary key, similar to other Nautobot database models, rather than its class_path . Via the Web UI \u00b6 Jobs can be run via the web UI by navigating to the job, completing any required form data (if any), and clicking the \"Run Job\" button. Once a job has been run, the latest JobResult for that job will be summarized in the job list view. Via the API \u00b6 To run a job via the REST API, issue a POST request to the job's endpoint /api/extras/jobs/<uuid>/run/ . You can optionally provide JSON data to set the commit flag, specify any required user input data , and/or provide optional scheduling information as described in the section on scheduling and approvals . For example, to run a job with no user inputs and without committing any anything to the database: curl -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/extras/jobs/$JOB_ID/run/ Or to run a job that expects user inputs, and commit changes to the database: curl -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/extras/jobs/$JOB_ID/run/ \\ --data '{\"data\": {\"string_variable\": \"somevalue\", \"integer_variable\": 123}, \"commit\": true}' When providing input data, it is possible to specify complex values contained in ObjectVar s, MultiObjectVar s, and IPAddressVar s. ObjectVar s can be specified by either using their primary key directly as the value, or as a dictionary containing a more complicated query that gets passed into the Django ORM as keyword arguments. MultiObjectVar s can be specified as a list of primary keys. IPAddressVar s can be provided as strings in CIDR notation. Via the CLI \u00b6 Jobs can be run from the CLI by invoking the management command: nautobot-server runjob [--username <username>] [--commit] [--local] [--data <data>] <class_path> Note See above for class_path definitions. Added in version 1.3.10 The --data and --local parameters were added. The --data parameter must be a JSON string, e.g. --data='{\"string_variable\": \"somevalue\", \"integer_variable\": 123}' Using the same example shown in the API: nautobot-server runjob --username myusername local/example/MyJobWithNoVars Warning The --username <username> parameter can be used to specify the user that will be identified as the requester of the job. It is optional if the job will not be modifying the database, but is mandatory if you are running with --commit , as the specified user will own any resulting database changes. Note that nautobot-server commands, like all management commands and other direct interactions with the Django database, are not gated by the usual Nautobot user authentication flow. It is possible to specify any existing --username with the nautobot-server runjob command in order to impersonate any defined user in Nautobot. Use this power wisely and be cautious who you allow to access it. Testing Jobs \u00b6 Jobs are Python code and can be tested as such, usually via Django unit-test features . That said, there are a few useful tricks specific to testing Jobs. While individual methods within your Job can and should be tested in isolation, you'll likely also want to test the entire execution of the Job. Nautobot 1.3.3 introduced a few enhancements to make this simpler to do, but it's also quite possible to test in earlier releases with a bit more effort. Nautobot 1.3.3 and later \u00b6 The simplest way to test the entire execution of Jobs from 1.3.3 on is via calling the nautobot.utilities.testing.run_job_for_testing() method, which is a helper wrapper around the run_job function used to execute a Job via Nautobot's Celery worker process. Because of the way run_job_for_testing and more specifically run_job() works, which is somewhat complex behind the scenes, you need to inherit from nautobot.utilities.testing.TransactionTestCase instead of django.test.TestCase (Refer to the Django documentation if you're interested in the differences between these classes - TransactionTestCase from Nautobot is a small wrapper around Django's TransactionTestCase ). When using TransactionTestCase (whether from Django or from Nautobot) each tests runs on a completely empty database. Furthermore, Nautobot requires new jobs to be enabled before they can run. Therefore, we need to make sure the job is enabled before each run which run_job_for_testing handles for us. A simple example of a Job test case for 1.3.3 and forward might look like the following: from nautobot.extras.models import Job , JobLogEntry from nautobot.utilities.testing import run_job_for_testing , TransactionTestCase class MyJobTestCase ( TransactionTestCase ): def test_my_job ( self ): # Testing of Job \"MyJob\" in file \"my_job_file.py\" in $JOBS_ROOT job = Job . objects . get ( job_class_name = \"MyJob\" , module_name = \"my_job_file\" , source = \"local\" ) # or, job = Job.objects.get_for_class_path(\"local/my_job_file/MyJob\") job_result = run_job_for_testing ( job , data = {}, commit = False ) # Since we ran with commit=False, any database changes made by the job won't persist, # but we can still inspect the logs created by running the job log_entries = JobLogEntry . objects . filter ( job_result = job_result ) for log_entry in log_entries : self . assertEqual ( log_entry . message , \"...\" ) Tip For more advanced examples (such as testing jobs executed with commit=True , for example) refer to the Nautobot source code, specifically nautobot/extras/tests/test_jobs.py . Nautobot 1.3.2 and earlier (including 1.2) \u00b6 If your test case needs to be backwards-compatible with test execution against Nautobot 1.3.2 and/or earlier, you need to handle a couple more things manually: Set up the \"job_logs\" database correctly for testing: from django.conf import settings if \"job_logs\" in settings . DATABASES : settings . DATABASES [ \"job_logs\" ] = settings . DATABASES [ \"job_logs\" ] . copy () settings . DATABASES [ \"job_logs\" ][ \"TEST\" ] = { \"MIRROR\" : \"default\" } Replicate the behavior of run_job_for_testing manually so that your test execution most closely resembles the way the celery worker would run the test: import uuid from django.contrib.auth import get_user_model from django.contrib.contenttypes.models import ContentType from nautobot.extras.context_managers import web_request_context from nautobot.extras.jobs import run_job from nautobot.extras.models import JobResult , Job def run_job_for_testing ( job , data = None , commit = True , username = \"test-user\" ): if data is None : data = {} user_model = get_user_model () user , _ = user_model . objects . get_or_create ( username = username , is_superuser = True , password = \"password\" ) job_result = JobResult . objects . create ( name = job . class_path , obj_type = ContentType . objects . get_for_model ( Job ), user = user , job_id = uuid . uuid4 (), ) with web_request_context ( user = user ) as request : run_job ( data = data , request = request , commit = commit , job_result_pk = job_result . pk ) return job_result Setup the databases field on the test class correctly, and re-create the default Statuses on setUp in your test classes, because django.test.TransactionTestCase truncates them on every tearDown : from django.apps import apps from django.conf import settings from django.test import TransactionTestCase from nautobot.extras.management import populate_status_choices class MyJobTestCase ( TransactionTestCase ): # 'job_logs' is a proxy connection to the same (default) database that's used exclusively for Job logging if \"job_logs\" in settings . DATABASES : databases = ( \"default\" , \"job_logs\" ) def setUp ( self ): super () . setUp () populate_status_choices ( apps , None ) Example Jobs \u00b6 Creating objects for a planned site \u00b6 This job prompts the user for three variables: The name of the new site The device model (a filtered list of defined device types) The number of access switches to create These variables are presented as a web form to be completed by the user. Once submitted, the job's run() method is called to create the appropriate objects, and it returns simple CSV output to the user summarizing the created objects. from django.utils.text import slugify from nautobot.dcim.models import Device , DeviceRole , DeviceType , Manufacturer , Site from nautobot.extras.models import Status from nautobot.extras.jobs import * class NewBranch ( Job ): class Meta : name = \"New Branch\" description = \"Provision a new branch site\" field_order = [ 'site_name' , 'switch_count' , 'switch_model' ] site_name = StringVar ( description = \"Name of the new site\" ) switch_count = IntegerVar ( description = \"Number of access switches to create\" ) manufacturer = ObjectVar ( model = Manufacturer , required = False ) switch_model = ObjectVar ( description = \"Access switch model\" , model = DeviceType , query_params = { 'manufacturer_id' : '$manufacturer' } ) def run ( self , data , commit ): STATUS_PLANNED = Status . objects . get ( slug = 'planned' ) # Create the new site site = Site ( name = data [ 'site_name' ], slug = slugify ( data [ 'site_name' ]), status = STATUS_PLANNED , ) site . validated_save () self . log_success ( obj = site , message = \"Created new site\" ) # Create access switches switch_role = DeviceRole . objects . get ( name = 'Access Switch' ) for i in range ( 1 , data [ 'switch_count' ] + 1 ): switch = Device ( device_type = data [ 'switch_model' ], name = f ' { site . slug } -switch { i } ' , site = site , status = STATUS_PLANNED , device_role = switch_role ) switch . validated_save () self . log_success ( obj = switch , message = \"Created new switch\" ) # Generate a CSV table of new devices output = [ 'name,make,model' ] for switch in Device . objects . filter ( site = site ): attrs = [ switch . name , switch . device_type . manufacturer . name , switch . device_type . model ] output . append ( ',' . join ( attrs )) return ' \\n ' . join ( output ) Device validation \u00b6 A job to perform various validation of Device data in Nautobot. As this job does not require any user input, it does not define any variables, nor does it implement a run() method. from nautobot.dcim.models import ConsolePort , Device , PowerPort from nautobot.extras.models import Status from nautobot.extras.jobs import Job class DeviceConnectionsReport ( Job ): description = \"Validate the minimum physical connections for each device\" def test_console_connection ( self ): STATUS_ACTIVE = Status . objects . get ( slug = 'active' ) # Check that every console port for every active device has a connection defined. for console_port in ConsolePort . objects . prefetch_related ( 'device' ) . filter ( device__status = STATUS_ACTIVE ): if console_port . connected_endpoint is None : self . log_failure ( obj = console_port . device , message = \"No console connection defined for {} \" . format ( console_port . name ) ) elif not console_port . connection_status : self . log_warning ( obj = console_port . device , message = \"Console connection for {} marked as planned\" . format ( console_port . name ) ) else : self . log_success ( obj = console_port . device ) def test_power_connections ( self ): STATUS_ACTIVE = Status . objects . get ( slug = 'active' ) # Check that every active device has at least two connected power supplies. for device in Device . objects . filter ( status = STATUS_ACTIVE ): connected_ports = 0 for power_port in PowerPort . objects . filter ( device = device ): if power_port . connected_endpoint is not None : connected_ports += 1 if not power_port . connection_status : self . log_warning ( obj = device , message = \"Power connection for {} marked as planned\" . format ( power_port . name ) ) if connected_ports < 2 : self . log_failure ( obj = device , message = \" {} connected power supplies found (2 needed)\" . format ( connected_ports ) ) else : self . log_success ( obj = device )","title":"Jobs"},{"location":"additional-features/jobs.html#jobs","text":"Jobs are a way for users to execute custom logic on demand from within the Nautobot UI. Jobs can interact directly with Nautobot data to accomplish various data creation, modification, and validation tasks, such as: Automatically populate new devices and cables in preparation for a new site deployment Create a range of new reserved prefixes or IP addresses Fetch data from an external source and import it to Nautobot Check and report whether all top-of-rack switches have a console connection Check and report whether every router has a loopback interface with an assigned IP address Check and report whether all IP addresses have a parent prefix ...and so on. Jobs are Python code and exist outside of the official Nautobot code base, so they can be updated and changed without interfering with the core Nautobot installation. And because they're completely customizable, there's practically no limit to what a job can accomplish. Note Jobs unify and supersede the functionality previously provided in NetBox by \"custom scripts\" and \"reports\". Jobs are backwards-compatible for now with the Script and Report class APIs, but you are urged to move to the new Job class API described below. Jobs may be optionally marked as read-only which equates to the Report functionally, but in all cases, user input is supported via job variables .","title":"Jobs"},{"location":"additional-features/jobs.html#writing-jobs","text":"Jobs may be installed in one of three ways: Manually installed as files in the JOBS_ROOT path (which defaults to $NAUTOBOT_ROOT/jobs/ ). The JOBS_ROOT directory must contain a file named __init__.py . Do not delete this file. Each file created within this path is considered a separate module; there is no support for cross-file dependencies (such as a file acting as a common \"library\" module of functions shared between jobs) for files installed in this way. Imported from an external Git repository . The repository's jobs/ directory must contain a file named __init__.py . Each Job file in the repository is considered a separate module; there is no support for cross-file dependencies (such as a file acting as a common \"library\" module of functions shared between jobs) for files installed in this way. Packaged as part of a plugin . Jobs installed this way are part of the plugin module and can import code from elsewhere in the plugin or even have dependencies on other packages, if needed, via the standard Python packaging mechanisms. In any case, each module holds one or more Jobs (Python classes), each of which serves a specific purpose. The logic of each job can be split into a number of distinct methods, each of which performs a discrete portion of the overall job logic. For example, we can create a module named devices.py to hold all of our jobs which pertain to devices in Nautobot. Within that module, we might define several jobs. Each job is defined as a Python class inheriting from extras.jobs.Job , which provides the base functionality needed to accept user input and log activity. Warning Make sure you are not inheriting extras.jobs.models.Job instead, otherwise Django will think you want to define a new database model. from nautobot.extras.jobs import Job class CreateDevices ( Job ): ... class DeviceConnectionsReport ( Job ): ... class DeviceIPsReport ( Job ): ... Each job class will implement some or all of the following components: Module and class attributes, providing for default behavior, documentation and discoverability a set of variables for user input via the Nautobot UI (if your job requires any user inputs) a run() method, which is executed first and receives the user input values, if any any number of test_*() methods, which will be invoked next in order of declaration. Log messages generated by the job will be grouped together by the test method they were invoked from. a post_run() method, which is executed last and can be used to handle any necessary cleanup or final events (such as sending an email or triggering a webhook). The status of the overall job is available at this time as self.failed and the JobResult data object is available as self.result . You can implement the entire job within the run() function, but for more complex jobs, you may want to provide more granularity in the output and logging of activity. For this purpose, you can implement portions of the logic as test_*() methods (i.e., methods whose name begins with test_* ) and/or a post_run() method. Log messages generated by the job logging APIs (more below on this topic) will be grouped together according to their base method ( run , test_a , test_b , ..., post_run ) which can aid in understanding the operation of the job. Note Your job can of course define additional Python methods to compartmentalize and reuse logic as required; however the run , test_* , and post_run methods are the only ones that will be automatically invoked by Nautobot. It's important to understand that jobs execute on the server asynchronously as background tasks; they log messages and report their status to the database by updating JobResult records and creating JobLogEntry records. Note When actively developing a Job utilizing a development environment it's important to understand that the \"automatically reload when code changes are detected\" debugging functionality provided by nautobot-server runserver does not automatically restart the Celery worker process when code changes are made; therefore, it is required to restart the worker after each update to your Job source code or else it will continue to run the version of the Job code that was present when it first started. Additionally, as of Nautobot 1.3, the Job database records corresponding to installed Jobs are not automatically refreshed when the development server auto-restarts. If you make changes to any of the class and module metadata attributes described in the following sections, the database will be refreshed to reflect these changes only after running nautobot-server migrate or nautobot-server post_upgrade (recommended) or if you manually edit a Job database record to force it to be refreshed.","title":"Writing Jobs"},{"location":"additional-features/jobs.html#module-metadata-attributes","text":"","title":"Module Metadata Attributes"},{"location":"additional-features/jobs.html#name-grouping","text":"You can define a global constant called name within a job module (the Python file which contains one or more job classes) to set the default grouping under which jobs in this module will be displayed in the Nautobot UI. If this value is not defined, the module's file name will be used. This \"grouping\" value may also be defined or overridden when editing Job records in the database. Note In some UI elements and API endpoints, the module file name is displayed in addition to or in place of this attribute, so even if defining this attribute, you should still choose an appropriately explanatory file name as well.","title":"name (Grouping)"},{"location":"additional-features/jobs.html#class-metadata-attributes","text":"Job-specific attributes may be defined under a class named Meta within each job class you implement. All of these are optional, but encouraged.","title":"Class Metadata Attributes"},{"location":"additional-features/jobs.html#name","text":"This is the human-friendly name of your job, as will be displayed in the Nautobot UI. If not set, the class name will be used. Note In some UI elements and API endpoints, the class name is displayed in addition to or in place of this attribute, so even if defining this attribute, you should still choose an appropriately explanatory class name as well.","title":"name"},{"location":"additional-features/jobs.html#description","text":"An optional human-friendly description of what this job does. This can accept either plain text or Markdown-formatted text. It can also be multiple lines: class ExampleJob ( Job ): class Meta : description = \"\"\" This job does a number of interesting things. 1. It hacks the Gibson 2. It immanentizes the eschaton 3. It's a floor wax *and* a dessert topping \"\"\" If you code a multi-line description, the first line only will be used in the description column of the jobs list, while the full description will be rendered in the job detail view, submission, approval, and results pages.","title":"description"},{"location":"additional-features/jobs.html#approval_required","text":"Default: False A boolean that will mark this job as requiring approval from another user to be run. For more details on approvals, please refer to the section on scheduling and approvals .","title":"approval_required"},{"location":"additional-features/jobs.html#commit_default","text":"Default: True The checkbox to commit database changes when executing a job is checked by default in the Nautobot UI. You can set commit_default to False under the Meta class if you want this option to instead be unchecked by default. class MyJob ( Job ): class Meta : commit_default = False","title":"commit_default"},{"location":"additional-features/jobs.html#field_order","text":"Default: [] A list of strings (field names) representing the order your job variables should be rendered as form fields in the job submission UI. If not defined, the variables will be listed in order of their definition in the code. If variables are defined on a parent class and no field order is defined, the parent class variables will appear before the subclass variables.","title":"field_order"},{"location":"additional-features/jobs.html#has_sensitive_variables","text":"Added in version 1.3.10 Default: True Unless set to False, it prevents the job's input parameters from being saved to the database. This defaults to True so as to protect against inadvertent database exposure of input parameters that may include sensitive data such as passwords or other user credentials. Review whether each job's inputs contain any such variables before setting this to False; if a job does contain sensitive inputs, if possible you should consider whether the job could be re-implemented using Nautobot's Secrets feature as a way to ensure that the sensitive data is not directly provided as a job variable at all. Important notes about jobs with sensitive variables: Such jobs cannot be scheduled to run in the future or on a recurring schedule (as scheduled jobs must by necessity store their variables in the database for future reference). Jobs with sensitive variables cannot be marked as requiring approval (as jobs pending approval must store their variables in the database until approved).","title":"has_sensitive_variables"},{"location":"additional-features/jobs.html#hidden","text":"Default: False A Boolean that if set to True prevents the job from being displayed by default in the list of Jobs in the Nautobot UI. Since the jobs execution framework is designed to be generic, there may be several technical jobs defined by users which interact with or are invoked by external systems. In such cases, these jobs are not meant to be executed by a human and likely do not make sense to expose to end users for execution, and thus having them exposed in the UI at all is extraneous. Important notes about hidden jobs: This is merely hiding them by default from the web interface. It is NOT a security feature. In the Jobs list view it is possible to filter to \"Hidden: (no selection)\" or even \"Hidden: Yes\" to list the hidden jobs. All Job UI and REST API endpoints still exist for hidden jobs and can be accessed by any user who is aware of their existence. Hidden jobs can still be executed through the UI or the REST API given the appropriate URL. Results for hidden jobs will still appear in the Job Results list after they are run.","title":"hidden"},{"location":"additional-features/jobs.html#read_only","text":"Added in version 1.1.0 Default: False A boolean that designates whether the job is able to make changes to data in the database. The value defaults to False but when set to True , any data modifications executed from the job's code will be automatically aborted at the end of the job. The job input form is also modified to remove the commit checkbox as it is irrelevant for read-only jobs. When a job is marked as read-only, log messages that are normally automatically emitted about the DB transaction state are not included because no changes to data are allowed. Note that user input may still be optionally collected with read-only jobs via job variables, as described below.","title":"read_only"},{"location":"additional-features/jobs.html#soft_time_limit","text":"Added in version 1.3.0 An int or float value, in seconds, which can be used to override the default soft time limit for a job task to complete. The celery.exceptions.SoftTimeLimitExceeded exception will be raised when this soft time limit is exceeded. The job task can catch this to clean up before the hard time limit (10 minutes by default) is reached: from celery.exceptions import SoftTimeLimitExceeded from nautobot.extras.jobs import Job class ExampleJobWithSoftTimeLimit ( Job ): class Meta : name = \"Soft Time Limit\" description = \"Set a soft time limit of 10 seconds`\" soft_time_limit = 10 def run ( self , data , commit ): try : # code which might take longer than 10 seconds to run job_code () except SoftTimeLimitExceeded : # any clean up code cleanup_in_a_hurry ()","title":"soft_time_limit"},{"location":"additional-features/jobs.html#template_name","text":"Added in version 1.4.0 A path relative to the job source code containing a Django template which provides additional code to customize the Job's submission form. This template should extend the existing job template, extras/job.html , otherwise the base form and functionality may not be available. A template can provide additional JavaScript, CSS, or even display HTML. A good starting template would be: {% extends 'extras/job.html' %} {% block extra_styles %} {{ block.super }} <!-- Add additional CSS here. --> {% endblock %} {% block content %} {{ block.super }} <!-- Add additional HTML here. --> {% endblock content %} {% block javascript %} {{ block.super }} <!-- Add additional JavaScript here. --> {% endblock javascript %} For another example checkout the template used in example plugin in the GitHub repo.","title":"template_name"},{"location":"additional-features/jobs.html#time_limit","text":"Added in version 1.3.0 An int or float value, in seconds, which can be used to override the default hard time limit (10 minutes by default) for a job task to complete. Unlike the soft_time_limit above, no exceptions are raised when a time_limit is exceeded. The task will just terminate silently: from nautobot.extras.jobs import Job class ExampleJobWithHardTimeLimit ( Job ): class Meta : name = \"Hard Time Limit\" description = \"Set a hard time limit of 10 seconds`\" time_limit = 10 def run ( self , data , commit ): # code which might take longer than 10 seconds to run # this code will fail silently if the time_limit is exceeded job_code () Note If the time_limit is set to a value less than or equal to the soft_time_limit , a warning log is generated to inform the user that this job will fail silently after the time_limit as the soft_time_limit will never be reached.","title":"time_limit"},{"location":"additional-features/jobs.html#variables","text":"Variables allow your job to accept user input via the Nautobot UI, but they are optional; if your job does not require any user input, there is no need to define any variables. Conversely, if you are making use of user input in your job, you must also implement the run() method, as it is the only entry point to your job that has visibility into the variable values provided by the user. from nautobot.extras.jobs import Job , StringVar , IntegerVar , ObjectVar class CreateDevices ( Job ): var1 = StringVar ( ... ) var2 = IntegerVar ( ... ) var3 = ObjectVar ( ... ) def run ( self , data , commit ): ... The remainder of this section documents the various supported variable types and how to make use of them.","title":"Variables"},{"location":"additional-features/jobs.html#default-variable-options","text":"All job variables support the following default options: default - The field's default value description - A brief user-friendly description of the field label - The field name to be displayed in the rendered form required - Indicates whether the field is mandatory (all fields are required by default) widget - The class of form widget to use (see the Django documentation )","title":"Default Variable Options"},{"location":"additional-features/jobs.html#stringvar","text":"Stores a string of characters (i.e. text). Options include: min_length - Minimum number of characters max_length - Maximum number of characters regex - A regular expression against which the provided value must match Note that min_length and max_length can be set to the same number to effect a fixed-length field.","title":"StringVar"},{"location":"additional-features/jobs.html#textvar","text":"Arbitrary text of any length. Renders as a multi-line text input field.","title":"TextVar"},{"location":"additional-features/jobs.html#integervar","text":"Stores a numeric integer. Options include: min_value - Minimum value max_value - Maximum value","title":"IntegerVar"},{"location":"additional-features/jobs.html#booleanvar","text":"A true/false flag. This field has no options beyond the defaults listed above.","title":"BooleanVar"},{"location":"additional-features/jobs.html#choicevar","text":"A set of choices from which the user can select one. choices - A list of (value, label) tuples representing the available choices. For example: CHOICES = ( ( 'n' , 'North' ), ( 's' , 'South' ), ( 'e' , 'East' ), ( 'w' , 'West' ) ) direction = ChoiceVar ( choices = CHOICES ) In the example above, selecting the choice labeled \"North\" will submit the value n .","title":"ChoiceVar"},{"location":"additional-features/jobs.html#multichoicevar","text":"Similar to ChoiceVar , but allows for the selection of multiple choices.","title":"MultiChoiceVar"},{"location":"additional-features/jobs.html#objectvar","text":"A particular object within Nautobot. Each ObjectVar must specify a particular model, and allows the user to select one of the available instances. ObjectVar accepts several arguments, listed below. model - The model class display_field - The name of the REST API object field to display in the selection list (default: 'display' ) query_params - A dictionary of REST API query parameters to use when retrieving available options (optional) null_option - A label representing a \"null\" or empty choice (optional) The display_field argument is useful in cases where using the display API field is not desired for referencing the object. For example, when displaying a list of IP Addresses, you might want to use the dns_name field: device_type = ObjectVar ( model = IPAddress , display_field = \"dns_name\" , ) To limit the selections available within the list, additional query parameters can be passed as the query_params dictionary. For example, to show only devices with an \"active\" status: device = ObjectVar ( model = Device , query_params = { 'status' : 'active' } ) Multiple values can be specified by assigning a list to the dictionary key. It is also possible to reference the value of other fields in the form by prepending a dollar sign ( $ ) to the variable's name. The keys you can use in this dictionary are the same ones that are available in the REST API - as an example it is also possible to filter the Site ObjectVar for its tenant_group_id . region = ObjectVar ( model = Region ) tenant_group = ObjectVar ( model = TenantGroup ) site = ObjectVar ( model = Site , query_params = { 'region_id' : '$region' , 'tenant_group_id' : '$tenant_group' } )","title":"ObjectVar"},{"location":"additional-features/jobs.html#multiobjectvar","text":"Similar to ObjectVar , but allows for the selection of multiple objects.","title":"MultiObjectVar"},{"location":"additional-features/jobs.html#filevar","text":"An uploaded file. Note that uploaded files are present in memory only for the duration of the job's execution: They will not be automatically saved for future use. The job is responsible for writing file contents to disk where necessary.","title":"FileVar"},{"location":"additional-features/jobs.html#ipaddressvar","text":"An IPv4 or IPv6 address, without a mask. Returns a netaddr.IPAddress object.","title":"IPAddressVar"},{"location":"additional-features/jobs.html#ipaddresswithmaskvar","text":"An IPv4 or IPv6 address with a mask. Returns a netaddr.IPNetwork object which includes the mask.","title":"IPAddressWithMaskVar"},{"location":"additional-features/jobs.html#ipnetworkvar","text":"An IPv4 or IPv6 network with a mask. Returns a netaddr.IPNetwork object. Two attributes are available to validate the provided mask: min_prefix_length - Minimum length of the mask max_prefix_length - Maximum length of the mask","title":"IPNetworkVar"},{"location":"additional-features/jobs.html#the-run-method","text":"The run() method, if you choose to implement it, should accept two arguments: data - A dictionary which will contain all of the variable data passed in by the user (via the web UI or REST API) commit - A boolean indicating whether database changes should be committed. If this is False , even if your Job attempts to make database changes, they will be automatically rolled back when the Job completes. from nautobot.extras.jobs import Job , StringVar , IntegerVar , ObjectVar class CreateDevices ( Job ): var1 = StringVar ( ... ) var2 = IntegerVar ( ... ) var3 = ObjectVar ( ... ) def run ( self , data , commit ): ... Again, defining user variables is totally optional; you may create a job with just a run() method if no user input is needed, in which case data will be an empty dictionary. Note The test_*() and post_run() methods do not accept any arguments; if you need to access user data or the commit flag, your run() method is responsible for storing these values in the job instance, such as: def run(self, data, commit): self.data = data self.commit = commit Warning When writing Jobs that create and manipulate data it is recommended to make use of the validated_save() convenience method which exists on all core models. This method saves the instance data but first enforces model validation logic. Simply calling save() on the model instance does not enforce validation automatically and may lead to bad data. See the development best practices . Warning The Django ORM provides methods to create/edit many objects at once, namely bulk_create() and update() . These are best avoided in most cases as they bypass a model's built-in validation and can easily lead to database corruption if not used carefully.","title":"The run() Method"},{"location":"additional-features/jobs.html#the-test_-methods","text":"If your job class defines any number of methods whose names begin with test_ , these will be automatically invoked after the run() method (if any) completes. These methods must take no arguments (other than self ). Log messages generated by any of these methods will be automatically grouped together by the test method they were invoked from, which can be helpful for readability.","title":"The test_*() Methods"},{"location":"additional-features/jobs.html#the-post_run-method","text":"If your job class implements a post_run() method (which must take no arguments other than self ), this method will be automatically invoked after the run() and test_*() methods (if any). It will be called even if one of the other methods raises an exception, so this method can be used to handle any necessary cleanup or final events (such as sending an email or triggering a webhook). The status of the overall job is available at this time as self.failed and the associated JobResult data field is available as self.results .","title":"The post_run() Method"},{"location":"additional-features/jobs.html#logging","text":"The following instance methods are available to log results from an executing job to be stored into JobLogEntry records associated with the current JobResult : self.log(message) self.log_debug(message) self.log_success(obj=None, message=None) self.log_info(obj=None, message=None) self.log_warning(obj=None, message=None) self.log_failure(obj=None, message=None) Messages recorded with log() or log_debug() will appear in a job's results but are never associated with a particular object; the other log_* functions may be invoked with or without a provided object to associate the message with. It is advised to log a message for each object that is evaluated so that the results will reflect how many objects are being manipulated or reported on. Markdown rendering is supported for log messages. Changed in version 1.3.4 As a security measure, the message passed to any of these methods will be passed through the nautobot.utilities.logging.sanitize() function in an attempt to strip out information such as usernames/passwords that should not be saved to the logs. This is of course best-effort only, and Job authors should take pains to ensure that such information is not passed to the logging APIs in the first place. The set of redaction rules used by the sanitize() function can be configured as settings.SANITIZER_PATTERNS . Note Using self.log_failure() , in addition to recording a log message, will flag the overall job as failed, but it will not stop the execution of the job, nor will it result in an automatic rollback of any database changes made by the job. To end a job early, you can use a Python raise or return as appropriate. Raising nautobot.utilities.exceptions.AbortTransaction will ensure that any database changes are rolled back as part of the process of ending the job.","title":"Logging"},{"location":"additional-features/jobs.html#accessing-request-data","text":"Details of the current HTTP request (the one being made to execute the job) are available as the instance attribute self.request . This can be used to infer, for example, the user executing the job and their client IP address: username = self . request . user . username ip_address = self . request . META . get ( 'HTTP_X_FORWARDED_FOR' ) or \\ self . request . META . get ( 'REMOTE_ADDR' ) self . log_info ( f \"Running as user { username } (IP: { ip_address } )...\" ) For a complete list of available request parameters, please see the Django documentation .","title":"Accessing Request Data"},{"location":"additional-features/jobs.html#reading-data-from-files","text":"The Job class provides two convenience methods for reading data from files: load_yaml load_json These two methods will load data in YAML or JSON format, respectively, from files within the local path (i.e. JOBS_ROOT/ ).","title":"Reading Data from Files"},{"location":"additional-features/jobs.html#managing-jobs","text":"As of Nautobot 1.3, each Job class installed in Nautobot is represented by a corresponding Job data record in the Nautobot database. These data records are refreshed when the nautobot-server migrate or nautobot-server post_upgrade command is run, or (for Jobs from a Git repository) when a Git repository is enabled or re-synced in Nautobot. These data records make it possible for an administrative user (or other user with appropriate access privileges) to exert a level of administrative control over the Jobs created and updated by Job authors.","title":"Managing Jobs"},{"location":"additional-features/jobs.html#enabling-jobs-for-running","text":"When a new Job record is created for a newly discovered Job class, it defaults to enabled = False , which prevents the Job from being run by any user. This is intended to provide a level of security and oversight regarding the installation of new Jobs into Nautobot. Important One exception to this default is when upgrading from a Nautobot release before 1.3 to Nautobot 1.3.0 or later. In this case, at the time of the upgrade, any Job class that shows evidence of having been run or scheduled under the older Nautobot version (that is, there is at least one JobResult and/or ScheduledJob record that references this Job class) will result in the creation of a Job database record with enabled = True . The reasoning for this feature is the assertion that because the Job has been run or scheduled previously, it has presumably already undergone appropriate review at that time, and so it should remain possible to run it as it was possible before the upgrade. An administrator or user with extras.change_job permission can edit the Job to change it to enabled = True , permitting running of the Job, when they have completed any appropriate review of the new Job to ensure that it meets their standards. Similarly, an obsolete or no-longer-used Job can be prevented from inadvertent execution by changing it back to enabled = False .","title":"Enabling Jobs for Running"},{"location":"additional-features/jobs.html#overriding-metadata","text":"An administrator or user with extras.change_job permission can also edit a Job database record to optionally override any or all of the following metadata attributes defined by the Job module or class: grouping name description approval_required commit_default hidden read_only soft_time_limit time_limit This is done by setting the corresponding \"override\" flag ( grouping_override , name_override , etc.) to True then providing a new value for the attribute in question. An overridden attribute will remain set to its overridden value even if the underlying Job class definition changes and nautobot-server <migrate|post_upgrade> gets run again. Conversely, clearing the \"override\" flag for an attribute and saving the database record will revert the attribute to the underlying value defined within the Job class source code.","title":"Overriding Metadata"},{"location":"additional-features/jobs.html#deleting-jobs","text":"When a previously installed Job class is removed, after running nautobot-server <migrate|post_upgrade> or refreshing the providing Git repository, the Job database record will not be automatically deleted, but will be flagged as installed = False and can no longer be run or scheduled. An administrator or user with extras.delete_job permissions may delete such a Job database record if desired, but be aware that doing so will result in any existing JobResult or ScheduledJob records that originated from this Job losing their association to the Job; this association will not be automatically restored even if the Job is later reinstalled or reintroduced.","title":"Deleting Jobs"},{"location":"additional-features/jobs.html#running-jobs","text":"Note To run any job, a user must be assigned the extras.run_job permission. This is achieved by assigning the user (or group) a permission on the extras > job object and specifying the run action in the admin UI as shown below. Similarly, to approve a job request by another user , a user must be assigned the extras.approve_job permission via the same process. Job approvers also need the extras.change_scheduledjob and/or extras.delete_scheduledjob permissions as job approvals are implemented via the ScheduledJob data model.","title":"Running Jobs"},{"location":"additional-features/jobs.html#jobs-and-class_path","text":"It is a key concept to understand the 3 class_path elements: grouping_name : which can be one of local , git , or plugins - depending on where the Job has been defined. module_name : which is the Python path to the job definition file, for a plugin-provided job, this might be something like my_plugin_name.jobs.my_job_filename or nautobot_golden_config.jobs and is the importable Python path name (which would not include the .py extension, as per Python syntax standards). JobClassName : which is the name of the class inheriting from nautobot.extras.jobs.Job contained in the above file. The class_path is often represented as a string in the format of <grouping_name>/<module_name>/<JobClassName> , such as local/example/MyJobWithNoVars or plugins/nautobot_golden_config.jobs/BackupJob . Understanding the definitions of these elements will be important in running jobs programmatically. Changed in version 1.3.0 With the addition of Job database models, it is now generally possible and preferable to refer to a job by its UUID primary key, similar to other Nautobot database models, rather than its class_path .","title":"Jobs and class_path"},{"location":"additional-features/jobs.html#via-the-web-ui","text":"Jobs can be run via the web UI by navigating to the job, completing any required form data (if any), and clicking the \"Run Job\" button. Once a job has been run, the latest JobResult for that job will be summarized in the job list view.","title":"Via the Web UI"},{"location":"additional-features/jobs.html#via-the-api","text":"To run a job via the REST API, issue a POST request to the job's endpoint /api/extras/jobs/<uuid>/run/ . You can optionally provide JSON data to set the commit flag, specify any required user input data , and/or provide optional scheduling information as described in the section on scheduling and approvals . For example, to run a job with no user inputs and without committing any anything to the database: curl -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/extras/jobs/$JOB_ID/run/ Or to run a job that expects user inputs, and commit changes to the database: curl -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/extras/jobs/$JOB_ID/run/ \\ --data '{\"data\": {\"string_variable\": \"somevalue\", \"integer_variable\": 123}, \"commit\": true}' When providing input data, it is possible to specify complex values contained in ObjectVar s, MultiObjectVar s, and IPAddressVar s. ObjectVar s can be specified by either using their primary key directly as the value, or as a dictionary containing a more complicated query that gets passed into the Django ORM as keyword arguments. MultiObjectVar s can be specified as a list of primary keys. IPAddressVar s can be provided as strings in CIDR notation.","title":"Via the API"},{"location":"additional-features/jobs.html#via-the-cli","text":"Jobs can be run from the CLI by invoking the management command: nautobot-server runjob [--username <username>] [--commit] [--local] [--data <data>] <class_path> Note See above for class_path definitions. Added in version 1.3.10 The --data and --local parameters were added. The --data parameter must be a JSON string, e.g. --data='{\"string_variable\": \"somevalue\", \"integer_variable\": 123}' Using the same example shown in the API: nautobot-server runjob --username myusername local/example/MyJobWithNoVars Warning The --username <username> parameter can be used to specify the user that will be identified as the requester of the job. It is optional if the job will not be modifying the database, but is mandatory if you are running with --commit , as the specified user will own any resulting database changes. Note that nautobot-server commands, like all management commands and other direct interactions with the Django database, are not gated by the usual Nautobot user authentication flow. It is possible to specify any existing --username with the nautobot-server runjob command in order to impersonate any defined user in Nautobot. Use this power wisely and be cautious who you allow to access it.","title":"Via the CLI"},{"location":"additional-features/jobs.html#testing-jobs","text":"Jobs are Python code and can be tested as such, usually via Django unit-test features . That said, there are a few useful tricks specific to testing Jobs. While individual methods within your Job can and should be tested in isolation, you'll likely also want to test the entire execution of the Job. Nautobot 1.3.3 introduced a few enhancements to make this simpler to do, but it's also quite possible to test in earlier releases with a bit more effort.","title":"Testing Jobs"},{"location":"additional-features/jobs.html#nautobot-133-and-later","text":"The simplest way to test the entire execution of Jobs from 1.3.3 on is via calling the nautobot.utilities.testing.run_job_for_testing() method, which is a helper wrapper around the run_job function used to execute a Job via Nautobot's Celery worker process. Because of the way run_job_for_testing and more specifically run_job() works, which is somewhat complex behind the scenes, you need to inherit from nautobot.utilities.testing.TransactionTestCase instead of django.test.TestCase (Refer to the Django documentation if you're interested in the differences between these classes - TransactionTestCase from Nautobot is a small wrapper around Django's TransactionTestCase ). When using TransactionTestCase (whether from Django or from Nautobot) each tests runs on a completely empty database. Furthermore, Nautobot requires new jobs to be enabled before they can run. Therefore, we need to make sure the job is enabled before each run which run_job_for_testing handles for us. A simple example of a Job test case for 1.3.3 and forward might look like the following: from nautobot.extras.models import Job , JobLogEntry from nautobot.utilities.testing import run_job_for_testing , TransactionTestCase class MyJobTestCase ( TransactionTestCase ): def test_my_job ( self ): # Testing of Job \"MyJob\" in file \"my_job_file.py\" in $JOBS_ROOT job = Job . objects . get ( job_class_name = \"MyJob\" , module_name = \"my_job_file\" , source = \"local\" ) # or, job = Job.objects.get_for_class_path(\"local/my_job_file/MyJob\") job_result = run_job_for_testing ( job , data = {}, commit = False ) # Since we ran with commit=False, any database changes made by the job won't persist, # but we can still inspect the logs created by running the job log_entries = JobLogEntry . objects . filter ( job_result = job_result ) for log_entry in log_entries : self . assertEqual ( log_entry . message , \"...\" ) Tip For more advanced examples (such as testing jobs executed with commit=True , for example) refer to the Nautobot source code, specifically nautobot/extras/tests/test_jobs.py .","title":"Nautobot 1.3.3 and later"},{"location":"additional-features/jobs.html#nautobot-132-and-earlier-including-12","text":"If your test case needs to be backwards-compatible with test execution against Nautobot 1.3.2 and/or earlier, you need to handle a couple more things manually: Set up the \"job_logs\" database correctly for testing: from django.conf import settings if \"job_logs\" in settings . DATABASES : settings . DATABASES [ \"job_logs\" ] = settings . DATABASES [ \"job_logs\" ] . copy () settings . DATABASES [ \"job_logs\" ][ \"TEST\" ] = { \"MIRROR\" : \"default\" } Replicate the behavior of run_job_for_testing manually so that your test execution most closely resembles the way the celery worker would run the test: import uuid from django.contrib.auth import get_user_model from django.contrib.contenttypes.models import ContentType from nautobot.extras.context_managers import web_request_context from nautobot.extras.jobs import run_job from nautobot.extras.models import JobResult , Job def run_job_for_testing ( job , data = None , commit = True , username = \"test-user\" ): if data is None : data = {} user_model = get_user_model () user , _ = user_model . objects . get_or_create ( username = username , is_superuser = True , password = \"password\" ) job_result = JobResult . objects . create ( name = job . class_path , obj_type = ContentType . objects . get_for_model ( Job ), user = user , job_id = uuid . uuid4 (), ) with web_request_context ( user = user ) as request : run_job ( data = data , request = request , commit = commit , job_result_pk = job_result . pk ) return job_result Setup the databases field on the test class correctly, and re-create the default Statuses on setUp in your test classes, because django.test.TransactionTestCase truncates them on every tearDown : from django.apps import apps from django.conf import settings from django.test import TransactionTestCase from nautobot.extras.management import populate_status_choices class MyJobTestCase ( TransactionTestCase ): # 'job_logs' is a proxy connection to the same (default) database that's used exclusively for Job logging if \"job_logs\" in settings . DATABASES : databases = ( \"default\" , \"job_logs\" ) def setUp ( self ): super () . setUp () populate_status_choices ( apps , None )","title":"Nautobot 1.3.2 and earlier (including 1.2)"},{"location":"additional-features/jobs.html#example-jobs","text":"","title":"Example Jobs"},{"location":"additional-features/jobs.html#creating-objects-for-a-planned-site","text":"This job prompts the user for three variables: The name of the new site The device model (a filtered list of defined device types) The number of access switches to create These variables are presented as a web form to be completed by the user. Once submitted, the job's run() method is called to create the appropriate objects, and it returns simple CSV output to the user summarizing the created objects. from django.utils.text import slugify from nautobot.dcim.models import Device , DeviceRole , DeviceType , Manufacturer , Site from nautobot.extras.models import Status from nautobot.extras.jobs import * class NewBranch ( Job ): class Meta : name = \"New Branch\" description = \"Provision a new branch site\" field_order = [ 'site_name' , 'switch_count' , 'switch_model' ] site_name = StringVar ( description = \"Name of the new site\" ) switch_count = IntegerVar ( description = \"Number of access switches to create\" ) manufacturer = ObjectVar ( model = Manufacturer , required = False ) switch_model = ObjectVar ( description = \"Access switch model\" , model = DeviceType , query_params = { 'manufacturer_id' : '$manufacturer' } ) def run ( self , data , commit ): STATUS_PLANNED = Status . objects . get ( slug = 'planned' ) # Create the new site site = Site ( name = data [ 'site_name' ], slug = slugify ( data [ 'site_name' ]), status = STATUS_PLANNED , ) site . validated_save () self . log_success ( obj = site , message = \"Created new site\" ) # Create access switches switch_role = DeviceRole . objects . get ( name = 'Access Switch' ) for i in range ( 1 , data [ 'switch_count' ] + 1 ): switch = Device ( device_type = data [ 'switch_model' ], name = f ' { site . slug } -switch { i } ' , site = site , status = STATUS_PLANNED , device_role = switch_role ) switch . validated_save () self . log_success ( obj = switch , message = \"Created new switch\" ) # Generate a CSV table of new devices output = [ 'name,make,model' ] for switch in Device . objects . filter ( site = site ): attrs = [ switch . name , switch . device_type . manufacturer . name , switch . device_type . model ] output . append ( ',' . join ( attrs )) return ' \\n ' . join ( output )","title":"Creating objects for a planned site"},{"location":"additional-features/jobs.html#device-validation","text":"A job to perform various validation of Device data in Nautobot. As this job does not require any user input, it does not define any variables, nor does it implement a run() method. from nautobot.dcim.models import ConsolePort , Device , PowerPort from nautobot.extras.models import Status from nautobot.extras.jobs import Job class DeviceConnectionsReport ( Job ): description = \"Validate the minimum physical connections for each device\" def test_console_connection ( self ): STATUS_ACTIVE = Status . objects . get ( slug = 'active' ) # Check that every console port for every active device has a connection defined. for console_port in ConsolePort . objects . prefetch_related ( 'device' ) . filter ( device__status = STATUS_ACTIVE ): if console_port . connected_endpoint is None : self . log_failure ( obj = console_port . device , message = \"No console connection defined for {} \" . format ( console_port . name ) ) elif not console_port . connection_status : self . log_warning ( obj = console_port . device , message = \"Console connection for {} marked as planned\" . format ( console_port . name ) ) else : self . log_success ( obj = console_port . device ) def test_power_connections ( self ): STATUS_ACTIVE = Status . objects . get ( slug = 'active' ) # Check that every active device has at least two connected power supplies. for device in Device . objects . filter ( status = STATUS_ACTIVE ): connected_ports = 0 for power_port in PowerPort . objects . filter ( device = device ): if power_port . connected_endpoint is not None : connected_ports += 1 if not power_port . connection_status : self . log_warning ( obj = device , message = \"Power connection for {} marked as planned\" . format ( power_port . name ) ) if connected_ports < 2 : self . log_failure ( obj = device , message = \" {} connected power supplies found (2 needed)\" . format ( connected_ports ) ) else : self . log_success ( obj = device )","title":"Device validation"},{"location":"additional-features/napalm.html","text":"NAPALM \u00b6 Nautobot supports integration with the NAPALM automation library. NAPALM allows Nautobot to serve a proxy for operational data, fetching live data from network devices and returning it to a requester via its REST API. Note that Nautobot does not store any NAPALM data locally. Note To enable this integration, the NAPALM library must be installed. See installation steps for more information. Below is an example REST API request and response: GET /api/dcim/devices/1/napalm/?method=get_environment { \"get_environment\": { ... } } Note To make NAPALM requests via the Nautobot REST API, a Nautobot user must have assigned a permission granting the napalm_read action for the device object type. Authentication \u00b6 As of Nautobot 1.2, there are three ways to specify the authentication credentials to use for a given device: NAPALM_USERNAME and NAPALM_PASSWORD configuration parameters, setting global defaults to use for all devices. Assigning an appropriately defined secrets group to the device to specify its specific credentials. In a REST API call, specifying the credentials as HTTP headers. Configuration Parameters \u00b6 By default, the NAPALM_USERNAME and NAPALM_PASSWORD configuration parameters are used for NAPALM authentication. Secrets Groups \u00b6 If a given device has an associated secrets group, and that secrets group contains secrets assigned as access type Generic and secrets types Username and Password (and optionally an additional Secret entry as well, which will be used for a Cisco enable secret as needed), these credentials will be used for NAPALM authentication, overriding any global defaults specified in nautobot_config.py . Note that in the case where many devices in your network share common credentials (such as a standardized service account), it's straightforward to define an appropriate secrets group and then use the device \"bulk editing\" functionality in Nautobot to quickly assign this group to a collection of devices. REST API HTTP Headers \u00b6 The NAPALM credentials specified by either of the above methods can be overridden for an individual REST API call by specifying the X-NAPALM-Username and X-NAPALM-Password headers. $ curl \"http://localhost/api/dcim/devices/1/napalm/?method=get_environment\" \\ -H \"Authorization: Token $TOKEN \" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; indent=4\" \\ -H \"X-NAPALM-Username: foo\" \\ -H \"X-NAPALM-Password: bar\" Method Support \u00b6 The list of supported NAPALM methods depends on the NAPALM driver configured for the platform of a device. Because there is no granular mechanism in place for limiting potentially disruptive requests, Nautobot supports only read-only get methods. Multiple Methods \u00b6 It is possible to request the output of multiple NAPALM methods in a single API request by passing multiple method parameters. For example: GET /api/dcim/devices/1/napalm/?method=get_ntp_servers&method=get_ntp_peers { \"get_ntp_servers\": { ... }, \"get_ntp_peers\": { ... } } Optional Arguments \u00b6 The behavior of NAPALM drivers can be adjusted according to the optional arguments . Nautobot exposes those arguments using headers prefixed with X-NAPALM- . For example, the SSH port is changed to 2222 in this API call: $ curl \"http://localhost/api/dcim/devices/1/napalm/?method=get_environment\" \\ -H \"Authorization: Token $TOKEN \" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; indent=4\" \\ -H \"X-NAPALM-port: 2222\"","title":"NAPALM"},{"location":"additional-features/napalm.html#napalm","text":"Nautobot supports integration with the NAPALM automation library. NAPALM allows Nautobot to serve a proxy for operational data, fetching live data from network devices and returning it to a requester via its REST API. Note that Nautobot does not store any NAPALM data locally. Note To enable this integration, the NAPALM library must be installed. See installation steps for more information. Below is an example REST API request and response: GET /api/dcim/devices/1/napalm/?method=get_environment { \"get_environment\": { ... } } Note To make NAPALM requests via the Nautobot REST API, a Nautobot user must have assigned a permission granting the napalm_read action for the device object type.","title":"NAPALM"},{"location":"additional-features/napalm.html#authentication","text":"As of Nautobot 1.2, there are three ways to specify the authentication credentials to use for a given device: NAPALM_USERNAME and NAPALM_PASSWORD configuration parameters, setting global defaults to use for all devices. Assigning an appropriately defined secrets group to the device to specify its specific credentials. In a REST API call, specifying the credentials as HTTP headers.","title":"Authentication"},{"location":"additional-features/napalm.html#configuration-parameters","text":"By default, the NAPALM_USERNAME and NAPALM_PASSWORD configuration parameters are used for NAPALM authentication.","title":"Configuration Parameters"},{"location":"additional-features/napalm.html#secrets-groups","text":"If a given device has an associated secrets group, and that secrets group contains secrets assigned as access type Generic and secrets types Username and Password (and optionally an additional Secret entry as well, which will be used for a Cisco enable secret as needed), these credentials will be used for NAPALM authentication, overriding any global defaults specified in nautobot_config.py . Note that in the case where many devices in your network share common credentials (such as a standardized service account), it's straightforward to define an appropriate secrets group and then use the device \"bulk editing\" functionality in Nautobot to quickly assign this group to a collection of devices.","title":"Secrets Groups"},{"location":"additional-features/napalm.html#rest-api-http-headers","text":"The NAPALM credentials specified by either of the above methods can be overridden for an individual REST API call by specifying the X-NAPALM-Username and X-NAPALM-Password headers. $ curl \"http://localhost/api/dcim/devices/1/napalm/?method=get_environment\" \\ -H \"Authorization: Token $TOKEN \" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; indent=4\" \\ -H \"X-NAPALM-Username: foo\" \\ -H \"X-NAPALM-Password: bar\"","title":"REST API HTTP Headers"},{"location":"additional-features/napalm.html#method-support","text":"The list of supported NAPALM methods depends on the NAPALM driver configured for the platform of a device. Because there is no granular mechanism in place for limiting potentially disruptive requests, Nautobot supports only read-only get methods.","title":"Method Support"},{"location":"additional-features/napalm.html#multiple-methods","text":"It is possible to request the output of multiple NAPALM methods in a single API request by passing multiple method parameters. For example: GET /api/dcim/devices/1/napalm/?method=get_ntp_servers&method=get_ntp_peers { \"get_ntp_servers\": { ... }, \"get_ntp_peers\": { ... } }","title":"Multiple Methods"},{"location":"additional-features/napalm.html#optional-arguments","text":"The behavior of NAPALM drivers can be adjusted according to the optional arguments . Nautobot exposes those arguments using headers prefixed with X-NAPALM- . For example, the SSH port is changed to 2222 in this API call: $ curl \"http://localhost/api/dcim/devices/1/napalm/?method=get_environment\" \\ -H \"Authorization: Token $TOKEN \" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; indent=4\" \\ -H \"X-NAPALM-port: 2222\"","title":"Optional Arguments"},{"location":"additional-features/prometheus-metrics.html","text":"Prometheus Metrics \u00b6 Nautobot supports optionally exposing native Prometheus metrics from the application. Prometheus is a popular time series metric platform used for monitoring. Configuration \u00b6 Metrics are not exposed by default. Metric exposition can be toggled with the METRICS_ENABLED configuration setting which exposes metrics at the /metrics HTTP endpoint, e.g. https://nautobot.local/metrics . In addition to the METRICS_ENABLED setting, database and/or caching metrics can also be enabled by changing the database engine and/or caching backends from django.db.backends / django_redis.cache to django_prometheus.db.backends / django_prometheus.cache.backends.redis : DATABASES = { \"default\" : { # Other settings... \"ENGINE\" : \"django_prometheus.db.backends.postgresql\" , # use \"django_prometheus.db.backends.mysql\" with MySQL } } # Other settings... CACHES = { \"default\" : { # Other settings... \"BACKEND\" : \"django_prometheus.cache.backends.redis.RedisCache\" , } } For more information see the django-prometheus docs. Metric Types \u00b6 Nautobot makes use of the django-prometheus library to export a number of different types of metrics, including: Per model insert, update, and delete counters Per view request counters Per view request latency histograms Request body size histograms Response body size histograms Response code counters Database connection, execution, and error counters Cache hit, miss, and invalidation counters Django middleware latency histograms Other Django related metadata metrics For the exhaustive list of exposed metrics, visit the /metrics endpoint on your Nautobot instance. Multi Processing Notes \u00b6 When deploying Nautobot in a multi-process manner (e.g. running multiple uWSGI workers) the Prometheus client library requires the use of a shared directory to collect metrics from all worker processes. To configure this, first create or designate a local directory to which the worker processes have read and write access, and then configure your WSGI service (e.g. uWSGI) to define this path as the prometheus_multiproc_dir environment variable. Warning If having accurate long-term metrics in a multi-process environment is crucial to your deployment, it's recommended you use the uwsgi library instead of gunicorn . The issue lies in the way gunicorn tracks worker processes (vs uwsgi ) which helps manage the metrics files created by the above configurations. If you're using Nautobot with gunicorn in a containerized environment following the one-process-per-container methodology, then you will likely not need to change to uwsgi . More details can be found in issue #3779 . Note Metrics from the celery worker are not available from Nautobot at this time. However, additional tools such as flower can be used to monitor the celery workers until these metrics are exposed through Nautobot.","title":"Prometheus Metrics"},{"location":"additional-features/prometheus-metrics.html#prometheus-metrics","text":"Nautobot supports optionally exposing native Prometheus metrics from the application. Prometheus is a popular time series metric platform used for monitoring.","title":"Prometheus Metrics"},{"location":"additional-features/prometheus-metrics.html#configuration","text":"Metrics are not exposed by default. Metric exposition can be toggled with the METRICS_ENABLED configuration setting which exposes metrics at the /metrics HTTP endpoint, e.g. https://nautobot.local/metrics . In addition to the METRICS_ENABLED setting, database and/or caching metrics can also be enabled by changing the database engine and/or caching backends from django.db.backends / django_redis.cache to django_prometheus.db.backends / django_prometheus.cache.backends.redis : DATABASES = { \"default\" : { # Other settings... \"ENGINE\" : \"django_prometheus.db.backends.postgresql\" , # use \"django_prometheus.db.backends.mysql\" with MySQL } } # Other settings... CACHES = { \"default\" : { # Other settings... \"BACKEND\" : \"django_prometheus.cache.backends.redis.RedisCache\" , } } For more information see the django-prometheus docs.","title":"Configuration"},{"location":"additional-features/prometheus-metrics.html#metric-types","text":"Nautobot makes use of the django-prometheus library to export a number of different types of metrics, including: Per model insert, update, and delete counters Per view request counters Per view request latency histograms Request body size histograms Response body size histograms Response code counters Database connection, execution, and error counters Cache hit, miss, and invalidation counters Django middleware latency histograms Other Django related metadata metrics For the exhaustive list of exposed metrics, visit the /metrics endpoint on your Nautobot instance.","title":"Metric Types"},{"location":"additional-features/prometheus-metrics.html#multi-processing-notes","text":"When deploying Nautobot in a multi-process manner (e.g. running multiple uWSGI workers) the Prometheus client library requires the use of a shared directory to collect metrics from all worker processes. To configure this, first create or designate a local directory to which the worker processes have read and write access, and then configure your WSGI service (e.g. uWSGI) to define this path as the prometheus_multiproc_dir environment variable. Warning If having accurate long-term metrics in a multi-process environment is crucial to your deployment, it's recommended you use the uwsgi library instead of gunicorn . The issue lies in the way gunicorn tracks worker processes (vs uwsgi ) which helps manage the metrics files created by the above configurations. If you're using Nautobot with gunicorn in a containerized environment following the one-process-per-container methodology, then you will likely not need to change to uwsgi . More details can be found in issue #3779 . Note Metrics from the celery worker are not available from Nautobot at this time. However, additional tools such as flower can be used to monitor the celery workers until these metrics are exposed through Nautobot.","title":"Multi Processing Notes"},{"location":"additional-features/template-filters.html","text":"Additional Template Filters \u00b6 Introduction \u00b6 Nautobot uses 2 template engines internally, Django Template and Jinja2. Django Template is used to render the UI pages and Jinja2 is used for features such as computed fields, custom links, export templates, etc. Note Jinja2 and Django Template are very similar, the main difference between them is the syntax of the template. Historically, Django Template has been the go-to solution to generate webpage in Django and Jinja2 is the industry standard outside of Django. Both Django Template and Jinja2 can be extended with a library of functions, called filters , that apply formatting or transformations to a provided input. Nautobot provides many built-in filters , including network specific filters from the netutils library . Netutils Filters \u00b6 Added in version 1.2.0 Netutils is an external library, maintained by Network to Code, that is focusing on providing a collection of functions for common network automation tasks. Please check the netutils documentation to see the list of available functions. All functions in Netutils are available in Nautobot in both Jinja2 filters and Django Template. Nautobot Built-In Filters \u00b6 The Nautobot project also provides the following built-in filters that can be used in both Jinja2 and Django Template. as_range \u00b6 Given a list of n items, return a corresponding range of n integers. # Django template {% for i in record.parents | as_range %} <i class=\"mdi mdi-circle-small\"></i> {% endfor %} bettertitle \u00b6 Alternative to the built-in title filter; capitalizes words without replacing letters that are already uppercase. For example, title(\"IP address\") == \"Ip Address\" , while bettertitle(\"IP address\") == \"IP Address\" . {{ obj_type.name | bettertitle }} divide \u00b6 Return x/y (rounded). # Django Template {{ powerfeed.available_power | divide : 3 }} VA # Jinja {{ powerfeed.available_power | divide ( 3 ) }} fgcolor \u00b6 Return the ideal foreground color (black \"#000000\" or white \"#ffffff\" ) given an arbitrary background color in RRGGBB format. color: {{ object.status.color | fgcolor }} get_docs_url \u00b6 Return the static URL of the documentation for the specified model. {{ obj | get_docs_url }} get_item \u00b6 Access a specific item/key in a dictionary. # Django Template {{ labels | get_item : key }} # Jinja {{ labels | get_item ( key ) }} has_one_or_more_perms \u00b6 Return True if the user has at least one of the permissions in the list. # Django Template {{ request.user | has_one_or_more_perms : panel_details.permissions }} # Jinja {{ request.user | has_one_or_more_perms ( panel_details.permissions ) }} has_perms \u00b6 Return True if the user has all permissions in the list. # Django Template {{ request.user | has_perms : group_item_details.permissions }} # Jinja {{ request.user | has_perms ( group_item_details.permissions ) }} humanize_speed \u00b6 Humanize speeds given in Kbps. 1544 => \"1.544 Mbps\" 100000 => \"100 Mbps\" 10000000 => \"10 Gbps\" {{ speed_value | humanize_speed }} hyperlinked_object \u00b6 Added in version 1.4.0 Render and link to a Django model instance, if any, or render a placeholder if not. Uses object.display if available, otherwise uses the string representation of the object. If the object defines get_absolute_url() this will be used to hyperlink the displayed object; additionally if there is an object.description this will be used as the title of the hyperlink. {{ device | hyperlinked_object }} meta \u00b6 Return the specified Meta attribute of a model. {{ obj | meta ( 'app_label' ) }} meters_to_feet \u00b6 Convert a length from meters to feet. {{ meter_value | meters_to_feet }} percentage \u00b6 Return x/y as a percentage. # Django Template {{ powerfeed.available_power | percentage : total_power }} VA # Jinja {{ powerfeed.available_power | percentage ( total_power ) }} placeholder \u00b6 Render a muted placeholder ( <span class=\"text-muted\">&mdash;</span> ) if value is falsey, else render the provided value. {{ html | placeholder }} render_boolean \u00b6 Render HTML from a computed boolean value. If value is (for example) a non-empty string or True or a non-zero number, this renders <span class=\"text-success\"><i class=\"mdi mdi-check-bold\" title=\"Yes\"></i></span> If value is (for example) \"\" or 0 or False, this renders <span class=\"text-danger\"><i class=\"mdi mdi-close-thick\" title=\"No\"></i></span> If value is None this renders <span class=\"text-muted\">&mdash;</span> {{ value | render_boolean }} render_json \u00b6 Render a dictionary as formatted JSON. {{ data | render_json }} render_markdown \u00b6 Render text as Markdown. {{ text | render_markdown }} render_yaml \u00b6 Render a dictionary as formatted YAML. {{ data | render_yaml }} settings_or_config \u00b6 Get a value from Django settings (if specified) or Constance configuration (otherwise). {{ \"RELEASE_CHECK_URL\" | settings_or_config }} split \u00b6 Split a string by the given value (default: comma) # Django Template {{ string | split }} {{ string | split :';' }} # Jinja {{ string | split }} {{ string | split ( ';' ) }} tzoffset \u00b6 Returns the hour offset of a given time zone using the current time. {{ object.time_zone | tzoffset }} validated_viewname \u00b6 Return the view name for the given model and action if valid, or None if invalid. # Django Template {{ obj | validated_viewname :'list' }} # Jinja {{ obj | validated_viewname ( 'list' ) }} viewname \u00b6 Return the view name for the given model and action. Does not perform any validation. # Django Template {{ obj | viewname :'list' }} # Jinja {{ obj | viewname ( 'list' ) }}","title":"Template Filters"},{"location":"additional-features/template-filters.html#additional-template-filters","text":"","title":"Additional Template Filters"},{"location":"additional-features/template-filters.html#introduction","text":"Nautobot uses 2 template engines internally, Django Template and Jinja2. Django Template is used to render the UI pages and Jinja2 is used for features such as computed fields, custom links, export templates, etc. Note Jinja2 and Django Template are very similar, the main difference between them is the syntax of the template. Historically, Django Template has been the go-to solution to generate webpage in Django and Jinja2 is the industry standard outside of Django. Both Django Template and Jinja2 can be extended with a library of functions, called filters , that apply formatting or transformations to a provided input. Nautobot provides many built-in filters , including network specific filters from the netutils library .","title":"Introduction"},{"location":"additional-features/template-filters.html#netutils-filters","text":"Added in version 1.2.0 Netutils is an external library, maintained by Network to Code, that is focusing on providing a collection of functions for common network automation tasks. Please check the netutils documentation to see the list of available functions. All functions in Netutils are available in Nautobot in both Jinja2 filters and Django Template.","title":"Netutils Filters"},{"location":"additional-features/template-filters.html#nautobot-built-in-filters","text":"The Nautobot project also provides the following built-in filters that can be used in both Jinja2 and Django Template.","title":"Nautobot Built-In Filters"},{"location":"additional-features/template-filters.html#as_range","text":"Given a list of n items, return a corresponding range of n integers. # Django template {% for i in record.parents | as_range %} <i class=\"mdi mdi-circle-small\"></i> {% endfor %}","title":"as_range"},{"location":"additional-features/template-filters.html#bettertitle","text":"Alternative to the built-in title filter; capitalizes words without replacing letters that are already uppercase. For example, title(\"IP address\") == \"Ip Address\" , while bettertitle(\"IP address\") == \"IP Address\" . {{ obj_type.name | bettertitle }}","title":"bettertitle"},{"location":"additional-features/template-filters.html#divide","text":"Return x/y (rounded). # Django Template {{ powerfeed.available_power | divide : 3 }} VA # Jinja {{ powerfeed.available_power | divide ( 3 ) }}","title":"divide"},{"location":"additional-features/template-filters.html#fgcolor","text":"Return the ideal foreground color (black \"#000000\" or white \"#ffffff\" ) given an arbitrary background color in RRGGBB format. color: {{ object.status.color | fgcolor }}","title":"fgcolor"},{"location":"additional-features/template-filters.html#get_docs_url","text":"Return the static URL of the documentation for the specified model. {{ obj | get_docs_url }}","title":"get_docs_url"},{"location":"additional-features/template-filters.html#get_item","text":"Access a specific item/key in a dictionary. # Django Template {{ labels | get_item : key }} # Jinja {{ labels | get_item ( key ) }}","title":"get_item"},{"location":"additional-features/template-filters.html#has_one_or_more_perms","text":"Return True if the user has at least one of the permissions in the list. # Django Template {{ request.user | has_one_or_more_perms : panel_details.permissions }} # Jinja {{ request.user | has_one_or_more_perms ( panel_details.permissions ) }}","title":"has_one_or_more_perms"},{"location":"additional-features/template-filters.html#has_perms","text":"Return True if the user has all permissions in the list. # Django Template {{ request.user | has_perms : group_item_details.permissions }} # Jinja {{ request.user | has_perms ( group_item_details.permissions ) }}","title":"has_perms"},{"location":"additional-features/template-filters.html#humanize_speed","text":"Humanize speeds given in Kbps. 1544 => \"1.544 Mbps\" 100000 => \"100 Mbps\" 10000000 => \"10 Gbps\" {{ speed_value | humanize_speed }}","title":"humanize_speed"},{"location":"additional-features/template-filters.html#hyperlinked_object","text":"Added in version 1.4.0 Render and link to a Django model instance, if any, or render a placeholder if not. Uses object.display if available, otherwise uses the string representation of the object. If the object defines get_absolute_url() this will be used to hyperlink the displayed object; additionally if there is an object.description this will be used as the title of the hyperlink. {{ device | hyperlinked_object }}","title":"hyperlinked_object"},{"location":"additional-features/template-filters.html#meta","text":"Return the specified Meta attribute of a model. {{ obj | meta ( 'app_label' ) }}","title":"meta"},{"location":"additional-features/template-filters.html#meters_to_feet","text":"Convert a length from meters to feet. {{ meter_value | meters_to_feet }}","title":"meters_to_feet"},{"location":"additional-features/template-filters.html#percentage","text":"Return x/y as a percentage. # Django Template {{ powerfeed.available_power | percentage : total_power }} VA # Jinja {{ powerfeed.available_power | percentage ( total_power ) }}","title":"percentage"},{"location":"additional-features/template-filters.html#placeholder","text":"Render a muted placeholder ( <span class=\"text-muted\">&mdash;</span> ) if value is falsey, else render the provided value. {{ html | placeholder }}","title":"placeholder"},{"location":"additional-features/template-filters.html#render_boolean","text":"Render HTML from a computed boolean value. If value is (for example) a non-empty string or True or a non-zero number, this renders <span class=\"text-success\"><i class=\"mdi mdi-check-bold\" title=\"Yes\"></i></span> If value is (for example) \"\" or 0 or False, this renders <span class=\"text-danger\"><i class=\"mdi mdi-close-thick\" title=\"No\"></i></span> If value is None this renders <span class=\"text-muted\">&mdash;</span> {{ value | render_boolean }}","title":"render_boolean"},{"location":"additional-features/template-filters.html#render_json","text":"Render a dictionary as formatted JSON. {{ data | render_json }}","title":"render_json"},{"location":"additional-features/template-filters.html#render_markdown","text":"Render text as Markdown. {{ text | render_markdown }}","title":"render_markdown"},{"location":"additional-features/template-filters.html#render_yaml","text":"Render a dictionary as formatted YAML. {{ data | render_yaml }}","title":"render_yaml"},{"location":"additional-features/template-filters.html#settings_or_config","text":"Get a value from Django settings (if specified) or Constance configuration (otherwise). {{ \"RELEASE_CHECK_URL\" | settings_or_config }}","title":"settings_or_config"},{"location":"additional-features/template-filters.html#split","text":"Split a string by the given value (default: comma) # Django Template {{ string | split }} {{ string | split :';' }} # Jinja {{ string | split }} {{ string | split ( ';' ) }}","title":"split"},{"location":"additional-features/template-filters.html#tzoffset","text":"Returns the hour offset of a given time zone using the current time. {{ object.time_zone | tzoffset }}","title":"tzoffset"},{"location":"additional-features/template-filters.html#validated_viewname","text":"Return the view name for the given model and action if valid, or None if invalid. # Django Template {{ obj | validated_viewname :'list' }} # Jinja {{ obj | validated_viewname ( 'list' ) }}","title":"validated_viewname"},{"location":"additional-features/template-filters.html#viewname","text":"Return the view name for the given model and action. Does not perform any validation. # Django Template {{ obj | viewname :'list' }} # Jinja {{ obj | viewname ( 'list' ) }}","title":"viewname"},{"location":"administration/nautobot-server.html","text":"The Nautobot Server Command \u00b6 Nautobot includes a command-line (CLI) management utility called nautobot-server , that is used as a single entrypoint for common administrative tasks. Background \u00b6 For those familiar with Django applications, this CLI utility works exactly as a project's manage.py script would, except that it comes bundled with the Nautobot code and therefore it gets automatically installed in the bin directory of your application environment. Important Since Nautobot is a Django application, there are a number of built-in management commands that will not be covered in this document. Please see the official Django documentation on management commands for more information. Important Django does not recognize nautobot-server . Anywhere python manage.py is mentioned, it is safe to replace with nautobot-server . Getting Help \u00b6 To see all available management commands: $ nautobot-server help All management commands have a -h/--help flag to list all available arguments for that command, for example: $ nautobot-server migrate --help Available Commands \u00b6 celery \u00b6 nautobot-server celery Celery command entrypoint which serves as a thin wrapper to the celery command that includes the Nautobot Celery application context. This allows us to execute Celery commands without having to worry about the chicken-and-egg problem with bootstrapping the Django settings. Most commonly you will be using this command to start the Celery worker process: $ nautobot-server celery worker --loglevel INFO --pidfile $(pwd)/nautobot-celery.pid -n worker1 celery@worker1 v5.1.1 (sun-harmonics) [config] .> app: nautobot:0x10c357eb0 .> transport: redis://localhost:6379/0 .> results: redis://localhost:6379/0 .> concurrency: 8 (prefork) .> task events: OFF (enable -E to monitor tasks in this worker) [queues] .> celery exchange=celery(direct) key=celery [tasks] . nautobot.extras.datasources.git.pull_git_repository_and_refresh_data . nautobot.extras.jobs.run_job . nautobot.extras.tasks.delete_custom_field_data . nautobot.extras.tasks.process_webhook . nautobot.extras.tasks.provision_field . nautobot.extras.tasks.update_custom_field_choice_data . nautobot.utilities.tasks.get_releases [2021-07-01 21:32:40,680: INFO/MainProcess] Connected to redis://localhost:6379/0 [2021-07-01 21:32:40,690: INFO/MainProcess] mingle: searching for neighbors [2021-07-01 21:32:41,713: INFO/MainProcess] mingle: all alone [2021-07-01 21:32:41,730: INFO/MainProcess] celery@worker1 ready. Note The internals of this command are built into Celery. Please see the official Celery workers guide for more information. collectstatic \u00b6 nautobot-server collectstatic Collect static files into STATIC_ROOT . $ nautobot-server collectstatic 965 static files copied to '/opt/nautobot/static'. Note This is a built-in Django command. Please see the official documentation on collectstatic for more information. createsuperuser \u00b6 nautobot-server createsuperuser Creates a superuser account that has all permissions. $ nautobot-server createsuperuser Username (leave blank to use 'jathan'): example Email address: example@localhost Password: Password (again): Superuser created successfully. Note This is a built-in Django command. Please see the official documentation on createsuperuser for more information. dbshell \u00b6 nautobot-server dbshell A shell for your database. This can be very useful in troubleshooting raw database issues. Danger This is an advanced feature that gives you direct access to run raw SQL queries. Use this very cautiously as you can cause irreparable damage to your Nautobot installation. $ nautobot-server dbshell psql (12.6 (Ubuntu 12.6-0ubuntu0.20.04.1)) SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off) Type \"help\" for help. nautobot=> \\conninfo You are connected to database \"nautobot\" as user \"nautobot\" on host \"localhost\" (address \"::1\") at port \"5432\". SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off) nautobot=> \\q Note This is a built-in Django command. Please see the official documentation on dbshell for more information. dumpdata \u00b6 Changed in version 1.3.0 extras.job should now be included in the dump (removed --exclude extras.job from the example usage) django_rq should now be excluded from the dump (added --exclude django_rq to the example usage) $ nautobot-server dumpdata \\ --natural-foreign \\ --natural-primary \\ --exclude contenttypes \\ --exclude auth.permission \\ --exclude django_rq \\ --format json \\ --indent 2 \\ --traceback > nautobot_dump.json Use this command to generate a JSON dump of the database contents. One example of using this command would be to export data from PostgreSQL . fix_custom_fields \u00b6 nautobot-server fix_custom_fields Adds/Removes any custom fields which should or should not exist on an object. This command should not be run unless a custom fields jobs has failed: $ nautobot-server fix_custom_fields Processing ContentType dcim | device Processing ContentType dcim | site Processing ContentType dcim | rack Processing ContentType dcim | cable Processing ContentType dcim | power feed Processing ContentType circuits | circuit Processing ContentType ipam | prefix ... (truncated for brevity of documentation) ... generate_secret_key \u00b6 nautobot-server generate_secret_key Generates a new SECRET_KEY that can be used in your nautobot_config.py : $ nautobot-server generate_secret_key e!j=vrlhz-!wl8p_3+q5s5cph29nzj$xm81eap-!&n!_9^du09 health_check \u00b6 nautobot-server health_check Run health checks and exit 0 if everything went well. $ nautobot-server health_check DatabaseBackend ... working DefaultFileStorageHealthCheck ... working RedisBackend ... working Please see the healthcheck documentation for more information. init \u00b6 nautobot-server init [config_path] Generates a new configuration with all of the default settings provided for you, and will also generate a unique SECRET_KEY . By default the file will be created at $HOME/.nautobot/nautobot_config.py : $ nautobot-server init Configuration file created at '/home/example/.nautobot/nautobot_config.py For more information on configuring Nautobot for the first time or on more advanced configuration patterns, please see the guide on Nautobot Configuration . invalidate \u00b6 nautobot-server invalidate Invalidates cache for entire app, model or particular instance. Most commonly you will see us recommend clearing the entire cache using invalidate all : $ nautobot-server invalidate all There are a number of other options not covered here. Note This is a built-in management command provided by the Cacheops plugin Nautobot for caching. Please see the official Cacheops documentation on invalidate for more information. loaddata \u00b6 nautobot-server loaddata --traceback nautobot_dump.json To import the data that was exported with nautobot-server dumpdata ... see the following documentation: Remove the auto-populated Status records from the database Import the database dump migrate \u00b6 nautobot-server migrate [app_label] [migration_name] Initialize a new database or run any pending database migrations to an existing database. $ nautobot-server migrate Wrapping model clean methods for custom validators failed because the ContentType table was not available or populated. This is normal during the execution of the migration command for the first time. Operations to perform: Apply all migrations: admin, auth, circuits, contenttypes, dcim, extras, ipam, sessions, taggit, tenancy, users, virtualization Running migrations: Applying contenttypes.0001_initial... OK Applying auth.0001_initial... OK Applying admin.0001_initial... OK ... (truncated for brevity of documentation) ... Note This is a built-in Django command. Please see the official documentation on migrate for more information. nbshell \u00b6 nautobot-server nbshell An interactive Python shell with all of the database models and various other utilities already imported for you. This is immensely useful for direct access to manipulating database objects. Danger This is an advanced feature that gives you direct access to the Django database models. Use this very cautiously as you can cause irreparable damage to your Nautobot installation. $ nautobot-server nbshell ### Nautobot interactive shell (localhost) ### Python 3.8.7 | Django 3.1.7 | Nautobot 1.0.0 ### lsmodels() will show available models. Use help(<model>) for more info. >>> Please see the dedicated guide on the Nautobot Shell for more information. post_upgrade \u00b6 nautobot-server post_upgrade Performs common server post-upgrade operations using a single entrypoint. This will run the following management commands with default settings, in order: migrate trace_paths collectstatic remove_stale_contenttypes clearsessions invalidate all Note Commands listed here that are not covered in this document here are Django built-in commands. --no-clearsessions Do not automatically clean out expired sessions. --no-collectstatic Do not automatically collect static files into a single location. --no-invalidate-all Do not automatically invalidate cache for entire application. --no-migrate Do not automatically perform any database migrations. --no-remove-stale-contenttypes Do not automatically remove stale content types. --no-trace-paths Do not automatically generate missing cable paths. $ nautobot-server post_upgrade Performing database migrations... Operations to perform: Apply all migrations: admin, auth, circuits, contenttypes, dcim, extras, ipam, sessions, taggit, tenancy, users, virtualization Running migrations: No migrations to apply. Generating cable paths... Found no missing circuit termination paths; skipping Found no missing console port paths; skipping Found no missing console server port paths; skipping Found no missing interface paths; skipping Found no missing power feed paths; skipping Found no missing power outlet paths; skipping Found no missing power port paths; skipping Finished. Collecting static files... 0 static files copied to '/opt/nautobot/static', 965 unmodified. Removing stale content types... Removing expired sessions... Invalidating cache... remove_stale_scheduled_jobs \u00b6 Added in version 1.3.10 nautobot-server remove_stale_scheduled_jobs [max-age of days] Delete non-recurring scheduled jobs that were scheduled to run more than max-age days ago. renaturalize \u00b6 nautobot-server renaturalize [app_label.ModelName [app_label.ModelName ...]] Recalculate natural ordering values for the specified models. This defaults to recalculating natural ordering on all models which have one or more fields of type NaturalOrderingField : $ nautobot-server renaturalize Renaturalizing 21 models. dcim.ConsolePort.name (_name)... 196 dcim.ConsoleServerPort.name (_name)... 0 dcim.PowerPort.name (_name)... 392 dcim.PowerOutlet.name (_name)... 0 dcim.Interface.name (_name)... 7161 dcim.FrontPort.name (_name)... 0 dcim.RearPort.name (_name)... 0 dcim.DeviceBay.name (_name)... 0 dcim.InventoryItem.name (_name)... 1 dcim.Device.name (_name)... 208 dcim.ConsolePortTemplate.name (_name)... 2 dcim.ConsoleServerPortTemplate.name (_name)... 0 dcim.PowerPortTemplate.name (_name)... 4 dcim.PowerOutletTemplate.name (_name)... 0 dcim.InterfaceTemplate.name (_name)... 221 dcim.FrontPortTemplate.name (_name)... 0 dcim.RearPortTemplate.name (_name)... 0 dcim.DeviceBayTemplate.name (_name)... 0 dcim.Rack.name (_name)... 156 dcim.Site.name (_name)... 22 virtualization.VMInterface.name (_name)... 0 Done. You may optionally specify or more specific models (each prefixed with its app_label) to renaturalize: $ nautobot-server renaturalize dcim.Device Renaturalizing 1 models. dcim.Device.name (_name)... 208 Done. runjob \u00b6 nautobot-server runjob [job] Run a job (script, report) to validate or update data in Nautobot. --commit Commit changes to DB (defaults to dry-run if unset). --username is mandatory if using this argument. --username <username> User account to impersonate as the requester of this job. $ nautobot-server runjob --commit --username someuser local/example/MyJobWithNoVars Note that there is presently no option to provide input parameters ( data ) for jobs via the CLI. Please see the guide on Jobs for more information on working with and running jobs. start \u00b6 nautobot-server start Directly invoke uWSGI to start a Nautobot server suitable for production use. This command behaves exactly as uWSGI does, but allows us to maintain a single entrypoint into the Nautobot application. Note uWSGI offers an overwhelming amount of command-line arguments that could not possibly be covered here. Please see the official uWSGI Options guide for more information. $ nautobot-server start --ini ./uwsgi.ini [uWSGI] getting INI configuration from ./uwsgi.ini [uwsgi-static] added mapping for /static => /opt/nautobot/static *** Starting uWSGI 2.0.19.1 (64bit) on [Thu Mar 11 21:13:22 2021] *** compiled with version: 8.3.1 20190311 (Red Hat 8.3.1-3) on 23 September 2020 02:39:40 os: Linux-5.4.0-52-generic #57-Ubuntu SMP Thu Oct 15 10:57:00 UTC 2020 nodename: jathan-nautobot-testing machine: x86_64 clock source: unix pcre jit disabled detected number of CPU cores: 48 current working directory: /opt/nautobot detected binary path: /usr/bin/python3.8 your processes number limit is 256070 your memory page size is 4096 bytes detected max file descriptor number: 1024 building mime-types dictionary from file /etc/mime.types...567 entry found lock engine: pthread robust mutexes thunder lock: disabled (you can enable it with --thunder-lock) uwsgi socket 0 bound to TCP address 0.0.0.0:9191 fd 7 Python version: 3.8.5 (default, Jan 27 2021, 15:41:15) [GCC 9.3.0] --- Python VM already initialized --- Python main interpreter initialized at 0x2573e30 python threads support enabled your server socket listen backlog is limited to 1024 connections your mercy for graceful operations on workers is 60 seconds mapped 636432 bytes (621 KB) for 15 cores *** Operational MODE: preforking+threaded *** WSGI app 0 (mountpoint='') ready in 0 seconds on interpreter 0x2573e30 pid: 112153 (default app) spawned uWSGI master process (pid: 112153) spawned uWSGI worker 1 (pid: 112159, cores: 3) spawned uWSGI worker 2 (pid: 112162, cores: 3) spawned uWSGI worker 3 (pid: 112165, cores: 3) spawned uWSGI worker 4 (pid: 112168, cores: 3) spawned uWSGI worker 5 (pid: 112171, cores: 3) Please see the guide on Deploying Nautobot Services for our recommended configuration for use with uWSGI. startplugin \u00b6 nautobot-server startplugin <name> [directory] Create a new plugin with name . This command is similar to the django-admin startapp command, but with a default template directory ( --template ) of nautobot/core/templates/plugin_template . This command assists with creating a basic file structure for beginning development of a new Nautobot plugin. Without passing in the destination directory, nautobot-server startplugin will use your current directory and the name argument provided to create a new directory. We recommend providing a directory so that the plugin can be installed or published easily. Here is an example: $ mkdir -p ~/myplugin/myplugin $ nautobot-server startplugin myplugin ~/myplugin/myplugin Additional options such as --name or --extension can be found in the Django documentation . trace_paths \u00b6 nautobot-server trace_paths Generate any missing cable paths among all cable termination objects in Nautobot. After upgrading the database or working with Cables, Circuits, or other related objects, there may be a need to rebuild cached cable paths. --force Force recalculation of all existing cable paths. --no-input Do not prompt user for any input/confirmation. $ nautobot-server trace_paths Found no missing circuit termination paths; skipping Found no missing console port paths; skipping Found no missing console server port paths; skipping Found no missing interface paths; skipping Found no missing power feed paths; skipping Found no missing power outlet paths; skipping Found no missing power port paths; skipping Finished. Note This command is safe to run at any time. If it does detect any changes, it will exit cleanly. webhook_receiver \u00b6 nautobot-server webhook_receiver Start a simple listener to display received HTTP requests. --port PORT Optional port number (default: 9000 ) --no-headers Hide HTTP request headers. $ nautobot-server webhook_receiver --port 9001 --no-headers Listening on port http://localhost:9000. Stop with CONTROL-C. Please see the guide on Troubleshooting Webhooks for more information.","title":"Nautobot Server"},{"location":"administration/nautobot-server.html#the-nautobot-server-command","text":"Nautobot includes a command-line (CLI) management utility called nautobot-server , that is used as a single entrypoint for common administrative tasks.","title":"The Nautobot Server Command"},{"location":"administration/nautobot-server.html#background","text":"For those familiar with Django applications, this CLI utility works exactly as a project's manage.py script would, except that it comes bundled with the Nautobot code and therefore it gets automatically installed in the bin directory of your application environment. Important Since Nautobot is a Django application, there are a number of built-in management commands that will not be covered in this document. Please see the official Django documentation on management commands for more information. Important Django does not recognize nautobot-server . Anywhere python manage.py is mentioned, it is safe to replace with nautobot-server .","title":"Background"},{"location":"administration/nautobot-server.html#getting-help","text":"To see all available management commands: $ nautobot-server help All management commands have a -h/--help flag to list all available arguments for that command, for example: $ nautobot-server migrate --help","title":"Getting Help"},{"location":"administration/nautobot-server.html#available-commands","text":"","title":"Available Commands"},{"location":"administration/nautobot-server.html#celery","text":"nautobot-server celery Celery command entrypoint which serves as a thin wrapper to the celery command that includes the Nautobot Celery application context. This allows us to execute Celery commands without having to worry about the chicken-and-egg problem with bootstrapping the Django settings. Most commonly you will be using this command to start the Celery worker process: $ nautobot-server celery worker --loglevel INFO --pidfile $(pwd)/nautobot-celery.pid -n worker1 celery@worker1 v5.1.1 (sun-harmonics) [config] .> app: nautobot:0x10c357eb0 .> transport: redis://localhost:6379/0 .> results: redis://localhost:6379/0 .> concurrency: 8 (prefork) .> task events: OFF (enable -E to monitor tasks in this worker) [queues] .> celery exchange=celery(direct) key=celery [tasks] . nautobot.extras.datasources.git.pull_git_repository_and_refresh_data . nautobot.extras.jobs.run_job . nautobot.extras.tasks.delete_custom_field_data . nautobot.extras.tasks.process_webhook . nautobot.extras.tasks.provision_field . nautobot.extras.tasks.update_custom_field_choice_data . nautobot.utilities.tasks.get_releases [2021-07-01 21:32:40,680: INFO/MainProcess] Connected to redis://localhost:6379/0 [2021-07-01 21:32:40,690: INFO/MainProcess] mingle: searching for neighbors [2021-07-01 21:32:41,713: INFO/MainProcess] mingle: all alone [2021-07-01 21:32:41,730: INFO/MainProcess] celery@worker1 ready. Note The internals of this command are built into Celery. Please see the official Celery workers guide for more information.","title":"celery"},{"location":"administration/nautobot-server.html#collectstatic","text":"nautobot-server collectstatic Collect static files into STATIC_ROOT . $ nautobot-server collectstatic 965 static files copied to '/opt/nautobot/static'. Note This is a built-in Django command. Please see the official documentation on collectstatic for more information.","title":"collectstatic"},{"location":"administration/nautobot-server.html#createsuperuser","text":"nautobot-server createsuperuser Creates a superuser account that has all permissions. $ nautobot-server createsuperuser Username (leave blank to use 'jathan'): example Email address: example@localhost Password: Password (again): Superuser created successfully. Note This is a built-in Django command. Please see the official documentation on createsuperuser for more information.","title":"createsuperuser"},{"location":"administration/nautobot-server.html#dbshell","text":"nautobot-server dbshell A shell for your database. This can be very useful in troubleshooting raw database issues. Danger This is an advanced feature that gives you direct access to run raw SQL queries. Use this very cautiously as you can cause irreparable damage to your Nautobot installation. $ nautobot-server dbshell psql (12.6 (Ubuntu 12.6-0ubuntu0.20.04.1)) SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off) Type \"help\" for help. nautobot=> \\conninfo You are connected to database \"nautobot\" as user \"nautobot\" on host \"localhost\" (address \"::1\") at port \"5432\". SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off) nautobot=> \\q Note This is a built-in Django command. Please see the official documentation on dbshell for more information.","title":"dbshell"},{"location":"administration/nautobot-server.html#dumpdata","text":"Changed in version 1.3.0 extras.job should now be included in the dump (removed --exclude extras.job from the example usage) django_rq should now be excluded from the dump (added --exclude django_rq to the example usage) $ nautobot-server dumpdata \\ --natural-foreign \\ --natural-primary \\ --exclude contenttypes \\ --exclude auth.permission \\ --exclude django_rq \\ --format json \\ --indent 2 \\ --traceback > nautobot_dump.json Use this command to generate a JSON dump of the database contents. One example of using this command would be to export data from PostgreSQL .","title":"dumpdata"},{"location":"administration/nautobot-server.html#fix_custom_fields","text":"nautobot-server fix_custom_fields Adds/Removes any custom fields which should or should not exist on an object. This command should not be run unless a custom fields jobs has failed: $ nautobot-server fix_custom_fields Processing ContentType dcim | device Processing ContentType dcim | site Processing ContentType dcim | rack Processing ContentType dcim | cable Processing ContentType dcim | power feed Processing ContentType circuits | circuit Processing ContentType ipam | prefix ... (truncated for brevity of documentation) ...","title":"fix_custom_fields"},{"location":"administration/nautobot-server.html#generate_secret_key","text":"nautobot-server generate_secret_key Generates a new SECRET_KEY that can be used in your nautobot_config.py : $ nautobot-server generate_secret_key e!j=vrlhz-!wl8p_3+q5s5cph29nzj$xm81eap-!&n!_9^du09","title":"generate_secret_key"},{"location":"administration/nautobot-server.html#health_check","text":"nautobot-server health_check Run health checks and exit 0 if everything went well. $ nautobot-server health_check DatabaseBackend ... working DefaultFileStorageHealthCheck ... working RedisBackend ... working Please see the healthcheck documentation for more information.","title":"health_check"},{"location":"administration/nautobot-server.html#init","text":"nautobot-server init [config_path] Generates a new configuration with all of the default settings provided for you, and will also generate a unique SECRET_KEY . By default the file will be created at $HOME/.nautobot/nautobot_config.py : $ nautobot-server init Configuration file created at '/home/example/.nautobot/nautobot_config.py For more information on configuring Nautobot for the first time or on more advanced configuration patterns, please see the guide on Nautobot Configuration .","title":"init"},{"location":"administration/nautobot-server.html#invalidate","text":"nautobot-server invalidate Invalidates cache for entire app, model or particular instance. Most commonly you will see us recommend clearing the entire cache using invalidate all : $ nautobot-server invalidate all There are a number of other options not covered here. Note This is a built-in management command provided by the Cacheops plugin Nautobot for caching. Please see the official Cacheops documentation on invalidate for more information.","title":"invalidate"},{"location":"administration/nautobot-server.html#loaddata","text":"nautobot-server loaddata --traceback nautobot_dump.json To import the data that was exported with nautobot-server dumpdata ... see the following documentation: Remove the auto-populated Status records from the database Import the database dump","title":"loaddata"},{"location":"administration/nautobot-server.html#migrate","text":"nautobot-server migrate [app_label] [migration_name] Initialize a new database or run any pending database migrations to an existing database. $ nautobot-server migrate Wrapping model clean methods for custom validators failed because the ContentType table was not available or populated. This is normal during the execution of the migration command for the first time. Operations to perform: Apply all migrations: admin, auth, circuits, contenttypes, dcim, extras, ipam, sessions, taggit, tenancy, users, virtualization Running migrations: Applying contenttypes.0001_initial... OK Applying auth.0001_initial... OK Applying admin.0001_initial... OK ... (truncated for brevity of documentation) ... Note This is a built-in Django command. Please see the official documentation on migrate for more information.","title":"migrate"},{"location":"administration/nautobot-server.html#nbshell","text":"nautobot-server nbshell An interactive Python shell with all of the database models and various other utilities already imported for you. This is immensely useful for direct access to manipulating database objects. Danger This is an advanced feature that gives you direct access to the Django database models. Use this very cautiously as you can cause irreparable damage to your Nautobot installation. $ nautobot-server nbshell ### Nautobot interactive shell (localhost) ### Python 3.8.7 | Django 3.1.7 | Nautobot 1.0.0 ### lsmodels() will show available models. Use help(<model>) for more info. >>> Please see the dedicated guide on the Nautobot Shell for more information.","title":"nbshell"},{"location":"administration/nautobot-server.html#post_upgrade","text":"nautobot-server post_upgrade Performs common server post-upgrade operations using a single entrypoint. This will run the following management commands with default settings, in order: migrate trace_paths collectstatic remove_stale_contenttypes clearsessions invalidate all Note Commands listed here that are not covered in this document here are Django built-in commands. --no-clearsessions Do not automatically clean out expired sessions. --no-collectstatic Do not automatically collect static files into a single location. --no-invalidate-all Do not automatically invalidate cache for entire application. --no-migrate Do not automatically perform any database migrations. --no-remove-stale-contenttypes Do not automatically remove stale content types. --no-trace-paths Do not automatically generate missing cable paths. $ nautobot-server post_upgrade Performing database migrations... Operations to perform: Apply all migrations: admin, auth, circuits, contenttypes, dcim, extras, ipam, sessions, taggit, tenancy, users, virtualization Running migrations: No migrations to apply. Generating cable paths... Found no missing circuit termination paths; skipping Found no missing console port paths; skipping Found no missing console server port paths; skipping Found no missing interface paths; skipping Found no missing power feed paths; skipping Found no missing power outlet paths; skipping Found no missing power port paths; skipping Finished. Collecting static files... 0 static files copied to '/opt/nautobot/static', 965 unmodified. Removing stale content types... Removing expired sessions... Invalidating cache...","title":"post_upgrade"},{"location":"administration/nautobot-server.html#remove_stale_scheduled_jobs","text":"Added in version 1.3.10 nautobot-server remove_stale_scheduled_jobs [max-age of days] Delete non-recurring scheduled jobs that were scheduled to run more than max-age days ago.","title":"remove_stale_scheduled_jobs"},{"location":"administration/nautobot-server.html#renaturalize","text":"nautobot-server renaturalize [app_label.ModelName [app_label.ModelName ...]] Recalculate natural ordering values for the specified models. This defaults to recalculating natural ordering on all models which have one or more fields of type NaturalOrderingField : $ nautobot-server renaturalize Renaturalizing 21 models. dcim.ConsolePort.name (_name)... 196 dcim.ConsoleServerPort.name (_name)... 0 dcim.PowerPort.name (_name)... 392 dcim.PowerOutlet.name (_name)... 0 dcim.Interface.name (_name)... 7161 dcim.FrontPort.name (_name)... 0 dcim.RearPort.name (_name)... 0 dcim.DeviceBay.name (_name)... 0 dcim.InventoryItem.name (_name)... 1 dcim.Device.name (_name)... 208 dcim.ConsolePortTemplate.name (_name)... 2 dcim.ConsoleServerPortTemplate.name (_name)... 0 dcim.PowerPortTemplate.name (_name)... 4 dcim.PowerOutletTemplate.name (_name)... 0 dcim.InterfaceTemplate.name (_name)... 221 dcim.FrontPortTemplate.name (_name)... 0 dcim.RearPortTemplate.name (_name)... 0 dcim.DeviceBayTemplate.name (_name)... 0 dcim.Rack.name (_name)... 156 dcim.Site.name (_name)... 22 virtualization.VMInterface.name (_name)... 0 Done. You may optionally specify or more specific models (each prefixed with its app_label) to renaturalize: $ nautobot-server renaturalize dcim.Device Renaturalizing 1 models. dcim.Device.name (_name)... 208 Done.","title":"renaturalize"},{"location":"administration/nautobot-server.html#runjob","text":"nautobot-server runjob [job] Run a job (script, report) to validate or update data in Nautobot. --commit Commit changes to DB (defaults to dry-run if unset). --username is mandatory if using this argument. --username <username> User account to impersonate as the requester of this job. $ nautobot-server runjob --commit --username someuser local/example/MyJobWithNoVars Note that there is presently no option to provide input parameters ( data ) for jobs via the CLI. Please see the guide on Jobs for more information on working with and running jobs.","title":"runjob"},{"location":"administration/nautobot-server.html#start","text":"nautobot-server start Directly invoke uWSGI to start a Nautobot server suitable for production use. This command behaves exactly as uWSGI does, but allows us to maintain a single entrypoint into the Nautobot application. Note uWSGI offers an overwhelming amount of command-line arguments that could not possibly be covered here. Please see the official uWSGI Options guide for more information. $ nautobot-server start --ini ./uwsgi.ini [uWSGI] getting INI configuration from ./uwsgi.ini [uwsgi-static] added mapping for /static => /opt/nautobot/static *** Starting uWSGI 2.0.19.1 (64bit) on [Thu Mar 11 21:13:22 2021] *** compiled with version: 8.3.1 20190311 (Red Hat 8.3.1-3) on 23 September 2020 02:39:40 os: Linux-5.4.0-52-generic #57-Ubuntu SMP Thu Oct 15 10:57:00 UTC 2020 nodename: jathan-nautobot-testing machine: x86_64 clock source: unix pcre jit disabled detected number of CPU cores: 48 current working directory: /opt/nautobot detected binary path: /usr/bin/python3.8 your processes number limit is 256070 your memory page size is 4096 bytes detected max file descriptor number: 1024 building mime-types dictionary from file /etc/mime.types...567 entry found lock engine: pthread robust mutexes thunder lock: disabled (you can enable it with --thunder-lock) uwsgi socket 0 bound to TCP address 0.0.0.0:9191 fd 7 Python version: 3.8.5 (default, Jan 27 2021, 15:41:15) [GCC 9.3.0] --- Python VM already initialized --- Python main interpreter initialized at 0x2573e30 python threads support enabled your server socket listen backlog is limited to 1024 connections your mercy for graceful operations on workers is 60 seconds mapped 636432 bytes (621 KB) for 15 cores *** Operational MODE: preforking+threaded *** WSGI app 0 (mountpoint='') ready in 0 seconds on interpreter 0x2573e30 pid: 112153 (default app) spawned uWSGI master process (pid: 112153) spawned uWSGI worker 1 (pid: 112159, cores: 3) spawned uWSGI worker 2 (pid: 112162, cores: 3) spawned uWSGI worker 3 (pid: 112165, cores: 3) spawned uWSGI worker 4 (pid: 112168, cores: 3) spawned uWSGI worker 5 (pid: 112171, cores: 3) Please see the guide on Deploying Nautobot Services for our recommended configuration for use with uWSGI.","title":"start"},{"location":"administration/nautobot-server.html#startplugin","text":"nautobot-server startplugin <name> [directory] Create a new plugin with name . This command is similar to the django-admin startapp command, but with a default template directory ( --template ) of nautobot/core/templates/plugin_template . This command assists with creating a basic file structure for beginning development of a new Nautobot plugin. Without passing in the destination directory, nautobot-server startplugin will use your current directory and the name argument provided to create a new directory. We recommend providing a directory so that the plugin can be installed or published easily. Here is an example: $ mkdir -p ~/myplugin/myplugin $ nautobot-server startplugin myplugin ~/myplugin/myplugin Additional options such as --name or --extension can be found in the Django documentation .","title":"startplugin"},{"location":"administration/nautobot-server.html#trace_paths","text":"nautobot-server trace_paths Generate any missing cable paths among all cable termination objects in Nautobot. After upgrading the database or working with Cables, Circuits, or other related objects, there may be a need to rebuild cached cable paths. --force Force recalculation of all existing cable paths. --no-input Do not prompt user for any input/confirmation. $ nautobot-server trace_paths Found no missing circuit termination paths; skipping Found no missing console port paths; skipping Found no missing console server port paths; skipping Found no missing interface paths; skipping Found no missing power feed paths; skipping Found no missing power outlet paths; skipping Found no missing power port paths; skipping Finished. Note This command is safe to run at any time. If it does detect any changes, it will exit cleanly.","title":"trace_paths"},{"location":"administration/nautobot-server.html#webhook_receiver","text":"nautobot-server webhook_receiver Start a simple listener to display received HTTP requests. --port PORT Optional port number (default: 9000 ) --no-headers Hide HTTP request headers. $ nautobot-server webhook_receiver --port 9001 --no-headers Listening on port http://localhost:9000. Stop with CONTROL-C. Please see the guide on Troubleshooting Webhooks for more information.","title":"webhook_receiver"},{"location":"administration/nautobot-shell.html","text":"The Nautobot Python Shell \u00b6 Nautobot includes a Python management shell within which objects can be directly queried, created, modified, and deleted. To enter the shell, run the following command: $ nautobot-server nbshell This will launch a lightly customized version of the built-in Django shell with all relevant Nautobot models pre-loaded. (If desired, the stock Django shell is also available by executing nautobot-server shell .) $ nautobot-server nbshell ### Nautobot interactive shell (localhost) ### Python 3.6.9 | Django 3.1 | Nautobot 1.0.0 ### lsmodels() will show available models. Use help(<model>) for more info. The function lsmodels() will print a list of all available Nautobot models: >>> lsmodels () DCIM : ConsolePort ConsolePortTemplate ConsoleServerPort ConsoleServerPortTemplate Device ... Warning The Nautobot shell affords direct access to Nautobot data and function with very little validation in place. As such, it is crucial to ensure that only authorized, knowledgeable users are ever granted access to it. Never perform any action in the management shell without having a full backup in place. Querying Objects \u00b6 Objects are retrieved from the database using a Django queryset . The base queryset for an object takes the form <model>.objects.all() , which will return a (truncated) list of all objects of that type. >>> Device . objects . all () < QuerySet [ < Device : TestDevice1 > , < Device : TestDevice2 > , < Device : TestDevice3 > , < Device : TestDevice4 > , < Device : TestDevice5 > , '...(remaining elements truncated)...' ] > Use a for loop to cycle through all objects in the list: >>> for device in Device . objects . all (): ... print ( device . name , device . device_type ) ... ( 'TestDevice1' , < DeviceType : PacketThingy 9000 > ) ( 'TestDevice2' , < DeviceType : PacketThingy 9000 > ) ( 'TestDevice3' , < DeviceType : PacketThingy 9000 > ) ( 'TestDevice4' , < DeviceType : PacketThingy 9000 > ) ( 'TestDevice5' , < DeviceType : PacketThingy 9000 > ) ... To count all objects matching the query, replace all() with count() : >>> Device . objects . count () 1274 To retrieve a particular object (typically by its primary key or other unique field), use get() : >>> Site . objects . get ( pk = \"8a2c9c3b-076e-4688-8a0b-89362f343a26\" ) < Site : Test Lab > Filtering Querysets \u00b6 In most cases, you will want to retrieve only a specific subset of objects. To filter a queryset, replace all() with filter() and pass one or more keyword arguments. For example: >>> Device . objects . filter ( status__slug = \"active\" ) < QuerySet [ < Device : TestDevice1 > , < Device : TestDevice2 > , < Device : TestDevice3 > , < Device : TestDevice8 > , < Device : TestDevice9 > , '...(remaining elements truncated)...' ] > Querysets support slicing to return a specific range of objects. >>> Device . objects . filter ( status__slug = \"active\" )[: 3 ] < QuerySet [ < Device : TestDevice1 > , < Device : TestDevice2 > , < Device : TestDevice3 > ] > The count() method can be appended to the queryset to return a count of objects rather than the full list. >>> Device . objects . filter ( status__slug = \"active\" ) . count () 982 Relationships with other models can be traversed by concatenating attribute names with a double-underscore. For example, the following will return all devices assigned to the tenant named \"Pied Piper.\" >>> Device . objects . filter ( tenant__name = \"Pied Piper\" ) This approach can span multiple levels of relations. For example, the following will return all IP addresses assigned to a device in North America: >>> IPAddress . objects . filter ( interface__device__site__region__slug = \"north-america\" ) Note While the above query is functional, it's not very efficient. There are ways to optimize such requests, however they are out of scope for this document. For more information, see the Django queryset method reference documentation. Reverse relationships can be traversed as well. For example, the following will find all devices with an interface named \"em0\": >>> Device . objects . filter ( interfaces__name = \"em0\" ) Character fields can be filtered against partial matches using the contains or icontains field lookup (the later of which is case-insensitive). >>> Device . objects . filter ( name__icontains = \"testdevice\" ) Similarly, numeric fields can be filtered by values less than, greater than, and/or equal to a given value. >>> VLAN . objects . filter ( vid__gt = 2000 ) Multiple filters can be combined to further refine a queryset. >>> VLAN . objects . filter ( vid__gt = 2000 , name__icontains = \"engineering\" ) To return the inverse of a filtered queryset, use exclude() instead of filter() . >>> Device . objects . count () 4479 >>> Device . objects . filter ( status = \"active\" ) . count () 4133 >>> Device . objects . exclude ( status = \"active\" ) . count () 346 Info The examples above are intended only to provide a cursory introduction to queryset filtering. For an exhaustive list of the available filters, please consult the Django queryset API documentation . Creating and Updating Objects \u00b6 New objects can be created by instantiating the desired model, defining values for all required attributes, and calling validated_save() on the instance. For example, we can create a new VLAN by specifying its numeric ID, name, and assigned site: >>> lab1 = Site . objects . get ( pk = \"8a2c9c3b-076e-4688-8a0b-89362f343a26\" ) >>> myvlan = VLAN ( vid = 123 , name = \"MyNewVLAN\" , site = lab1 ) >>> myvlan . validated_save () Alternatively, the above can be performed as a single operation. (Note, however, that validated_save() does not return the new instance for reuse.) >>> VLAN ( vid = 123 , name = \"MyNewVLAN\" , site = Site . objects . get ( pk = \"8a2c9c3b-076e-4688-8a0b-89362f343a26\" )) . validated_save () To modify an existing object, we retrieve it, update the desired field(s), and call validated_save() again. >>> vlan = VLAN . objects . get ( pk = \"b4b4344f-f6bb-4ceb-85bc-4f169c753157\" ) >>> vlan . name 'MyNewVLAN' >>> vlan . name = 'BetterName' >>> vlan . validated_save () >>> VLAN . objects . get ( pk = \"b4b4344f-f6bb-4ceb-85bc-4f169c753157\" ) . name 'BetterName' Warning It is recommended to make use of the validated_save() convenience method which exists on all core models. While the Django save() method still exists, the validated_save() method saves the instance data but first enforces model validation logic. Simply calling save() on the model instance does not enforce validation automatically and may lead to bad data. See the development best practices . Warning The Django ORM provides methods to create/edit many objects at once, namely bulk_create() and update() . These are best avoided in most cases as they bypass a model's built-in validation and can easily lead to database corruption if not used carefully. Deleting Objects \u00b6 To delete an object, simply call delete() on its instance. This will return a dictionary of all objects (including related objects) which have been deleted as a result of this operation. >>> vlan < VLAN : 123 ( BetterName ) > >>> vlan . delete () ( 1 , { 'ipam.VLAN' : 1 }) To delete multiple objects at once, call delete() on a filtered queryset. It's a good idea to always sanity-check the count of selected objects before deleting them. >>> Device . objects . filter ( name__icontains = 'test' ) . count () 27 >>> Device . objects . filter ( name__icontains = 'test' ) . delete () ( 35 , { 'dcim.DeviceBay' : 0 , 'dcim.InterfaceConnection' : 4 , 'extras.ImageAttachment' : 0 , 'dcim.Device' : 27 , 'dcim.Interface' : 4 , 'dcim.ConsolePort' : 0 , 'dcim.PowerPort' : 0 }) Warning Deletions are immediate and irreversible. Always consider the impact of deleting objects carefully before calling delete() on an instance or queryset. Change Logging and Webhooks \u00b6 Note that Nautobot's change logging and webhook processing features operate under the context of an HTTP request. As such, these functions do not work automatically when using the ORM directly, either through the nbshell or otherwise. A special context manager is provided to allow these features to operate under an emulated HTTP request context. This context manager must be explicitly invoked for change log entries and webhooks to be created when interacting with objects through the ORM. Here is an example using the web_request_context context manager within the nbshell: >>> from nautobot.extras.context_managers import web_request_context >>> user = User . objects . get ( username = \"admin\" ) >>> with web_request_context ( user ): ... lax = Site ( name = \"LAX\" ) ... lax . validated_save () A User object must be provided. A WSGIRequest may optionally be passed and one will automatically be created if not provided.","title":"Nautobot Shell"},{"location":"administration/nautobot-shell.html#the-nautobot-python-shell","text":"Nautobot includes a Python management shell within which objects can be directly queried, created, modified, and deleted. To enter the shell, run the following command: $ nautobot-server nbshell This will launch a lightly customized version of the built-in Django shell with all relevant Nautobot models pre-loaded. (If desired, the stock Django shell is also available by executing nautobot-server shell .) $ nautobot-server nbshell ### Nautobot interactive shell (localhost) ### Python 3.6.9 | Django 3.1 | Nautobot 1.0.0 ### lsmodels() will show available models. Use help(<model>) for more info. The function lsmodels() will print a list of all available Nautobot models: >>> lsmodels () DCIM : ConsolePort ConsolePortTemplate ConsoleServerPort ConsoleServerPortTemplate Device ... Warning The Nautobot shell affords direct access to Nautobot data and function with very little validation in place. As such, it is crucial to ensure that only authorized, knowledgeable users are ever granted access to it. Never perform any action in the management shell without having a full backup in place.","title":"The Nautobot Python Shell"},{"location":"administration/nautobot-shell.html#querying-objects","text":"Objects are retrieved from the database using a Django queryset . The base queryset for an object takes the form <model>.objects.all() , which will return a (truncated) list of all objects of that type. >>> Device . objects . all () < QuerySet [ < Device : TestDevice1 > , < Device : TestDevice2 > , < Device : TestDevice3 > , < Device : TestDevice4 > , < Device : TestDevice5 > , '...(remaining elements truncated)...' ] > Use a for loop to cycle through all objects in the list: >>> for device in Device . objects . all (): ... print ( device . name , device . device_type ) ... ( 'TestDevice1' , < DeviceType : PacketThingy 9000 > ) ( 'TestDevice2' , < DeviceType : PacketThingy 9000 > ) ( 'TestDevice3' , < DeviceType : PacketThingy 9000 > ) ( 'TestDevice4' , < DeviceType : PacketThingy 9000 > ) ( 'TestDevice5' , < DeviceType : PacketThingy 9000 > ) ... To count all objects matching the query, replace all() with count() : >>> Device . objects . count () 1274 To retrieve a particular object (typically by its primary key or other unique field), use get() : >>> Site . objects . get ( pk = \"8a2c9c3b-076e-4688-8a0b-89362f343a26\" ) < Site : Test Lab >","title":"Querying Objects"},{"location":"administration/nautobot-shell.html#filtering-querysets","text":"In most cases, you will want to retrieve only a specific subset of objects. To filter a queryset, replace all() with filter() and pass one or more keyword arguments. For example: >>> Device . objects . filter ( status__slug = \"active\" ) < QuerySet [ < Device : TestDevice1 > , < Device : TestDevice2 > , < Device : TestDevice3 > , < Device : TestDevice8 > , < Device : TestDevice9 > , '...(remaining elements truncated)...' ] > Querysets support slicing to return a specific range of objects. >>> Device . objects . filter ( status__slug = \"active\" )[: 3 ] < QuerySet [ < Device : TestDevice1 > , < Device : TestDevice2 > , < Device : TestDevice3 > ] > The count() method can be appended to the queryset to return a count of objects rather than the full list. >>> Device . objects . filter ( status__slug = \"active\" ) . count () 982 Relationships with other models can be traversed by concatenating attribute names with a double-underscore. For example, the following will return all devices assigned to the tenant named \"Pied Piper.\" >>> Device . objects . filter ( tenant__name = \"Pied Piper\" ) This approach can span multiple levels of relations. For example, the following will return all IP addresses assigned to a device in North America: >>> IPAddress . objects . filter ( interface__device__site__region__slug = \"north-america\" ) Note While the above query is functional, it's not very efficient. There are ways to optimize such requests, however they are out of scope for this document. For more information, see the Django queryset method reference documentation. Reverse relationships can be traversed as well. For example, the following will find all devices with an interface named \"em0\": >>> Device . objects . filter ( interfaces__name = \"em0\" ) Character fields can be filtered against partial matches using the contains or icontains field lookup (the later of which is case-insensitive). >>> Device . objects . filter ( name__icontains = \"testdevice\" ) Similarly, numeric fields can be filtered by values less than, greater than, and/or equal to a given value. >>> VLAN . objects . filter ( vid__gt = 2000 ) Multiple filters can be combined to further refine a queryset. >>> VLAN . objects . filter ( vid__gt = 2000 , name__icontains = \"engineering\" ) To return the inverse of a filtered queryset, use exclude() instead of filter() . >>> Device . objects . count () 4479 >>> Device . objects . filter ( status = \"active\" ) . count () 4133 >>> Device . objects . exclude ( status = \"active\" ) . count () 346 Info The examples above are intended only to provide a cursory introduction to queryset filtering. For an exhaustive list of the available filters, please consult the Django queryset API documentation .","title":"Filtering Querysets"},{"location":"administration/nautobot-shell.html#creating-and-updating-objects","text":"New objects can be created by instantiating the desired model, defining values for all required attributes, and calling validated_save() on the instance. For example, we can create a new VLAN by specifying its numeric ID, name, and assigned site: >>> lab1 = Site . objects . get ( pk = \"8a2c9c3b-076e-4688-8a0b-89362f343a26\" ) >>> myvlan = VLAN ( vid = 123 , name = \"MyNewVLAN\" , site = lab1 ) >>> myvlan . validated_save () Alternatively, the above can be performed as a single operation. (Note, however, that validated_save() does not return the new instance for reuse.) >>> VLAN ( vid = 123 , name = \"MyNewVLAN\" , site = Site . objects . get ( pk = \"8a2c9c3b-076e-4688-8a0b-89362f343a26\" )) . validated_save () To modify an existing object, we retrieve it, update the desired field(s), and call validated_save() again. >>> vlan = VLAN . objects . get ( pk = \"b4b4344f-f6bb-4ceb-85bc-4f169c753157\" ) >>> vlan . name 'MyNewVLAN' >>> vlan . name = 'BetterName' >>> vlan . validated_save () >>> VLAN . objects . get ( pk = \"b4b4344f-f6bb-4ceb-85bc-4f169c753157\" ) . name 'BetterName' Warning It is recommended to make use of the validated_save() convenience method which exists on all core models. While the Django save() method still exists, the validated_save() method saves the instance data but first enforces model validation logic. Simply calling save() on the model instance does not enforce validation automatically and may lead to bad data. See the development best practices . Warning The Django ORM provides methods to create/edit many objects at once, namely bulk_create() and update() . These are best avoided in most cases as they bypass a model's built-in validation and can easily lead to database corruption if not used carefully.","title":"Creating and Updating Objects"},{"location":"administration/nautobot-shell.html#deleting-objects","text":"To delete an object, simply call delete() on its instance. This will return a dictionary of all objects (including related objects) which have been deleted as a result of this operation. >>> vlan < VLAN : 123 ( BetterName ) > >>> vlan . delete () ( 1 , { 'ipam.VLAN' : 1 }) To delete multiple objects at once, call delete() on a filtered queryset. It's a good idea to always sanity-check the count of selected objects before deleting them. >>> Device . objects . filter ( name__icontains = 'test' ) . count () 27 >>> Device . objects . filter ( name__icontains = 'test' ) . delete () ( 35 , { 'dcim.DeviceBay' : 0 , 'dcim.InterfaceConnection' : 4 , 'extras.ImageAttachment' : 0 , 'dcim.Device' : 27 , 'dcim.Interface' : 4 , 'dcim.ConsolePort' : 0 , 'dcim.PowerPort' : 0 }) Warning Deletions are immediate and irreversible. Always consider the impact of deleting objects carefully before calling delete() on an instance or queryset.","title":"Deleting Objects"},{"location":"administration/nautobot-shell.html#change-logging-and-webhooks","text":"Note that Nautobot's change logging and webhook processing features operate under the context of an HTTP request. As such, these functions do not work automatically when using the ORM directly, either through the nbshell or otherwise. A special context manager is provided to allow these features to operate under an emulated HTTP request context. This context manager must be explicitly invoked for change log entries and webhooks to be created when interacting with objects through the ORM. Here is an example using the web_request_context context manager within the nbshell: >>> from nautobot.extras.context_managers import web_request_context >>> user = User . objects . get ( username = \"admin\" ) >>> with web_request_context ( user ): ... lax = Site ( name = \"LAX\" ) ... lax . validated_save () A User object must be provided. A WSGIRequest may optionally be passed and one will automatically be created if not provided.","title":"Change Logging and Webhooks"},{"location":"administration/permissions.html","text":"Permissions \u00b6 Nautobot provides an object-based permissions framework, which replace's Django's built-in permissions model. Object-based permissions enable an administrator to grant users or groups the ability to perform an action on arbitrary subsets of objects in Nautobot, rather than all objects of a certain type. For example, it is possible to grant a user permission to view only sites within a particular region, or to modify only VLANs with a numeric ID within a certain range. Object Permissions \u00b6 A permission in Nautobot represents a relationship shared by several components: Object type(s) - One or more types of object in Nautobot User(s)/Group(s) - One or more users or groups of users Action(s) - The action(s) that can be performed on an object Constraints - An arbitrary filter used to limit the granted action(s) to a specific subset of objects At a minimum, a permission assignment must specify one object type, one user or group, and one action. The specification of constraints is optional: A permission without any constraints specified will apply to all instances of the selected model(s). Actions \u00b6 There are four core actions that can be permitted for each type of object within Nautobot, roughly analogous to the CRUD convention (create, read, update, and delete): View - Retrieve an object from the database Add - Create a new object Change - Modify an existing object Delete - Delete an existing object In addition to these, permissions can also grant custom actions that may be required by a specific model or plugin. For example, the napalm_read permission on the device model allows a user to execute NAPALM queries on a device via Nautobot's REST API. These can be specified when granting a permission in the \"additional actions\" field. Note Internally, all actions granted by a permission (both built-in and custom) are stored as strings in an array field named actions . Constraints \u00b6 Constraints are expressed as a JSON object or list representing a Django query filter . This is the same syntax that you would pass to the QuerySet filter() method when performing a query using the Django ORM. As with query filters, double underscores can be used to traverse related objects or invoke lookup expressions. Some example queries and their corresponding definitions are shown below. All attributes defined within a single JSON object are applied with a logical AND. For example, suppose you assign a permission for the site model with the following constraints. { \"status\" : \"active\" , \"region__name\" : \"Americas\" } The permission will grant access only to sites which have a status of \"active\" and which are assigned to the \"Americas\" region. To achieve a logical OR with a different set of constraints, define multiple objects within a list. For example, if you want to constrain the permission to VLANs with an ID between 100 and 199 or a status of \"reserved,\" do the following: [ { \"vid__gte\" : 100 , \"vid__lt\" : 200 }, { \"status\" : \"reserved\" } ] Additionally, where multiple permissions have been assigned for an object type, their collective constraints will be merged using a logical \"OR\" operation. Example Constraint Definitions \u00b6 Constraints Description {\"status\": \"active\"} Status is active {\"status__in\": [\"planned\", \"reserved\"]} Status is active OR reserved {\"status\": \"active\", \"role\": \"testing\"} Status is active OR role is testing {\"name__startswith\": \"Foo\"} Name starts with \"Foo\" (case-sensitive) {\"name__iendswith\": \"bar\"} Name ends with \"bar\" (case-insensitive) {\"vid__gte\": 100, \"vid__lt\": 200} VLAN ID is greater than or equal to 100 AND less than 200 [{\"vid__lt\": 200}, {\"status\": \"reserved\"}] VLAN ID is less than 200 OR status is reserved Permissions Enforcement \u00b6 Viewing Objects \u00b6 Object-based permissions work by filtering the database query generated by a user's request to restrict the set of objects returned. When a request is received, Nautobot first determines whether the user is authenticated and has been granted to perform the requested action. For example, if the requested URL is /dcim/devices/ , Nautobot will check for the dcim.view_device permission. If the user has not been assigned this permission (either directly or via a group assignment), Nautobot will return a 403 (forbidden) HTTP response. If the permission has been granted, Nautobot will compile any specified constraints for the model and action. For example, suppose two permissions have been assigned to the user granting view access to the device model, with the following constraints: [ { \"site__name__in\" : [ \"NYC1\" , \"NYC2\" ]}, { \"status\" : \"offline\" , \"tenant__isnull\" : true } ] This grants the user access to view any device that is assigned to a site named NYC1 or NYC2, or which has a status of \"offline\" and has no tenant assigned. These constraints are equivalent to the following ORM query: Site.objects.filter( Q(site__name__in=['NYC1', 'NYC2']), Q(status='active', tenant__isnull=True) ) Creating and Modifying Objects \u00b6 The same sort of logic is in play when a user attempts to create or modify an object in Nautobot, with a twist. Once validation has completed, Nautobot starts an atomic database transaction to facilitate the change, and the object is created or saved normally. Next, still within the transaction, Nautobot issues a second query to retrieve the newly created/updated object, filtering the restricted queryset with the object's primary key. If this query fails to return the object, Nautobot knows that the new revision does not match the constraints imposed by the permission. The transaction is then rolled back, leaving the database in its original state prior to the change, and the user is informed of the violation. Assigning Permissions \u00b6 Permissions are implemented by assigning them to specific users and/or to groups of users. Users can have a combination of permissions and groups assigned to their account. All of the permissions granted to the user's groups and directly to the user's account will be used when determining authorization to access an object or view. Assigning Permissions to Individual Users \u00b6 Permissions can be related directly to users from the Admin UI or the API: - Admin UI API Superusers Yes Yes Staff users with users.add_permission or users.change_permission Yes Yes Regular users with users.add_permission or users.change_permission No Yes Multiple permissions can be assigned to a user account. Info User permission relationships can be managed in the Admin UI by modifying the user or the permission. Warning Granting a user users.change_permission or users.add_permission gives the user the ability to modify their own permissions. This permission should be restricted to trusted accounts and should be considered the same as giving a user full access. Creating Groups \u00b6 Groups of users can be created to provide role-based access control and simplify user permissions management. Permissions related to a group will apply to all users in the group. A user can belong to any number of groups. Groups can be created from the Admin UI or the API: - Admin UI API Superusers Yes Yes Users with auth.add_group or auth.change_group No Yes Adding Users to Groups \u00b6 Users can be added to groups through the Admin UI by superusers or automatically assigned to externally authenticated users through the EXTERNAL_AUTH_DEFAULT_GROUPS and EXTERNAL_AUTH_DEFAULT_PERMISSIONS settings. Nautobot groups can optionally be mapped to LDAP groups when using LDAP authentication . Assigning Permissions to Groups \u00b6 Permissions can be related to groups by superusers or users with users.add_permission or users.change_permission permissions. - Admin UI API Superusers Yes Yes Staff users with users.add_permission or users.change_permission Yes Yes Regular users with users.add_permission or users.change_permission No Yes Multiple permissions can be assigned to a user group. Info Group permission relationships can be managed in the Admin UI by modifying the group (superusers only) or the permission.","title":"Permissions"},{"location":"administration/permissions.html#permissions","text":"Nautobot provides an object-based permissions framework, which replace's Django's built-in permissions model. Object-based permissions enable an administrator to grant users or groups the ability to perform an action on arbitrary subsets of objects in Nautobot, rather than all objects of a certain type. For example, it is possible to grant a user permission to view only sites within a particular region, or to modify only VLANs with a numeric ID within a certain range.","title":"Permissions"},{"location":"administration/permissions.html#object-permissions","text":"A permission in Nautobot represents a relationship shared by several components: Object type(s) - One or more types of object in Nautobot User(s)/Group(s) - One or more users or groups of users Action(s) - The action(s) that can be performed on an object Constraints - An arbitrary filter used to limit the granted action(s) to a specific subset of objects At a minimum, a permission assignment must specify one object type, one user or group, and one action. The specification of constraints is optional: A permission without any constraints specified will apply to all instances of the selected model(s).","title":"Object Permissions"},{"location":"administration/permissions.html#actions","text":"There are four core actions that can be permitted for each type of object within Nautobot, roughly analogous to the CRUD convention (create, read, update, and delete): View - Retrieve an object from the database Add - Create a new object Change - Modify an existing object Delete - Delete an existing object In addition to these, permissions can also grant custom actions that may be required by a specific model or plugin. For example, the napalm_read permission on the device model allows a user to execute NAPALM queries on a device via Nautobot's REST API. These can be specified when granting a permission in the \"additional actions\" field. Note Internally, all actions granted by a permission (both built-in and custom) are stored as strings in an array field named actions .","title":"Actions"},{"location":"administration/permissions.html#constraints","text":"Constraints are expressed as a JSON object or list representing a Django query filter . This is the same syntax that you would pass to the QuerySet filter() method when performing a query using the Django ORM. As with query filters, double underscores can be used to traverse related objects or invoke lookup expressions. Some example queries and their corresponding definitions are shown below. All attributes defined within a single JSON object are applied with a logical AND. For example, suppose you assign a permission for the site model with the following constraints. { \"status\" : \"active\" , \"region__name\" : \"Americas\" } The permission will grant access only to sites which have a status of \"active\" and which are assigned to the \"Americas\" region. To achieve a logical OR with a different set of constraints, define multiple objects within a list. For example, if you want to constrain the permission to VLANs with an ID between 100 and 199 or a status of \"reserved,\" do the following: [ { \"vid__gte\" : 100 , \"vid__lt\" : 200 }, { \"status\" : \"reserved\" } ] Additionally, where multiple permissions have been assigned for an object type, their collective constraints will be merged using a logical \"OR\" operation.","title":"Constraints"},{"location":"administration/permissions.html#example-constraint-definitions","text":"Constraints Description {\"status\": \"active\"} Status is active {\"status__in\": [\"planned\", \"reserved\"]} Status is active OR reserved {\"status\": \"active\", \"role\": \"testing\"} Status is active OR role is testing {\"name__startswith\": \"Foo\"} Name starts with \"Foo\" (case-sensitive) {\"name__iendswith\": \"bar\"} Name ends with \"bar\" (case-insensitive) {\"vid__gte\": 100, \"vid__lt\": 200} VLAN ID is greater than or equal to 100 AND less than 200 [{\"vid__lt\": 200}, {\"status\": \"reserved\"}] VLAN ID is less than 200 OR status is reserved","title":"Example Constraint Definitions"},{"location":"administration/permissions.html#permissions-enforcement","text":"","title":"Permissions Enforcement"},{"location":"administration/permissions.html#viewing-objects","text":"Object-based permissions work by filtering the database query generated by a user's request to restrict the set of objects returned. When a request is received, Nautobot first determines whether the user is authenticated and has been granted to perform the requested action. For example, if the requested URL is /dcim/devices/ , Nautobot will check for the dcim.view_device permission. If the user has not been assigned this permission (either directly or via a group assignment), Nautobot will return a 403 (forbidden) HTTP response. If the permission has been granted, Nautobot will compile any specified constraints for the model and action. For example, suppose two permissions have been assigned to the user granting view access to the device model, with the following constraints: [ { \"site__name__in\" : [ \"NYC1\" , \"NYC2\" ]}, { \"status\" : \"offline\" , \"tenant__isnull\" : true } ] This grants the user access to view any device that is assigned to a site named NYC1 or NYC2, or which has a status of \"offline\" and has no tenant assigned. These constraints are equivalent to the following ORM query: Site.objects.filter( Q(site__name__in=['NYC1', 'NYC2']), Q(status='active', tenant__isnull=True) )","title":"Viewing Objects"},{"location":"administration/permissions.html#creating-and-modifying-objects","text":"The same sort of logic is in play when a user attempts to create or modify an object in Nautobot, with a twist. Once validation has completed, Nautobot starts an atomic database transaction to facilitate the change, and the object is created or saved normally. Next, still within the transaction, Nautobot issues a second query to retrieve the newly created/updated object, filtering the restricted queryset with the object's primary key. If this query fails to return the object, Nautobot knows that the new revision does not match the constraints imposed by the permission. The transaction is then rolled back, leaving the database in its original state prior to the change, and the user is informed of the violation.","title":"Creating and Modifying Objects"},{"location":"administration/permissions.html#assigning-permissions","text":"Permissions are implemented by assigning them to specific users and/or to groups of users. Users can have a combination of permissions and groups assigned to their account. All of the permissions granted to the user's groups and directly to the user's account will be used when determining authorization to access an object or view.","title":"Assigning Permissions"},{"location":"administration/permissions.html#assigning-permissions-to-individual-users","text":"Permissions can be related directly to users from the Admin UI or the API: - Admin UI API Superusers Yes Yes Staff users with users.add_permission or users.change_permission Yes Yes Regular users with users.add_permission or users.change_permission No Yes Multiple permissions can be assigned to a user account. Info User permission relationships can be managed in the Admin UI by modifying the user or the permission. Warning Granting a user users.change_permission or users.add_permission gives the user the ability to modify their own permissions. This permission should be restricted to trusted accounts and should be considered the same as giving a user full access.","title":"Assigning Permissions to Individual Users"},{"location":"administration/permissions.html#creating-groups","text":"Groups of users can be created to provide role-based access control and simplify user permissions management. Permissions related to a group will apply to all users in the group. A user can belong to any number of groups. Groups can be created from the Admin UI or the API: - Admin UI API Superusers Yes Yes Users with auth.add_group or auth.change_group No Yes","title":"Creating Groups"},{"location":"administration/permissions.html#adding-users-to-groups","text":"Users can be added to groups through the Admin UI by superusers or automatically assigned to externally authenticated users through the EXTERNAL_AUTH_DEFAULT_GROUPS and EXTERNAL_AUTH_DEFAULT_PERMISSIONS settings. Nautobot groups can optionally be mapped to LDAP groups when using LDAP authentication .","title":"Adding Users to Groups"},{"location":"administration/permissions.html#assigning-permissions-to-groups","text":"Permissions can be related to groups by superusers or users with users.add_permission or users.change_permission permissions. - Admin UI API Superusers Yes Yes Staff users with users.add_permission or users.change_permission Yes Yes Regular users with users.add_permission or users.change_permission No Yes Multiple permissions can be assigned to a user group. Info Group permission relationships can be managed in the Admin UI by modifying the group (superusers only) or the permission.","title":"Assigning Permissions to Groups"},{"location":"administration/replicating-nautobot.html","text":"Replicating Nautobot \u00b6 Replicating the Database \u00b6 Nautobot employs a PostgreSQL database, so general PostgreSQL best practices apply here. The database can be written to a file and restored using the pg_dump and psql utilities, respectively. Note The examples below assume that your database is named nautobot . Export the Database \u00b6 Use the pg_dump utility to export the entire database to a file: pg_dump nautobot > nautobot.sql When replicating a production database for development purposes, you may find it convenient to exclude changelog data, which can easily account for the bulk of a database's size. To do this, exclude the extras_objectchange table data from the export. The table will still be included in the output file, but will not be populated with any data. pg_dump --exclude-table-data=extras_objectchange nautobot > nautobot.sql Load an Exported Database \u00b6 When restoring a database from a file, it's recommended to delete any existing database first to avoid potential conflicts. Warning The following will destroy and replace any existing instance of the database. psql -c 'drop database nautobot' psql -c 'create database nautobot' psql nautobot < nautobot.sql Keep in mind that PostgreSQL user accounts and permissions are not included with the dump: You will need to create those manually if you want to fully replicate the original database (see the installation docs ). When setting up a development instance of Nautobot, it's strongly recommended to use different credentials anyway. Export the Database Schema \u00b6 If you want to export only the database schema, and not the data itself (e.g. for development reference), do the following: pg_dump -s nautobot > nautobot_schema.sql Replicating Uploaded Media \u00b6 By default, Nautobot stores uploaded files (such as image attachments) in its media directory. To fully replicate an instance of Nautobot, you'll need to copy both the database and the media files. Note These operations are not necessary if your installation is utilizing a remote storage backend . Archive the Media Directory \u00b6 Execute the following command (which may need to be changed if you're using non-default storage path settings): tar -czf nautobot_media.tar.gz $NAUTOBOT_ROOT/media/ Restore the Media Directory \u00b6 To extract the saved archive into a new installation, run the following from the installation root: tar -xf nautobot_media.tar.gz Cache Invalidation \u00b6 If you are migrating your instance of Nautobot to a different machine, be sure to first invalidate the cache on the original instance by issuing the invalidate all management command (within the Python virtual environment): $ nautobot-server invalidate all","title":"Replicating Nautobot"},{"location":"administration/replicating-nautobot.html#replicating-nautobot","text":"","title":"Replicating Nautobot"},{"location":"administration/replicating-nautobot.html#replicating-the-database","text":"Nautobot employs a PostgreSQL database, so general PostgreSQL best practices apply here. The database can be written to a file and restored using the pg_dump and psql utilities, respectively. Note The examples below assume that your database is named nautobot .","title":"Replicating the Database"},{"location":"administration/replicating-nautobot.html#export-the-database","text":"Use the pg_dump utility to export the entire database to a file: pg_dump nautobot > nautobot.sql When replicating a production database for development purposes, you may find it convenient to exclude changelog data, which can easily account for the bulk of a database's size. To do this, exclude the extras_objectchange table data from the export. The table will still be included in the output file, but will not be populated with any data. pg_dump --exclude-table-data=extras_objectchange nautobot > nautobot.sql","title":"Export the Database"},{"location":"administration/replicating-nautobot.html#load-an-exported-database","text":"When restoring a database from a file, it's recommended to delete any existing database first to avoid potential conflicts. Warning The following will destroy and replace any existing instance of the database. psql -c 'drop database nautobot' psql -c 'create database nautobot' psql nautobot < nautobot.sql Keep in mind that PostgreSQL user accounts and permissions are not included with the dump: You will need to create those manually if you want to fully replicate the original database (see the installation docs ). When setting up a development instance of Nautobot, it's strongly recommended to use different credentials anyway.","title":"Load an Exported Database"},{"location":"administration/replicating-nautobot.html#export-the-database-schema","text":"If you want to export only the database schema, and not the data itself (e.g. for development reference), do the following: pg_dump -s nautobot > nautobot_schema.sql","title":"Export the Database Schema"},{"location":"administration/replicating-nautobot.html#replicating-uploaded-media","text":"By default, Nautobot stores uploaded files (such as image attachments) in its media directory. To fully replicate an instance of Nautobot, you'll need to copy both the database and the media files. Note These operations are not necessary if your installation is utilizing a remote storage backend .","title":"Replicating Uploaded Media"},{"location":"administration/replicating-nautobot.html#archive-the-media-directory","text":"Execute the following command (which may need to be changed if you're using non-default storage path settings): tar -czf nautobot_media.tar.gz $NAUTOBOT_ROOT/media/","title":"Archive the Media Directory"},{"location":"administration/replicating-nautobot.html#restore-the-media-directory","text":"To extract the saved archive into a new installation, run the following from the installation root: tar -xf nautobot_media.tar.gz","title":"Restore the Media Directory"},{"location":"administration/replicating-nautobot.html#cache-invalidation","text":"If you are migrating your instance of Nautobot to a different machine, be sure to first invalidate the cache on the original instance by issuing the invalidate all management command (within the Python virtual environment): $ nautobot-server invalidate all","title":"Cache Invalidation"},{"location":"apps/index.html","text":"Nautobot Apps Overview \u00b6 To view and search the full list of Apps, head over to the Nautobot App Ecosystem Page on networktocode.com . Below you will only find links to the documentation for the apps listed in the table. Community-Developed Nautobot Apps \u00b6 Find out more about what documentation is available for the various community developed Nautobot Apps by visiting the Community Apps page. Network to Code Nautobot Apps \u00b6 These Nautobot Apps have their documentation hosted as a subproject of this docs site and they are built and structured according to the Network To Code documentation standards. App Name Description Nautobot Golden Configuration Automate configuration backups, perform configuration compliance, and generate intended configurations. Nautobot Device Onboarding Simplify the onboarding process of a new device by allowing the user to specify a small amount of info and having the plugin populate a much larger amount of device data in Nautobot. Nautobot Single Source of Truth (SSoT) Integrate and synchronize data between various \"source of truth\" (SoT) systems, with Nautobot acting as a central clearinghouse for data - a Single Source of Truth Nautobot Device Lifecycle Management Make related associations to Devices, Device Types, and Inventory Items to help provide data about the hardware end of life notices, appropriate software versions to be running on the devices, and the maintenance contracts associated with devices. Nautobot Firewall Models Construct firewall policies in Nautobot with the help of the provided collection of relevant models. Nautobot ChatOps Add chatbot functionality to Nautobot to facilitate getting data from Nautobot directly from a chat platform. The ChatOps app is a multi-platform chatbot for network operations and engineering teams. It is built to seamlessly work across Slack, MS Teams, WebEx Teams, and Mattermost, but also as a framework to help developers add more chat platforms in the future.","title":"Overview"},{"location":"apps/index.html#nautobot-apps-overview","text":"To view and search the full list of Apps, head over to the Nautobot App Ecosystem Page on networktocode.com . Below you will only find links to the documentation for the apps listed in the table.","title":"Nautobot Apps Overview"},{"location":"apps/index.html#community-developed-nautobot-apps","text":"Find out more about what documentation is available for the various community developed Nautobot Apps by visiting the Community Apps page.","title":"Community-Developed Nautobot Apps"},{"location":"apps/index.html#network-to-code-nautobot-apps","text":"These Nautobot Apps have their documentation hosted as a subproject of this docs site and they are built and structured according to the Network To Code documentation standards. App Name Description Nautobot Golden Configuration Automate configuration backups, perform configuration compliance, and generate intended configurations. Nautobot Device Onboarding Simplify the onboarding process of a new device by allowing the user to specify a small amount of info and having the plugin populate a much larger amount of device data in Nautobot. Nautobot Single Source of Truth (SSoT) Integrate and synchronize data between various \"source of truth\" (SoT) systems, with Nautobot acting as a central clearinghouse for data - a Single Source of Truth Nautobot Device Lifecycle Management Make related associations to Devices, Device Types, and Inventory Items to help provide data about the hardware end of life notices, appropriate software versions to be running on the devices, and the maintenance contracts associated with devices. Nautobot Firewall Models Construct firewall policies in Nautobot with the help of the provided collection of relevant models. Nautobot ChatOps Add chatbot functionality to Nautobot to facilitate getting data from Nautobot directly from a chat platform. The ChatOps app is a multi-platform chatbot for network operations and engineering teams. It is built to seamlessly work across Slack, MS Teams, WebEx Teams, and Mattermost, but also as a framework to help developers add more chat platforms in the future.","title":"Network to Code Nautobot Apps"},{"location":"apps/nautobot-apps.html","text":"Nautobot Community Apps \u00b6 The following is a manually curated list of Apps/Plugins from the wider Nautobot community, which have some sort of documentation. App Name Links Description Nautobot SSoT vSphere Docs GitHub A plugin for Nautobot that leverages the SSoT plugin to create Virtual Machines, VMInterfaces, IPAddresses, Clusters, and Cluster Groups from VMWare vSphere. GWDG Networking Team Plugins GitLab nautobot-move nautobot-bulk-connect nautobot-cable-utils nautobot-evpn nautobot-sfp-inventory nautobot-type-reapply and more.","title":"Community Apps"},{"location":"apps/nautobot-apps.html#nautobot-community-apps","text":"The following is a manually curated list of Apps/Plugins from the wider Nautobot community, which have some sort of documentation. App Name Links Description Nautobot SSoT vSphere Docs GitHub A plugin for Nautobot that leverages the SSoT plugin to create Virtual Machines, VMInterfaces, IPAddresses, Clusters, and Cluster Groups from VMWare vSphere. GWDG Networking Team Plugins GitLab nautobot-move nautobot-bulk-connect nautobot-cable-utils nautobot-evpn nautobot-sfp-inventory nautobot-type-reapply and more.","title":"Nautobot Community Apps"},{"location":"configuration/index.html","text":"Nautobot Configuration \u00b6 This section describes how to get started with configuring Nautobot. Initializing the Configuration \u00b6 An initial configuration can be created by executing nautobot-server init . This will generate a new configuration with all of the default settings provided for you, and will also generate a unique SECRET_KEY . By default (if you haven't set NAUTOBOT_ROOT to some other value), the file will be created at $HOME/.nautobot/nautobot_config.py : $ nautobot-server init Configuration file created at '/opt/nautobot/nautobot_config.py' Tip The Nautobot Installation Docs example sets NAUTOBOT_ROOT to /opt/nautobot , so nautobot_config.py would be found at /opt/nautobot/nautobot_config.py . You may specify a different location for the configuration as the argument to init : $ nautobot-server init /tmp/custom_config.py Configuration file created at '/tmp/custom_config.py' Note Throughout the documentation, the configuration file will be referred to by name as nautobot_config.py . If you use a custom file name, you must use that instead. Specifying your Configuration \u00b6 If you place your configuration in the default location at $HOME/.nautobot/nautobot_config.py , you may utilize the nautobot-server command and it will use that location automatically. If you do not wish to utilize the default location, you have two options: Config argument \u00b6 You may provide the --config argument when executing nautobot-server to tell Nautobot where to find your configuration. For example, to start a shell with the configuration in an alternate location: $ nautobot-server --config=/etc/nautobot_config.py nbshell Environment variable \u00b6 You may also set the NAUTOBOT_CONFIG environment variable to the location of your configuration file so that you don't have to keep providing the --config argument. If set, this overrides the default location. $ export NAUTOBOT_CONFIG=/etc/nautobot_config.py $ nautobot-server nbshell Nautobot Root Directory \u00b6 By default, Nautobot will always read or store files in ~/.nautobot to allow for installation without requiring superuser (root) permissions. The NAUTOBOT_ROOT configuration setting specifies where these files will be stored on your file system. You may customize this location by setting the NAUTOBOT_ROOT environment variable. For example: $ export NAUTOBOT_ROOT=/opt/nautobot This setting is also used in the Nautobot deployment guide to make the nautobot-server command easier to find and use. Note The --config argument and the NAUTOBOT_CONFIG environment variable will always take precedence over NAUTOBOT_ROOT for the purpose of telling Nautobot where your nautobot_config.py can be found. Warning Do not override NAUTOBOT_ROOT in your nautobot_config.py . It will not work as expected. If you need to customize this setting, please always set the NAUTOBOT_ROOT environment variable. File Storage \u00b6 Nautobot is capable of storing various types of files. This includes Jobs , Git repositories , image attachments , and static files (CSS, JavaScript, etc.). Each of the features requiring use of file storage default to being stored in NAUTOBOT_ROOT . If desired, you may customize each one individually. Please see each feature's respective documentation linked above for how to do that. Configuration Parameters \u00b6 While Nautobot has many configuration settings, only a few of them must be defined at the time of installation. These configuration parameters may be set in nautobot_config.py or by default many of them may also be set by environment variables. Please see the following links for more information: Required settings Optional settings Optional Authentication Configuration \u00b6 LDAP Authentication Remote User Authentication SSO Authentication Changing the Configuration \u00b6 Configuration settings may be changed at any time. However, the WSGI service (e.g. uWSGI) must be restarted before the changes will take effect. For example, if you're running Nautobot using systemd: $ sudo systemctl restart nautobot nautobot-worker Advanced Configuration \u00b6 Troubleshooting the Configuration \u00b6 To facilitate troubleshooting and debugging of settings, try inspecting the settings from a shell. First get a shell and load the Django settings: $ nautobot-server nbshell ### Nautobot interactive shell (localhost) ### Python 3.9.1 | Django 3.1.3 | Nautobot 1.0.0 ### lsmodels() will show available models. Use help(<model>) for more info. >>> from django.conf import settings Inspect the SETTINGS_PATH variable. Does it match the configuration you're expecting to be loading? >>> settings.SETTINGS_PATH '/home/example/.nautobot/nautobot_config.py' If not, double check that you haven't set the NAUTOBOT_CONFIG environment variable, or if you did, that the path defined there is the correct one. $ echo $NAUTOBOT_CONFIG Adding your own dependencies \u00b6 Warning Be cautious not to confuse extra applications with Nautobot plugins which are installed using the PLUGINS setting. They are similar, but distinctly different! Nautobot, being a Django application, allows for installation of additional dependencies utilizing the INSTALLED_APPS settings. Due to the highly specialized nature of Nautobot, you cannot safely do this . For example, let's assume that you want to install the popular django-health-check plugin to your Nautobot deployment which requires you to add one or more health_check entries to your INSTALLED_APPS . If you attempt to modify INSTALLED_APPS yourself, you might see an error such as this: Traceback ( most recent call last ): File \"/usr/local/bin/nautobot-server\" , line 8 , in < module > sys . exit ( main ()) File \"/usr/local/lib/python3.7/site-packages/nautobot/core/cli.py\" , line 53 , in main initializer = _configure_settings , # Called after defaults File \"/usr/local/lib/python3.7/site-packages/nautobot/core/runner/runner.py\" , line 193 , in run_app management . execute_from_command_line ([ runner_name , command ] + command_args ) File \"/usr/local/lib/python3.7/site-packages/django/core/management/__init__.py\" , line 401 , in execute_from_command_line utility . execute () File \"/usr/local/lib/python3.7/site-packages/django/core/management/__init__.py\" , line 377 , in execute django . setup () File \"/usr/local/lib/python3.7/site-packages/django/__init__.py\" , line 24 , in setup apps . populate ( settings . INSTALLED_APPS ) File \"/usr/local/lib/python3.7/site-packages/django/apps/registry.py\" , line 95 , in populate \"duplicates: %s \" % app_config . label ) django . core . exceptions . ImproperlyConfigured : Application labels aren 't unique, duplicates: health_check To make it work, you would simply specify EXTRA_INSTALLED_APPS instead: EXTRA_INSTALLED_APPS = [ 'health_check' , ... ] For more information on installing extra applications, please see the documentation on Extra Applications . For more information on installing or developing Nautobot plugins, please see the documentation on Plugins .","title":"Configuring Nautobot"},{"location":"configuration/index.html#nautobot-configuration","text":"This section describes how to get started with configuring Nautobot.","title":"Nautobot Configuration"},{"location":"configuration/index.html#initializing-the-configuration","text":"An initial configuration can be created by executing nautobot-server init . This will generate a new configuration with all of the default settings provided for you, and will also generate a unique SECRET_KEY . By default (if you haven't set NAUTOBOT_ROOT to some other value), the file will be created at $HOME/.nautobot/nautobot_config.py : $ nautobot-server init Configuration file created at '/opt/nautobot/nautobot_config.py' Tip The Nautobot Installation Docs example sets NAUTOBOT_ROOT to /opt/nautobot , so nautobot_config.py would be found at /opt/nautobot/nautobot_config.py . You may specify a different location for the configuration as the argument to init : $ nautobot-server init /tmp/custom_config.py Configuration file created at '/tmp/custom_config.py' Note Throughout the documentation, the configuration file will be referred to by name as nautobot_config.py . If you use a custom file name, you must use that instead.","title":"Initializing the Configuration"},{"location":"configuration/index.html#specifying-your-configuration","text":"If you place your configuration in the default location at $HOME/.nautobot/nautobot_config.py , you may utilize the nautobot-server command and it will use that location automatically. If you do not wish to utilize the default location, you have two options:","title":"Specifying your Configuration"},{"location":"configuration/index.html#config-argument","text":"You may provide the --config argument when executing nautobot-server to tell Nautobot where to find your configuration. For example, to start a shell with the configuration in an alternate location: $ nautobot-server --config=/etc/nautobot_config.py nbshell","title":"Config argument"},{"location":"configuration/index.html#environment-variable","text":"You may also set the NAUTOBOT_CONFIG environment variable to the location of your configuration file so that you don't have to keep providing the --config argument. If set, this overrides the default location. $ export NAUTOBOT_CONFIG=/etc/nautobot_config.py $ nautobot-server nbshell","title":"Environment variable"},{"location":"configuration/index.html#nautobot-root-directory","text":"By default, Nautobot will always read or store files in ~/.nautobot to allow for installation without requiring superuser (root) permissions. The NAUTOBOT_ROOT configuration setting specifies where these files will be stored on your file system. You may customize this location by setting the NAUTOBOT_ROOT environment variable. For example: $ export NAUTOBOT_ROOT=/opt/nautobot This setting is also used in the Nautobot deployment guide to make the nautobot-server command easier to find and use. Note The --config argument and the NAUTOBOT_CONFIG environment variable will always take precedence over NAUTOBOT_ROOT for the purpose of telling Nautobot where your nautobot_config.py can be found. Warning Do not override NAUTOBOT_ROOT in your nautobot_config.py . It will not work as expected. If you need to customize this setting, please always set the NAUTOBOT_ROOT environment variable.","title":"Nautobot Root Directory"},{"location":"configuration/index.html#file-storage","text":"Nautobot is capable of storing various types of files. This includes Jobs , Git repositories , image attachments , and static files (CSS, JavaScript, etc.). Each of the features requiring use of file storage default to being stored in NAUTOBOT_ROOT . If desired, you may customize each one individually. Please see each feature's respective documentation linked above for how to do that.","title":"File Storage"},{"location":"configuration/index.html#configuration-parameters","text":"While Nautobot has many configuration settings, only a few of them must be defined at the time of installation. These configuration parameters may be set in nautobot_config.py or by default many of them may also be set by environment variables. Please see the following links for more information: Required settings Optional settings","title":"Configuration Parameters"},{"location":"configuration/index.html#optional-authentication-configuration","text":"LDAP Authentication Remote User Authentication SSO Authentication","title":"Optional Authentication Configuration"},{"location":"configuration/index.html#changing-the-configuration","text":"Configuration settings may be changed at any time. However, the WSGI service (e.g. uWSGI) must be restarted before the changes will take effect. For example, if you're running Nautobot using systemd: $ sudo systemctl restart nautobot nautobot-worker","title":"Changing the Configuration"},{"location":"configuration/index.html#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"configuration/index.html#troubleshooting-the-configuration","text":"To facilitate troubleshooting and debugging of settings, try inspecting the settings from a shell. First get a shell and load the Django settings: $ nautobot-server nbshell ### Nautobot interactive shell (localhost) ### Python 3.9.1 | Django 3.1.3 | Nautobot 1.0.0 ### lsmodels() will show available models. Use help(<model>) for more info. >>> from django.conf import settings Inspect the SETTINGS_PATH variable. Does it match the configuration you're expecting to be loading? >>> settings.SETTINGS_PATH '/home/example/.nautobot/nautobot_config.py' If not, double check that you haven't set the NAUTOBOT_CONFIG environment variable, or if you did, that the path defined there is the correct one. $ echo $NAUTOBOT_CONFIG","title":"Troubleshooting the Configuration"},{"location":"configuration/index.html#adding-your-own-dependencies","text":"Warning Be cautious not to confuse extra applications with Nautobot plugins which are installed using the PLUGINS setting. They are similar, but distinctly different! Nautobot, being a Django application, allows for installation of additional dependencies utilizing the INSTALLED_APPS settings. Due to the highly specialized nature of Nautobot, you cannot safely do this . For example, let's assume that you want to install the popular django-health-check plugin to your Nautobot deployment which requires you to add one or more health_check entries to your INSTALLED_APPS . If you attempt to modify INSTALLED_APPS yourself, you might see an error such as this: Traceback ( most recent call last ): File \"/usr/local/bin/nautobot-server\" , line 8 , in < module > sys . exit ( main ()) File \"/usr/local/lib/python3.7/site-packages/nautobot/core/cli.py\" , line 53 , in main initializer = _configure_settings , # Called after defaults File \"/usr/local/lib/python3.7/site-packages/nautobot/core/runner/runner.py\" , line 193 , in run_app management . execute_from_command_line ([ runner_name , command ] + command_args ) File \"/usr/local/lib/python3.7/site-packages/django/core/management/__init__.py\" , line 401 , in execute_from_command_line utility . execute () File \"/usr/local/lib/python3.7/site-packages/django/core/management/__init__.py\" , line 377 , in execute django . setup () File \"/usr/local/lib/python3.7/site-packages/django/__init__.py\" , line 24 , in setup apps . populate ( settings . INSTALLED_APPS ) File \"/usr/local/lib/python3.7/site-packages/django/apps/registry.py\" , line 95 , in populate \"duplicates: %s \" % app_config . label ) django . core . exceptions . ImproperlyConfigured : Application labels aren 't unique, duplicates: health_check To make it work, you would simply specify EXTRA_INSTALLED_APPS instead: EXTRA_INSTALLED_APPS = [ 'health_check' , ... ] For more information on installing extra applications, please see the documentation on Extra Applications . For more information on installing or developing Nautobot plugins, please see the documentation on Plugins .","title":"Adding your own dependencies"},{"location":"configuration/optional-settings.html","text":"Optional Configuration Settings \u00b6 Administratively Configurable Settings \u00b6 Added in version 1.2.0 A number of settings can alternatively be configured via the Nautobot Admin UI. To do so, these settings must not be defined in your nautobot_config.py , as any settings defined there will take precedence over any values defined in the Admin UI. Settings that are currently configurable via the Admin UI include: BANNER_BOTTOM BANNER_LOGIN BANNER_TOP CHANGELOG_RETENTION HIDE_RESTRICTED_UI MAX_PAGE_SIZE PAGINATE_COUNT PER_PAGE_DEFAULTS PREFER_IPV4 RACK_ELEVATION_DEFAULT_UNIT_HEIGHT RACK_ELEVATION_DEFAULT_UNIT_WIDTH RELEASE_CHECK_TIMEOUT RELEASE_CHECK_URL Extra Applications \u00b6 A need may arise to allow the user to register additional settings. These will automatically apply based on keynames prefixed with EXTRA_ assuming the base key (the latter part of the setting name) is of type list or tuple. For example, to register additional INSTALLED_APPS , you would simply specify this in your custom (user) configuration:: EXTRA_INSTALLED_APPS = [ 'foo.bar' , ] This will ensure your default setting's INSTALLED_APPS do not have to be modified, and the user can specify additional apps with ease. Similarly, additional MIDDLEWARE can be added using EXTRA_MIDDLEWARE . ALLOWED_URL_SCHEMES \u00b6 Default: ('file', 'ftp', 'ftps', 'http', 'https', 'irc', 'mailto', 'sftp', 'ssh', 'tel', 'telnet', 'tftp', 'vnc', 'xmpp') A list of permitted URL schemes referenced when rendering links within Nautobot. Note that only the schemes specified in this list will be accepted: If adding your own, be sure to replicate all of the default values as well (excluding those schemes which are not desirable). BANNER_TOP \u00b6 BANNER_BOTTOM \u00b6 Default: \"\" (Empty string) Setting these variables will display custom content in a banner at the top and/or bottom of the page, respectively. HTML is allowed. To replicate the content of the top banner in the bottom banner, set: BANNER_TOP = 'Your banner text' BANNER_BOTTOM = BANNER_TOP Added in version 1.2.0 If you do not set a value for these settings in your nautobot_config.py , they can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for either setting in nautobot_config.py , it will override any dynamically configured value. BANNER_LOGIN \u00b6 Default: \"\" (Empty string) This defines custom content to be displayed on the login page above the login form. HTML is allowed. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value. BRANDING_FILEPATHS \u00b6 Default: { \"logo\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_LOGO\" , None ), # Navbar logo \"favicon\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_FAVICON\" , None ), # Browser favicon \"icon_16\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_ICON_16\" , None ), # 16x16px icon \"icon_32\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_ICON_32\" , None ), # 32x32px icon \"icon_180\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_ICON_180\" , None ), # 180x180px icon - used for the apple-touch-icon header \"icon_192\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_ICON_192\" , None ), # 192x192px icon \"icon_mask\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_ICON_MASK\" , None ), # mono-chrome icon used for the mask-icon header } A set of filepaths relative to the MEDIA_ROOT which locate image assets used for custom branding. Each of these assets takes the place of the corresponding stock Nautobot asset. This allows for instance, providing your own navbar logo and favicon. These environment variables may be used to specify the values: NAUTOBOT_BRANDING_FILEPATHS_LOGO NAUTOBOT_BRANDING_FILEPATHS_FAVICON NAUTOBOT_BRANDING_FILEPATHS_ICON_16 NAUTOBOT_BRANDING_FILEPATHS_ICON_32 NAUTOBOT_BRANDING_FILEPATHS_ICON_180 NAUTOBOT_BRANDING_FILEPATHS_ICON_192 NAUTOBOT_BRANDING_FILEPATHS_ICON_MASK If a custom image asset is not provided for any of the above options, the stock Nautobot asset is used. BRANDING_PREPENDED_FILENAME \u00b6 Added in version 1.3.4 Default: \"nautobot_\" Environment Variable: NAUTOBOT_BRANDING_PREPENDED_FILENAME Defines the prefix of the filename when exporting to CSV/YAML or export templates. BRANDING_TITLE \u00b6 Default: \"Nautobot\" Environment Variable: NAUTOBOT_BRANDING_TITLE The defines the custom branding title that should be used in place of \"Nautobot\" within user facing areas of the application like the HTML title of web pages. BRANDING_URLS \u00b6 Default: { \"code\" : os . getenv ( \"NAUTOBOT_BRANDING_URLS_CODE\" , \"https://github.com/nautobot/nautobot\" ), # Code link in the footer \"docs\" : os . getenv ( \"NAUTOBOT_BRANDING_URLS_DOCS\" , \"<STATIC_URL>docs/index.html\" ), # Docs link in the footer \"help\" : os . getenv ( \"NAUTOBOT_BRANDING_URLS_HELP\" , \"https://github.com/nautobot/nautobot/wiki\" ), # Help link in the footer } A set of URLs that correspond to helpful links in the right of the footer on every web page. These environment variables may be used to specify the values: NAUTOBOT_BRANDING_URLS_CODE NAUTOBOT_BRANDING_URLS_DOCS NAUTOBOT_BRANDING_URLS_HELP If a custom URL is not provided for any of the links, the default link within the Nautobot community is used. CACHEOPS_DEFAULTS \u00b6 Default: {'timeout': 900} (15 minutes, in seconds) Environment Variable: NAUTOBOT_CACHEOPS_TIMEOUT (timeout value only) Warning It is an error to set the timeout value to 0 . If you wish to disable caching, please use CACHEOPS_ENABLED . Various defaults for caching, the most important of which being the cache timeout. The timeout is the number of seconds that cache entries will be retained before expiring. CACHEOPS_ENABLED \u00b6 Default: True Environment Variable: NAUTOBOT_CACHEOPS_ENABLED A boolean that turns on/off caching. If set to False , all caching is bypassed and Nautobot operates as if there is no cache. CACHEOPS_HEALTH_CHECK_ENABLED \u00b6 Default: False A boolean that turns on/off health checks for the Redis server connection utilized by Cacheops. Most deployments share a Redis server with django-redis as such we only need to check the health of Redis one time. If you are using a separate Redis deployment for Cacheops, please consider enabling this to monitor that Redis deployment. Keep in mind the more health checks enabled the longer the health checks will take and timeouts might need to be increased. CACHEOPS_REDIS \u00b6 Default: 'redis://localhost:6379/1' Environment Variable: NAUTOBOT_CACHEOPS_REDIS The Redis connection string to use for caching. CELERY_BROKER_TRANSPORT_OPTIONS \u00b6 Default: {} A dict of additional options passed to the Celery broker transport. This is only required when configuring Celery to utilize Redis Sentinel . CELERY_BROKER_URL \u00b6 Environment Variable: NAUTOBOT_CELERY_BROKER_URL Default: 'redis://localhost:6379/0' Celery broker URL used to tell workers where queues are located. CELERY_RESULT_BACKEND \u00b6 Environment Variable: NAUTOBOT_CELERY_RESULT_BACKEND Default: 'redis://localhost:6379/0' Celery result backend used to tell workers where to store task results (tombstones). CELERY_RESULT_BACKEND_TRANSPORT_OPTIONS \u00b6 Default: {} A dict of additional options passed to the Celery result backend transport. This is only required when configuring Celery to utilize Redis Sentinel . CELERY_TASK_SOFT_TIME_LIMIT \u00b6 Default: 300 (5 minutes) Environment Variable: NAUTOBOT_CELERY_TASK_SOFT_TIME_LIMIT The global Celery task soft timeout (in seconds). Any background task that exceeds this duration will receive a SoftTimeLimitExceeded exception and is responsible for handling this exception and performing any necessary cleanup or final operations before ending. See also CELERY_TASK_TIME_LIMIT below. CELERY_TASK_TIME_LIMIT \u00b6 Default: 600 (10 minutes) Environment Variable: NAUTOBOT_CELERY_TASK_TIME_LIMIT The global Celery task hard timeout (in seconds). Any background task that exceeds this duration will be forcibly killed with a SIGKILL signal. CHANGELOG_RETENTION \u00b6 Default: 90 The number of days to retain logged changes (object creations, updates, and deletions). Set this to 0 to retain changes in the database indefinitely. Warning If enabling indefinite changelog retention, it is recommended to periodically delete old entries. Otherwise, the database may eventually exceed capacity. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value. CORS_ALLOW_ALL_ORIGINS \u00b6 Default: False Environment Variable: NAUTOBOT_CORS_ALLOW_ALL_ORIGINS If True , all origins will be allowed. Other settings restricting allowed origins will be ignored. Setting this to True can be dangerous, as it allows any website to make cross-origin requests to yours. Generally you'll want to restrict the list of allowed origins with CORS_ALLOWED_ORIGINS or CORS_ALLOWED_ORIGIN_REGEXES . Previously this setting was called CORS_ORIGIN_ALLOW_ALL , which still works as an alias, with the new name taking precedence. CORS_ALLOWED_ORIGINS \u00b6 Default: [] (Empty list) A list of origins that are authorized to make cross-site HTTP requests. An Origin is defined by the CORS RFC Section 3.2 as a URI scheme + hostname + port , or one of the special values 'null' or 'file://' . Default ports (HTTPS = 443, HTTP = 80) are optional here. The special value null is sent by the browser in \"privacy-sensitive contexts\" , such as when the client is running from a file:// domain. The special value file:// is sent accidentally by some versions of Chrome on Android as per this bug. Example: CORS_ALLOWED_ORIGINS = [ \"https://example.com\" , \"https://sub.example.com\" , \"http://localhost:8080\" , \"http://127.0.0.1:9000\" ] Previously this setting was called CORS_ORIGIN_WHITELIST , which still works as an alias, with the new name taking precedence. CORS_ALLOWED_ORIGIN_REGEXES \u00b6 Default: [] A list of strings representing regexes that match Origins that are authorized to make cross-site HTTP requests. Useful when CORS_ALLOWED_ORIGINS is impractical, such as when you have a large number of subdomains. Example: CORS_ALLOWED_ORIGIN_REGEXES = [ r \"^https://\\w+\\.example\\.com$\" , ] Previously this setting was called CORS_ORIGIN_REGEX_WHITELIST , which still works as an alias, with the new name taking precedence. DISABLE_PREFIX_LIST_HIERARCHY \u00b6 Default: False Environment Variable: NAUTOBOT_DISABLE_PREFIX_LIST_HIERARCHY This setting disables rendering of the IP prefix hierarchy (parent/child relationships) in the IPAM prefix list view. With large sets of prefixes, users may encounter a performance penalty when trying to load the prefix list view due to the nature of calculating the parent/child relationships. This setting allows users to disable the hierarchy and instead only render a flat list of all prefixes in the table. A later release of Nautobot will address the underlying performance issues, and likely remove this configuration option. ENFORCE_GLOBAL_UNIQUE \u00b6 Default: False Environment Variable: NAUTOBOT_ENFORCE_GLOBAL_UNIQUE By default, Nautobot will permit users to create duplicate prefixes and IP addresses in the global table (that is, those which are not assigned to any VRF). This behavior can be disabled by setting ENFORCE_GLOBAL_UNIQUE to True . EXEMPT_VIEW_PERMISSIONS \u00b6 Default: [] (Empty list) A list of Nautobot models to exempt from the enforcement of view permissions. Models listed here will be viewable by all users, both authenticated and anonymous. List models in the form <app>.<model> . For example: EXEMPT_VIEW_PERMISSIONS = [ 'dcim.site' , 'dcim.region' , 'ipam.prefix' , ] To exempt all models from view permission enforcement, set the following. (Note that EXEMPT_VIEW_PERMISSIONS must be an iterable.) EXEMPT_VIEW_PERMISSIONS = [ '*' ] Note Using a wildcard will not affect certain potentially sensitive models, such as user permissions. If there is a need to exempt these models, they must be specified individually. EXTERNAL_AUTH_DEFAULT_GROUPS \u00b6 Default: [] (Empty list) The list of group names to assign a new user account when created using 3rd-party authentication. EXTERNAL_AUTH_DEFAULT_PERMISSIONS \u00b6 Default: {} (Empty dictionary) A mapping of permissions to assign a new user account when created using SSO authentication. Each key in the dictionary will be the permission name specified as <app_label>.<action>_<model> , and the value should be set to the permission constraints , or None to allow all objects. Example Permissions \u00b6 Permission Description {'dcim.view_device': {}} or {'dcim.view_device': None} Users can view all devices {'dcim.add_device': {}} Users can add devices, see note below {'dcim.view_device': {\"site__name__in\": [\"HQ\"]}} Users can view all devices in the HQ site Warning Permissions can be complicated! Be careful when restricting permissions to also add any required prerequisite permissions. For example, when adding Devices the Device Role, Device Type, Site, and Status fields are all required fields in order for the UI to function properly. Users will also need view permissions for those fields or the corresponding field selections in the UI will be unavailable and potentially prevent objects from being able to be created or edited. The following example gives a user a reasonable amount of access to add devices to a single site (HQ in this case): { 'dcim.add_device' : { \"site__name__in\" : [ \"HQ\" ]}, 'dcim.view_device' : { \"site__name__in\" : [ \"HQ\" ]}, 'dcim.view_devicerole' : None , 'dcim.view_devicetype' : None , 'extras.view_status' : None , 'dcim.view_site' : { \"name__in\" : [ \"HQ\" ]}, 'dcim.view_manufacturer' : None , 'dcim.view_region' : None , 'dcim.view_rack' : None , 'dcim.view_rackgroup' : None , 'dcim.view_platform' : None , 'virtualization.view_cluster' : None , 'virtualization.view_clustergroup' : None , 'tenancy.view_tenant' : None , 'tenancy.view_tenantgroup' : None , } Please see the object permissions page for more information. GIT_ROOT \u00b6 Default: os.path.join(NAUTOBOT_ROOT, \"git\") Environment Variable: NAUTOBOT_GIT_ROOT The file path to a directory where cloned Git repositories will be located. The value of this variable can also be customized by setting the environment variable NAUTOBOT_GIT_ROOT to a directory path of your choosing. GRAPHQL_CUSTOM_FIELD_PREFIX \u00b6 Default: cf By default, all custom fields in GraphQL will be prefixed with cf . A custom field name my_field will appear in GraphQL as cf_my_field by default. It's possible to change or remove the prefix by setting the value of GRAPHQL_CUSTOM_FIELD_PREFIX . HIDE_RESTRICTED_UI \u00b6 Default: False When set to True , users with limited permissions will only be able to see items in the UI they have access to. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value. Added in version 1.3.10 When this setting is set to True , logged out users will be redirected to the login page when navigating to the Nautobot home page. HTTP_PROXIES \u00b6 Default: None (Disabled) A dictionary of HTTP proxies to use for outbound requests originating from Nautobot (e.g. when sending webhook requests). Proxies should be specified by schema (HTTP and HTTPS) as per the Python requests library documentation . For example: HTTP_PROXIES = { 'http' : 'http://10.10.1.10:3128' , 'https' : 'http://10.10.1.10:1080' , } Note When using Git repositories within Nautobot the Python library GitPython needs extra proxy configuration: git config --global http.proxy http://192.0.2.1:3128 git config --global https.proxy http://192.0.2.1:3128 JOBS_ROOT \u00b6 Default: os.path.join(NAUTOBOT_ROOT, \"jobs\") Environment Variable: NAUTOBOT_JOBS_ROOT The file path to a directory where Jobs can be discovered. Caution This directory must contain an __init__.py file. MAINTENANCE_MODE \u00b6 Default: False Environment Variable: NAUTOBOT_MAINTENANCE_MODE Setting this to True will display a \"maintenance mode\" banner at the top of every page. Additionally, Nautobot will no longer update a user's \"last active\" time upon login. This is to allow new logins when the database is in a read-only state. Recording of login times will resume when maintenance mode is disabled. Note The default SESSION_ENGINE configuration will store sessions in the database, this obviously will not work when MAINTENANCE_MODE is True and the database is in a read-only state for maintenance. Consider setting SESSION_ENGINE to django.contrib.sessions.backends.cache when enabling MAINTENANCE_MODE . Note The Docker container normally attempts to run migrations on startup; however, if the database is in a read-only state the Docker container will fail to start. Setting the environment variable NAUTOBOT_DOCKER_SKIP_INIT to true will prevent the migrations from occurring. Note If you are using django-auth-ldap for LDAP authentication, django-auth-ldap by default will try to update a user object on every log in. If the database is in a read-only state django-auth-ldap will fail. You will also need to set AUTH_LDAP_ALWAYS_UPDATE_USER=False and AUTH_LDAP_NO_NEW_USERS=True to avoid this, please see the django-auth-ldap documentation for more information. MAX_PAGE_SIZE \u00b6 Default: 1000 A web user or API consumer can request an arbitrary number of objects by appending the \"limit\" parameter to the URL (e.g. ?limit=1000 ). This parameter defines the maximum acceptable limit. Setting this to 0 or None will allow a client to retrieve all matching objects at once with no limit by specifying ?limit=0 . Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value. METRICS_ENABLED \u00b6 Default: False Environment Variable: NAUTOBOT_METRICS_ENABLED Toggle the availability Prometheus-compatible metrics at /metrics . See the Prometheus Metrics documentation for more details. NAPALM_USERNAME \u00b6 NAPALM_PASSWORD \u00b6 Default: \"\" (Empty string) Environment Variables: NAUTOBOT_NAPALM_USERNAME and NAUTOBOT_NAPALM_PASSWORD Nautobot will use these credentials when authenticating to remote devices via the NAPALM library , if installed. Both parameters are optional. Note If SSH public key authentication has been set up on the remote device(s) for the system account under which Nautobot runs, these parameters are not needed. Note If a given device has an appropriately populated secrets group assigned to it, the secrets defined in that group will take precedence over these default values. NAPALM_ARGS \u00b6 Default: {} (Empty dictionary) A dictionary of optional arguments to pass to NAPALM when instantiating a network driver. See the NAPALM documentation for a complete list of optional arguments . An example: NAPALM_ARGS = { 'api_key' : '472071a93b60a1bd1fafb401d9f8ef41' , 'port' : 2222 , } Some platforms (e.g. Cisco IOS) require an enable password to be passed in addition to the normal password. If desired, you can use the configured NAPALM_PASSWORD as the value for this argument: NAPALM_USERNAME = 'username' NAPALM_PASSWORD = 'MySecretPassword' NAPALM_ARGS = { 'secret' : NAPALM_PASSWORD , # ios and nxos_ssh 'enable_password' : NAPALM_PASSWORD , # eos # Include any additional args here } Note If a given device has an appropriately populated secrets group assigned to it, a secret defined in that group can override the NAPALM_ARGS[\"secret\"] or NAPALM_ARGS[\"enable_password\"] default value defined here. NAPALM_TIMEOUT \u00b6 Default: 30 Environment Variable: NAUTOBOT_NAPALM_TIMEOUT The amount of time (in seconds) to wait for NAPALM to connect to a device. PAGINATE_COUNT \u00b6 Default: 50 The default maximum number of objects to display per page within each list of objects. Applies to both the UI and the REST API. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value. PER_PAGE_DEFAULTS \u00b6 Default: [25, 50, 100, 250, 500, 1000] The options displayed in the web interface dropdown to limit the number of objects per page. For proper user experience, this list should include the PAGINATE_COUNT and MAX_PAGE_SIZE values as options. PLUGINS \u00b6 Default: [] (Empty list) A list of installed Nautobot plugins to enable. Plugins will not take effect unless they are listed here. Warning Plugins extend Nautobot by allowing external code to run with the same access and privileges as Nautobot itself. Only install plugins from trusted sources. The Nautobot maintainers make absolutely no guarantees about the integrity or security of your installation with plugins enabled. PLUGINS_CONFIG \u00b6 Default: {} (Empty dictionary) This parameter holds configuration settings for individual Nautobot plugins. It is defined as a dictionary, with each key using the name of an installed plugin. The specific parameters supported are unique to each plugin: Reference the plugin's documentation to determine the supported parameters. An example configuration is shown below: PLUGINS_CONFIG = { 'plugin1' : { 'foo' : 123 , 'bar' : True }, 'plugin2' : { 'foo' : 456 , }, } Note that a plugin must be listed in PLUGINS for its configuration to take effect. PREFER_IPV4 \u00b6 Default: False When determining the primary IP address for a device, IPv6 is preferred over IPv4 by default. Set this to True to prefer IPv4 instead. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value. RACK_ELEVATION_DEFAULT_UNIT_HEIGHT \u00b6 Default: 22 Default height (in pixels) of a unit within a rack elevation. For best results, this should be approximately one tenth of RACK_ELEVATION_DEFAULT_UNIT_WIDTH . Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value. RACK_ELEVATION_DEFAULT_UNIT_WIDTH \u00b6 Default: 220 Default width (in pixels) of a unit within a rack elevation. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value. REDIS_LOCK_TIMEOUT \u00b6 Default: 600 Environment Variable: NAUTOBOT_REDIS_LOCK_TIMEOUT Maximum duration of a Redis lock created when calling /api/ipam/prefixes/{id}/available-prefixes/ or /api/ipam/prefixes/{id}/available-ips/ to avoid inadvertently allocating the same prefix or IP to multiple simultaneous callers. Default is set to 600 seconds (10 minutes) to be longer than any theoretical API call time. This is to prevent a deadlock scenario where the server did not gracefully exit the with block when acquiring the Redis lock. RELEASE_CHECK_TIMEOUT \u00b6 Default: 86400 (24 hours) The number of seconds to retain the latest version that is fetched from the GitHub API before automatically invalidating it and fetching it from the API again. Warning This must be set to at least one hour ( 3600 seconds). Setting it to a value lower than this is an error. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value. RELEASE_CHECK_URL \u00b6 Default: None (disabled) This parameter defines the URL of the repository that will be checked periodically for new Nautobot releases. When a new release is detected, a message will be displayed to administrative users on the home page. This can be set to the official repository ( 'https://api.github.com/repos/nautobot/nautobot/releases' ) or a custom fork. Set this to None to disable automatic update checks. Note The URL provided must be compatible with the GitHub REST API . Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value. SANITIZER_PATTERNS \u00b6 Added in version 1.3.4 Default: [ ( re . compile ( r \"(https?://)?\\S+\\s*@\" , re . IGNORECASE ), r \"\\1 {replacement} @\" ), ( re . compile ( r \"(username|password|passwd|pwd)(\\s*i?s?\\s*:?\\s*)?\\S+\" , re . IGNORECASE ), r \"\\1\\2 {replacement} \" ), ] List of (regular expression, replacement pattern) tuples used by the nautobot.utilities.logging.sanitize() function. As of Nautobot 1.3.4 this function is used primarily for sanitization of Job log entries, but it may be used in other scopes in the future. STORAGE_BACKEND \u00b6 Default: None (local storage) The backend storage engine for handling uploaded files (e.g. image attachments). Nautobot supports integration with the django-storages package, which provides backends for several popular file storage services. If not configured, local filesystem storage will be used. The configuration parameters for the specified storage backend are defined under the STORAGE_CONFIG setting. STORAGE_CONFIG \u00b6 Default: {} (Empty dictionary) A dictionary of configuration parameters for the storage backend configured as STORAGE_BACKEND . The specific parameters to be used here are specific to each backend; see the django-storages documentation for more detail. If STORAGE_BACKEND is not defined, this setting will be ignored. STRICT_FILTERING \u00b6 Added in version 1.4.0 Default: True Environment Variable: NAUTOBOT_STRICT_FILTERING If set to True (default), UI and REST API filtering of object lists will fail if an unknown/unrecognized filter parameter is provided as a URL parameter. (For example, /dcim/devices/?ice_cream_flavor=chocolate or /api/dcim/sites/?ice_cream_flavor=chocolate ). UI list (table) views will report an error message in this case and display no filtered objects; REST API list endpoints will return a 400 Bad Request response with an explanatory error message. If set to False , unknown/unrecognized filter parameters will be discarded and ignored, although Nautobot will log a warning message. Warning Setting this to False can result in unexpected filtering results in the case of user error, for example /dcim/devices/?has_primry_ip=false (note the typo primry ) will result in a list of all devices, rather than the intended list of only devices that lack a primary IP address. In the case of Jobs or external automation making use of such a filter, this could have wide-ranging consequences. UI_RACK_VIEW_TRUNCATE_FUNCTION \u00b6 Added in version 1.4.0 Default: def UI_RACK_VIEW_TRUNCATE_FUNCTION ( device_display_name ): return str ( device_display_name ) . split ( \".\" )[ 0 ] This setting function is used to perform the rack elevation truncation feature. This provides a way to tailor the truncation behavior to best suit the needs of the installation. The function must take only one argument: the device display name, as a string, attempting to be rendered on the rack elevation. The function must return only one argument: a string of the truncated device display name. Environment-Variable-Only Settings \u00b6 Warning The following settings are only configurable as environment variables, and not via nautobot_config.py or similar. GIT_SSL_NO_VERIFY \u00b6 Default: Unset If you are using a self-signed git repository, you will need to set the environment variable GIT_SSL_NO_VERIFY=\"1\" in order for the repository to sync. Warning This must be specified as an environment variable. Setting it in nautobot_config.py will not have the desired effect. NAUTOBOT_ROOT \u00b6 Default: ~/.nautobot/ The filesystem path to use to store Nautobot files (Jobs, uploaded images, Git repositories, etc.). This setting is used internally in the core settings to provide default locations for features that require file storage , and the default location of the nautobot_config.py . Warning Do not override NAUTOBOT_ROOT in your nautobot_config.py . It will not work as expected. If you need to customize this setting, please always set the NAUTOBOT_ROOT environment variable. Django Configuration Settings \u00b6 While the official Django documentation documents all Django settings, the below is provided where either the setting is common in Nautobot deployments and/or there is a supported NAUTOBOT_* environment variable. ADMINS \u00b6 Default: [] (Empty list) Nautobot will email details about critical errors to the administrators listed here. This should be a list of (name, email) tuples. For example: ADMINS = [ [ 'Hank Hill' , 'hhill@example.com' ], [ 'Dale Gribble' , 'dgribble@example.com' ], ] Please see the official Django documentation on ADMINS for more information. CSRF_TRUSTED_ORIGINS \u00b6 Default: [] A list of hosts (fully-qualified domain names (FQDNs) or subdomains) that are considered trusted origins for cross-site secure requests such as HTTPS POST. For more information, please see the official Django documentation on CSRF_TRUSTED_ORIGINS and more generally the official Django documentation on CSRF protection Date and Time Formatting \u00b6 You may define custom formatting for date and times. For detailed instructions on writing format strings, please see the Django documentation . Default formats are listed below. DATE_FORMAT = 'N j, Y' # June 26, 2016 SHORT_DATE_FORMAT = 'Y-m-d' # 2016-06-26 TIME_FORMAT = 'g:i a' # 1:23 p.m. DATETIME_FORMAT = 'N j, Y g:i a' # June 26, 2016 1:23 p.m. SHORT_DATETIME_FORMAT = 'Y-m-d H:i' # 2016-06-26 13:23 Environment Variables: NAUTOBOT_DATE_FORMAT NAUTOBOT_SHORT_DATE_FORMAT NAUTOBOT_TIME_FORMAT NAUTOBOT_SHORT_TIME_FORMAT NAUTOBOT_DATETIME_FORMAT NAUTOBOT_SHORT_DATETIME_FORMAT DEBUG \u00b6 Default: False Environment Variable: NAUTOBOT_DEBUG This setting enables debugging. Debugging should be enabled only during development or troubleshooting. Note that only clients which access Nautobot from a recognized internal IP address will see debugging tools in the user interface. Warning Never enable debugging on a production system, as it can expose sensitive data to unauthenticated users and impose a substantial performance penalty. Please see the official Django documentation on DEBUG for more information. FORCE_SCRIPT_NAME \u00b6 Default: None If not None , this will be used as the value of the SCRIPT_NAME environment variable in any HTTP request. This setting can be used to override the server-provided value of SCRIPT_NAME , which is most commonly used for hosting Nautobot in a subdirectory (e.g. example.com/nautobot/ ). Important To host Nautobot under a subdirectory you must set this value to match the same prefix configured on your HTTP server. For example, if you configure NGINX to serve Nautobot at /nautobot/ , you must set FORCE_SCRIPT_NAME = \"/nautobot/\" . Please see the official Django documentation on FORCE_SCRIPT_NAME for more information. INTERNAL_IPS \u00b6 Default: ('127.0.0.1', '::1') A list of IP addresses recognized as internal to the system, used to control the display of debugging output. For example, the Django debugging toolbar , if installed, will be viewable only when a client is accessing Nautobot from one of the listed IP addresses (and DEBUG is true). LOGGING \u00b6 Default: {} (Empty dictionary) By default, all messages of INFO severity or higher will be logged to the console. Additionally, if DEBUG is False and email access has been configured, ERROR and CRITICAL messages will be emailed to the users defined in ADMINS . The Django framework on which Nautobot runs allows for the customization of logging format and destination. Please consult the Django logging documentation for more information on configuring this setting. Below is an example which will write all INFO and higher messages to a local file and log DEBUG and higher messages from Nautobot itself with higher verbosity: LOGGING = { 'version' : 1 , 'disable_existing_loggers' : False , 'formatters' : { 'normal' : { 'format' : ' %(asctime)s . %(msecs)03d %(levelname)-7s %(name)s : %(message)s ' , 'datefmt' : '%H:%M:%S' , }, 'verbose' : { 'format' : ' %(asctime)s . %(msecs)03d %(levelname)-7s %(name)-20s %(filename)-15s %(funcName)30s () : \\n %(message)s ' , 'datefmt' : '%H:%M:%S' , }, }, 'handlers' : { 'file' : { 'level' : 'INFO' , 'class' : 'logging.FileHandler' , 'filename' : '/var/log/nautobot.log' , 'formatter' : 'normal' }, 'normal_console' : { 'level' : 'INFO' , 'class' : 'logging.StreamHandler' , 'formatter' : 'normal' }, 'verbose_console' : { 'level' : 'DEBUG' , 'class' : 'logging.StreamHandler' , 'formatter' : 'verbose' }, }, 'loggers' : { 'django' : { 'handlers' : [ 'file' , 'normal_console' ], 'level' : 'INFO' }, 'nautobot' : { 'handlers' : [ 'file' , 'verbose_console' ], 'level' : 'DEBUG' }, }, } Additional examples are available in /examples/logging . Available Loggers \u00b6 django.* - Generic Django operations (HTTP requests/responses, etc.) nautobot.<app>.<module> - Generic form for model- or module-specific log messages nautobot.auth.* - Authentication events nautobot.api.views.* - Views which handle business logic for the REST API nautobot.jobs.* - Job execution ( * = JobClassName ) nautobot.graphql.* - GraphQL initialization and operation. nautobot.plugins.* - Plugin loading and activity nautobot.views.* - Views which handle business logic for the web UI rq.worker - Background task handling MEDIA_ROOT \u00b6 Default: os.path.join(NAUTOBOT_ROOT, \"media\") The file path to the location where media files (such as image attachments ) are stored. Please see the official Django documentation on MEDIA_ROOT for more information. SESSION_COOKIE_AGE \u00b6 Default: 1209600 (2 weeks, in seconds) Environment Variable: NAUTOBOT_SESSION_COOKIE_AGE The age of session cookies, in seconds. SESSION_ENGINE \u00b6 Default: 'django.contrib.sessions.backends.db' Controls where Nautobot stores session data. To use cache-based sessions, set this to 'django.contrib.sessions.backends.cache' . To use file-based sessions, set this to 'django.contrib.sessions.backends.file' . See the official Django documentation on Configuring the session engine for more details. SESSION_FILE_PATH \u00b6 Default: None Environment Variable: NAUTOBOT_SESSION_FILE_PATH HTTP session data is used to track authenticated users when they access Nautobot. By default, Nautobot stores session data in its database. However, this inhibits authentication to a standby instance of Nautobot without write access to the database. Alternatively, a local file path may be specified here and Nautobot will store session data as files instead of using the database. Note that the Nautobot system user must have read and write permissions to this path. When the default value ( None ) is used, Nautobot will use the standard temporary directory for the system. If you set this value, you must also enable file-based sessions as explained above using SESSION_ENGINE . STATIC_ROOT \u00b6 Default: os.path.join(NAUTOBOT_ROOT, \"static\") The location where static files (such as CSS, JavaScript, fonts, or images) used to serve the web interface will be staged by the nautobot-server collectstatic command. Please see the official Django documentation on STATIC_ROOT for more information. TIME_ZONE \u00b6 Default: \"UTC\" Environment Variable: NAUTOBOT_TIME_ZONE The time zone Nautobot will use when dealing with dates and times. It is recommended to use UTC time unless you have a specific need to use a local time zone. Please see the list of available time zones . Please see the official Django documentation on TIME_ZONE for more information.","title":"Optional Settings"},{"location":"configuration/optional-settings.html#optional-configuration-settings","text":"","title":"Optional Configuration Settings"},{"location":"configuration/optional-settings.html#administratively-configurable-settings","text":"Added in version 1.2.0 A number of settings can alternatively be configured via the Nautobot Admin UI. To do so, these settings must not be defined in your nautobot_config.py , as any settings defined there will take precedence over any values defined in the Admin UI. Settings that are currently configurable via the Admin UI include: BANNER_BOTTOM BANNER_LOGIN BANNER_TOP CHANGELOG_RETENTION HIDE_RESTRICTED_UI MAX_PAGE_SIZE PAGINATE_COUNT PER_PAGE_DEFAULTS PREFER_IPV4 RACK_ELEVATION_DEFAULT_UNIT_HEIGHT RACK_ELEVATION_DEFAULT_UNIT_WIDTH RELEASE_CHECK_TIMEOUT RELEASE_CHECK_URL","title":"Administratively Configurable Settings"},{"location":"configuration/optional-settings.html#extra-applications","text":"A need may arise to allow the user to register additional settings. These will automatically apply based on keynames prefixed with EXTRA_ assuming the base key (the latter part of the setting name) is of type list or tuple. For example, to register additional INSTALLED_APPS , you would simply specify this in your custom (user) configuration:: EXTRA_INSTALLED_APPS = [ 'foo.bar' , ] This will ensure your default setting's INSTALLED_APPS do not have to be modified, and the user can specify additional apps with ease. Similarly, additional MIDDLEWARE can be added using EXTRA_MIDDLEWARE .","title":"Extra Applications"},{"location":"configuration/optional-settings.html#allowed_url_schemes","text":"Default: ('file', 'ftp', 'ftps', 'http', 'https', 'irc', 'mailto', 'sftp', 'ssh', 'tel', 'telnet', 'tftp', 'vnc', 'xmpp') A list of permitted URL schemes referenced when rendering links within Nautobot. Note that only the schemes specified in this list will be accepted: If adding your own, be sure to replicate all of the default values as well (excluding those schemes which are not desirable).","title":"ALLOWED_URL_SCHEMES"},{"location":"configuration/optional-settings.html#banner_top","text":"","title":"BANNER_TOP"},{"location":"configuration/optional-settings.html#banner_bottom","text":"Default: \"\" (Empty string) Setting these variables will display custom content in a banner at the top and/or bottom of the page, respectively. HTML is allowed. To replicate the content of the top banner in the bottom banner, set: BANNER_TOP = 'Your banner text' BANNER_BOTTOM = BANNER_TOP Added in version 1.2.0 If you do not set a value for these settings in your nautobot_config.py , they can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for either setting in nautobot_config.py , it will override any dynamically configured value.","title":"BANNER_BOTTOM"},{"location":"configuration/optional-settings.html#banner_login","text":"Default: \"\" (Empty string) This defines custom content to be displayed on the login page above the login form. HTML is allowed. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value.","title":"BANNER_LOGIN"},{"location":"configuration/optional-settings.html#branding_filepaths","text":"Default: { \"logo\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_LOGO\" , None ), # Navbar logo \"favicon\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_FAVICON\" , None ), # Browser favicon \"icon_16\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_ICON_16\" , None ), # 16x16px icon \"icon_32\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_ICON_32\" , None ), # 32x32px icon \"icon_180\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_ICON_180\" , None ), # 180x180px icon - used for the apple-touch-icon header \"icon_192\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_ICON_192\" , None ), # 192x192px icon \"icon_mask\" : os . getenv ( \"NAUTOBOT_BRANDING_FILEPATHS_ICON_MASK\" , None ), # mono-chrome icon used for the mask-icon header } A set of filepaths relative to the MEDIA_ROOT which locate image assets used for custom branding. Each of these assets takes the place of the corresponding stock Nautobot asset. This allows for instance, providing your own navbar logo and favicon. These environment variables may be used to specify the values: NAUTOBOT_BRANDING_FILEPATHS_LOGO NAUTOBOT_BRANDING_FILEPATHS_FAVICON NAUTOBOT_BRANDING_FILEPATHS_ICON_16 NAUTOBOT_BRANDING_FILEPATHS_ICON_32 NAUTOBOT_BRANDING_FILEPATHS_ICON_180 NAUTOBOT_BRANDING_FILEPATHS_ICON_192 NAUTOBOT_BRANDING_FILEPATHS_ICON_MASK If a custom image asset is not provided for any of the above options, the stock Nautobot asset is used.","title":"BRANDING_FILEPATHS"},{"location":"configuration/optional-settings.html#branding_prepended_filename","text":"Added in version 1.3.4 Default: \"nautobot_\" Environment Variable: NAUTOBOT_BRANDING_PREPENDED_FILENAME Defines the prefix of the filename when exporting to CSV/YAML or export templates.","title":"BRANDING_PREPENDED_FILENAME"},{"location":"configuration/optional-settings.html#branding_title","text":"Default: \"Nautobot\" Environment Variable: NAUTOBOT_BRANDING_TITLE The defines the custom branding title that should be used in place of \"Nautobot\" within user facing areas of the application like the HTML title of web pages.","title":"BRANDING_TITLE"},{"location":"configuration/optional-settings.html#branding_urls","text":"Default: { \"code\" : os . getenv ( \"NAUTOBOT_BRANDING_URLS_CODE\" , \"https://github.com/nautobot/nautobot\" ), # Code link in the footer \"docs\" : os . getenv ( \"NAUTOBOT_BRANDING_URLS_DOCS\" , \"<STATIC_URL>docs/index.html\" ), # Docs link in the footer \"help\" : os . getenv ( \"NAUTOBOT_BRANDING_URLS_HELP\" , \"https://github.com/nautobot/nautobot/wiki\" ), # Help link in the footer } A set of URLs that correspond to helpful links in the right of the footer on every web page. These environment variables may be used to specify the values: NAUTOBOT_BRANDING_URLS_CODE NAUTOBOT_BRANDING_URLS_DOCS NAUTOBOT_BRANDING_URLS_HELP If a custom URL is not provided for any of the links, the default link within the Nautobot community is used.","title":"BRANDING_URLS"},{"location":"configuration/optional-settings.html#cacheops_defaults","text":"Default: {'timeout': 900} (15 minutes, in seconds) Environment Variable: NAUTOBOT_CACHEOPS_TIMEOUT (timeout value only) Warning It is an error to set the timeout value to 0 . If you wish to disable caching, please use CACHEOPS_ENABLED . Various defaults for caching, the most important of which being the cache timeout. The timeout is the number of seconds that cache entries will be retained before expiring.","title":"CACHEOPS_DEFAULTS"},{"location":"configuration/optional-settings.html#cacheops_enabled","text":"Default: True Environment Variable: NAUTOBOT_CACHEOPS_ENABLED A boolean that turns on/off caching. If set to False , all caching is bypassed and Nautobot operates as if there is no cache.","title":"CACHEOPS_ENABLED"},{"location":"configuration/optional-settings.html#cacheops_health_check_enabled","text":"Default: False A boolean that turns on/off health checks for the Redis server connection utilized by Cacheops. Most deployments share a Redis server with django-redis as such we only need to check the health of Redis one time. If you are using a separate Redis deployment for Cacheops, please consider enabling this to monitor that Redis deployment. Keep in mind the more health checks enabled the longer the health checks will take and timeouts might need to be increased.","title":"CACHEOPS_HEALTH_CHECK_ENABLED"},{"location":"configuration/optional-settings.html#cacheops_redis","text":"Default: 'redis://localhost:6379/1' Environment Variable: NAUTOBOT_CACHEOPS_REDIS The Redis connection string to use for caching.","title":"CACHEOPS_REDIS"},{"location":"configuration/optional-settings.html#celery_broker_transport_options","text":"Default: {} A dict of additional options passed to the Celery broker transport. This is only required when configuring Celery to utilize Redis Sentinel .","title":"CELERY_BROKER_TRANSPORT_OPTIONS"},{"location":"configuration/optional-settings.html#celery_broker_url","text":"Environment Variable: NAUTOBOT_CELERY_BROKER_URL Default: 'redis://localhost:6379/0' Celery broker URL used to tell workers where queues are located.","title":"CELERY_BROKER_URL"},{"location":"configuration/optional-settings.html#celery_result_backend","text":"Environment Variable: NAUTOBOT_CELERY_RESULT_BACKEND Default: 'redis://localhost:6379/0' Celery result backend used to tell workers where to store task results (tombstones).","title":"CELERY_RESULT_BACKEND"},{"location":"configuration/optional-settings.html#celery_result_backend_transport_options","text":"Default: {} A dict of additional options passed to the Celery result backend transport. This is only required when configuring Celery to utilize Redis Sentinel .","title":"CELERY_RESULT_BACKEND_TRANSPORT_OPTIONS"},{"location":"configuration/optional-settings.html#celery_task_soft_time_limit","text":"Default: 300 (5 minutes) Environment Variable: NAUTOBOT_CELERY_TASK_SOFT_TIME_LIMIT The global Celery task soft timeout (in seconds). Any background task that exceeds this duration will receive a SoftTimeLimitExceeded exception and is responsible for handling this exception and performing any necessary cleanup or final operations before ending. See also CELERY_TASK_TIME_LIMIT below.","title":"CELERY_TASK_SOFT_TIME_LIMIT"},{"location":"configuration/optional-settings.html#celery_task_time_limit","text":"Default: 600 (10 minutes) Environment Variable: NAUTOBOT_CELERY_TASK_TIME_LIMIT The global Celery task hard timeout (in seconds). Any background task that exceeds this duration will be forcibly killed with a SIGKILL signal.","title":"CELERY_TASK_TIME_LIMIT"},{"location":"configuration/optional-settings.html#changelog_retention","text":"Default: 90 The number of days to retain logged changes (object creations, updates, and deletions). Set this to 0 to retain changes in the database indefinitely. Warning If enabling indefinite changelog retention, it is recommended to periodically delete old entries. Otherwise, the database may eventually exceed capacity. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value.","title":"CHANGELOG_RETENTION"},{"location":"configuration/optional-settings.html#cors_allow_all_origins","text":"Default: False Environment Variable: NAUTOBOT_CORS_ALLOW_ALL_ORIGINS If True , all origins will be allowed. Other settings restricting allowed origins will be ignored. Setting this to True can be dangerous, as it allows any website to make cross-origin requests to yours. Generally you'll want to restrict the list of allowed origins with CORS_ALLOWED_ORIGINS or CORS_ALLOWED_ORIGIN_REGEXES . Previously this setting was called CORS_ORIGIN_ALLOW_ALL , which still works as an alias, with the new name taking precedence.","title":"CORS_ALLOW_ALL_ORIGINS"},{"location":"configuration/optional-settings.html#cors_allowed_origins","text":"Default: [] (Empty list) A list of origins that are authorized to make cross-site HTTP requests. An Origin is defined by the CORS RFC Section 3.2 as a URI scheme + hostname + port , or one of the special values 'null' or 'file://' . Default ports (HTTPS = 443, HTTP = 80) are optional here. The special value null is sent by the browser in \"privacy-sensitive contexts\" , such as when the client is running from a file:// domain. The special value file:// is sent accidentally by some versions of Chrome on Android as per this bug. Example: CORS_ALLOWED_ORIGINS = [ \"https://example.com\" , \"https://sub.example.com\" , \"http://localhost:8080\" , \"http://127.0.0.1:9000\" ] Previously this setting was called CORS_ORIGIN_WHITELIST , which still works as an alias, with the new name taking precedence.","title":"CORS_ALLOWED_ORIGINS"},{"location":"configuration/optional-settings.html#cors_allowed_origin_regexes","text":"Default: [] A list of strings representing regexes that match Origins that are authorized to make cross-site HTTP requests. Useful when CORS_ALLOWED_ORIGINS is impractical, such as when you have a large number of subdomains. Example: CORS_ALLOWED_ORIGIN_REGEXES = [ r \"^https://\\w+\\.example\\.com$\" , ] Previously this setting was called CORS_ORIGIN_REGEX_WHITELIST , which still works as an alias, with the new name taking precedence.","title":"CORS_ALLOWED_ORIGIN_REGEXES"},{"location":"configuration/optional-settings.html#disable_prefix_list_hierarchy","text":"Default: False Environment Variable: NAUTOBOT_DISABLE_PREFIX_LIST_HIERARCHY This setting disables rendering of the IP prefix hierarchy (parent/child relationships) in the IPAM prefix list view. With large sets of prefixes, users may encounter a performance penalty when trying to load the prefix list view due to the nature of calculating the parent/child relationships. This setting allows users to disable the hierarchy and instead only render a flat list of all prefixes in the table. A later release of Nautobot will address the underlying performance issues, and likely remove this configuration option.","title":"DISABLE_PREFIX_LIST_HIERARCHY"},{"location":"configuration/optional-settings.html#enforce_global_unique","text":"Default: False Environment Variable: NAUTOBOT_ENFORCE_GLOBAL_UNIQUE By default, Nautobot will permit users to create duplicate prefixes and IP addresses in the global table (that is, those which are not assigned to any VRF). This behavior can be disabled by setting ENFORCE_GLOBAL_UNIQUE to True .","title":"ENFORCE_GLOBAL_UNIQUE"},{"location":"configuration/optional-settings.html#exempt_view_permissions","text":"Default: [] (Empty list) A list of Nautobot models to exempt from the enforcement of view permissions. Models listed here will be viewable by all users, both authenticated and anonymous. List models in the form <app>.<model> . For example: EXEMPT_VIEW_PERMISSIONS = [ 'dcim.site' , 'dcim.region' , 'ipam.prefix' , ] To exempt all models from view permission enforcement, set the following. (Note that EXEMPT_VIEW_PERMISSIONS must be an iterable.) EXEMPT_VIEW_PERMISSIONS = [ '*' ] Note Using a wildcard will not affect certain potentially sensitive models, such as user permissions. If there is a need to exempt these models, they must be specified individually.","title":"EXEMPT_VIEW_PERMISSIONS"},{"location":"configuration/optional-settings.html#external_auth_default_groups","text":"Default: [] (Empty list) The list of group names to assign a new user account when created using 3rd-party authentication.","title":"EXTERNAL_AUTH_DEFAULT_GROUPS"},{"location":"configuration/optional-settings.html#external_auth_default_permissions","text":"Default: {} (Empty dictionary) A mapping of permissions to assign a new user account when created using SSO authentication. Each key in the dictionary will be the permission name specified as <app_label>.<action>_<model> , and the value should be set to the permission constraints , or None to allow all objects.","title":"EXTERNAL_AUTH_DEFAULT_PERMISSIONS"},{"location":"configuration/optional-settings.html#example-permissions","text":"Permission Description {'dcim.view_device': {}} or {'dcim.view_device': None} Users can view all devices {'dcim.add_device': {}} Users can add devices, see note below {'dcim.view_device': {\"site__name__in\": [\"HQ\"]}} Users can view all devices in the HQ site Warning Permissions can be complicated! Be careful when restricting permissions to also add any required prerequisite permissions. For example, when adding Devices the Device Role, Device Type, Site, and Status fields are all required fields in order for the UI to function properly. Users will also need view permissions for those fields or the corresponding field selections in the UI will be unavailable and potentially prevent objects from being able to be created or edited. The following example gives a user a reasonable amount of access to add devices to a single site (HQ in this case): { 'dcim.add_device' : { \"site__name__in\" : [ \"HQ\" ]}, 'dcim.view_device' : { \"site__name__in\" : [ \"HQ\" ]}, 'dcim.view_devicerole' : None , 'dcim.view_devicetype' : None , 'extras.view_status' : None , 'dcim.view_site' : { \"name__in\" : [ \"HQ\" ]}, 'dcim.view_manufacturer' : None , 'dcim.view_region' : None , 'dcim.view_rack' : None , 'dcim.view_rackgroup' : None , 'dcim.view_platform' : None , 'virtualization.view_cluster' : None , 'virtualization.view_clustergroup' : None , 'tenancy.view_tenant' : None , 'tenancy.view_tenantgroup' : None , } Please see the object permissions page for more information.","title":"Example Permissions"},{"location":"configuration/optional-settings.html#git_root","text":"Default: os.path.join(NAUTOBOT_ROOT, \"git\") Environment Variable: NAUTOBOT_GIT_ROOT The file path to a directory where cloned Git repositories will be located. The value of this variable can also be customized by setting the environment variable NAUTOBOT_GIT_ROOT to a directory path of your choosing.","title":"GIT_ROOT"},{"location":"configuration/optional-settings.html#graphql_custom_field_prefix","text":"Default: cf By default, all custom fields in GraphQL will be prefixed with cf . A custom field name my_field will appear in GraphQL as cf_my_field by default. It's possible to change or remove the prefix by setting the value of GRAPHQL_CUSTOM_FIELD_PREFIX .","title":"GRAPHQL_CUSTOM_FIELD_PREFIX"},{"location":"configuration/optional-settings.html#hide_restricted_ui","text":"Default: False When set to True , users with limited permissions will only be able to see items in the UI they have access to. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value. Added in version 1.3.10 When this setting is set to True , logged out users will be redirected to the login page when navigating to the Nautobot home page.","title":"HIDE_RESTRICTED_UI"},{"location":"configuration/optional-settings.html#http_proxies","text":"Default: None (Disabled) A dictionary of HTTP proxies to use for outbound requests originating from Nautobot (e.g. when sending webhook requests). Proxies should be specified by schema (HTTP and HTTPS) as per the Python requests library documentation . For example: HTTP_PROXIES = { 'http' : 'http://10.10.1.10:3128' , 'https' : 'http://10.10.1.10:1080' , } Note When using Git repositories within Nautobot the Python library GitPython needs extra proxy configuration: git config --global http.proxy http://192.0.2.1:3128 git config --global https.proxy http://192.0.2.1:3128","title":"HTTP_PROXIES"},{"location":"configuration/optional-settings.html#jobs_root","text":"Default: os.path.join(NAUTOBOT_ROOT, \"jobs\") Environment Variable: NAUTOBOT_JOBS_ROOT The file path to a directory where Jobs can be discovered. Caution This directory must contain an __init__.py file.","title":"JOBS_ROOT"},{"location":"configuration/optional-settings.html#maintenance_mode","text":"Default: False Environment Variable: NAUTOBOT_MAINTENANCE_MODE Setting this to True will display a \"maintenance mode\" banner at the top of every page. Additionally, Nautobot will no longer update a user's \"last active\" time upon login. This is to allow new logins when the database is in a read-only state. Recording of login times will resume when maintenance mode is disabled. Note The default SESSION_ENGINE configuration will store sessions in the database, this obviously will not work when MAINTENANCE_MODE is True and the database is in a read-only state for maintenance. Consider setting SESSION_ENGINE to django.contrib.sessions.backends.cache when enabling MAINTENANCE_MODE . Note The Docker container normally attempts to run migrations on startup; however, if the database is in a read-only state the Docker container will fail to start. Setting the environment variable NAUTOBOT_DOCKER_SKIP_INIT to true will prevent the migrations from occurring. Note If you are using django-auth-ldap for LDAP authentication, django-auth-ldap by default will try to update a user object on every log in. If the database is in a read-only state django-auth-ldap will fail. You will also need to set AUTH_LDAP_ALWAYS_UPDATE_USER=False and AUTH_LDAP_NO_NEW_USERS=True to avoid this, please see the django-auth-ldap documentation for more information.","title":"MAINTENANCE_MODE"},{"location":"configuration/optional-settings.html#max_page_size","text":"Default: 1000 A web user or API consumer can request an arbitrary number of objects by appending the \"limit\" parameter to the URL (e.g. ?limit=1000 ). This parameter defines the maximum acceptable limit. Setting this to 0 or None will allow a client to retrieve all matching objects at once with no limit by specifying ?limit=0 . Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value.","title":"MAX_PAGE_SIZE"},{"location":"configuration/optional-settings.html#metrics_enabled","text":"Default: False Environment Variable: NAUTOBOT_METRICS_ENABLED Toggle the availability Prometheus-compatible metrics at /metrics . See the Prometheus Metrics documentation for more details.","title":"METRICS_ENABLED"},{"location":"configuration/optional-settings.html#napalm_username","text":"","title":"NAPALM_USERNAME"},{"location":"configuration/optional-settings.html#napalm_password","text":"Default: \"\" (Empty string) Environment Variables: NAUTOBOT_NAPALM_USERNAME and NAUTOBOT_NAPALM_PASSWORD Nautobot will use these credentials when authenticating to remote devices via the NAPALM library , if installed. Both parameters are optional. Note If SSH public key authentication has been set up on the remote device(s) for the system account under which Nautobot runs, these parameters are not needed. Note If a given device has an appropriately populated secrets group assigned to it, the secrets defined in that group will take precedence over these default values.","title":"NAPALM_PASSWORD"},{"location":"configuration/optional-settings.html#napalm_args","text":"Default: {} (Empty dictionary) A dictionary of optional arguments to pass to NAPALM when instantiating a network driver. See the NAPALM documentation for a complete list of optional arguments . An example: NAPALM_ARGS = { 'api_key' : '472071a93b60a1bd1fafb401d9f8ef41' , 'port' : 2222 , } Some platforms (e.g. Cisco IOS) require an enable password to be passed in addition to the normal password. If desired, you can use the configured NAPALM_PASSWORD as the value for this argument: NAPALM_USERNAME = 'username' NAPALM_PASSWORD = 'MySecretPassword' NAPALM_ARGS = { 'secret' : NAPALM_PASSWORD , # ios and nxos_ssh 'enable_password' : NAPALM_PASSWORD , # eos # Include any additional args here } Note If a given device has an appropriately populated secrets group assigned to it, a secret defined in that group can override the NAPALM_ARGS[\"secret\"] or NAPALM_ARGS[\"enable_password\"] default value defined here.","title":"NAPALM_ARGS"},{"location":"configuration/optional-settings.html#napalm_timeout","text":"Default: 30 Environment Variable: NAUTOBOT_NAPALM_TIMEOUT The amount of time (in seconds) to wait for NAPALM to connect to a device.","title":"NAPALM_TIMEOUT"},{"location":"configuration/optional-settings.html#paginate_count","text":"Default: 50 The default maximum number of objects to display per page within each list of objects. Applies to both the UI and the REST API. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value.","title":"PAGINATE_COUNT"},{"location":"configuration/optional-settings.html#per_page_defaults","text":"Default: [25, 50, 100, 250, 500, 1000] The options displayed in the web interface dropdown to limit the number of objects per page. For proper user experience, this list should include the PAGINATE_COUNT and MAX_PAGE_SIZE values as options.","title":"PER_PAGE_DEFAULTS"},{"location":"configuration/optional-settings.html#plugins","text":"Default: [] (Empty list) A list of installed Nautobot plugins to enable. Plugins will not take effect unless they are listed here. Warning Plugins extend Nautobot by allowing external code to run with the same access and privileges as Nautobot itself. Only install plugins from trusted sources. The Nautobot maintainers make absolutely no guarantees about the integrity or security of your installation with plugins enabled.","title":"PLUGINS"},{"location":"configuration/optional-settings.html#plugins_config","text":"Default: {} (Empty dictionary) This parameter holds configuration settings for individual Nautobot plugins. It is defined as a dictionary, with each key using the name of an installed plugin. The specific parameters supported are unique to each plugin: Reference the plugin's documentation to determine the supported parameters. An example configuration is shown below: PLUGINS_CONFIG = { 'plugin1' : { 'foo' : 123 , 'bar' : True }, 'plugin2' : { 'foo' : 456 , }, } Note that a plugin must be listed in PLUGINS for its configuration to take effect.","title":"PLUGINS_CONFIG"},{"location":"configuration/optional-settings.html#prefer_ipv4","text":"Default: False When determining the primary IP address for a device, IPv6 is preferred over IPv4 by default. Set this to True to prefer IPv4 instead. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value.","title":"PREFER_IPV4"},{"location":"configuration/optional-settings.html#rack_elevation_default_unit_height","text":"Default: 22 Default height (in pixels) of a unit within a rack elevation. For best results, this should be approximately one tenth of RACK_ELEVATION_DEFAULT_UNIT_WIDTH . Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value.","title":"RACK_ELEVATION_DEFAULT_UNIT_HEIGHT"},{"location":"configuration/optional-settings.html#rack_elevation_default_unit_width","text":"Default: 220 Default width (in pixels) of a unit within a rack elevation. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value.","title":"RACK_ELEVATION_DEFAULT_UNIT_WIDTH"},{"location":"configuration/optional-settings.html#redis_lock_timeout","text":"Default: 600 Environment Variable: NAUTOBOT_REDIS_LOCK_TIMEOUT Maximum duration of a Redis lock created when calling /api/ipam/prefixes/{id}/available-prefixes/ or /api/ipam/prefixes/{id}/available-ips/ to avoid inadvertently allocating the same prefix or IP to multiple simultaneous callers. Default is set to 600 seconds (10 minutes) to be longer than any theoretical API call time. This is to prevent a deadlock scenario where the server did not gracefully exit the with block when acquiring the Redis lock.","title":"REDIS_LOCK_TIMEOUT"},{"location":"configuration/optional-settings.html#release_check_timeout","text":"Default: 86400 (24 hours) The number of seconds to retain the latest version that is fetched from the GitHub API before automatically invalidating it and fetching it from the API again. Warning This must be set to at least one hour ( 3600 seconds). Setting it to a value lower than this is an error. Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value.","title":"RELEASE_CHECK_TIMEOUT"},{"location":"configuration/optional-settings.html#release_check_url","text":"Default: None (disabled) This parameter defines the URL of the repository that will be checked periodically for new Nautobot releases. When a new release is detected, a message will be displayed to administrative users on the home page. This can be set to the official repository ( 'https://api.github.com/repos/nautobot/nautobot/releases' ) or a custom fork. Set this to None to disable automatic update checks. Note The URL provided must be compatible with the GitHub REST API . Added in version 1.2.0 If you do not set a value for this setting in your nautobot_config.py , it can be configured dynamically by an admin user via the Nautobot Admin UI. If you do have a value for this setting in nautobot_config.py , it will override any dynamically configured value.","title":"RELEASE_CHECK_URL"},{"location":"configuration/optional-settings.html#sanitizer_patterns","text":"Added in version 1.3.4 Default: [ ( re . compile ( r \"(https?://)?\\S+\\s*@\" , re . IGNORECASE ), r \"\\1 {replacement} @\" ), ( re . compile ( r \"(username|password|passwd|pwd)(\\s*i?s?\\s*:?\\s*)?\\S+\" , re . IGNORECASE ), r \"\\1\\2 {replacement} \" ), ] List of (regular expression, replacement pattern) tuples used by the nautobot.utilities.logging.sanitize() function. As of Nautobot 1.3.4 this function is used primarily for sanitization of Job log entries, but it may be used in other scopes in the future.","title":"SANITIZER_PATTERNS"},{"location":"configuration/optional-settings.html#storage_backend","text":"Default: None (local storage) The backend storage engine for handling uploaded files (e.g. image attachments). Nautobot supports integration with the django-storages package, which provides backends for several popular file storage services. If not configured, local filesystem storage will be used. The configuration parameters for the specified storage backend are defined under the STORAGE_CONFIG setting.","title":"STORAGE_BACKEND"},{"location":"configuration/optional-settings.html#storage_config","text":"Default: {} (Empty dictionary) A dictionary of configuration parameters for the storage backend configured as STORAGE_BACKEND . The specific parameters to be used here are specific to each backend; see the django-storages documentation for more detail. If STORAGE_BACKEND is not defined, this setting will be ignored.","title":"STORAGE_CONFIG"},{"location":"configuration/optional-settings.html#strict_filtering","text":"Added in version 1.4.0 Default: True Environment Variable: NAUTOBOT_STRICT_FILTERING If set to True (default), UI and REST API filtering of object lists will fail if an unknown/unrecognized filter parameter is provided as a URL parameter. (For example, /dcim/devices/?ice_cream_flavor=chocolate or /api/dcim/sites/?ice_cream_flavor=chocolate ). UI list (table) views will report an error message in this case and display no filtered objects; REST API list endpoints will return a 400 Bad Request response with an explanatory error message. If set to False , unknown/unrecognized filter parameters will be discarded and ignored, although Nautobot will log a warning message. Warning Setting this to False can result in unexpected filtering results in the case of user error, for example /dcim/devices/?has_primry_ip=false (note the typo primry ) will result in a list of all devices, rather than the intended list of only devices that lack a primary IP address. In the case of Jobs or external automation making use of such a filter, this could have wide-ranging consequences.","title":"STRICT_FILTERING"},{"location":"configuration/optional-settings.html#ui_rack_view_truncate_function","text":"Added in version 1.4.0 Default: def UI_RACK_VIEW_TRUNCATE_FUNCTION ( device_display_name ): return str ( device_display_name ) . split ( \".\" )[ 0 ] This setting function is used to perform the rack elevation truncation feature. This provides a way to tailor the truncation behavior to best suit the needs of the installation. The function must take only one argument: the device display name, as a string, attempting to be rendered on the rack elevation. The function must return only one argument: a string of the truncated device display name.","title":"UI_RACK_VIEW_TRUNCATE_FUNCTION"},{"location":"configuration/optional-settings.html#environment-variable-only-settings","text":"Warning The following settings are only configurable as environment variables, and not via nautobot_config.py or similar.","title":"Environment-Variable-Only Settings"},{"location":"configuration/optional-settings.html#git_ssl_no_verify","text":"Default: Unset If you are using a self-signed git repository, you will need to set the environment variable GIT_SSL_NO_VERIFY=\"1\" in order for the repository to sync. Warning This must be specified as an environment variable. Setting it in nautobot_config.py will not have the desired effect.","title":"GIT_SSL_NO_VERIFY"},{"location":"configuration/optional-settings.html#nautobot_root","text":"Default: ~/.nautobot/ The filesystem path to use to store Nautobot files (Jobs, uploaded images, Git repositories, etc.). This setting is used internally in the core settings to provide default locations for features that require file storage , and the default location of the nautobot_config.py . Warning Do not override NAUTOBOT_ROOT in your nautobot_config.py . It will not work as expected. If you need to customize this setting, please always set the NAUTOBOT_ROOT environment variable.","title":"NAUTOBOT_ROOT"},{"location":"configuration/optional-settings.html#django-configuration-settings","text":"While the official Django documentation documents all Django settings, the below is provided where either the setting is common in Nautobot deployments and/or there is a supported NAUTOBOT_* environment variable.","title":"Django Configuration Settings"},{"location":"configuration/optional-settings.html#admins","text":"Default: [] (Empty list) Nautobot will email details about critical errors to the administrators listed here. This should be a list of (name, email) tuples. For example: ADMINS = [ [ 'Hank Hill' , 'hhill@example.com' ], [ 'Dale Gribble' , 'dgribble@example.com' ], ] Please see the official Django documentation on ADMINS for more information.","title":"ADMINS"},{"location":"configuration/optional-settings.html#csrf_trusted_origins","text":"Default: [] A list of hosts (fully-qualified domain names (FQDNs) or subdomains) that are considered trusted origins for cross-site secure requests such as HTTPS POST. For more information, please see the official Django documentation on CSRF_TRUSTED_ORIGINS and more generally the official Django documentation on CSRF protection","title":"CSRF_TRUSTED_ORIGINS"},{"location":"configuration/optional-settings.html#date-and-time-formatting","text":"You may define custom formatting for date and times. For detailed instructions on writing format strings, please see the Django documentation . Default formats are listed below. DATE_FORMAT = 'N j, Y' # June 26, 2016 SHORT_DATE_FORMAT = 'Y-m-d' # 2016-06-26 TIME_FORMAT = 'g:i a' # 1:23 p.m. DATETIME_FORMAT = 'N j, Y g:i a' # June 26, 2016 1:23 p.m. SHORT_DATETIME_FORMAT = 'Y-m-d H:i' # 2016-06-26 13:23 Environment Variables: NAUTOBOT_DATE_FORMAT NAUTOBOT_SHORT_DATE_FORMAT NAUTOBOT_TIME_FORMAT NAUTOBOT_SHORT_TIME_FORMAT NAUTOBOT_DATETIME_FORMAT NAUTOBOT_SHORT_DATETIME_FORMAT","title":"Date and Time Formatting"},{"location":"configuration/optional-settings.html#debug","text":"Default: False Environment Variable: NAUTOBOT_DEBUG This setting enables debugging. Debugging should be enabled only during development or troubleshooting. Note that only clients which access Nautobot from a recognized internal IP address will see debugging tools in the user interface. Warning Never enable debugging on a production system, as it can expose sensitive data to unauthenticated users and impose a substantial performance penalty. Please see the official Django documentation on DEBUG for more information.","title":"DEBUG"},{"location":"configuration/optional-settings.html#force_script_name","text":"Default: None If not None , this will be used as the value of the SCRIPT_NAME environment variable in any HTTP request. This setting can be used to override the server-provided value of SCRIPT_NAME , which is most commonly used for hosting Nautobot in a subdirectory (e.g. example.com/nautobot/ ). Important To host Nautobot under a subdirectory you must set this value to match the same prefix configured on your HTTP server. For example, if you configure NGINX to serve Nautobot at /nautobot/ , you must set FORCE_SCRIPT_NAME = \"/nautobot/\" . Please see the official Django documentation on FORCE_SCRIPT_NAME for more information.","title":"FORCE_SCRIPT_NAME"},{"location":"configuration/optional-settings.html#internal_ips","text":"Default: ('127.0.0.1', '::1') A list of IP addresses recognized as internal to the system, used to control the display of debugging output. For example, the Django debugging toolbar , if installed, will be viewable only when a client is accessing Nautobot from one of the listed IP addresses (and DEBUG is true).","title":"INTERNAL_IPS"},{"location":"configuration/optional-settings.html#logging","text":"Default: {} (Empty dictionary) By default, all messages of INFO severity or higher will be logged to the console. Additionally, if DEBUG is False and email access has been configured, ERROR and CRITICAL messages will be emailed to the users defined in ADMINS . The Django framework on which Nautobot runs allows for the customization of logging format and destination. Please consult the Django logging documentation for more information on configuring this setting. Below is an example which will write all INFO and higher messages to a local file and log DEBUG and higher messages from Nautobot itself with higher verbosity: LOGGING = { 'version' : 1 , 'disable_existing_loggers' : False , 'formatters' : { 'normal' : { 'format' : ' %(asctime)s . %(msecs)03d %(levelname)-7s %(name)s : %(message)s ' , 'datefmt' : '%H:%M:%S' , }, 'verbose' : { 'format' : ' %(asctime)s . %(msecs)03d %(levelname)-7s %(name)-20s %(filename)-15s %(funcName)30s () : \\n %(message)s ' , 'datefmt' : '%H:%M:%S' , }, }, 'handlers' : { 'file' : { 'level' : 'INFO' , 'class' : 'logging.FileHandler' , 'filename' : '/var/log/nautobot.log' , 'formatter' : 'normal' }, 'normal_console' : { 'level' : 'INFO' , 'class' : 'logging.StreamHandler' , 'formatter' : 'normal' }, 'verbose_console' : { 'level' : 'DEBUG' , 'class' : 'logging.StreamHandler' , 'formatter' : 'verbose' }, }, 'loggers' : { 'django' : { 'handlers' : [ 'file' , 'normal_console' ], 'level' : 'INFO' }, 'nautobot' : { 'handlers' : [ 'file' , 'verbose_console' ], 'level' : 'DEBUG' }, }, } Additional examples are available in /examples/logging .","title":"LOGGING"},{"location":"configuration/optional-settings.html#available-loggers","text":"django.* - Generic Django operations (HTTP requests/responses, etc.) nautobot.<app>.<module> - Generic form for model- or module-specific log messages nautobot.auth.* - Authentication events nautobot.api.views.* - Views which handle business logic for the REST API nautobot.jobs.* - Job execution ( * = JobClassName ) nautobot.graphql.* - GraphQL initialization and operation. nautobot.plugins.* - Plugin loading and activity nautobot.views.* - Views which handle business logic for the web UI rq.worker - Background task handling","title":"Available Loggers"},{"location":"configuration/optional-settings.html#media_root","text":"Default: os.path.join(NAUTOBOT_ROOT, \"media\") The file path to the location where media files (such as image attachments ) are stored. Please see the official Django documentation on MEDIA_ROOT for more information.","title":"MEDIA_ROOT"},{"location":"configuration/optional-settings.html#session_cookie_age","text":"Default: 1209600 (2 weeks, in seconds) Environment Variable: NAUTOBOT_SESSION_COOKIE_AGE The age of session cookies, in seconds.","title":"SESSION_COOKIE_AGE"},{"location":"configuration/optional-settings.html#session_engine","text":"Default: 'django.contrib.sessions.backends.db' Controls where Nautobot stores session data. To use cache-based sessions, set this to 'django.contrib.sessions.backends.cache' . To use file-based sessions, set this to 'django.contrib.sessions.backends.file' . See the official Django documentation on Configuring the session engine for more details.","title":"SESSION_ENGINE"},{"location":"configuration/optional-settings.html#session_file_path","text":"Default: None Environment Variable: NAUTOBOT_SESSION_FILE_PATH HTTP session data is used to track authenticated users when they access Nautobot. By default, Nautobot stores session data in its database. However, this inhibits authentication to a standby instance of Nautobot without write access to the database. Alternatively, a local file path may be specified here and Nautobot will store session data as files instead of using the database. Note that the Nautobot system user must have read and write permissions to this path. When the default value ( None ) is used, Nautobot will use the standard temporary directory for the system. If you set this value, you must also enable file-based sessions as explained above using SESSION_ENGINE .","title":"SESSION_FILE_PATH"},{"location":"configuration/optional-settings.html#static_root","text":"Default: os.path.join(NAUTOBOT_ROOT, \"static\") The location where static files (such as CSS, JavaScript, fonts, or images) used to serve the web interface will be staged by the nautobot-server collectstatic command. Please see the official Django documentation on STATIC_ROOT for more information.","title":"STATIC_ROOT"},{"location":"configuration/optional-settings.html#time_zone","text":"Default: \"UTC\" Environment Variable: NAUTOBOT_TIME_ZONE The time zone Nautobot will use when dealing with dates and times. It is recommended to use UTC time unless you have a specific need to use a local time zone. Please see the list of available time zones . Please see the official Django documentation on TIME_ZONE for more information.","title":"TIME_ZONE"},{"location":"configuration/required-settings.html","text":"Required Configuration Settings \u00b6 ALLOWED_HOSTS \u00b6 Environment Variable: NAUTOBOT_ALLOWED_HOSTS specified as a space-separated quoted string (e.g. NAUTOBOT_ALLOWED_HOSTS=\"localhost 127.0.0.1 example.com\" ). This is a list of valid fully-qualified domain names (FQDNs) and/or IP addresses that can be used to reach the Nautobot service. Usually this is the same as the hostname for the Nautobot server, but can also be different; for example, when using a reverse proxy serving the Nautobot website under a different FQDN than the hostname of the Nautobot server. To help guard against HTTP Host header attacks , Nautobot will not permit access to the server via any other hostnames (or IPs). Keep in mind that by default Nautobot sets USE_X_FORWARDED_HOST to True , which means that if you're using a reverse proxy, the FQDN used to reach that reverse proxy needs to be in this list. Note This parameter must always be defined as a list or tuple, even if only a single value is provided. Example: ALLOWED_HOSTS = [ 'nautobot.example.com' , '192.0.2.123' ] Tip If there is more than one hostname in this list, you may also need to set CSRF_TRUSTED_ORIGINS as well. If you are not yet sure what the domain name and/or IP address of the Nautobot installation will be, and are comfortable accepting the risks in doing so, you can set this to a wildcard (asterisk) to allow all host values: ALLOWED_HOSTS = [ '*' ] Warning It is not recommended to leave this value as ['*'] for production deployments. Please see the official Django documentation on ALLOWED_HOSTS for help. DATABASES \u00b6 Nautobot requires access to a supported database service to store data. This service can run locally on the Nautobot server or on a remote system. The following parameters must be defined within the DATABASES dictionary: NAME - Database name USER - Database username PASSWORD - Database password HOST - Name or IP address of the database server (use localhost if running locally) PORT - The port to use when connecting to the database. An empty string means the default port for your selected backend. (PostgreSQL: 5432 , MySQL: 3306 ) CONN_MAX_AGE - Lifetime of a persistent database connection , in seconds (300 is the default) ENGINE - The database backend to use. This can be either django.db.backends.postgresql or django.db.backends.mysql . The following environment variables may also be set for each of the above values: NAUTOBOT_DB_NAME NAUTOBOT_DB_USER NAUTOBOT_DB_PASSWORD NAUTOBOT_DB_HOST NAUTOBOT_DB_PORT NAUTOBOT_DB_TIMEOUT NAUTOBOT_DB_ENGINE Added in version 1.1.0 The NAUTOBOT_DB_ENGINE setting was added along with support for MySQL. Warning Nautobot supports either MySQL or PostgreSQL as a database backend. You must make sure that the ENGINE setting matches your selected database backend or you will be unable to connect to the database . Example: DATABASES = { 'default' : { 'NAME' : 'nautobot' , # Database name 'USER' : 'nautobot' , # Database username 'PASSWORD' : 'awesome_password' , # Database password 'HOST' : 'localhost' , # Database server 'PORT' : '' , # Database port (leave blank for default) 'CONN_MAX_AGE' : 300 , # Max database connection age 'ENGINE' : 'django.db.backends.postgresql' , # Database driver (\"mysql\" or \"postgresql\") } } Note Nautobot supports all database options supported by the underlying Django framework. For a complete list of available parameters, please see the official Django documentation on DATABASES . MySQL Unicode Settings \u00b6 Tip By default, MySQL is case-insensitive in its handling of text strings. This is different from PostgreSQL which is case-sensitive by default. We strongly recommend that you configure MySQL to be case-sensitive for use with Nautobot, either when you enable the MySQL server, or when you create the Nautobot database in MySQL. If you follow the provided installation instructions for CentOS or Ubuntu, the recommended steps there will include the appropriate database configuration. When using MySQL as a database backend, and you want to enable support for Unicode characters like the beloved poop emoji, you'll need to update your settings. If you try to use emojis without this setting, you will encounter a server error along the lines of Incorrect string value , because you are running afoul of the legacy implementation of Unicode (aka utf8 ) encoding in MySQL. The utf8 encoding in MySQL is limited to 3-bytes per character. Newer Unicode emoji require 4-bytes. To properly support using such characters, you will need to create an entry in DATABASES -> default -> OPTIONS with the value {\"charset\": \"utf8mb4\"} in your nautobot_config.py and restart all Nautobot services. This will tell MySQL to always use utf8mb4 character set for database client connections. For example: DATABASES = { \"default\" : { # Other settings... \"OPTIONS\" : { \"charset\" : \"utf8mb4\" }, # Add this line } } Tip Starting in v1.1.0, if you have generated a new nautobot_config.py using nautobot-server init , this line is already there for you in your config. You'll just need to uncomment it! Redis Settings \u00b6 Redis is an in-memory data store similar to memcached. It is required to support Nautobot's caching, task queueing, and webhook features. The connection settings are explained here, allowing Nautobot to connect to different Redis instances/databases per feature. Warning It is highly recommended to keep the Redis databases for caching and tasks separate. Using the same database number on the same Redis instance for both may result in queued background tasks being lost during cache flushing events. For this reason, the default settings utilize database 1 for caching and database 0 for tasks. Tip The default Redis settings in your nautobot_config.py should be suitable for most deployments and should only require customization for more advanced configurations. Caching \u00b6 Nautobot supports database query caching using django-cacheops . Caching is configured by defining the CACHEOPS_REDIS setting which in its simplest form is just a URL. For more details on Nautobot's caching, including TLS and HA configuration, see the guide on Caching . Important Nautobot does not utilize the built-in Django cache framework to perform caching, as django-cacheops takes its place. CACHEOPS_REDIS \u00b6 Default: \"redis://localhost:6379/1\" Environment Variable: NAUTOBOT_CACHEOPS_REDIS If you wish to use SSL, you may set the URL scheme to rediss:// , for example: CACHEOPS_REDIS = \"rediss://localhost:6379/1\" This setting may also be a dictionary style to provide additional options such as custom TLS/SSL settings, for example: import ssl CACHEOPS_REDIS = { \"host\" : os . getenv ( \"NAUTOBOT_REDIS_HOST\" , \"localhost\" ), \"port\" : int ( os . getenv ( \"NAUTOBOT_REDIS_PORT\" , 6379 )), \"password\" : os . getenv ( \"NAUTOBOT_REDIS_PASSWORD\" , \"\" ), \"ssl\" : True , \"ssl_cert_reqs\" : ssl . CERT_REQUIRED , \"ssl_ca_certs\" : \"/opt/nautobot/redis/ca.crt\" , \"ssl_certfile\" : \"/opt/nautobot/redis/tls.crt\" , \"ssl_keyfile\" : \"/opt/nautobot/redis/tls.key\" , } Additional settings may be available and are not covered here. Please see the official guide on Cacheops setup . CACHEOPS_SENTINEL \u00b6 Default: undefined If you are using Redis Sentinel for high-availability purposes, you must replace the CACHEOPS_REDIS setting with CACHEOPS_SENTINEL . For more details on configuring Nautobot to use Redis Sentinel see Using Redis Sentinel . For more details on how to configure Cacheops specifically to use Redis Sentinel see the official guide on Cacheops setup . Warning CACHEOPS_REDIS and CACHEOPS_SENTINEL are mutually exclusive and will result in an error if both are set. Task Queuing \u00b6 CACHES \u00b6 The django-redis Django plugin is used to enable Redis as a concurrent write lock for preventing race conditions when allocating IP address objects, and also to define centralized Redis connection settings that will be used by RQ. The CACHES setting is required to to simplify the configuration for defining queues. It is not used for caching at this time. Important Nautobot does not utilize the built-in Django cache framework (which also relies on the CACHES setting) to perform caching because Cacheops is being used instead as detailed just above. Yes, we know this is confusing, which is why this is being called out explicitly! Default: # Uncomment the following line to configure TLS/SSL # import ssl CACHES = { \"default\" : { \"BACKEND\" : \"django_redis.cache.RedisCache\" , \"LOCATION\" : \"redis://localhost:6379/0\" , \"TIMEOUT\" : 300 , \"OPTIONS\" : { \"CLIENT_CLASS\" : \"django_redis.client.DefaultClient\" , # Uncomment the following lines to configure TLS/SSL # \"CONNECTION_POOL_KWARGS\": { # \"ssl_cert_reqs\": ssl.CERT_REQUIRED, # \"ssl_ca_certs\": \"/opt/nautobot/redis/ca.crt\", # \"ssl_certfile\": \"/opt/nautobot/redis/tls.crt\", # \"ssl_keyfile\": \"/opt/nautobot/redis/tls.key\", # }, }, } } Task Queuing with RQ \u00b6 Changed in version 1.1.0 Using task queueing with RQ is deprecated in exchange for using Celery. Support for RQ will be removed entirely starting in Nautobot 2.0. Task queues are configured by defining them within the RQ_QUEUES setting. Nautobot's core functionality relies on several distinct queues and these represent the minimum required set of queues that must be defined. By default, these use identical connection settings as defined in CACHES (yes, that's confusing and we'll explain below). In most cases the default settings will be suitable for production use, but it is up to you to modify the task queues for your environment and know that other use cases such as utilizing specific plugins may require additional queues to be defined. RQ_QUEUES \u00b6 The default value for this setting defines the queues and instructs RQ to use the default Redis connection defined in CACHES . This is intended to simplify default configuration for the common case. Please see the official django-rq documentation on support for django-redis connection settings for more information. Changed in version 1.1.0 The check_releases , custom_fields , and webhooks queues are no longer in use by Nautobot but maintained here for backwards compatibility; they will be removed in Nautobot 2.0. Default: RQ_QUEUES = { \"default\" : { \"USE_REDIS_CACHE\" : \"default\" , }, \"check_releases\" : { \"USE_REDIS_CACHE\" : \"default\" , }, \"custom_fields\" : { \"USE_REDIS_CACHE\" : \"default\" , }, \"webhooks\" : { \"USE_REDIS_CACHE\" : \"default\" , }, } More verbose dictionary-style configuration is still supported, but is not required unless you absolutely need more advanced task queuing configuration. An example configuration follows: RQ_QUEUES = { \"default\" : { \"HOST\" : \"localhost\" , \"PORT\" : 6379 , \"DB\" : 0 , \"PASSWORD\" : \"\" , \"SSL\" : False , \"DEFAULT_TIMEOUT\" : 300 }, \"webhooks\" : { \"HOST\" : \"localhost\" , \"PORT\" : 6379 , \"DB\" : 0 , \"PASSWORD\" : \"\" , \"SSL\" : False , \"DEFAULT_TIMEOUT\" : 300 }, \"check_releases\" : { \"HOST\" : \"localhost\" , \"PORT\" : 6379 , \"DB\" : 0 , \"PASSWORD\" : \"\" , \"SSL\" : False , \"DEFAULT_TIMEOUT\" : 300 }, \"custom_fields\" : { \"HOST\" : \"localhost\" , \"PORT\" : 6379 , \"DB\" : 0 , \"PASSWORD\" : \"\" , \"SSL\" : False , \"DEFAULT_TIMEOUT\" : 300 } } HOST - Name or IP address of the Redis server (use localhost if running locally) PORT - TCP port of the Redis service; leave blank for default port (6379) PASSWORD - Redis password (if set) DB - Numeric database ID SSL - Use SSL connection to Redis DEFAULT_TIMEOUT - The maximum execution time of a background task (such as running a Job ), in seconds. The following environment variables may also be set for some of the above values: NAUTOBOT_REDIS_HOST NAUTOBOT_REDIS_PORT NAUTOBOT_REDIS_PASSWORD NAUTOBOT_REDIS_USERNAME NAUTOBOT_REDIS_SSL NAUTOBOT_REDIS_TIMEOUT Note If you overload any of the default values in CACHES or RQ_QUEUES you may be unable to utilize the environment variables, depending on what you change. For more details on configuring RQ, please see the documentation for Django RQ installation . Task Queuing with Celery \u00b6 Out of the box you do not need to make any changes to utilize task queueing with Celery. All of the default settings are sufficient for most installations. In the event you do need to make customizations to how Celery interacts with the message broker such as for more advanced clustered deployments, the following settings are required. CELERY_BROKER_URL \u00b6 This setting tells Celery and its workers how and where to communicate with the message broker. The default value for this points to redis://localhost:6379/0 . Please see the optional settings documentation for CELERY_BROKER_URL for more information on customizing this setting. CELERY_RESULT_BACKEND \u00b6 This setting tells Celery and its workers how and where to store message results. This defaults to the same value as CELERY_BROKER_URL . In some more advanced setups it may be required for these to be separate locations, however in our configuration guides these are always the same. Please see the optional settings documentation for CELERY_RESULT_BACKEND for more information on customizing this setting. Configuring Celery with TLS \u00b6 Optionally, you can configure Celery to use custom SSL certificates to connect to redis by setting the following variables: import ssl CELERY_REDIS_BACKEND_USE_SSL = { \"ssl_cert_reqs\" : ssl . CERT_REQUIRED , \"ssl_ca_certs\" : \"/opt/nautobot/redis/ca.crt\" , \"ssl_certfile\" : \"/opt/nautobot/redis/tls.crt\" , \"ssl_keyfile\" : \"/opt/nautobot/redis/tls.key\" , } CELERY_BROKER_USE_SSL = CELERY_REDIS_BACKEND_USE_SSL Please see the celery documentation for additional details. Configuring Celery for High Availability \u00b6 High availability clustering of Redis for use with Celery can be performed using Redis Sentinel. Please see documentation section on configuring Celery for Redis Sentinel for more information. SECRET_KEY \u00b6 Environment Variable: NAUTOBOT_SECRET_KEY This is a secret, random string used to assist in the creation new cryptographic hashes for passwords and HTTP cookies. The key defined here should not be shared outside of the configuration file. SECRET_KEY can be changed at any time, however be aware that doing so will invalidate all existing sessions. Please note that this key is not used directly for hashing user passwords or for the encrypted storage of secret data in Nautobot. SECRET_KEY should be at least 50 characters in length and contain a random mix of letters, digits, and symbols. Note A unique SECRET_KEY is generated for you automatically when you use nautobot-server init to create a new nautobot_config.py . You may run nautobot-server generate_secret_key to generate a new key at any time. $ nautobot-server generate_secret_key +$_kw69oq&fbkfk6&q-+ksbgzw1&061ghw%420u3(wen54w(m Alternatively use the following command to generate a secret even before nautobot-server is runnable: $ LC_ALL=C tr -cd '[:lower:][:digit:]!@#$%^&*(\\-_=+)' < /dev/urandom | fold -w50 | head -n1 9.V$@Kxkc@@Kd@z<a/=.J-Y;rYc79<y@](9o9(L(*sS)Q+ud5P Warning In the case of a highly available installation with multiple web servers, SECRET_KEY must be identical among all servers in order to maintain a persistent user session state. For more details see Nautobot Configuration .","title":"Required Settings"},{"location":"configuration/required-settings.html#required-configuration-settings","text":"","title":"Required Configuration Settings"},{"location":"configuration/required-settings.html#allowed_hosts","text":"Environment Variable: NAUTOBOT_ALLOWED_HOSTS specified as a space-separated quoted string (e.g. NAUTOBOT_ALLOWED_HOSTS=\"localhost 127.0.0.1 example.com\" ). This is a list of valid fully-qualified domain names (FQDNs) and/or IP addresses that can be used to reach the Nautobot service. Usually this is the same as the hostname for the Nautobot server, but can also be different; for example, when using a reverse proxy serving the Nautobot website under a different FQDN than the hostname of the Nautobot server. To help guard against HTTP Host header attacks , Nautobot will not permit access to the server via any other hostnames (or IPs). Keep in mind that by default Nautobot sets USE_X_FORWARDED_HOST to True , which means that if you're using a reverse proxy, the FQDN used to reach that reverse proxy needs to be in this list. Note This parameter must always be defined as a list or tuple, even if only a single value is provided. Example: ALLOWED_HOSTS = [ 'nautobot.example.com' , '192.0.2.123' ] Tip If there is more than one hostname in this list, you may also need to set CSRF_TRUSTED_ORIGINS as well. If you are not yet sure what the domain name and/or IP address of the Nautobot installation will be, and are comfortable accepting the risks in doing so, you can set this to a wildcard (asterisk) to allow all host values: ALLOWED_HOSTS = [ '*' ] Warning It is not recommended to leave this value as ['*'] for production deployments. Please see the official Django documentation on ALLOWED_HOSTS for help.","title":"ALLOWED_HOSTS"},{"location":"configuration/required-settings.html#databases","text":"Nautobot requires access to a supported database service to store data. This service can run locally on the Nautobot server or on a remote system. The following parameters must be defined within the DATABASES dictionary: NAME - Database name USER - Database username PASSWORD - Database password HOST - Name or IP address of the database server (use localhost if running locally) PORT - The port to use when connecting to the database. An empty string means the default port for your selected backend. (PostgreSQL: 5432 , MySQL: 3306 ) CONN_MAX_AGE - Lifetime of a persistent database connection , in seconds (300 is the default) ENGINE - The database backend to use. This can be either django.db.backends.postgresql or django.db.backends.mysql . The following environment variables may also be set for each of the above values: NAUTOBOT_DB_NAME NAUTOBOT_DB_USER NAUTOBOT_DB_PASSWORD NAUTOBOT_DB_HOST NAUTOBOT_DB_PORT NAUTOBOT_DB_TIMEOUT NAUTOBOT_DB_ENGINE Added in version 1.1.0 The NAUTOBOT_DB_ENGINE setting was added along with support for MySQL. Warning Nautobot supports either MySQL or PostgreSQL as a database backend. You must make sure that the ENGINE setting matches your selected database backend or you will be unable to connect to the database . Example: DATABASES = { 'default' : { 'NAME' : 'nautobot' , # Database name 'USER' : 'nautobot' , # Database username 'PASSWORD' : 'awesome_password' , # Database password 'HOST' : 'localhost' , # Database server 'PORT' : '' , # Database port (leave blank for default) 'CONN_MAX_AGE' : 300 , # Max database connection age 'ENGINE' : 'django.db.backends.postgresql' , # Database driver (\"mysql\" or \"postgresql\") } } Note Nautobot supports all database options supported by the underlying Django framework. For a complete list of available parameters, please see the official Django documentation on DATABASES .","title":"DATABASES"},{"location":"configuration/required-settings.html#mysql-unicode-settings","text":"Tip By default, MySQL is case-insensitive in its handling of text strings. This is different from PostgreSQL which is case-sensitive by default. We strongly recommend that you configure MySQL to be case-sensitive for use with Nautobot, either when you enable the MySQL server, or when you create the Nautobot database in MySQL. If you follow the provided installation instructions for CentOS or Ubuntu, the recommended steps there will include the appropriate database configuration. When using MySQL as a database backend, and you want to enable support for Unicode characters like the beloved poop emoji, you'll need to update your settings. If you try to use emojis without this setting, you will encounter a server error along the lines of Incorrect string value , because you are running afoul of the legacy implementation of Unicode (aka utf8 ) encoding in MySQL. The utf8 encoding in MySQL is limited to 3-bytes per character. Newer Unicode emoji require 4-bytes. To properly support using such characters, you will need to create an entry in DATABASES -> default -> OPTIONS with the value {\"charset\": \"utf8mb4\"} in your nautobot_config.py and restart all Nautobot services. This will tell MySQL to always use utf8mb4 character set for database client connections. For example: DATABASES = { \"default\" : { # Other settings... \"OPTIONS\" : { \"charset\" : \"utf8mb4\" }, # Add this line } } Tip Starting in v1.1.0, if you have generated a new nautobot_config.py using nautobot-server init , this line is already there for you in your config. You'll just need to uncomment it!","title":"MySQL Unicode Settings"},{"location":"configuration/required-settings.html#redis-settings","text":"Redis is an in-memory data store similar to memcached. It is required to support Nautobot's caching, task queueing, and webhook features. The connection settings are explained here, allowing Nautobot to connect to different Redis instances/databases per feature. Warning It is highly recommended to keep the Redis databases for caching and tasks separate. Using the same database number on the same Redis instance for both may result in queued background tasks being lost during cache flushing events. For this reason, the default settings utilize database 1 for caching and database 0 for tasks. Tip The default Redis settings in your nautobot_config.py should be suitable for most deployments and should only require customization for more advanced configurations.","title":"Redis Settings"},{"location":"configuration/required-settings.html#caching","text":"Nautobot supports database query caching using django-cacheops . Caching is configured by defining the CACHEOPS_REDIS setting which in its simplest form is just a URL. For more details on Nautobot's caching, including TLS and HA configuration, see the guide on Caching . Important Nautobot does not utilize the built-in Django cache framework to perform caching, as django-cacheops takes its place.","title":"Caching"},{"location":"configuration/required-settings.html#cacheops_redis","text":"Default: \"redis://localhost:6379/1\" Environment Variable: NAUTOBOT_CACHEOPS_REDIS If you wish to use SSL, you may set the URL scheme to rediss:// , for example: CACHEOPS_REDIS = \"rediss://localhost:6379/1\" This setting may also be a dictionary style to provide additional options such as custom TLS/SSL settings, for example: import ssl CACHEOPS_REDIS = { \"host\" : os . getenv ( \"NAUTOBOT_REDIS_HOST\" , \"localhost\" ), \"port\" : int ( os . getenv ( \"NAUTOBOT_REDIS_PORT\" , 6379 )), \"password\" : os . getenv ( \"NAUTOBOT_REDIS_PASSWORD\" , \"\" ), \"ssl\" : True , \"ssl_cert_reqs\" : ssl . CERT_REQUIRED , \"ssl_ca_certs\" : \"/opt/nautobot/redis/ca.crt\" , \"ssl_certfile\" : \"/opt/nautobot/redis/tls.crt\" , \"ssl_keyfile\" : \"/opt/nautobot/redis/tls.key\" , } Additional settings may be available and are not covered here. Please see the official guide on Cacheops setup .","title":"CACHEOPS_REDIS"},{"location":"configuration/required-settings.html#cacheops_sentinel","text":"Default: undefined If you are using Redis Sentinel for high-availability purposes, you must replace the CACHEOPS_REDIS setting with CACHEOPS_SENTINEL . For more details on configuring Nautobot to use Redis Sentinel see Using Redis Sentinel . For more details on how to configure Cacheops specifically to use Redis Sentinel see the official guide on Cacheops setup . Warning CACHEOPS_REDIS and CACHEOPS_SENTINEL are mutually exclusive and will result in an error if both are set.","title":"CACHEOPS_SENTINEL"},{"location":"configuration/required-settings.html#task-queuing","text":"","title":"Task Queuing"},{"location":"configuration/required-settings.html#caches","text":"The django-redis Django plugin is used to enable Redis as a concurrent write lock for preventing race conditions when allocating IP address objects, and also to define centralized Redis connection settings that will be used by RQ. The CACHES setting is required to to simplify the configuration for defining queues. It is not used for caching at this time. Important Nautobot does not utilize the built-in Django cache framework (which also relies on the CACHES setting) to perform caching because Cacheops is being used instead as detailed just above. Yes, we know this is confusing, which is why this is being called out explicitly! Default: # Uncomment the following line to configure TLS/SSL # import ssl CACHES = { \"default\" : { \"BACKEND\" : \"django_redis.cache.RedisCache\" , \"LOCATION\" : \"redis://localhost:6379/0\" , \"TIMEOUT\" : 300 , \"OPTIONS\" : { \"CLIENT_CLASS\" : \"django_redis.client.DefaultClient\" , # Uncomment the following lines to configure TLS/SSL # \"CONNECTION_POOL_KWARGS\": { # \"ssl_cert_reqs\": ssl.CERT_REQUIRED, # \"ssl_ca_certs\": \"/opt/nautobot/redis/ca.crt\", # \"ssl_certfile\": \"/opt/nautobot/redis/tls.crt\", # \"ssl_keyfile\": \"/opt/nautobot/redis/tls.key\", # }, }, } }","title":"CACHES"},{"location":"configuration/required-settings.html#task-queuing-with-rq","text":"Changed in version 1.1.0 Using task queueing with RQ is deprecated in exchange for using Celery. Support for RQ will be removed entirely starting in Nautobot 2.0. Task queues are configured by defining them within the RQ_QUEUES setting. Nautobot's core functionality relies on several distinct queues and these represent the minimum required set of queues that must be defined. By default, these use identical connection settings as defined in CACHES (yes, that's confusing and we'll explain below). In most cases the default settings will be suitable for production use, but it is up to you to modify the task queues for your environment and know that other use cases such as utilizing specific plugins may require additional queues to be defined.","title":"Task Queuing with RQ"},{"location":"configuration/required-settings.html#rq_queues","text":"The default value for this setting defines the queues and instructs RQ to use the default Redis connection defined in CACHES . This is intended to simplify default configuration for the common case. Please see the official django-rq documentation on support for django-redis connection settings for more information. Changed in version 1.1.0 The check_releases , custom_fields , and webhooks queues are no longer in use by Nautobot but maintained here for backwards compatibility; they will be removed in Nautobot 2.0. Default: RQ_QUEUES = { \"default\" : { \"USE_REDIS_CACHE\" : \"default\" , }, \"check_releases\" : { \"USE_REDIS_CACHE\" : \"default\" , }, \"custom_fields\" : { \"USE_REDIS_CACHE\" : \"default\" , }, \"webhooks\" : { \"USE_REDIS_CACHE\" : \"default\" , }, } More verbose dictionary-style configuration is still supported, but is not required unless you absolutely need more advanced task queuing configuration. An example configuration follows: RQ_QUEUES = { \"default\" : { \"HOST\" : \"localhost\" , \"PORT\" : 6379 , \"DB\" : 0 , \"PASSWORD\" : \"\" , \"SSL\" : False , \"DEFAULT_TIMEOUT\" : 300 }, \"webhooks\" : { \"HOST\" : \"localhost\" , \"PORT\" : 6379 , \"DB\" : 0 , \"PASSWORD\" : \"\" , \"SSL\" : False , \"DEFAULT_TIMEOUT\" : 300 }, \"check_releases\" : { \"HOST\" : \"localhost\" , \"PORT\" : 6379 , \"DB\" : 0 , \"PASSWORD\" : \"\" , \"SSL\" : False , \"DEFAULT_TIMEOUT\" : 300 }, \"custom_fields\" : { \"HOST\" : \"localhost\" , \"PORT\" : 6379 , \"DB\" : 0 , \"PASSWORD\" : \"\" , \"SSL\" : False , \"DEFAULT_TIMEOUT\" : 300 } } HOST - Name or IP address of the Redis server (use localhost if running locally) PORT - TCP port of the Redis service; leave blank for default port (6379) PASSWORD - Redis password (if set) DB - Numeric database ID SSL - Use SSL connection to Redis DEFAULT_TIMEOUT - The maximum execution time of a background task (such as running a Job ), in seconds. The following environment variables may also be set for some of the above values: NAUTOBOT_REDIS_HOST NAUTOBOT_REDIS_PORT NAUTOBOT_REDIS_PASSWORD NAUTOBOT_REDIS_USERNAME NAUTOBOT_REDIS_SSL NAUTOBOT_REDIS_TIMEOUT Note If you overload any of the default values in CACHES or RQ_QUEUES you may be unable to utilize the environment variables, depending on what you change. For more details on configuring RQ, please see the documentation for Django RQ installation .","title":"RQ_QUEUES"},{"location":"configuration/required-settings.html#task-queuing-with-celery","text":"Out of the box you do not need to make any changes to utilize task queueing with Celery. All of the default settings are sufficient for most installations. In the event you do need to make customizations to how Celery interacts with the message broker such as for more advanced clustered deployments, the following settings are required.","title":"Task Queuing with Celery"},{"location":"configuration/required-settings.html#celery_broker_url","text":"This setting tells Celery and its workers how and where to communicate with the message broker. The default value for this points to redis://localhost:6379/0 . Please see the optional settings documentation for CELERY_BROKER_URL for more information on customizing this setting.","title":"CELERY_BROKER_URL"},{"location":"configuration/required-settings.html#celery_result_backend","text":"This setting tells Celery and its workers how and where to store message results. This defaults to the same value as CELERY_BROKER_URL . In some more advanced setups it may be required for these to be separate locations, however in our configuration guides these are always the same. Please see the optional settings documentation for CELERY_RESULT_BACKEND for more information on customizing this setting.","title":"CELERY_RESULT_BACKEND"},{"location":"configuration/required-settings.html#configuring-celery-with-tls","text":"Optionally, you can configure Celery to use custom SSL certificates to connect to redis by setting the following variables: import ssl CELERY_REDIS_BACKEND_USE_SSL = { \"ssl_cert_reqs\" : ssl . CERT_REQUIRED , \"ssl_ca_certs\" : \"/opt/nautobot/redis/ca.crt\" , \"ssl_certfile\" : \"/opt/nautobot/redis/tls.crt\" , \"ssl_keyfile\" : \"/opt/nautobot/redis/tls.key\" , } CELERY_BROKER_USE_SSL = CELERY_REDIS_BACKEND_USE_SSL Please see the celery documentation for additional details.","title":"Configuring Celery with TLS"},{"location":"configuration/required-settings.html#configuring-celery-for-high-availability","text":"High availability clustering of Redis for use with Celery can be performed using Redis Sentinel. Please see documentation section on configuring Celery for Redis Sentinel for more information.","title":"Configuring Celery for High Availability"},{"location":"configuration/required-settings.html#secret_key","text":"Environment Variable: NAUTOBOT_SECRET_KEY This is a secret, random string used to assist in the creation new cryptographic hashes for passwords and HTTP cookies. The key defined here should not be shared outside of the configuration file. SECRET_KEY can be changed at any time, however be aware that doing so will invalidate all existing sessions. Please note that this key is not used directly for hashing user passwords or for the encrypted storage of secret data in Nautobot. SECRET_KEY should be at least 50 characters in length and contain a random mix of letters, digits, and symbols. Note A unique SECRET_KEY is generated for you automatically when you use nautobot-server init to create a new nautobot_config.py . You may run nautobot-server generate_secret_key to generate a new key at any time. $ nautobot-server generate_secret_key +$_kw69oq&fbkfk6&q-+ksbgzw1&061ghw%420u3(wen54w(m Alternatively use the following command to generate a secret even before nautobot-server is runnable: $ LC_ALL=C tr -cd '[:lower:][:digit:]!@#$%^&*(\\-_=+)' < /dev/urandom | fold -w50 | head -n1 9.V$@Kxkc@@Kd@z<a/=.J-Y;rYc79<y@](9o9(L(*sS)Q+ud5P Warning In the case of a highly available installation with multiple web servers, SECRET_KEY must be identical among all servers in order to maintain a persistent user session state. For more details see Nautobot Configuration .","title":"SECRET_KEY"},{"location":"configuration/authentication/ldap.html","text":"LDAP Authentication \u00b6 This guide explains how to implement LDAP authentication using an external server. User authentication will fall back to built-in Django users in the event of a failure. Install Requirements \u00b6 Install System Packages \u00b6 On Ubuntu: $ sudo apt install -y libldap-dev libsasl2-dev On CentOS: $ sudo dnf install -y openldap-devel Install django-auth-ldap \u00b6 Warning This and all remaining steps in this document should all be performed as the nautobot user! Hint: Use sudo -iu nautobot Activate the Python virtual environment and install the django-auth-ldap package using pip: $ source /opt/nautobot/bin/activate (nautobot) $ pip3 install \"nautobot[ldap]\" Once installed, add the package to local_requirements.txt to ensure it is re-installed during future rebuilds of the virtual environment: (nautobot) $ echo \"nautobot[ldap]\" >> /opt/nautobot/local_requirements.txt Configuration \u00b6 Enable the LDAP authentication backend by adding the following to your nautobot_config.py : Note It is critical that you include the ObjectPermissionsBackend provided by Nautobot after the LDAPBackend so that object-level permissions features can work properly. AUTHENTICATION_BACKENDS = [ 'django_auth_ldap.backend.LDAPBackend' , 'nautobot.core.authentication.ObjectPermissionBackend' , ] General Server Configuration \u00b6 Define all of the parameters required below in your nautobot_config.py . Complete documentation of all django-auth-ldap configuration options is included in the project's official documentation . Info When using Windows Server 2012 you may wish to use the Global Catalog by specifying a port on AUTH_LDAP_SERVER_URI . Use 3269 for secure ( ldaps:// ), or 3268 for non-secure. import ldap # Server URI AUTH_LDAP_SERVER_URI = \"ldap://ad.example.com\" # The following may be needed if you are binding to Active Directory. AUTH_LDAP_CONNECTION_OPTIONS = { ldap . OPT_REFERRALS : 0 } # Set the DN and password for the Nautobot service account. AUTH_LDAP_BIND_DN = \"CN=NAUTOBOTSA, OU=Service Accounts,DC=example,DC=com\" AUTH_LDAP_BIND_PASSWORD = \"demo\" Encryption Options \u00b6 It is recommended when using LDAP to use STARTTLS, however SSL can also be used. TLS Options \u00b6 STARTTLS can be configured by setting AUTH_LDAP_START_TLS = True and using the ldap:// URI scheme. AUTH_LDAP_SERVER_URI = \"ldap://ad.example.com\" AUTH_LDAP_START_TLS = True SSL Options \u00b6 SSL can also be used by using the ldaps:// URI scheme. AUTH_LDAP_SERVER_URI = \"ldaps://ad.example.com\" Certificate Validation \u00b6 When using either TLS or SSL it is necessary to validate the certificate from your LDAP server. Copy your CA cert to /opt/nautobot/ca.pem . # Set the path to the trusted CA certificates and create a new internal SSL context. AUTH_LDAP_CONNECTION_OPTIONS = { ldap . OPT_X_TLS_CACERTFILE : \"/opt/nautobot/ca.pem\" , ldap . OPT_X_TLS_NEWCTX : 0 } If you prefer you can ignore the certificate, however, this is only recommended in development and not production. # WARNING: You should not do this in production! AUTH_LDAP_CONNECTION_OPTIONS = { ldap . OPT_X_TLS_REQUIRE_CERT : ldap . OPT_X_TLS_NEVER , } Additional ldap connection options can be found in the python-ldap documentation . User Authentication \u00b6 Info When using Windows Server 2012, AUTH_LDAP_USER_DN_TEMPLATE should be set to None. from django_auth_ldap.config import LDAPSearch # This search matches users with the sAMAccountName equal to the provided username. This is required if the user's # username is not in their DN (Active Directory). AUTH_LDAP_USER_SEARCH = LDAPSearch ( \"ou=Users,dc=example,dc=com\" , ldap . SCOPE_SUBTREE , \"(sAMAccountName= %(user)s )\" ) # If a user's DN is producible from their username, we don't need to search. AUTH_LDAP_USER_DN_TEMPLATE = \"uid= %(user)s ,ou=users,dc=example,dc=com\" # You can map user attributes to Django attributes as so. AUTH_LDAP_USER_ATTR_MAP = { \"first_name\" : \"givenName\" , \"last_name\" : \"sn\" , \"email\" : \"mail\" } Searching in Multiple LDAP Groups \u00b6 Define the user-groups in your environment, such as a *.env file (delimiter ';' ): # Groups to search for user objects. \"(sAMAccountName=%(user)s),...\" NAUTOBOT_AUTH_LDAP_USER_SEARCH_DN = OU = IT - Admins , OU = special - users , OU = Acme - User , DC = Acme , DC = local ; OU = Infrastruktur , OU = IT , OU = my - location , OU = User , OU = Acme - User , DC = Acme , DC = local Import LDAPSearchUnion in nautobot_config.py , and replace the AUTH_LDAP_USER_SEARCH command from above: from django_auth_ldap.config import ... , LDAPSearchUnion # ... AUTH_LDAP_USER_SEARCH_DN = os . getenv ( \"NAUTOBOT_AUTH_LDAP_USER_SEARCH_DN\" , \"\" ) if AUTH_LDAP_USER_SEARCH_DN != \"\" : user_search_dn_list = str ( AUTH_LDAP_USER_SEARCH_DN ) . split ( \";\" ) ldapsearch_objects = [] for sdn in user_search_dn_list : ldapsearch_objects . append ( LDAPSearch ( sdn . strip (), ldap . SCOPE_SUBTREE , \"(sAMAccountName= %(user)s )\" )) AUTH_LDAP_USER_SEARCH = LDAPSearchUnion ( * ldapsearch_objects ) User Groups for Permissions \u00b6 Info When using Microsoft Active Directory, support for nested groups can be activated by using NestedGroupOfNamesType() instead of GroupOfNamesType() for AUTH_LDAP_GROUP_TYPE . You will also need to modify the import line to use NestedGroupOfNamesType instead of GroupOfNamesType . from django_auth_ldap.config import LDAPSearch , GroupOfNamesType # This search ought to return all groups to which the user belongs. django_auth_ldap uses this to determine group # hierarchy. AUTH_LDAP_GROUP_SEARCH = LDAPSearch ( \"dc=example,dc=com\" , ldap . SCOPE_SUBTREE , \"(objectClass=group)\" ) AUTH_LDAP_GROUP_TYPE = GroupOfNamesType () # Define a group required to login. AUTH_LDAP_REQUIRE_GROUP = \"CN=NAUTOBOT_USERS,DC=example,DC=com\" # Define special user types using groups. Exercise great caution when assigning superuser status. AUTH_LDAP_USER_FLAGS_BY_GROUP = { \"is_active\" : \"cn=active,ou=groups,dc=example,dc=com\" , \"is_staff\" : \"cn=staff,ou=groups,dc=example,dc=com\" , \"is_superuser\" : \"cn=superuser,ou=groups,dc=example,dc=com\" } # For more granular permissions, we can map LDAP groups to Django groups. AUTH_LDAP_FIND_GROUP_PERMS = True # Cache groups for one hour to reduce LDAP traffic AUTH_LDAP_CACHE_TIMEOUT = 3600 is_active - All users must be mapped to at least this group to enable authentication. Without this, users cannot log in. is_staff - Users mapped to this group are enabled for access to the administration tools; this is the equivalent of checking the \"staff status\" box on a manually created user. This doesn't grant any specific permissions. is_superuser - Users mapped to this group will be granted superuser status. Superusers are implicitly granted all permissions. Warning Authentication will fail if the groups (the distinguished names) do not exist in the LDAP directory. Multiple LDAP Server Support \u00b6 Multiple servers can be supported in django-auth-ldap by the use of additional LDAP backends, as described in the library's documentation . In order to define and load additional backends into Nautobot a plugin can be used. This plugin will allow the backend(s) to be loaded into the Django settings for use within the nautobot_config.py file. At the simplest form the plugin should have a custom backend(s) defined: # my_customer_backends.py from django_auth_ldap.backend import LDAPBackend class LDAPBackendSecondary ( LDAPBackend ): settings_prefix = \"AUTH_LDAP_SECONDARY_\" If the plugin is named nautobot_ldap_plugin , the following snippet could be used to load the additional LDAP backend: # nautobot_config.py AUTHENTICATION_BACKENDS = [ 'django_auth_ldap.backend.LDAPBackend' , 'nautobot_ldap_plugin.my_customer_backends.LDAPBackendSecondary' , # path to the custom LDAP Backend 'nautobot.core.authentication.ObjectPermissionBackend' , ] Once the custom backend is loaded into the settings all the configuration items mentioned previously need to be completed for each server. As a simplified example defining the URIs would be accomplished by the following two lines in the nautobot_config.py file. A similar approach would be done to define the rest of the settings. # nautobot_config.py # Server URI which uses django_auth_ldap.backend.LDAPBackend AUTH_LDAP_SERVER_URI = \"ldap://ad.example.com\" # Server URI which uses nautobot_ldap_plugin.my_customer_backends.LDAPBackendSecondary AUTH_LDAP_SECONDARY_SERVER_URI = \"ldap://secondary-ad.example.com\" Info In this example the default LDAPBackend was still used as the first LDAP server, which utilized the AUTH_LDAP_* environment variables. It is also possible to remove the default backend and create multiple custom backends instead to normalize the environment variable naming scheme. Troubleshooting LDAP \u00b6 systemctl restart nautobot restarts the Nautobot service, and initiates any changes made to nautobot_config.py . If there are syntax errors present, the Nautobot process will not spawn an instance, and errors should be logged to /var/log/messages . For troubleshooting LDAP user/group queries, add or merge the following logging configuration to nautobot_config.py : LOGGING = { 'version' : 1 , 'disable_existing_loggers' : False , 'handlers' : { 'nautobot_auth_log' : { 'level' : 'DEBUG' , 'class' : 'logging.handlers.RotatingFileHandler' , 'filename' : '/opt/nautobot/logs/django-ldap-debug.log' , 'maxBytes' : 1024 * 500 , 'backupCount' : 5 , }, }, 'loggers' : { 'django_auth_ldap' : { 'handlers' : [ 'nautobot_auth_log' ], 'level' : 'DEBUG' , }, }, } Ensure the file and path specified in logfile exist and are writable and executable by the application service account. Restart the nautobot service and attempt to log into the site to trigger log entries to this file. Be sure to configure EXTERNAL_AUTH_DEFAULT_GROUPS and EXTERNAL_AUTH_DEFAULT_PERMISSIONS next.","title":"LDAP"},{"location":"configuration/authentication/ldap.html#ldap-authentication","text":"This guide explains how to implement LDAP authentication using an external server. User authentication will fall back to built-in Django users in the event of a failure.","title":"LDAP Authentication"},{"location":"configuration/authentication/ldap.html#install-requirements","text":"","title":"Install Requirements"},{"location":"configuration/authentication/ldap.html#install-system-packages","text":"On Ubuntu: $ sudo apt install -y libldap-dev libsasl2-dev On CentOS: $ sudo dnf install -y openldap-devel","title":"Install System Packages"},{"location":"configuration/authentication/ldap.html#install-django-auth-ldap","text":"Warning This and all remaining steps in this document should all be performed as the nautobot user! Hint: Use sudo -iu nautobot Activate the Python virtual environment and install the django-auth-ldap package using pip: $ source /opt/nautobot/bin/activate (nautobot) $ pip3 install \"nautobot[ldap]\" Once installed, add the package to local_requirements.txt to ensure it is re-installed during future rebuilds of the virtual environment: (nautobot) $ echo \"nautobot[ldap]\" >> /opt/nautobot/local_requirements.txt","title":"Install django-auth-ldap"},{"location":"configuration/authentication/ldap.html#configuration","text":"Enable the LDAP authentication backend by adding the following to your nautobot_config.py : Note It is critical that you include the ObjectPermissionsBackend provided by Nautobot after the LDAPBackend so that object-level permissions features can work properly. AUTHENTICATION_BACKENDS = [ 'django_auth_ldap.backend.LDAPBackend' , 'nautobot.core.authentication.ObjectPermissionBackend' , ]","title":"Configuration"},{"location":"configuration/authentication/ldap.html#general-server-configuration","text":"Define all of the parameters required below in your nautobot_config.py . Complete documentation of all django-auth-ldap configuration options is included in the project's official documentation . Info When using Windows Server 2012 you may wish to use the Global Catalog by specifying a port on AUTH_LDAP_SERVER_URI . Use 3269 for secure ( ldaps:// ), or 3268 for non-secure. import ldap # Server URI AUTH_LDAP_SERVER_URI = \"ldap://ad.example.com\" # The following may be needed if you are binding to Active Directory. AUTH_LDAP_CONNECTION_OPTIONS = { ldap . OPT_REFERRALS : 0 } # Set the DN and password for the Nautobot service account. AUTH_LDAP_BIND_DN = \"CN=NAUTOBOTSA, OU=Service Accounts,DC=example,DC=com\" AUTH_LDAP_BIND_PASSWORD = \"demo\"","title":"General Server Configuration"},{"location":"configuration/authentication/ldap.html#encryption-options","text":"It is recommended when using LDAP to use STARTTLS, however SSL can also be used.","title":"Encryption Options"},{"location":"configuration/authentication/ldap.html#tls-options","text":"STARTTLS can be configured by setting AUTH_LDAP_START_TLS = True and using the ldap:// URI scheme. AUTH_LDAP_SERVER_URI = \"ldap://ad.example.com\" AUTH_LDAP_START_TLS = True","title":"TLS Options"},{"location":"configuration/authentication/ldap.html#ssl-options","text":"SSL can also be used by using the ldaps:// URI scheme. AUTH_LDAP_SERVER_URI = \"ldaps://ad.example.com\"","title":"SSL Options"},{"location":"configuration/authentication/ldap.html#certificate-validation","text":"When using either TLS or SSL it is necessary to validate the certificate from your LDAP server. Copy your CA cert to /opt/nautobot/ca.pem . # Set the path to the trusted CA certificates and create a new internal SSL context. AUTH_LDAP_CONNECTION_OPTIONS = { ldap . OPT_X_TLS_CACERTFILE : \"/opt/nautobot/ca.pem\" , ldap . OPT_X_TLS_NEWCTX : 0 } If you prefer you can ignore the certificate, however, this is only recommended in development and not production. # WARNING: You should not do this in production! AUTH_LDAP_CONNECTION_OPTIONS = { ldap . OPT_X_TLS_REQUIRE_CERT : ldap . OPT_X_TLS_NEVER , } Additional ldap connection options can be found in the python-ldap documentation .","title":"Certificate Validation"},{"location":"configuration/authentication/ldap.html#user-authentication","text":"Info When using Windows Server 2012, AUTH_LDAP_USER_DN_TEMPLATE should be set to None. from django_auth_ldap.config import LDAPSearch # This search matches users with the sAMAccountName equal to the provided username. This is required if the user's # username is not in their DN (Active Directory). AUTH_LDAP_USER_SEARCH = LDAPSearch ( \"ou=Users,dc=example,dc=com\" , ldap . SCOPE_SUBTREE , \"(sAMAccountName= %(user)s )\" ) # If a user's DN is producible from their username, we don't need to search. AUTH_LDAP_USER_DN_TEMPLATE = \"uid= %(user)s ,ou=users,dc=example,dc=com\" # You can map user attributes to Django attributes as so. AUTH_LDAP_USER_ATTR_MAP = { \"first_name\" : \"givenName\" , \"last_name\" : \"sn\" , \"email\" : \"mail\" }","title":"User Authentication"},{"location":"configuration/authentication/ldap.html#searching-in-multiple-ldap-groups","text":"Define the user-groups in your environment, such as a *.env file (delimiter ';' ): # Groups to search for user objects. \"(sAMAccountName=%(user)s),...\" NAUTOBOT_AUTH_LDAP_USER_SEARCH_DN = OU = IT - Admins , OU = special - users , OU = Acme - User , DC = Acme , DC = local ; OU = Infrastruktur , OU = IT , OU = my - location , OU = User , OU = Acme - User , DC = Acme , DC = local Import LDAPSearchUnion in nautobot_config.py , and replace the AUTH_LDAP_USER_SEARCH command from above: from django_auth_ldap.config import ... , LDAPSearchUnion # ... AUTH_LDAP_USER_SEARCH_DN = os . getenv ( \"NAUTOBOT_AUTH_LDAP_USER_SEARCH_DN\" , \"\" ) if AUTH_LDAP_USER_SEARCH_DN != \"\" : user_search_dn_list = str ( AUTH_LDAP_USER_SEARCH_DN ) . split ( \";\" ) ldapsearch_objects = [] for sdn in user_search_dn_list : ldapsearch_objects . append ( LDAPSearch ( sdn . strip (), ldap . SCOPE_SUBTREE , \"(sAMAccountName= %(user)s )\" )) AUTH_LDAP_USER_SEARCH = LDAPSearchUnion ( * ldapsearch_objects )","title":"Searching in Multiple LDAP Groups"},{"location":"configuration/authentication/ldap.html#user-groups-for-permissions","text":"Info When using Microsoft Active Directory, support for nested groups can be activated by using NestedGroupOfNamesType() instead of GroupOfNamesType() for AUTH_LDAP_GROUP_TYPE . You will also need to modify the import line to use NestedGroupOfNamesType instead of GroupOfNamesType . from django_auth_ldap.config import LDAPSearch , GroupOfNamesType # This search ought to return all groups to which the user belongs. django_auth_ldap uses this to determine group # hierarchy. AUTH_LDAP_GROUP_SEARCH = LDAPSearch ( \"dc=example,dc=com\" , ldap . SCOPE_SUBTREE , \"(objectClass=group)\" ) AUTH_LDAP_GROUP_TYPE = GroupOfNamesType () # Define a group required to login. AUTH_LDAP_REQUIRE_GROUP = \"CN=NAUTOBOT_USERS,DC=example,DC=com\" # Define special user types using groups. Exercise great caution when assigning superuser status. AUTH_LDAP_USER_FLAGS_BY_GROUP = { \"is_active\" : \"cn=active,ou=groups,dc=example,dc=com\" , \"is_staff\" : \"cn=staff,ou=groups,dc=example,dc=com\" , \"is_superuser\" : \"cn=superuser,ou=groups,dc=example,dc=com\" } # For more granular permissions, we can map LDAP groups to Django groups. AUTH_LDAP_FIND_GROUP_PERMS = True # Cache groups for one hour to reduce LDAP traffic AUTH_LDAP_CACHE_TIMEOUT = 3600 is_active - All users must be mapped to at least this group to enable authentication. Without this, users cannot log in. is_staff - Users mapped to this group are enabled for access to the administration tools; this is the equivalent of checking the \"staff status\" box on a manually created user. This doesn't grant any specific permissions. is_superuser - Users mapped to this group will be granted superuser status. Superusers are implicitly granted all permissions. Warning Authentication will fail if the groups (the distinguished names) do not exist in the LDAP directory.","title":"User Groups for Permissions"},{"location":"configuration/authentication/ldap.html#multiple-ldap-server-support","text":"Multiple servers can be supported in django-auth-ldap by the use of additional LDAP backends, as described in the library's documentation . In order to define and load additional backends into Nautobot a plugin can be used. This plugin will allow the backend(s) to be loaded into the Django settings for use within the nautobot_config.py file. At the simplest form the plugin should have a custom backend(s) defined: # my_customer_backends.py from django_auth_ldap.backend import LDAPBackend class LDAPBackendSecondary ( LDAPBackend ): settings_prefix = \"AUTH_LDAP_SECONDARY_\" If the plugin is named nautobot_ldap_plugin , the following snippet could be used to load the additional LDAP backend: # nautobot_config.py AUTHENTICATION_BACKENDS = [ 'django_auth_ldap.backend.LDAPBackend' , 'nautobot_ldap_plugin.my_customer_backends.LDAPBackendSecondary' , # path to the custom LDAP Backend 'nautobot.core.authentication.ObjectPermissionBackend' , ] Once the custom backend is loaded into the settings all the configuration items mentioned previously need to be completed for each server. As a simplified example defining the URIs would be accomplished by the following two lines in the nautobot_config.py file. A similar approach would be done to define the rest of the settings. # nautobot_config.py # Server URI which uses django_auth_ldap.backend.LDAPBackend AUTH_LDAP_SERVER_URI = \"ldap://ad.example.com\" # Server URI which uses nautobot_ldap_plugin.my_customer_backends.LDAPBackendSecondary AUTH_LDAP_SECONDARY_SERVER_URI = \"ldap://secondary-ad.example.com\" Info In this example the default LDAPBackend was still used as the first LDAP server, which utilized the AUTH_LDAP_* environment variables. It is also possible to remove the default backend and create multiple custom backends instead to normalize the environment variable naming scheme.","title":"Multiple LDAP Server Support"},{"location":"configuration/authentication/ldap.html#troubleshooting-ldap","text":"systemctl restart nautobot restarts the Nautobot service, and initiates any changes made to nautobot_config.py . If there are syntax errors present, the Nautobot process will not spawn an instance, and errors should be logged to /var/log/messages . For troubleshooting LDAP user/group queries, add or merge the following logging configuration to nautobot_config.py : LOGGING = { 'version' : 1 , 'disable_existing_loggers' : False , 'handlers' : { 'nautobot_auth_log' : { 'level' : 'DEBUG' , 'class' : 'logging.handlers.RotatingFileHandler' , 'filename' : '/opt/nautobot/logs/django-ldap-debug.log' , 'maxBytes' : 1024 * 500 , 'backupCount' : 5 , }, }, 'loggers' : { 'django_auth_ldap' : { 'handlers' : [ 'nautobot_auth_log' ], 'level' : 'DEBUG' , }, }, } Ensure the file and path specified in logfile exist and are writable and executable by the application service account. Restart the nautobot service and attempt to log into the site to trigger log entries to this file. Be sure to configure EXTERNAL_AUTH_DEFAULT_GROUPS and EXTERNAL_AUTH_DEFAULT_PERMISSIONS next.","title":"Troubleshooting LDAP"},{"location":"configuration/authentication/remote.html","text":"Remote User Authentication \u00b6 Nautobot can be configured to support remote user authentication by inferring users from an HTTP header set by an authenticating reverse proxy (e.g. NGINX). This document describes how to make use of an external authentication source (where the Web server sets the REMOTE_USER environment variable). This type of authentication solution is typically seen on intranet sites, with single sign-on solutions. User authentication will still fall back to built-in Django users in the event of a failure in remote authentication. Installation \u00b6 Enable the remote user authentication backend by adding the following to your nautobot_config.py : Note It is critical that you include the ObjectPermissionsBackend provided by Nautobot after the RemoteUserBackend so that object-level permissions features can work properly. AUTHENTICATION_BACKENDS = [ 'nautobot.core.authentication.RemoteUserBackend' , 'nautobot.core.authentication.ObjectPermissionBackend' , ] Configuration \u00b6 The following configuration variables describe the default values and as long as RemoteUserBackend has been installed as described above, no changes are required. If you do require customizing any of these settings, they must be set in your nautobot_config.py . REMOTE_AUTH_AUTO_CREATE_USER \u00b6 Default: False If set to True , local accounts will be automatically created for users authenticated via a remote service. REMOTE_AUTH_HEADER \u00b6 Default: 'HTTP_REMOTE_USER' When remote user authentication is in use, this is the name of the HTTP header which informs Nautobot of the currently authenticated user. For example, to use the request header X-Remote-User it needs to be set to HTTP_X_REMOTE_USER . Be sure to configure EXTERNAL_AUTH_DEFAULT_GROUPS and EXTERNAL_AUTH_DEFAULT_PERMISSIONS next.","title":"Remote Header"},{"location":"configuration/authentication/remote.html#remote-user-authentication","text":"Nautobot can be configured to support remote user authentication by inferring users from an HTTP header set by an authenticating reverse proxy (e.g. NGINX). This document describes how to make use of an external authentication source (where the Web server sets the REMOTE_USER environment variable). This type of authentication solution is typically seen on intranet sites, with single sign-on solutions. User authentication will still fall back to built-in Django users in the event of a failure in remote authentication.","title":"Remote User Authentication"},{"location":"configuration/authentication/remote.html#installation","text":"Enable the remote user authentication backend by adding the following to your nautobot_config.py : Note It is critical that you include the ObjectPermissionsBackend provided by Nautobot after the RemoteUserBackend so that object-level permissions features can work properly. AUTHENTICATION_BACKENDS = [ 'nautobot.core.authentication.RemoteUserBackend' , 'nautobot.core.authentication.ObjectPermissionBackend' , ]","title":"Installation"},{"location":"configuration/authentication/remote.html#configuration","text":"The following configuration variables describe the default values and as long as RemoteUserBackend has been installed as described above, no changes are required. If you do require customizing any of these settings, they must be set in your nautobot_config.py .","title":"Configuration"},{"location":"configuration/authentication/remote.html#remote_auth_auto_create_user","text":"Default: False If set to True , local accounts will be automatically created for users authenticated via a remote service.","title":"REMOTE_AUTH_AUTO_CREATE_USER"},{"location":"configuration/authentication/remote.html#remote_auth_header","text":"Default: 'HTTP_REMOTE_USER' When remote user authentication is in use, this is the name of the HTTP header which informs Nautobot of the currently authenticated user. For example, to use the request header X-Remote-User it needs to be set to HTTP_X_REMOTE_USER . Be sure to configure EXTERNAL_AUTH_DEFAULT_GROUPS and EXTERNAL_AUTH_DEFAULT_PERMISSIONS next.","title":"REMOTE_AUTH_HEADER"},{"location":"configuration/authentication/sso.html","text":"Single Sign On \u00b6 Nautobot supports several different authentication mechanisms including OAuth (1 and 2), OpenID, SAML, and others. To accomplish this, Nautobot comes preinstalled with the social-auth-app-django Python module. This module supports several authentication backends by default including: Google Microsoft Azure Active Directory Okta And many more... Installation \u00b6 Warning Unless otherwise noted, all remaining steps in this document should all be performed as the nautobot user! Hint: Use sudo -iu nautobot Install Dependencies \u00b6 If you are using OpenID Connect or SAML you will also need to install the extra dependencies for those. OpenID Connect Dependencies \u00b6 For OpenID connect, you'll need to install the sso Python extra. $ pip3 install \"nautobot[sso]\" SAML Dependencies \u00b6 For SAML, additional system-level dependencies are required so that the specialized XML libraries can be built and compiled for your system. Note These instructions have only been certified on Ubuntu 20.04 at this time. Install the system dependencies as root : $ sudo apt install -y libxmlsec1-dev libxmlsec1-openssl pkg-config Install the sso Python extra as the nautobot user. $ pip3 install \"nautobot[sso]\" Please see the SAML configuration guide below for an example of how to configure Nautobot to authenticate using SAML with Google as the identity provider. Configuration \u00b6 Authentication Backends \u00b6 To use external authentication, you'll need to define AUTHENTICATION_BACKENDS in your nautobot_config.py . Insert the desired external authentication backend as the first item in the list. This step is key to properly redirecting when users click the login button. You must also ensure that nautobot.core.authentication.ObjectPermissionBackend is always the second item in the list. It is an error to exclude this backend. Note It is critical that you include the ObjectPermissionsBackend provided by Nautobot after the desired backend so that object-level permissions features can work properly. For example, if you wanted to use Google OAuth2 as your authentication backend: AUTHENTICATION_BACKENDS = [ \"social_core.backends.google.GoogleOAuth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] Note Many backends have settings specific to that backend that are not covered in this guide. Please consult the documentation for your desired backend linked in the next section. Warning You should only enable one social authentication authentication backend. It is technically possible to use multiple backends but we cannot officially support more than one at this time. Custom Authentication Backends \u00b6 The default external authentication supported is social-auth-app-django as stated above. If you have developed your own external authentication backend, you will need to configure SOCIAL_AUTH_BACKEND_PREFIX to use your backend instead and correctly enable the SSO redirect when the login button is clicked. For example, if your custom authentication backend is available at custom_auth.backends.custom.Oauth2 , you would set things as follows: SOCIAL_AUTH_BACKEND_PREFIX = \"custom_auth.backends\" AUTHENTICATION_BACKENDS = [ \"custom_auth.backends.custom.Oauth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] In the example above, SOCIAL_AUTH_BACKEND_PREFIX was set to custom_auth.backends within the nautobot_config.py for our custom authentication plugin we created ( custom_auth.backends.custom.Oauth2 ). This will enable the SSO redirect for users when they click the login button. Select your Authentication Backend \u00b6 You will need to select the correct social authentication module name for your desired method of external authentication. Please see the official Python Social Auth documentation on supported backends for more the full list of backends and any specific configuration or required settings. Some common backend module names include: Backend Social Auth Backend Module Name Microsoft Azure Active Directory social_core.backends.azuread.AzureADOAuth2 social_core.backends.azuread_b2c.AzureADB2COAuth2 social_core.backends.azuread_tenant.AzureADTenantOAuth2 social_core.backends.azuread_tenant.AzureADV2TenantOAuth2 Google social_core.backends.gae.GoogleAppEngineAuth social_core.backends.google.GoogleOAuth2 social_core.backends.google.GoogleOAuth social_core.backends.google_openidconnect.GoogleOpenIdConnect Okta social_core.backends.okta.OktaOAuth2 social_core.backends.okta_openidconnect.OktaOpenIdConnect SAML social_core.backends.saml.SAMLAuth User Permissions \u00b6 By default, once authenticated, if the user has never logged in before a new user account will be created for the user. This new user will not be a member of any group or have any permissions assigned. If you would like to create users with a default set of permissions there are some additional variables to configure the permissions. Please see the documentation on EXTERNAL_AUTH_DEFAULT_GROUPS and EXTERNAL_AUTH_DEFAULT_PERMISSIONS for more information. Configuration Guides \u00b6 The following guides are provided for some of the most common authentication methods. Okta \u00b6 In the Okta admin portal, create a new Web application Configure the application as follows: Base URIs : should be the URI of your Nautobot application such as https://nautobot.example.com Login redirect URIs : should be the Base URI plus /complete/okta-openidconnect/ such as https://nautobot.example.com/complete/okta-openidconnect/ Logout redirect URIs : should be the Base URI plus /disconnect/okta-openidconnect/ such as https://nautobot.example.com/disconnect/okta-openidconnect/ Once the application is configured in Okta, SSO can either be configured with OAuth2 or OpenID Connect (OIDC). When using an organization's authentication server OAuth2 is preferred; with custom Okta authentication backends, use OIDC. Okta - OAuth2 \u00b6 Edit your nautobot_config.py as follows: AUTHENTICATION_BACKENDS = [ \"social_core.backends.okta.OktaOAuth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] SOCIAL_AUTH_OKTA_OAUTH2_KEY = '<Client ID from Okta>' SOCIAL_AUTH_OKTA_OAUTH2_SECRET = '<Client Secret From Okta>' SOCIAL_AUTH_OKTA_OAUTH2_API_URL = 'https://<Okta URL>' Okta - OpenID \u00b6 Edit your nautobot_config.py as follows: AUTHENTICATION_BACKENDS = [ \"social_core.backends.okta_openidconnect.OktaOpenIdConnect\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] SOCIAL_AUTH_OKTA_OPENIDCONNECT_KEY = '<Client ID from Okta>' SOCIAL_AUTH_OKTA_OPENIDCONNECT_SECRET = '<Client Secret From Okta>' SOCIAL_AUTH_OKTA_OPENIDCONNECT_API_URL = 'https://<Okta URL>/oauth2/<Authentication Server>' The /default authentication server can be used for testing, however, it should not be used in production. Okta - Additional Scopes \u00b6 It is possible to get additional OAuth scopes from okta by adding them to the SOCIAL_AUTH_{BACKEND}_SCOPE list. For example to get the groups scope from Okta using OAuth2 add the following to your nautobot_config.py : SOCIAL_AUTH_OKTA_OAUTH2_SCOPE = [ 'groups' ] for OpenID: SOCIAL_AUTH_OKTA_OPENIDCONNECT_SCOPE = [ 'groups' ] In order to use this returned scope a custom function needs to be written and added to the SOCIAL_AUTH_PIPELINE as described in the python-social-auth authentication pipeline documentation . An example to sync groups with Okta is provided in the examples/okta folder in the root of the Nautobot repository. Google - OAuth2 \u00b6 The following instructions guide you through the process of configuring Google for OAuth2 authentication. Important Please note there is further guidance provided by python-social-auth as well as Google . For more information please utilize these additional resources. In the Google API Console create a new project or select an existing one. Select OAuth consent screen from the menu on the left side of the page For User Type select Internal and click Create Configure as follows: App name : Acme Corp Nautobot User support email : select an email App logo : The Nautobot logo can be found at nautobot/project-static/img/nautobot_logo.png Click Save and Continue No additional scopes are needed click Save and Continue Select Credentials from the menu on the left side of the page Click + Create Credentials at the top of the page and select OAuth client ID Configure as follows: Application type : Web application Name : Nautobot Authorized redirect URIs : should be the Nautobot URL plus /complete/google-oauth2/ for example https://nautobot.example.com/complete/google-oauth2/ Click Create Edit your nautobot_config.py as follows: AUTHENTICATION_BACKENDS = [ \"social_core.backends.google.GoogleOAuth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] SOCIAL_AUTH_GOOGLE_OAUTH2_KEY = '<Client ID from Google>' SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET = '<Secret ID from Google>' SOCIAL_AUTH_GOOGLE_OAUTH2_SCOPE = [ 'openid' ] SAML \u00b6 This guide will walk you through configuring Nautobot to authenticate using SAML with Google as the identity provider. Important Please note that there is further guidance provided by python-social-auth and Google . For more information please utilize these additional resources. Prerequisites \u00b6 Warning SAML will not work without end-to-end encryption. These requirements are not flexible. Before you begin you will need the following: The fully-qualified domain name (FQDN) of your Nautobot host must be registered in DNS. For this example we will be using nautobot.example.com . A valid publicly trusted SSL certificate matching the FQDN of your host. You cannot use a self-signed certificate. Google validates this certificate to assert authenticity of SAML authentication requests. The name and email address for a technical point of contact. For this example we will use \"Bob Jones, bob@example.com\". The name and email address for a support point of contact. For this example we will use \"Alice Jenkins, alice@example.com.\" Setup SAML in Google \u00b6 Visit the Web and mobile apps console in the Google Admin dashboard. Follow Google's official document to Set up your own custom SAML application , pausing at step 6. From step 6 of the instructions, capture the SSO URL , Entity ID , and Certificate . You will use these in later steps to configure Nautobot. Each of these will be referred to as GOOGLE_SSO_URL , GOOGLE_ENTITY_ID , and GOOGLE_CERTIFICATE respectively. Skip step 7 in the instructions, as that does not apply here because we will be configuring Nautobot directly. For step 9 of the instructions under Service provider details , provide the following ACS URL : https://nautobot.example.com/complete/saml/ Entity ID: https://nautobot.example.com/ Start URL: Leave this field blank Skip step 10 in the instructions, as a signed response is not required. For step 11 of the instructions, under Name ID , set the following: Name ID Format : Select EMAIL Name ID: Select Basic Information > Primary Email For step 13 of the instructions, on the Attribute mapping page, add the following mappings for Google Directory attributes to App attributes : Primary email --> email First name --> first_name Last name --> last_name Click Finish Configure Nautobot \u00b6 There is a lot to configure to inform Nautobot how to integrate with SAML, so please provide the following configuration very carefully. All of these values must be correct in your nautobot_config.py . Important Refer to the official Python Social Auth documentation for required SAML configuration if you run into any issues. # Django authentication backends AUTHENTICATION_BACKENDS = [ \"social_core.backends.saml.SAMLAuth\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] # The https FQDN to your Nautobot instance SOCIAL_AUTH_SAML_SP_ENTITY_ID = \"https://nautobot.example.com/\" # X.509 cert/key pair used for host verification are not used for this example because # Nautobot is directly authenticating itself to Google. Set them to empty strings. SOCIAL_AUTH_SAML_SP_PUBLIC_CERT = \"\" SOCIAL_AUTH_SAML_SP_PRIVATE_KEY = \"\" # A dictionary that contains information about your app. You must specify values for # English at a minimum. SOCIAL_AUTH_SAML_ORG_INFO = { \"en-US\" : { \"name\" : \"Nautobot\" , \"displayname\" : \"Nautobot\" , \"url\" : \"https://nautobot.example.com\" , } } # Technical point of contact SOCIAL_AUTH_SAML_TECHNICAL_CONTACT = { \"givenName\" : \"Bob Jones\" , \"emailAddress\" : \"bob@example.com\" } # Support point of contact SOCIAL_AUTH_SAML_SUPPORT_CONTACT = { \"givenName\" : \"Alice Jenkins\" , \"emailAddress\" : \"alice@example.com\" } # The Entity ID URL for Google from step 3 GOOGLE_ENTITY_ID = \"<Entity ID from Google>\" # The SSO URL for Google from step 3 GOOGLE_SSO_URL = \"<SSO URL from Google>\" # The Certificate for Google from step 3 GOOGLE_CERTIFICATE = \"<Certificate from Google>\" # The most important setting. List the Entity ID, SSO URL, and x.509 public key certificate # for each provider that you app wants to support. We are only supporting Google for this # example. SOCIAL_AUTH_SAML_ENABLED_IDPS = { \"google\" : { \"entity_id\" : GOOGLE_ENTITY_ID , \"url\" : GOOGLE_SSO_URL , \"x509cert\" : GOOGLE_CERTIFICATE , # These are used to map to User object fields in Nautobot using Google # attribute fields we configured in step 8 of \"Setup SAML in Google\". \"attr_user_permanent_id\" : \"email\" , \"attr_first_name\" : \"first_name\" , \"attr_last_name\" : \"last_name\" , \"attr_username\" : \"email\" , \"attr_email\" : \"email\" , } } # Required for correctly redirecting when behind SSL proxy (NGINX). You may or may not need # these depending on your production deployment. They are provided here just in case. SECURE_SSL_REDIRECT = True SECURE_PROXY_SSL_HEADER = ( 'HTTP_X_FORWARDED_PROTO' , 'https' ) Enable SAML in Google \u00b6 Now that you've configured both Google and Nautobot for SAML, you still need to enable SAML for your users in your Google domain. On Google's official site to Set up your own custom SAML application , scroll down to Turn on your SAML app and follow the remaining instructions to enable and verify SAML in Google. Login with SAML \u00b6 Note the provider entry we configured in SOCIAL_AUTH_SAML_ENABLED_IDPS as google . This will be used to login and will be referenced in the query parameter using idp=google . For example /login/saml/?idp=google . This should be the URL that is mapped to the \"Log in\" button on the top right of the index page when you navigate to Nautobot in your browser. Clicking this link should automatically redirect you to Google, ask you to \"Choose an account\", log you in and redirect you back to the Nautobot home page. Your email address will also be your username. Be sure to configure EXTERNAL_AUTH_DEFAULT_GROUPS and EXTERNAL_AUTH_DEFAULT_PERMISSIONS next. Azure AD \u00b6 In the Azure admin portal, search for and select Azure Active Directory . Under Manage , select App registrations -> New registration . Configure the application as follows: Name : This is the user-facing display name for the app. Supported account types : This specifies the AD directories that you're allowing to authenticate with this app. Redirect URIs : Don't fill this out yet, it will be configured in the following steps. Once the application is configured in Azure, you'll be shown the app registration's Overview page. Please take note of the Application (client) ID for use later. SSO with Azure can either be configured with OAuth2 or OpenID Connect (OIDC). When using an organization's authentication server OAuth2 is preferred; with custom Azure authentication backends, use OIDC. From the App registration page, click on Authentication . Under Platform configurations , select Add a platform and select Web . Click on the Add a Redirect URI link on the page and configure it as follows: Redirect URIs : should be the Base URI plus /complete/azuread-oauth2/ such as https://nautobot.example.com/complete/azuread-oauth2/ Once the Redirect URI is set, the last thing you'll need is to generate a client secret . To do so, click on Certificates & secrets and then the New client secret option. At this point you'll need to specify the expiration for the secret. Microsoft recommends less than 12 months with a maximum of 24 months as an option. Ensure you make a note of the secret that's generated for the next step. With the client secret generated, edit your nautobot_config.py as follows: Azure AD - OAuth2 \u00b6 If your app is linked to the common tenant, you'll want to edit your nautobot_config.py as follows: AUTHENTICATION_BACKENDS = [ \"social_core.backends.azuread.AzureADOAuth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] SOCIAL_AUTH_AZUREAD_OAUTH2_KEY = \"<Client ID from Azure>\" SOCIAL_AUTH_AZUREAD_OAUTH2_SECRET = \"<Client Secret From Azure>\" Azure - Tenant Support \u00b6 If your app is linked to a specific tenant instead of the common tenant, you'll want to edit your nautobot_config.py as follows: AUTHENTICATION_BACKENDS = [ \"social_core.backends.azuread.AzureADTenantOAuth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_KEY = \"<Client ID from Azure>\" SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_SECRET = \"<Client Secret From Azure>\" SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_TENANT_ID = \"<Tenant ID from Azure>\" With those settings in place your users should be able to authenticate against Azure AD and successfully login to Nautobot. However, that user will not be placed in any groups or given any permissions. In order to do so, you'll need to utilize a script to synchronize the groups passed from Azure to Nautobot after authentication succeeds. Any group permissions will need to be set manually in the Nautobot admin panel. An example to sync groups with Azure is provided in the examples/azure_ad folder in the root of the Nautobot repository.","title":"SSO"},{"location":"configuration/authentication/sso.html#single-sign-on","text":"Nautobot supports several different authentication mechanisms including OAuth (1 and 2), OpenID, SAML, and others. To accomplish this, Nautobot comes preinstalled with the social-auth-app-django Python module. This module supports several authentication backends by default including: Google Microsoft Azure Active Directory Okta And many more...","title":"Single Sign On"},{"location":"configuration/authentication/sso.html#installation","text":"Warning Unless otherwise noted, all remaining steps in this document should all be performed as the nautobot user! Hint: Use sudo -iu nautobot","title":"Installation"},{"location":"configuration/authentication/sso.html#install-dependencies","text":"If you are using OpenID Connect or SAML you will also need to install the extra dependencies for those.","title":"Install Dependencies"},{"location":"configuration/authentication/sso.html#openid-connect-dependencies","text":"For OpenID connect, you'll need to install the sso Python extra. $ pip3 install \"nautobot[sso]\"","title":"OpenID Connect Dependencies"},{"location":"configuration/authentication/sso.html#saml-dependencies","text":"For SAML, additional system-level dependencies are required so that the specialized XML libraries can be built and compiled for your system. Note These instructions have only been certified on Ubuntu 20.04 at this time. Install the system dependencies as root : $ sudo apt install -y libxmlsec1-dev libxmlsec1-openssl pkg-config Install the sso Python extra as the nautobot user. $ pip3 install \"nautobot[sso]\" Please see the SAML configuration guide below for an example of how to configure Nautobot to authenticate using SAML with Google as the identity provider.","title":"SAML Dependencies"},{"location":"configuration/authentication/sso.html#configuration","text":"","title":"Configuration"},{"location":"configuration/authentication/sso.html#authentication-backends","text":"To use external authentication, you'll need to define AUTHENTICATION_BACKENDS in your nautobot_config.py . Insert the desired external authentication backend as the first item in the list. This step is key to properly redirecting when users click the login button. You must also ensure that nautobot.core.authentication.ObjectPermissionBackend is always the second item in the list. It is an error to exclude this backend. Note It is critical that you include the ObjectPermissionsBackend provided by Nautobot after the desired backend so that object-level permissions features can work properly. For example, if you wanted to use Google OAuth2 as your authentication backend: AUTHENTICATION_BACKENDS = [ \"social_core.backends.google.GoogleOAuth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] Note Many backends have settings specific to that backend that are not covered in this guide. Please consult the documentation for your desired backend linked in the next section. Warning You should only enable one social authentication authentication backend. It is technically possible to use multiple backends but we cannot officially support more than one at this time.","title":"Authentication Backends"},{"location":"configuration/authentication/sso.html#custom-authentication-backends","text":"The default external authentication supported is social-auth-app-django as stated above. If you have developed your own external authentication backend, you will need to configure SOCIAL_AUTH_BACKEND_PREFIX to use your backend instead and correctly enable the SSO redirect when the login button is clicked. For example, if your custom authentication backend is available at custom_auth.backends.custom.Oauth2 , you would set things as follows: SOCIAL_AUTH_BACKEND_PREFIX = \"custom_auth.backends\" AUTHENTICATION_BACKENDS = [ \"custom_auth.backends.custom.Oauth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] In the example above, SOCIAL_AUTH_BACKEND_PREFIX was set to custom_auth.backends within the nautobot_config.py for our custom authentication plugin we created ( custom_auth.backends.custom.Oauth2 ). This will enable the SSO redirect for users when they click the login button.","title":"Custom Authentication Backends"},{"location":"configuration/authentication/sso.html#select-your-authentication-backend","text":"You will need to select the correct social authentication module name for your desired method of external authentication. Please see the official Python Social Auth documentation on supported backends for more the full list of backends and any specific configuration or required settings. Some common backend module names include: Backend Social Auth Backend Module Name Microsoft Azure Active Directory social_core.backends.azuread.AzureADOAuth2 social_core.backends.azuread_b2c.AzureADB2COAuth2 social_core.backends.azuread_tenant.AzureADTenantOAuth2 social_core.backends.azuread_tenant.AzureADV2TenantOAuth2 Google social_core.backends.gae.GoogleAppEngineAuth social_core.backends.google.GoogleOAuth2 social_core.backends.google.GoogleOAuth social_core.backends.google_openidconnect.GoogleOpenIdConnect Okta social_core.backends.okta.OktaOAuth2 social_core.backends.okta_openidconnect.OktaOpenIdConnect SAML social_core.backends.saml.SAMLAuth","title":"Select your Authentication Backend"},{"location":"configuration/authentication/sso.html#user-permissions","text":"By default, once authenticated, if the user has never logged in before a new user account will be created for the user. This new user will not be a member of any group or have any permissions assigned. If you would like to create users with a default set of permissions there are some additional variables to configure the permissions. Please see the documentation on EXTERNAL_AUTH_DEFAULT_GROUPS and EXTERNAL_AUTH_DEFAULT_PERMISSIONS for more information.","title":"User Permissions"},{"location":"configuration/authentication/sso.html#configuration-guides","text":"The following guides are provided for some of the most common authentication methods.","title":"Configuration Guides"},{"location":"configuration/authentication/sso.html#okta","text":"In the Okta admin portal, create a new Web application Configure the application as follows: Base URIs : should be the URI of your Nautobot application such as https://nautobot.example.com Login redirect URIs : should be the Base URI plus /complete/okta-openidconnect/ such as https://nautobot.example.com/complete/okta-openidconnect/ Logout redirect URIs : should be the Base URI plus /disconnect/okta-openidconnect/ such as https://nautobot.example.com/disconnect/okta-openidconnect/ Once the application is configured in Okta, SSO can either be configured with OAuth2 or OpenID Connect (OIDC). When using an organization's authentication server OAuth2 is preferred; with custom Okta authentication backends, use OIDC.","title":"Okta"},{"location":"configuration/authentication/sso.html#okta-oauth2","text":"Edit your nautobot_config.py as follows: AUTHENTICATION_BACKENDS = [ \"social_core.backends.okta.OktaOAuth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] SOCIAL_AUTH_OKTA_OAUTH2_KEY = '<Client ID from Okta>' SOCIAL_AUTH_OKTA_OAUTH2_SECRET = '<Client Secret From Okta>' SOCIAL_AUTH_OKTA_OAUTH2_API_URL = 'https://<Okta URL>'","title":"Okta - OAuth2"},{"location":"configuration/authentication/sso.html#okta-openid","text":"Edit your nautobot_config.py as follows: AUTHENTICATION_BACKENDS = [ \"social_core.backends.okta_openidconnect.OktaOpenIdConnect\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] SOCIAL_AUTH_OKTA_OPENIDCONNECT_KEY = '<Client ID from Okta>' SOCIAL_AUTH_OKTA_OPENIDCONNECT_SECRET = '<Client Secret From Okta>' SOCIAL_AUTH_OKTA_OPENIDCONNECT_API_URL = 'https://<Okta URL>/oauth2/<Authentication Server>' The /default authentication server can be used for testing, however, it should not be used in production.","title":"Okta - OpenID"},{"location":"configuration/authentication/sso.html#okta-additional-scopes","text":"It is possible to get additional OAuth scopes from okta by adding them to the SOCIAL_AUTH_{BACKEND}_SCOPE list. For example to get the groups scope from Okta using OAuth2 add the following to your nautobot_config.py : SOCIAL_AUTH_OKTA_OAUTH2_SCOPE = [ 'groups' ] for OpenID: SOCIAL_AUTH_OKTA_OPENIDCONNECT_SCOPE = [ 'groups' ] In order to use this returned scope a custom function needs to be written and added to the SOCIAL_AUTH_PIPELINE as described in the python-social-auth authentication pipeline documentation . An example to sync groups with Okta is provided in the examples/okta folder in the root of the Nautobot repository.","title":"Okta - Additional Scopes"},{"location":"configuration/authentication/sso.html#google-oauth2","text":"The following instructions guide you through the process of configuring Google for OAuth2 authentication. Important Please note there is further guidance provided by python-social-auth as well as Google . For more information please utilize these additional resources. In the Google API Console create a new project or select an existing one. Select OAuth consent screen from the menu on the left side of the page For User Type select Internal and click Create Configure as follows: App name : Acme Corp Nautobot User support email : select an email App logo : The Nautobot logo can be found at nautobot/project-static/img/nautobot_logo.png Click Save and Continue No additional scopes are needed click Save and Continue Select Credentials from the menu on the left side of the page Click + Create Credentials at the top of the page and select OAuth client ID Configure as follows: Application type : Web application Name : Nautobot Authorized redirect URIs : should be the Nautobot URL plus /complete/google-oauth2/ for example https://nautobot.example.com/complete/google-oauth2/ Click Create Edit your nautobot_config.py as follows: AUTHENTICATION_BACKENDS = [ \"social_core.backends.google.GoogleOAuth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] SOCIAL_AUTH_GOOGLE_OAUTH2_KEY = '<Client ID from Google>' SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET = '<Secret ID from Google>' SOCIAL_AUTH_GOOGLE_OAUTH2_SCOPE = [ 'openid' ]","title":"Google - OAuth2"},{"location":"configuration/authentication/sso.html#saml","text":"This guide will walk you through configuring Nautobot to authenticate using SAML with Google as the identity provider. Important Please note that there is further guidance provided by python-social-auth and Google . For more information please utilize these additional resources.","title":"SAML"},{"location":"configuration/authentication/sso.html#prerequisites","text":"Warning SAML will not work without end-to-end encryption. These requirements are not flexible. Before you begin you will need the following: The fully-qualified domain name (FQDN) of your Nautobot host must be registered in DNS. For this example we will be using nautobot.example.com . A valid publicly trusted SSL certificate matching the FQDN of your host. You cannot use a self-signed certificate. Google validates this certificate to assert authenticity of SAML authentication requests. The name and email address for a technical point of contact. For this example we will use \"Bob Jones, bob@example.com\". The name and email address for a support point of contact. For this example we will use \"Alice Jenkins, alice@example.com.\"","title":"Prerequisites"},{"location":"configuration/authentication/sso.html#setup-saml-in-google","text":"Visit the Web and mobile apps console in the Google Admin dashboard. Follow Google's official document to Set up your own custom SAML application , pausing at step 6. From step 6 of the instructions, capture the SSO URL , Entity ID , and Certificate . You will use these in later steps to configure Nautobot. Each of these will be referred to as GOOGLE_SSO_URL , GOOGLE_ENTITY_ID , and GOOGLE_CERTIFICATE respectively. Skip step 7 in the instructions, as that does not apply here because we will be configuring Nautobot directly. For step 9 of the instructions under Service provider details , provide the following ACS URL : https://nautobot.example.com/complete/saml/ Entity ID: https://nautobot.example.com/ Start URL: Leave this field blank Skip step 10 in the instructions, as a signed response is not required. For step 11 of the instructions, under Name ID , set the following: Name ID Format : Select EMAIL Name ID: Select Basic Information > Primary Email For step 13 of the instructions, on the Attribute mapping page, add the following mappings for Google Directory attributes to App attributes : Primary email --> email First name --> first_name Last name --> last_name Click Finish","title":"Setup SAML in Google"},{"location":"configuration/authentication/sso.html#configure-nautobot","text":"There is a lot to configure to inform Nautobot how to integrate with SAML, so please provide the following configuration very carefully. All of these values must be correct in your nautobot_config.py . Important Refer to the official Python Social Auth documentation for required SAML configuration if you run into any issues. # Django authentication backends AUTHENTICATION_BACKENDS = [ \"social_core.backends.saml.SAMLAuth\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] # The https FQDN to your Nautobot instance SOCIAL_AUTH_SAML_SP_ENTITY_ID = \"https://nautobot.example.com/\" # X.509 cert/key pair used for host verification are not used for this example because # Nautobot is directly authenticating itself to Google. Set them to empty strings. SOCIAL_AUTH_SAML_SP_PUBLIC_CERT = \"\" SOCIAL_AUTH_SAML_SP_PRIVATE_KEY = \"\" # A dictionary that contains information about your app. You must specify values for # English at a minimum. SOCIAL_AUTH_SAML_ORG_INFO = { \"en-US\" : { \"name\" : \"Nautobot\" , \"displayname\" : \"Nautobot\" , \"url\" : \"https://nautobot.example.com\" , } } # Technical point of contact SOCIAL_AUTH_SAML_TECHNICAL_CONTACT = { \"givenName\" : \"Bob Jones\" , \"emailAddress\" : \"bob@example.com\" } # Support point of contact SOCIAL_AUTH_SAML_SUPPORT_CONTACT = { \"givenName\" : \"Alice Jenkins\" , \"emailAddress\" : \"alice@example.com\" } # The Entity ID URL for Google from step 3 GOOGLE_ENTITY_ID = \"<Entity ID from Google>\" # The SSO URL for Google from step 3 GOOGLE_SSO_URL = \"<SSO URL from Google>\" # The Certificate for Google from step 3 GOOGLE_CERTIFICATE = \"<Certificate from Google>\" # The most important setting. List the Entity ID, SSO URL, and x.509 public key certificate # for each provider that you app wants to support. We are only supporting Google for this # example. SOCIAL_AUTH_SAML_ENABLED_IDPS = { \"google\" : { \"entity_id\" : GOOGLE_ENTITY_ID , \"url\" : GOOGLE_SSO_URL , \"x509cert\" : GOOGLE_CERTIFICATE , # These are used to map to User object fields in Nautobot using Google # attribute fields we configured in step 8 of \"Setup SAML in Google\". \"attr_user_permanent_id\" : \"email\" , \"attr_first_name\" : \"first_name\" , \"attr_last_name\" : \"last_name\" , \"attr_username\" : \"email\" , \"attr_email\" : \"email\" , } } # Required for correctly redirecting when behind SSL proxy (NGINX). You may or may not need # these depending on your production deployment. They are provided here just in case. SECURE_SSL_REDIRECT = True SECURE_PROXY_SSL_HEADER = ( 'HTTP_X_FORWARDED_PROTO' , 'https' )","title":"Configure Nautobot"},{"location":"configuration/authentication/sso.html#enable-saml-in-google","text":"Now that you've configured both Google and Nautobot for SAML, you still need to enable SAML for your users in your Google domain. On Google's official site to Set up your own custom SAML application , scroll down to Turn on your SAML app and follow the remaining instructions to enable and verify SAML in Google.","title":"Enable SAML in Google"},{"location":"configuration/authentication/sso.html#login-with-saml","text":"Note the provider entry we configured in SOCIAL_AUTH_SAML_ENABLED_IDPS as google . This will be used to login and will be referenced in the query parameter using idp=google . For example /login/saml/?idp=google . This should be the URL that is mapped to the \"Log in\" button on the top right of the index page when you navigate to Nautobot in your browser. Clicking this link should automatically redirect you to Google, ask you to \"Choose an account\", log you in and redirect you back to the Nautobot home page. Your email address will also be your username. Be sure to configure EXTERNAL_AUTH_DEFAULT_GROUPS and EXTERNAL_AUTH_DEFAULT_PERMISSIONS next.","title":"Login with SAML"},{"location":"configuration/authentication/sso.html#azure-ad","text":"In the Azure admin portal, search for and select Azure Active Directory . Under Manage , select App registrations -> New registration . Configure the application as follows: Name : This is the user-facing display name for the app. Supported account types : This specifies the AD directories that you're allowing to authenticate with this app. Redirect URIs : Don't fill this out yet, it will be configured in the following steps. Once the application is configured in Azure, you'll be shown the app registration's Overview page. Please take note of the Application (client) ID for use later. SSO with Azure can either be configured with OAuth2 or OpenID Connect (OIDC). When using an organization's authentication server OAuth2 is preferred; with custom Azure authentication backends, use OIDC. From the App registration page, click on Authentication . Under Platform configurations , select Add a platform and select Web . Click on the Add a Redirect URI link on the page and configure it as follows: Redirect URIs : should be the Base URI plus /complete/azuread-oauth2/ such as https://nautobot.example.com/complete/azuread-oauth2/ Once the Redirect URI is set, the last thing you'll need is to generate a client secret . To do so, click on Certificates & secrets and then the New client secret option. At this point you'll need to specify the expiration for the secret. Microsoft recommends less than 12 months with a maximum of 24 months as an option. Ensure you make a note of the secret that's generated for the next step. With the client secret generated, edit your nautobot_config.py as follows:","title":"Azure AD"},{"location":"configuration/authentication/sso.html#azure-ad-oauth2","text":"If your app is linked to the common tenant, you'll want to edit your nautobot_config.py as follows: AUTHENTICATION_BACKENDS = [ \"social_core.backends.azuread.AzureADOAuth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] SOCIAL_AUTH_AZUREAD_OAUTH2_KEY = \"<Client ID from Azure>\" SOCIAL_AUTH_AZUREAD_OAUTH2_SECRET = \"<Client Secret From Azure>\"","title":"Azure AD - OAuth2"},{"location":"configuration/authentication/sso.html#azure-tenant-support","text":"If your app is linked to a specific tenant instead of the common tenant, you'll want to edit your nautobot_config.py as follows: AUTHENTICATION_BACKENDS = [ \"social_core.backends.azuread.AzureADTenantOAuth2\" , \"nautobot.core.authentication.ObjectPermissionBackend\" , ] SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_KEY = \"<Client ID from Azure>\" SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_SECRET = \"<Client Secret From Azure>\" SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_TENANT_ID = \"<Tenant ID from Azure>\" With those settings in place your users should be able to authenticate against Azure AD and successfully login to Nautobot. However, that user will not be placed in any groups or given any permissions. In order to do so, you'll need to utilize a script to synchronize the groups passed from Azure to Nautobot after authentication succeeds. Any group permissions will need to be set manually in the Nautobot admin panel. An example to sync groups with Azure is provided in the examples/azure_ad folder in the root of the Nautobot repository.","title":"Azure - Tenant Support"},{"location":"core-functionality/circuits.html","text":"Circuits \u00b6 Providers \u00b6 A circuit provider is any entity which provides some form of connectivity of among sites or organizations within a site. While this obviously includes carriers which offer Internet and private transit service, it might also include Internet exchange (IX) points and even organizations with whom you peer directly. Each circuit within Nautobot must be assigned a provider and a circuit ID which is unique to that provider. Each provider may be assigned an autonomous system number (ASN), an account number, and contact information. Provider Network \u00b6 Added in version 1.3.0 A provider network represents an abstract portion of network topology, just like in a topology diagram. For example, a provider network may be used to represent a provider's MPLS network. Each provider network must be assigned to a provider. A circuit may terminate to either a provider network or to a site. Circuits \u00b6 A communications circuit represents a single physical link connecting exactly two endpoints, commonly referred to as its A and Z terminations. A circuit in Nautobot may have zero, one, or two terminations defined. It is common to have only one termination defined when you don't necessarily care about the details of the provider side of the circuit, e.g. for Internet access circuits. Both terminations would likely be modeled for circuits which connect one customer site to another. Each circuit is associated with a provider and a user-defined type. For example, you might have Internet access circuits delivered to each site by one provider, and private MPLS circuits delivered by another. Each circuit must be assigned a circuit ID, each of which must be unique per provider. Each circuit must be assigned to a status . The following statuses are available by default: Planned Provisioning Active Offline Deprovisioning Decommissioned Circuits also have optional fields for annotating their installation date and commit rate, and may be assigned to Nautobot tenants. Note Nautobot currently models only physical circuits: those which have exactly two endpoints. It is common to layer virtualized constructs ( virtual circuits ) such as MPLS or EVPN tunnels on top of these, however Nautobot does not yet support virtual circuit modeling. Circuit Types \u00b6 Circuits are classified by functional type. These types are completely customizable, and are typically used to convey the type of service being delivered over a circuit. For example, you might define circuit types for: Internet transit Out-of-band connectivity Peering Private backhaul Circuit Terminations \u00b6 The association of a circuit with a particular site, location, and/or device is modeled separately as a circuit termination. A circuit may have up to two terminations, labeled A and Z. A single-termination circuit can be used when you don't know (or care) about the far end of a circuit (for example, an Internet access circuit which connects to a transit provider). A dual-termination circuit is useful for tracking circuits which connect two sites. Each circuit termination is attached to either a site (and optionally also a location within the site) or a provider network. Site terminations may optionally be connected via a cable to a specific device interface or port within that site. Each termination must be assigned a port speed, and can optionally be assigned an upstream speed if it differs from the downstream speed (a common scenario with e.g. DOCSIS cable modems). Fields are also available to track cross-connect and patch panel details. In adherence with Nautobot's philosophy of closely modeling the real world, a circuit may be connected only to a physical interface. For example, circuits may not terminate to LAG interfaces, which are virtual in nature. In such cases, a separate physical circuit is associated with each LAG member interface and each needs to be modeled discretely. Note A circuit in Nautobot represents a physical link, and cannot have more than two endpoints. When modeling a multi-point topology, each leg of the topology must be defined as a discrete circuit, with one end terminating within the provider's infrastructure. The provider network model is ideal for representing these networks.","title":"Circuits"},{"location":"core-functionality/circuits.html#circuits","text":"","title":"Circuits"},{"location":"core-functionality/circuits.html#providers","text":"A circuit provider is any entity which provides some form of connectivity of among sites or organizations within a site. While this obviously includes carriers which offer Internet and private transit service, it might also include Internet exchange (IX) points and even organizations with whom you peer directly. Each circuit within Nautobot must be assigned a provider and a circuit ID which is unique to that provider. Each provider may be assigned an autonomous system number (ASN), an account number, and contact information.","title":"Providers"},{"location":"core-functionality/circuits.html#provider-network","text":"Added in version 1.3.0 A provider network represents an abstract portion of network topology, just like in a topology diagram. For example, a provider network may be used to represent a provider's MPLS network. Each provider network must be assigned to a provider. A circuit may terminate to either a provider network or to a site.","title":"Provider Network"},{"location":"core-functionality/circuits.html#circuits_1","text":"A communications circuit represents a single physical link connecting exactly two endpoints, commonly referred to as its A and Z terminations. A circuit in Nautobot may have zero, one, or two terminations defined. It is common to have only one termination defined when you don't necessarily care about the details of the provider side of the circuit, e.g. for Internet access circuits. Both terminations would likely be modeled for circuits which connect one customer site to another. Each circuit is associated with a provider and a user-defined type. For example, you might have Internet access circuits delivered to each site by one provider, and private MPLS circuits delivered by another. Each circuit must be assigned a circuit ID, each of which must be unique per provider. Each circuit must be assigned to a status . The following statuses are available by default: Planned Provisioning Active Offline Deprovisioning Decommissioned Circuits also have optional fields for annotating their installation date and commit rate, and may be assigned to Nautobot tenants. Note Nautobot currently models only physical circuits: those which have exactly two endpoints. It is common to layer virtualized constructs ( virtual circuits ) such as MPLS or EVPN tunnels on top of these, however Nautobot does not yet support virtual circuit modeling.","title":"Circuits"},{"location":"core-functionality/circuits.html#circuit-types","text":"Circuits are classified by functional type. These types are completely customizable, and are typically used to convey the type of service being delivered over a circuit. For example, you might define circuit types for: Internet transit Out-of-band connectivity Peering Private backhaul","title":"Circuit Types"},{"location":"core-functionality/circuits.html#circuit-terminations","text":"The association of a circuit with a particular site, location, and/or device is modeled separately as a circuit termination. A circuit may have up to two terminations, labeled A and Z. A single-termination circuit can be used when you don't know (or care) about the far end of a circuit (for example, an Internet access circuit which connects to a transit provider). A dual-termination circuit is useful for tracking circuits which connect two sites. Each circuit termination is attached to either a site (and optionally also a location within the site) or a provider network. Site terminations may optionally be connected via a cable to a specific device interface or port within that site. Each termination must be assigned a port speed, and can optionally be assigned an upstream speed if it differs from the downstream speed (a common scenario with e.g. DOCSIS cable modems). Fields are also available to track cross-connect and patch panel details. In adherence with Nautobot's philosophy of closely modeling the real world, a circuit may be connected only to a physical interface. For example, circuits may not terminate to LAG interfaces, which are virtual in nature. In such cases, a separate physical circuit is associated with each LAG member interface and each needs to be modeled discretely. Note A circuit in Nautobot represents a physical link, and cannot have more than two endpoints. When modeling a multi-point topology, each leg of the topology must be defined as a discrete circuit, with one end terminating within the provider's infrastructure. The provider network model is ideal for representing these networks.","title":"Circuit Terminations"},{"location":"core-functionality/device-types.html","text":"Device Types \u00b6 Device Types \u00b6 A device type represents a particular make and model of hardware that exists in the real world. Device types define the physical attributes of a device (rack height and depth) and its individual components (console, power, network interfaces, and so on). Device types are instantiated as devices installed within sites and/or equipment racks. For example, you might define a device type to represent a Juniper EX4300-48T network switch with 48 Ethernet interfaces. You can then create multiple instances of this type named \"switch1,\" \"switch2,\" and so on. Each device will automatically inherit the components (such as interfaces) of its device type at the time of creation. However, changes made to a device type will not apply to instances of that device type retroactively. Some devices house child devices which share physical resources, like space and power, but which functional independently from one another. A common example of this is blade server chassis. Each device type is designated as one of the following: A parent device (which has device bays) A child device (which must be installed within a device bay) Neither Note This parent/child relationship is not suitable for modeling chassis-based devices, wherein child members share a common control plane. Instead, line cards and similarly non-autonomous hardware should be modeled as inventory items within a device, with any associated interfaces or other components assigned directly to the device. Manufacturers \u00b6 A manufacturer represents the \"make\" of a device; e.g. Cisco or Dell. Each device type must be assigned to a manufacturer. (Inventory items and platforms may also be associated with manufacturers.) Each manufacturer must have a unique name and may have a description assigned to it. Device Component Templates \u00b6 Added in version 1.4.5 The fields created and last_updated were added to all device component template models. If you upgraded from Nautobot 1.4.4 or earlier, the values for these fields will default to None (null). Each device type is assigned a number of component templates which define the physical components within a device. These are: Console ports Console server ports Power ports Power outlets Network interfaces Front ports Rear ports Device bays (which house child devices) Whenever a new device is created, its components are automatically created per the templates assigned to its device type. For example, a Juniper EX4300-48T device type might have the following component templates defined: One template for a console port (\"Console\") Two templates for power ports (\"PSU0\" and \"PSU1\") 48 templates for 1GE interfaces (\"ge-0/0/0\" through \"ge-0/0/47\") Four templates for 10GE interfaces (\"xe-0/2/0\" through \"xe-0/2/3\") Once component templates have been created, every new device that you create as an instance of this type will automatically be assigned each of the components listed above. However, this automation occurs only when a device is first created: Adding or removing components on a device type does not automatically change existing device instances based on that device type. Note Assignment of components from templates occurs only at the time of device creation. If you modify the templates of a device type, it will not affect devices which have already been created. This is intentional and by design as device type templates may evolve over time in your organization. However, you always have the option of adding, modifying, or deleting components on existing devices. Consider also that automatically changing components of existing devices when a device-type changes would require Nautobot to make very dangerous assumptions. For example, suppose you had a device type which included a specific line card with a specific set of interfaces. After creating some devices, representing your existing deployment of this device type, your deployment for new devices of this type changed to include a different line card and hence different interfaces, so you decided to update the device-type templates accordingly. If Nautobot were to automatically propagate this change to all existing devices of this type, it would result in an incorrect reflection of the reality that all existing devices have not yet been retrofitted with the new line card. Console Port Templates \u00b6 A template for a console port that will be created on all instantiations of the parent device type. Each console port can be assigned a physical type. Console Server Port Templates \u00b6 A template for a console server port that will be created on all instantiations of the parent device type. Each console server port can be assigned a physical type. Power Port Templates \u00b6 A template for a power port that will be created on all instantiations of the parent device type. Each power port can be assigned a physical type, as well as a maximum and allocated draw in watts. Power Outlet Templates \u00b6 A template for a power outlet that will be created on all instantiations of the parent device type. Each power outlet can be assigned a physical type, and its power source may be mapped to a specific feed leg and power port template. This association will be automatically replicated when the device type is instantiated. Interface Templates \u00b6 A template for a network interface that will be created on all instantiations of the parent device type. Each interface may be assigned a physical or virtual type, and may be designated as \"management-only.\" Front Port Templates \u00b6 A template for a front-facing pass-through port that will be created on all instantiations of the parent device type. Front ports may have a physical type assigned, and must be associated with a corresponding rear port and position. This association will be automatically replicated when the device type is instantiated. Rear Port Templates \u00b6 A template for a rear-facing pass-through port that will be created on all instantiations of the parent device type. Each rear port may have a physical type and one or more front port templates assigned to it. The number of positions associated with a rear port determines how many front ports can be assigned to it (the maximum is 1024). Device Bay Templates \u00b6 A template for a device bay that will be created on all instantiations of the parent device type.","title":"Device Types"},{"location":"core-functionality/device-types.html#device-types","text":"","title":"Device Types"},{"location":"core-functionality/device-types.html#device-types_1","text":"A device type represents a particular make and model of hardware that exists in the real world. Device types define the physical attributes of a device (rack height and depth) and its individual components (console, power, network interfaces, and so on). Device types are instantiated as devices installed within sites and/or equipment racks. For example, you might define a device type to represent a Juniper EX4300-48T network switch with 48 Ethernet interfaces. You can then create multiple instances of this type named \"switch1,\" \"switch2,\" and so on. Each device will automatically inherit the components (such as interfaces) of its device type at the time of creation. However, changes made to a device type will not apply to instances of that device type retroactively. Some devices house child devices which share physical resources, like space and power, but which functional independently from one another. A common example of this is blade server chassis. Each device type is designated as one of the following: A parent device (which has device bays) A child device (which must be installed within a device bay) Neither Note This parent/child relationship is not suitable for modeling chassis-based devices, wherein child members share a common control plane. Instead, line cards and similarly non-autonomous hardware should be modeled as inventory items within a device, with any associated interfaces or other components assigned directly to the device.","title":"Device Types"},{"location":"core-functionality/device-types.html#manufacturers","text":"A manufacturer represents the \"make\" of a device; e.g. Cisco or Dell. Each device type must be assigned to a manufacturer. (Inventory items and platforms may also be associated with manufacturers.) Each manufacturer must have a unique name and may have a description assigned to it.","title":"Manufacturers"},{"location":"core-functionality/device-types.html#device-component-templates","text":"Added in version 1.4.5 The fields created and last_updated were added to all device component template models. If you upgraded from Nautobot 1.4.4 or earlier, the values for these fields will default to None (null). Each device type is assigned a number of component templates which define the physical components within a device. These are: Console ports Console server ports Power ports Power outlets Network interfaces Front ports Rear ports Device bays (which house child devices) Whenever a new device is created, its components are automatically created per the templates assigned to its device type. For example, a Juniper EX4300-48T device type might have the following component templates defined: One template for a console port (\"Console\") Two templates for power ports (\"PSU0\" and \"PSU1\") 48 templates for 1GE interfaces (\"ge-0/0/0\" through \"ge-0/0/47\") Four templates for 10GE interfaces (\"xe-0/2/0\" through \"xe-0/2/3\") Once component templates have been created, every new device that you create as an instance of this type will automatically be assigned each of the components listed above. However, this automation occurs only when a device is first created: Adding or removing components on a device type does not automatically change existing device instances based on that device type. Note Assignment of components from templates occurs only at the time of device creation. If you modify the templates of a device type, it will not affect devices which have already been created. This is intentional and by design as device type templates may evolve over time in your organization. However, you always have the option of adding, modifying, or deleting components on existing devices. Consider also that automatically changing components of existing devices when a device-type changes would require Nautobot to make very dangerous assumptions. For example, suppose you had a device type which included a specific line card with a specific set of interfaces. After creating some devices, representing your existing deployment of this device type, your deployment for new devices of this type changed to include a different line card and hence different interfaces, so you decided to update the device-type templates accordingly. If Nautobot were to automatically propagate this change to all existing devices of this type, it would result in an incorrect reflection of the reality that all existing devices have not yet been retrofitted with the new line card.","title":"Device Component Templates"},{"location":"core-functionality/device-types.html#console-port-templates","text":"A template for a console port that will be created on all instantiations of the parent device type. Each console port can be assigned a physical type.","title":"Console Port Templates"},{"location":"core-functionality/device-types.html#console-server-port-templates","text":"A template for a console server port that will be created on all instantiations of the parent device type. Each console server port can be assigned a physical type.","title":"Console Server Port Templates"},{"location":"core-functionality/device-types.html#power-port-templates","text":"A template for a power port that will be created on all instantiations of the parent device type. Each power port can be assigned a physical type, as well as a maximum and allocated draw in watts.","title":"Power Port Templates"},{"location":"core-functionality/device-types.html#power-outlet-templates","text":"A template for a power outlet that will be created on all instantiations of the parent device type. Each power outlet can be assigned a physical type, and its power source may be mapped to a specific feed leg and power port template. This association will be automatically replicated when the device type is instantiated.","title":"Power Outlet Templates"},{"location":"core-functionality/device-types.html#interface-templates","text":"A template for a network interface that will be created on all instantiations of the parent device type. Each interface may be assigned a physical or virtual type, and may be designated as \"management-only.\"","title":"Interface Templates"},{"location":"core-functionality/device-types.html#front-port-templates","text":"A template for a front-facing pass-through port that will be created on all instantiations of the parent device type. Front ports may have a physical type assigned, and must be associated with a corresponding rear port and position. This association will be automatically replicated when the device type is instantiated.","title":"Front Port Templates"},{"location":"core-functionality/device-types.html#rear-port-templates","text":"A template for a rear-facing pass-through port that will be created on all instantiations of the parent device type. Each rear port may have a physical type and one or more front port templates assigned to it. The number of positions associated with a rear port determines how many front ports can be assigned to it (the maximum is 1024).","title":"Rear Port Templates"},{"location":"core-functionality/device-types.html#device-bay-templates","text":"A template for a device bay that will be created on all instantiations of the parent device type.","title":"Device Bay Templates"},{"location":"core-functionality/devices.html","text":"Devices and Cabling \u00b6 Devices \u00b6 Every piece of hardware which is installed within a site or rack exists in Nautobot as a device. Devices are measured in rack units (U) and can be half depth or full depth. A device may have a height of 0U: These devices do not consume vertical rack space and cannot be assigned to a particular rack unit. A common example of a 0U device is a vertically-mounted PDU. When assigning a multi-U device to a rack, it is considered to be mounted in the lowest-numbered rack unit which it occupies. For example, a 3U device which occupies U8 through U10 is said to be mounted in U8. This logic applies to racks with both ascending and descending unit numbering. A device is said to be full-depth if its installation on one rack face prevents the installation of any other device on the opposite face within the same rack unit(s). This could be either because the device is physically too deep to allow a device behind it, or because the installation of an opposing device would impede airflow. Each device must be instantiated from a pre-created device type, and its default components (console ports, power ports, interfaces, etc.) will be created automatically. (The device type associated with a device may be changed after its creation, however its components will not be updated retroactively.) Each device must be assigned a site, device role, and operational status , and may optionally be assigned to a specific location and/or rack within a site. A platform, serial number, and asset tag may optionally be assigned to each device. Device names must be unique within a site, unless the device has been assigned to a tenant. Devices may also be unnamed. When a device has one or more interfaces with IP addresses assigned, a primary IP for the device can be designated, for both IPv4 and IPv6. Device Roles \u00b6 Devices can be organized by functional roles, which are fully customizable by the user. For example, you might create roles for core switches, distribution switches, and access switches within your network. Platforms \u00b6 A platform defines the type of software running on a device or virtual machine. This can be helpful to model when it is necessary to distinguish between different versions or feature sets. Note that two devices of the same type may be assigned different platforms: For example, one Juniper MX240 might run Junos 14 while another runs Junos 15. Platforms may optionally be limited by manufacturer: If a platform is assigned to a particular manufacturer, it can only be assigned to devices with a type belonging to that manufacturer. The platform model is also used to indicate which NAPALM driver and any associated arguments Nautobot should use when connecting to a remote device. The name of the driver along with optional parameters are stored with the platform. The assignment of platforms to devices is an optional feature, and may be disregarded if not desired. Device Components \u00b6 Added in version 1.4.5 The fields created and last_updated were added to all device component models. If you upgraded from Nautobot 1.4.4 or earlier, the values for these fields will default to None (null). Console Ports \u00b6 A console port provides connectivity to the physical console of a device. These are typically used for temporary access by someone who is physically near the device, or for remote out-of-band access provided via a networked console server. Each console port may be assigned a physical type. Cables can connect console ports to console server ports or pass-through ports. Console Server Ports \u00b6 A console server is a device which provides remote access to the local consoles of connected devices. They are typically used to provide remote out-of-band access to network devices. Each console server port may be assigned a physical type. Cables can connect console server ports to console ports or pass-through ports. Power Ports \u00b6 A power port represents the inlet of a device where it draws its power, i.e. the connection port(s) on a device's power supply. Each power port may be assigned a physical type, as well as allocated and maximum draw values (in watts). These values can be used to calculate the overall utilization of an upstream power feed. Info When creating a power port on a device which supplies power to downstream devices, the allocated and maximum draw numbers should be left blank. Utilization will be calculated by taking the sum of all power ports of devices connected downstream. Cables can connect power ports only to power outlets or power feeds. (Pass-through ports cannot be used to model power distribution.) Power Outlets \u00b6 Power outlets represent the outlets on a power distribution unit (PDU) or other device that supply power to dependent devices. Each power port may be assigned a physical type, and may be associated with a specific feed leg (where three-phase power is used) and/or a specific upstream power port. This association can be used to model the distribution of power within a device. For example, imagine a PDU with one power port which draws from a three-phase feed and 48 power outlets arranged into three banks of 16 outlets each. Outlets 1-16 would be associated with leg A on the port, and outlets 17-32 and 33-48 would be associated with legs B and C, respectively. Cables can connect power outlets only to downstream power ports. (Pass-through ports cannot be used to model power distribution.) Interfaces \u00b6 Interfaces in Nautobot represent network interfaces used to exchange data with connected devices. On modern networks, these are most commonly Ethernet, but other types are supported as well. Each interface must be assigned a type, an operational status and may optionally be assigned a MAC address, MTU, and IEEE 802.1Q mode (tagged or access). Each interface can also be enabled or disabled, and optionally designated as management-only (for out-of-band management). The following operational statuses are available by default: Planned Maintenance Active Decommissioning Failed Added in version 1.4.0 Added bridge field. Added parent_interface field. Added status field. Interfaces may be physical or virtual in nature, but only physical interfaces may be connected via cables. Cables can connect interfaces to pass-through ports, circuit terminations, or other interfaces. Physical interfaces may be arranged into a link aggregation group (LAG) and associated with a parent LAG (virtual) interface. LAG interfaces can be recursively nested to model bonding of trunk groups. Like all virtual interfaces, LAG interfaces cannot be connected physically. IP addresses can be assigned to interfaces. VLANs can also be assigned to each interface as either tagged or untagged. (An interface may have only one untagged VLAN.) Note Although devices and virtual machines both can have interfaces, a separate model is used for each. Thus, device interfaces have some properties that are not present on virtual machine interfaces and vice versa. Front Ports \u00b6 Front ports are pass-through ports used to represent physical cable connections that comprise part of a longer path. For example, the ports on the front face of a UTP patch panel would be modeled in Nautobot as front ports. Each port is assigned a physical type, and must be mapped to a specific rear port on the same device. A single rear port may be mapped to multiple rear ports, using numeric positions to annotate the specific alignment of each. Rear Ports \u00b6 Like front ports, rear ports are pass-through ports which represent the continuation of a path from one cable to the next. Each rear port is defined with its physical type and a number of positions: Rear ports with more than one position can be mapped to multiple front ports. This can be useful for modeling instances where multiple paths share a common cable (for example, six discrete two-strand fiber connections sharing a 12-strand MPO cable). Note Front and rear ports need not necessarily reside on the actual front or rear device face. This terminology is used primarily to distinguish between the two components in a pass-through port pairing. Device Bays \u00b6 Device bays represent a space or slot within a parent device in which a child device may be installed. For example, a 2U parent chassis might house four individual blade servers. The chassis would appear in the rack elevation as a 2U device with four device bays, and each server within it would be defined as a 0U device installed in one of the device bays. Child devices do not appear within rack elevations or count as consuming rack units. Child devices are first-class Devices in their own right: That is, they are fully independent managed entities which don't share any control plane with the parent. Just like normal devices, child devices have their own platform (OS), role, tags, and components. LAG interfaces may not group interfaces belonging to different child devices. Note Device bays are not suitable for modeling line cards (such as those commonly found in chassis-based routers and switches), as these components depend on the control plane of the parent device to operate. Instead, line cards and similarly non-autonomous hardware should be modeled as inventory items within a device, with any associated interfaces or other components assigned directly to the device. Inventory Items \u00b6 Inventory items represent hardware components installed within a device, such as a power supply or CPU or line card. Inventory items are distinct from other device components in that they cannot be templatized on a device type, and cannot be connected by cables. They are intended to be used primarily for inventory purposes. Each inventory item can be assigned a manufacturer, part ID, serial number, and asset tag (all optional). A boolean toggle is also provided to indicate whether each item was entered manually or discovered automatically (by some process outside of Nautobot). Inventory items are hierarchical in nature, such that any individual item may be designated as the parent for other items. For example, an inventory item might be created to represent a line card which houses several SFP optics, each of which exists as a child item within the device. Virtual Chassis \u00b6 A virtual chassis represents a set of devices which share a common control plane. A common example of this is a stack of switches which are connected and configured to operate as a single device. A virtual chassis must be assigned a name and may be assigned a domain. Each device in the virtual chassis is referred to as a VC member, and assigned a position and (optionally) a priority. VC member devices commonly reside within the same rack, though this is not a requirement. One of the devices may be designated as the VC master: This device will typically be assigned a name, services, and other attributes related to managing the VC. Note It's important to recognize the distinction between a virtual chassis and a chassis-based device. A virtual chassis is not suitable for modeling a chassis-based switch with removable line cards (such as the Juniper EX9208), as its line cards are not physically autonomous devices. Cables \u00b6 All connections between device components in Nautobot are represented using cables. A cable represents a direct physical connection between two termination points, such as between a console port and a patch panel port, or between two network interfaces. Each cable must have two endpoints defined. These endpoints are sometimes referenced as A and B for clarity, however cables are direction-agnostic and the order in which terminations are made has no meaning. Cables may be connected to the following objects: Circuit terminations Console ports Console server ports Interfaces Pass-through ports (front and rear) Power feeds Power outlets Power ports Each cable may be assigned a type, label, length, and color. Each cable must also assigned to an operational status . The following statuses are available by default: Active Planned Decommissioning Tracing Cables \u00b6 A cable may be traced from either of its endpoints by clicking the \"trace\" button. (A REST API endpoint also provides this functionality.) Nautobot will follow the path of connected cables from this termination across the directly connected cable to the far-end termination. If the cable connects to a pass-through port, and the peer port has another cable connected, Nautobot will continue following the cable path until it encounters a non-pass-through or unconnected termination point. The entire path will be displayed to the user. In the example below, three individual cables comprise a path between devices A and D: Traced from Interface 1 on Device A, Nautobot will show the following path: Cable 1: Interface 1 to Front Port 1 Cable 2: Rear Port 1 to Rear Port 2 Cable 3: Front Port 2 to Interface 2 A cable can also be traced through a circuit. Traced from Interface 1 on Device A, Nautobot will show the following path: Cable 1: Interface 1 to Side A Cable 2: Side Z to Interface 2","title":"Devices and Cabling"},{"location":"core-functionality/devices.html#devices-and-cabling","text":"","title":"Devices and Cabling"},{"location":"core-functionality/devices.html#devices","text":"Every piece of hardware which is installed within a site or rack exists in Nautobot as a device. Devices are measured in rack units (U) and can be half depth or full depth. A device may have a height of 0U: These devices do not consume vertical rack space and cannot be assigned to a particular rack unit. A common example of a 0U device is a vertically-mounted PDU. When assigning a multi-U device to a rack, it is considered to be mounted in the lowest-numbered rack unit which it occupies. For example, a 3U device which occupies U8 through U10 is said to be mounted in U8. This logic applies to racks with both ascending and descending unit numbering. A device is said to be full-depth if its installation on one rack face prevents the installation of any other device on the opposite face within the same rack unit(s). This could be either because the device is physically too deep to allow a device behind it, or because the installation of an opposing device would impede airflow. Each device must be instantiated from a pre-created device type, and its default components (console ports, power ports, interfaces, etc.) will be created automatically. (The device type associated with a device may be changed after its creation, however its components will not be updated retroactively.) Each device must be assigned a site, device role, and operational status , and may optionally be assigned to a specific location and/or rack within a site. A platform, serial number, and asset tag may optionally be assigned to each device. Device names must be unique within a site, unless the device has been assigned to a tenant. Devices may also be unnamed. When a device has one or more interfaces with IP addresses assigned, a primary IP for the device can be designated, for both IPv4 and IPv6.","title":"Devices"},{"location":"core-functionality/devices.html#device-roles","text":"Devices can be organized by functional roles, which are fully customizable by the user. For example, you might create roles for core switches, distribution switches, and access switches within your network.","title":"Device Roles"},{"location":"core-functionality/devices.html#platforms","text":"A platform defines the type of software running on a device or virtual machine. This can be helpful to model when it is necessary to distinguish between different versions or feature sets. Note that two devices of the same type may be assigned different platforms: For example, one Juniper MX240 might run Junos 14 while another runs Junos 15. Platforms may optionally be limited by manufacturer: If a platform is assigned to a particular manufacturer, it can only be assigned to devices with a type belonging to that manufacturer. The platform model is also used to indicate which NAPALM driver and any associated arguments Nautobot should use when connecting to a remote device. The name of the driver along with optional parameters are stored with the platform. The assignment of platforms to devices is an optional feature, and may be disregarded if not desired.","title":"Platforms"},{"location":"core-functionality/devices.html#device-components","text":"Added in version 1.4.5 The fields created and last_updated were added to all device component models. If you upgraded from Nautobot 1.4.4 or earlier, the values for these fields will default to None (null).","title":"Device Components"},{"location":"core-functionality/devices.html#console-ports","text":"A console port provides connectivity to the physical console of a device. These are typically used for temporary access by someone who is physically near the device, or for remote out-of-band access provided via a networked console server. Each console port may be assigned a physical type. Cables can connect console ports to console server ports or pass-through ports.","title":"Console Ports"},{"location":"core-functionality/devices.html#console-server-ports","text":"A console server is a device which provides remote access to the local consoles of connected devices. They are typically used to provide remote out-of-band access to network devices. Each console server port may be assigned a physical type. Cables can connect console server ports to console ports or pass-through ports.","title":"Console Server Ports"},{"location":"core-functionality/devices.html#power-ports","text":"A power port represents the inlet of a device where it draws its power, i.e. the connection port(s) on a device's power supply. Each power port may be assigned a physical type, as well as allocated and maximum draw values (in watts). These values can be used to calculate the overall utilization of an upstream power feed. Info When creating a power port on a device which supplies power to downstream devices, the allocated and maximum draw numbers should be left blank. Utilization will be calculated by taking the sum of all power ports of devices connected downstream. Cables can connect power ports only to power outlets or power feeds. (Pass-through ports cannot be used to model power distribution.)","title":"Power Ports"},{"location":"core-functionality/devices.html#power-outlets","text":"Power outlets represent the outlets on a power distribution unit (PDU) or other device that supply power to dependent devices. Each power port may be assigned a physical type, and may be associated with a specific feed leg (where three-phase power is used) and/or a specific upstream power port. This association can be used to model the distribution of power within a device. For example, imagine a PDU with one power port which draws from a three-phase feed and 48 power outlets arranged into three banks of 16 outlets each. Outlets 1-16 would be associated with leg A on the port, and outlets 17-32 and 33-48 would be associated with legs B and C, respectively. Cables can connect power outlets only to downstream power ports. (Pass-through ports cannot be used to model power distribution.)","title":"Power Outlets"},{"location":"core-functionality/devices.html#interfaces","text":"Interfaces in Nautobot represent network interfaces used to exchange data with connected devices. On modern networks, these are most commonly Ethernet, but other types are supported as well. Each interface must be assigned a type, an operational status and may optionally be assigned a MAC address, MTU, and IEEE 802.1Q mode (tagged or access). Each interface can also be enabled or disabled, and optionally designated as management-only (for out-of-band management). The following operational statuses are available by default: Planned Maintenance Active Decommissioning Failed Added in version 1.4.0 Added bridge field. Added parent_interface field. Added status field. Interfaces may be physical or virtual in nature, but only physical interfaces may be connected via cables. Cables can connect interfaces to pass-through ports, circuit terminations, or other interfaces. Physical interfaces may be arranged into a link aggregation group (LAG) and associated with a parent LAG (virtual) interface. LAG interfaces can be recursively nested to model bonding of trunk groups. Like all virtual interfaces, LAG interfaces cannot be connected physically. IP addresses can be assigned to interfaces. VLANs can also be assigned to each interface as either tagged or untagged. (An interface may have only one untagged VLAN.) Note Although devices and virtual machines both can have interfaces, a separate model is used for each. Thus, device interfaces have some properties that are not present on virtual machine interfaces and vice versa.","title":"Interfaces"},{"location":"core-functionality/devices.html#front-ports","text":"Front ports are pass-through ports used to represent physical cable connections that comprise part of a longer path. For example, the ports on the front face of a UTP patch panel would be modeled in Nautobot as front ports. Each port is assigned a physical type, and must be mapped to a specific rear port on the same device. A single rear port may be mapped to multiple rear ports, using numeric positions to annotate the specific alignment of each.","title":"Front Ports"},{"location":"core-functionality/devices.html#rear-ports","text":"Like front ports, rear ports are pass-through ports which represent the continuation of a path from one cable to the next. Each rear port is defined with its physical type and a number of positions: Rear ports with more than one position can be mapped to multiple front ports. This can be useful for modeling instances where multiple paths share a common cable (for example, six discrete two-strand fiber connections sharing a 12-strand MPO cable). Note Front and rear ports need not necessarily reside on the actual front or rear device face. This terminology is used primarily to distinguish between the two components in a pass-through port pairing.","title":"Rear Ports"},{"location":"core-functionality/devices.html#device-bays","text":"Device bays represent a space or slot within a parent device in which a child device may be installed. For example, a 2U parent chassis might house four individual blade servers. The chassis would appear in the rack elevation as a 2U device with four device bays, and each server within it would be defined as a 0U device installed in one of the device bays. Child devices do not appear within rack elevations or count as consuming rack units. Child devices are first-class Devices in their own right: That is, they are fully independent managed entities which don't share any control plane with the parent. Just like normal devices, child devices have their own platform (OS), role, tags, and components. LAG interfaces may not group interfaces belonging to different child devices. Note Device bays are not suitable for modeling line cards (such as those commonly found in chassis-based routers and switches), as these components depend on the control plane of the parent device to operate. Instead, line cards and similarly non-autonomous hardware should be modeled as inventory items within a device, with any associated interfaces or other components assigned directly to the device.","title":"Device Bays"},{"location":"core-functionality/devices.html#inventory-items","text":"Inventory items represent hardware components installed within a device, such as a power supply or CPU or line card. Inventory items are distinct from other device components in that they cannot be templatized on a device type, and cannot be connected by cables. They are intended to be used primarily for inventory purposes. Each inventory item can be assigned a manufacturer, part ID, serial number, and asset tag (all optional). A boolean toggle is also provided to indicate whether each item was entered manually or discovered automatically (by some process outside of Nautobot). Inventory items are hierarchical in nature, such that any individual item may be designated as the parent for other items. For example, an inventory item might be created to represent a line card which houses several SFP optics, each of which exists as a child item within the device.","title":"Inventory Items"},{"location":"core-functionality/devices.html#virtual-chassis","text":"A virtual chassis represents a set of devices which share a common control plane. A common example of this is a stack of switches which are connected and configured to operate as a single device. A virtual chassis must be assigned a name and may be assigned a domain. Each device in the virtual chassis is referred to as a VC member, and assigned a position and (optionally) a priority. VC member devices commonly reside within the same rack, though this is not a requirement. One of the devices may be designated as the VC master: This device will typically be assigned a name, services, and other attributes related to managing the VC. Note It's important to recognize the distinction between a virtual chassis and a chassis-based device. A virtual chassis is not suitable for modeling a chassis-based switch with removable line cards (such as the Juniper EX9208), as its line cards are not physically autonomous devices.","title":"Virtual Chassis"},{"location":"core-functionality/devices.html#cables","text":"All connections between device components in Nautobot are represented using cables. A cable represents a direct physical connection between two termination points, such as between a console port and a patch panel port, or between two network interfaces. Each cable must have two endpoints defined. These endpoints are sometimes referenced as A and B for clarity, however cables are direction-agnostic and the order in which terminations are made has no meaning. Cables may be connected to the following objects: Circuit terminations Console ports Console server ports Interfaces Pass-through ports (front and rear) Power feeds Power outlets Power ports Each cable may be assigned a type, label, length, and color. Each cable must also assigned to an operational status . The following statuses are available by default: Active Planned Decommissioning","title":"Cables"},{"location":"core-functionality/devices.html#tracing-cables","text":"A cable may be traced from either of its endpoints by clicking the \"trace\" button. (A REST API endpoint also provides this functionality.) Nautobot will follow the path of connected cables from this termination across the directly connected cable to the far-end termination. If the cable connects to a pass-through port, and the peer port has another cable connected, Nautobot will continue following the cable path until it encounters a non-pass-through or unconnected termination point. The entire path will be displayed to the user. In the example below, three individual cables comprise a path between devices A and D: Traced from Interface 1 on Device A, Nautobot will show the following path: Cable 1: Interface 1 to Front Port 1 Cable 2: Rear Port 1 to Rear Port 2 Cable 3: Front Port 2 to Interface 2 A cable can also be traced through a circuit. Traced from Interface 1 on Device A, Nautobot will show the following path: Cable 1: Interface 1 to Side A Cable 2: Side Z to Interface 2","title":"Tracing Cables"},{"location":"core-functionality/ipam.html","text":"IP Address Management \u00b6 Aggregates \u00b6 IP addressing is by nature hierarchical. The first few levels of the IPv4 hierarchy, for example, look like this: 0.0.0.0/0 0.0.0.0/1 0.0.0.0/2 64.0.0.0/2 128.0.0.0/1 128.0.0.0/2 192.0.0.0/2 This hierarchy comprises 33 tiers of addressing, from /0 all the way down to individual /32 address (and much, much further to /128 for IPv6). Of course, most organizations are concerned with only relatively small portions of the total IP space, so tracking the uppermost of these tiers isn't necessary. Nautobot allows us to specify the portions of IP space that are interesting to us by defining aggregates . Typically, an aggregate will correspond to either an allocation of public (globally routable) IP space granted by a regional authority, or a private (internally-routable) designation. Common private designations include: 10.0.0.0/8 (RFC 1918) 100.64.0.0/10 (RFC 6598) 172.16.0.0/12 (RFC 1918) 192.168.0.0/16 (RFC 1918) One or more /48s within fd00::/8 (IPv6 unique local addressing) Each aggregate is assigned to a RIR. For \"public\" aggregates, this will be the real-world authority which has granted your organization permission to use the specified IP space on the public Internet. For \"private\" aggregates, this will be a statutory authority, such as RFC 1918. Each aggregate can also annotate that date on which it was allocated, where applicable. Prefixes are automatically arranged beneath their parent aggregates in Nautobot. Typically you'll want to create aggregates only for the prefixes and IP addresses that your organization actually manages: There is no need to define aggregates for provider-assigned space which is only used on Internet circuits, for example. Note Because aggregates represent swaths of the global IP space, they cannot overlap with one another: They can only exist side-by-side. For instance, you cannot define both 10.0.0.0/8 and 10.16.0.0/16 as aggregates, because they overlap. 10.16.0.0/16 in this example would be created as a container prefix and automatically grouped under the 10.0.0.0/8 aggregate. Remember, the purpose of aggregates is to establish the root of your IP addressing hierarchy. Regional Internet Registries (RIRs) \u00b6 Regional Internet registries are responsible for the allocation of globally-routable address space. The five RIRs are ARIN, RIPE, APNIC, LACNIC, and AFRINIC. However, some address space has been set aside for internal use, such as defined in RFCs 1918 and 6598. Nautobot considers these RFCs as a sort of RIR as well; that is, an authority which \"owns\" certain address space. There also exist lower-tier registries which serve particular geographic areas. Users can create whatever RIRs they like, but each aggregate must be assigned to one RIR. The RIR model includes a boolean flag which indicates whether the RIR allocates only private IP space. For example, suppose your organization has been allocated 104.131.0.0/16 by ARIN. It also makes use of RFC 1918 addressing internally. You would first create RIRs named \"ARIN\" and \"RFC 1918,\" then create an aggregate for each of these top-level prefixes, assigning it to its respective RIR. Prefixes \u00b6 A prefix is an IPv4 or IPv6 network and mask expressed in CIDR notation (e.g. 192.0.2.0/24). A prefix entails only the \"network portion\" of an IP address: All bits in the address not covered by the mask must be zero. (In other words, a prefix cannot be a specific IP address.) Prefixes are automatically organized by their parent aggregates. Additionally, each prefix can be assigned to a particular site (optionally also to a location within the site) and a virtual routing and forwarding instance (VRF). Each VRF represents a separate IP space or routing table. All prefixes not assigned to a VRF are considered to be in the \"global\" table. Each prefix must be assigned a status and can optionally be assigned a role. These terms are often used interchangeably so it's important to recognize the difference between them. The status defines a prefix's operational state. The following statuses are provided by default: Container - A summary of child prefixes Active - Provisioned and in use Reserved - Designated for future use Deprecated - No longer in use On the other hand, a prefix's role defines its function. Role assignment is optional and roles are fully customizable. For example, you might create roles to differentiate between production and development infrastructure. A prefix may also be assigned to a VLAN. This association is helpful for associating address space with layer two domains. A VLAN may have multiple prefixes assigned to it. The prefix model include an \"is pool\" flag. If enabled, Nautobot will treat this prefix as a range (such as a NAT pool) wherein every IP address is valid and assignable. This logic is used when identifying available IP addresses within a prefix. If this flag is disabled, Nautobot will assume that the first and last (broadcast) address within an IPv4 prefix are unusable. Prefix/VLAN Roles \u00b6 A role indicates the function of a prefix or VLAN. For example, you might define Data, Voice, and Security roles. Generally, a prefix will be assigned the same functional role as the VLAN to which it is assigned (if any). IP Addresses \u00b6 An IP address comprises a single host address (either IPv4 or IPv6) and its subnet mask. Its mask should match exactly how the IP address is configured on an interface in the real world. Like a prefix, an IP address can optionally be assigned to a VRF (otherwise, it will appear in the \"global\" table). IP addresses are automatically arranged under parent prefixes within their respective VRFs according to the IP hierarchy. Each IP address can also be assigned an operational status and a functional role. The following statuses are available by default: Active Reserved Deprecated DHCP SLAAC (IPv6 Stateless Address Autoconfiguration) Roles are used to indicate some special attribute of an IP address; for example, use as a loopback or as the the virtual IP for a VRRP group. (Note that functional roles are conceptual in nature, and thus cannot be customized by the user.) Available roles include: Loopback Secondary Anycast VIP VRRP HSRP GLBP An IP address can be assigned to any device or virtual machine interface, and an interface may have multiple IP addresses assigned to it. Further, each device and virtual machine may have one of its interface IPs designated as its primary IP per address family (one for IPv4 and one for IPv6). Note When primary IPs are set for both IPv4 and IPv6, Nautobot will prefer IPv6. This can be changed by setting the PREFER_IPV4 configuration parameter. Network Address Translation (NAT) \u00b6 An IP address can be designated as the network address translation (NAT) inside IP address for one or more other IP addresses. This is useful primarily to denote a translation between public and private IP addresses. This relationship is followed in both directions: For example, if 10.0.0.1 is assigned as the inside IP for 192.0.2.1, 192.0.2.1 will be displayed as the outside IP for 10.0.0.1. Added in version 1.3.0 Support for multiple outside NAT IP addresses was added. Virtual Routing and Forwarding (VRF) \u00b6 A VRF object in Nautobot represents a virtual routing and forwarding (VRF) domain. Each VRF is essentially a separate routing table. VRFs are commonly used to isolate customers or organizations from one another within a network, or to route overlapping address space (e.g. multiple instances of the 10.0.0.0/8 space). Each VRF may be assigned to a specific tenant to aid in organizing the available IP space by customer or internal user. Each VRF is assigned a unique name and an optional route distinguisher (RD). The RD is expected to take one of the forms prescribed in RFC 4364 , however its formatting is not strictly enforced. Each prefix and IP address may be assigned to one (and only one) VRF. If you have a prefix or IP address which exists in multiple VRFs, you will need to create a separate instance of it in Nautobot for each VRF. Any prefix or IP address not assigned to a VRF is said to belong to the \"global\" table. By default, Nautobot will allow duplicate prefixes to be assigned to a VRF. This behavior can be toggled by setting the \"enforce unique\" flag on the VRF model. Note Enforcement of unique IP space can be toggled for global table (non-VRF prefixes) using the ENFORCE_GLOBAL_UNIQUE configuration setting. Each VRF may have one or more import and/or export route targets applied to it. Route targets are used to control the exchange of routes (prefixes) among VRFs in L3VPNs. Route Targets \u00b6 A route target is a particular type of extended BGP community used to control the redistribution of routes among VRF tables in a network. Route targets can be assigned to individual VRFs in Nautobot as import or export targets (or both) to model this exchange in an L3VPN. Each route target must be given a unique name, which should be in a format prescribed by RFC 4364 , similar to a VR route distinguisher. Each route target can optionally be assigned to a tenant, and may have tags assigned to it.","title":"IP Address Management"},{"location":"core-functionality/ipam.html#ip-address-management","text":"","title":"IP Address Management"},{"location":"core-functionality/ipam.html#aggregates","text":"IP addressing is by nature hierarchical. The first few levels of the IPv4 hierarchy, for example, look like this: 0.0.0.0/0 0.0.0.0/1 0.0.0.0/2 64.0.0.0/2 128.0.0.0/1 128.0.0.0/2 192.0.0.0/2 This hierarchy comprises 33 tiers of addressing, from /0 all the way down to individual /32 address (and much, much further to /128 for IPv6). Of course, most organizations are concerned with only relatively small portions of the total IP space, so tracking the uppermost of these tiers isn't necessary. Nautobot allows us to specify the portions of IP space that are interesting to us by defining aggregates . Typically, an aggregate will correspond to either an allocation of public (globally routable) IP space granted by a regional authority, or a private (internally-routable) designation. Common private designations include: 10.0.0.0/8 (RFC 1918) 100.64.0.0/10 (RFC 6598) 172.16.0.0/12 (RFC 1918) 192.168.0.0/16 (RFC 1918) One or more /48s within fd00::/8 (IPv6 unique local addressing) Each aggregate is assigned to a RIR. For \"public\" aggregates, this will be the real-world authority which has granted your organization permission to use the specified IP space on the public Internet. For \"private\" aggregates, this will be a statutory authority, such as RFC 1918. Each aggregate can also annotate that date on which it was allocated, where applicable. Prefixes are automatically arranged beneath their parent aggregates in Nautobot. Typically you'll want to create aggregates only for the prefixes and IP addresses that your organization actually manages: There is no need to define aggregates for provider-assigned space which is only used on Internet circuits, for example. Note Because aggregates represent swaths of the global IP space, they cannot overlap with one another: They can only exist side-by-side. For instance, you cannot define both 10.0.0.0/8 and 10.16.0.0/16 as aggregates, because they overlap. 10.16.0.0/16 in this example would be created as a container prefix and automatically grouped under the 10.0.0.0/8 aggregate. Remember, the purpose of aggregates is to establish the root of your IP addressing hierarchy.","title":"Aggregates"},{"location":"core-functionality/ipam.html#regional-internet-registries-rirs","text":"Regional Internet registries are responsible for the allocation of globally-routable address space. The five RIRs are ARIN, RIPE, APNIC, LACNIC, and AFRINIC. However, some address space has been set aside for internal use, such as defined in RFCs 1918 and 6598. Nautobot considers these RFCs as a sort of RIR as well; that is, an authority which \"owns\" certain address space. There also exist lower-tier registries which serve particular geographic areas. Users can create whatever RIRs they like, but each aggregate must be assigned to one RIR. The RIR model includes a boolean flag which indicates whether the RIR allocates only private IP space. For example, suppose your organization has been allocated 104.131.0.0/16 by ARIN. It also makes use of RFC 1918 addressing internally. You would first create RIRs named \"ARIN\" and \"RFC 1918,\" then create an aggregate for each of these top-level prefixes, assigning it to its respective RIR.","title":"Regional Internet Registries (RIRs)"},{"location":"core-functionality/ipam.html#prefixes","text":"A prefix is an IPv4 or IPv6 network and mask expressed in CIDR notation (e.g. 192.0.2.0/24). A prefix entails only the \"network portion\" of an IP address: All bits in the address not covered by the mask must be zero. (In other words, a prefix cannot be a specific IP address.) Prefixes are automatically organized by their parent aggregates. Additionally, each prefix can be assigned to a particular site (optionally also to a location within the site) and a virtual routing and forwarding instance (VRF). Each VRF represents a separate IP space or routing table. All prefixes not assigned to a VRF are considered to be in the \"global\" table. Each prefix must be assigned a status and can optionally be assigned a role. These terms are often used interchangeably so it's important to recognize the difference between them. The status defines a prefix's operational state. The following statuses are provided by default: Container - A summary of child prefixes Active - Provisioned and in use Reserved - Designated for future use Deprecated - No longer in use On the other hand, a prefix's role defines its function. Role assignment is optional and roles are fully customizable. For example, you might create roles to differentiate between production and development infrastructure. A prefix may also be assigned to a VLAN. This association is helpful for associating address space with layer two domains. A VLAN may have multiple prefixes assigned to it. The prefix model include an \"is pool\" flag. If enabled, Nautobot will treat this prefix as a range (such as a NAT pool) wherein every IP address is valid and assignable. This logic is used when identifying available IP addresses within a prefix. If this flag is disabled, Nautobot will assume that the first and last (broadcast) address within an IPv4 prefix are unusable.","title":"Prefixes"},{"location":"core-functionality/ipam.html#prefixvlan-roles","text":"A role indicates the function of a prefix or VLAN. For example, you might define Data, Voice, and Security roles. Generally, a prefix will be assigned the same functional role as the VLAN to which it is assigned (if any).","title":"Prefix/VLAN Roles"},{"location":"core-functionality/ipam.html#ip-addresses","text":"An IP address comprises a single host address (either IPv4 or IPv6) and its subnet mask. Its mask should match exactly how the IP address is configured on an interface in the real world. Like a prefix, an IP address can optionally be assigned to a VRF (otherwise, it will appear in the \"global\" table). IP addresses are automatically arranged under parent prefixes within their respective VRFs according to the IP hierarchy. Each IP address can also be assigned an operational status and a functional role. The following statuses are available by default: Active Reserved Deprecated DHCP SLAAC (IPv6 Stateless Address Autoconfiguration) Roles are used to indicate some special attribute of an IP address; for example, use as a loopback or as the the virtual IP for a VRRP group. (Note that functional roles are conceptual in nature, and thus cannot be customized by the user.) Available roles include: Loopback Secondary Anycast VIP VRRP HSRP GLBP An IP address can be assigned to any device or virtual machine interface, and an interface may have multiple IP addresses assigned to it. Further, each device and virtual machine may have one of its interface IPs designated as its primary IP per address family (one for IPv4 and one for IPv6). Note When primary IPs are set for both IPv4 and IPv6, Nautobot will prefer IPv6. This can be changed by setting the PREFER_IPV4 configuration parameter.","title":"IP Addresses"},{"location":"core-functionality/ipam.html#network-address-translation-nat","text":"An IP address can be designated as the network address translation (NAT) inside IP address for one or more other IP addresses. This is useful primarily to denote a translation between public and private IP addresses. This relationship is followed in both directions: For example, if 10.0.0.1 is assigned as the inside IP for 192.0.2.1, 192.0.2.1 will be displayed as the outside IP for 10.0.0.1. Added in version 1.3.0 Support for multiple outside NAT IP addresses was added.","title":"Network Address Translation (NAT)"},{"location":"core-functionality/ipam.html#virtual-routing-and-forwarding-vrf","text":"A VRF object in Nautobot represents a virtual routing and forwarding (VRF) domain. Each VRF is essentially a separate routing table. VRFs are commonly used to isolate customers or organizations from one another within a network, or to route overlapping address space (e.g. multiple instances of the 10.0.0.0/8 space). Each VRF may be assigned to a specific tenant to aid in organizing the available IP space by customer or internal user. Each VRF is assigned a unique name and an optional route distinguisher (RD). The RD is expected to take one of the forms prescribed in RFC 4364 , however its formatting is not strictly enforced. Each prefix and IP address may be assigned to one (and only one) VRF. If you have a prefix or IP address which exists in multiple VRFs, you will need to create a separate instance of it in Nautobot for each VRF. Any prefix or IP address not assigned to a VRF is said to belong to the \"global\" table. By default, Nautobot will allow duplicate prefixes to be assigned to a VRF. This behavior can be toggled by setting the \"enforce unique\" flag on the VRF model. Note Enforcement of unique IP space can be toggled for global table (non-VRF prefixes) using the ENFORCE_GLOBAL_UNIQUE configuration setting. Each VRF may have one or more import and/or export route targets applied to it. Route targets are used to control the exchange of routes (prefixes) among VRFs in L3VPNs.","title":"Virtual Routing and Forwarding (VRF)"},{"location":"core-functionality/ipam.html#route-targets","text":"A route target is a particular type of extended BGP community used to control the redistribution of routes among VRF tables in a network. Route targets can be assigned to individual VRFs in Nautobot as import or export targets (or both) to model this exchange in an L3VPN. Each route target must be given a unique name, which should be in a format prescribed by RFC 4364 , similar to a VR route distinguisher. Each route target can optionally be assigned to a tenant, and may have tags assigned to it.","title":"Route Targets"},{"location":"core-functionality/power.html","text":"Power Tracking \u00b6 Power Panel \u00b6 A power panel represents the origin point in Nautobot for electrical power being disseminated by one or more power feeds. In a data center environment, one power panel often serves a group of racks, with an individual power feed extending to each rack, though this is not always the case. It is common to have two sets of panels and feeds arranged in parallel to provide redundant power to each rack. Each power panel must be assigned to a site, and may optionally be assigned to a particular location and/or rack group. Note Nautobot does not model the mechanism by which power is delivered to a power panel. Power panels define the root level of the power distribution hierarchy in Nautobot. Power Feed \u00b6 A power feed represents the distribution of power from a power panel to a particular device, typically a power distribution unit (PDU). The power pot (inlet) on a device can be connected via a cable to a power feed. A power feed may optionally be assigned to a rack to allow more easily tracking the distribution of power among racks. Each power feed is assigned an operational type (primary or redundant) and a status . The following statuses are available by default: Offline Active Planned Failed Each power feed also defines the electrical characteristics of the circuit which it represents. These include the following: Supply type (AC or DC) Phase (single or three-phase) Voltage Amperage Maximum utilization (percentage) Info The power utilization of a rack is calculated when one or more power feeds are assigned to the rack and connected to devices that draw power. Example Power Topology \u00b6","title":"Power Tracking"},{"location":"core-functionality/power.html#power-tracking","text":"","title":"Power Tracking"},{"location":"core-functionality/power.html#power-panel","text":"A power panel represents the origin point in Nautobot for electrical power being disseminated by one or more power feeds. In a data center environment, one power panel often serves a group of racks, with an individual power feed extending to each rack, though this is not always the case. It is common to have two sets of panels and feeds arranged in parallel to provide redundant power to each rack. Each power panel must be assigned to a site, and may optionally be assigned to a particular location and/or rack group. Note Nautobot does not model the mechanism by which power is delivered to a power panel. Power panels define the root level of the power distribution hierarchy in Nautobot.","title":"Power Panel"},{"location":"core-functionality/power.html#power-feed","text":"A power feed represents the distribution of power from a power panel to a particular device, typically a power distribution unit (PDU). The power pot (inlet) on a device can be connected via a cable to a power feed. A power feed may optionally be assigned to a rack to allow more easily tracking the distribution of power among racks. Each power feed is assigned an operational type (primary or redundant) and a status . The following statuses are available by default: Offline Active Planned Failed Each power feed also defines the electrical characteristics of the circuit which it represents. These include the following: Supply type (AC or DC) Phase (single or three-phase) Voltage Amperage Maximum utilization (percentage) Info The power utilization of a rack is calculated when one or more power feeds are assigned to the rack and connected to devices that draw power.","title":"Power Feed"},{"location":"core-functionality/power.html#example-power-topology","text":"","title":"Example Power Topology"},{"location":"core-functionality/secrets.html","text":"Secrets \u00b6 Secrets \u00b6 Added in version 1.2.0 For security reasons, Nautobot generally does not store sensitive secrets (device access credentials, systems-integration API tokens, etc.) in its own database. There are other approaches and systems better suited to this purpose, ranging from simple solutions such as process-specific environment variables or restricted-access files on disk, all the way through to dedicated systems such as Hashicorp Vault or AWS Secrets Manager. However, any number of Nautobot features (including, but not limited to, device access via NAPALM, Git repository access, custom Jobs, and various plugins seeking to integrate with third-party systems) do need the ability to retrieve and make use of such secrets. Towards that end, Nautobot provides a Secret database model. This model does not store the secret value itself, but instead defines how Nautobot can retrieve the secret value as and when it is needed. By using this model as an abstraction of the underlying secrets storage implementation, this makes it possible for any Nautobot feature to make use of secret values without needing to know or care where or how the secret is actually stored. Secrets can be grouped and assigned a specific purpose as members of a Secrets Group, which can then be attached to a Git repository, device, or other data model as needed for a given purpose. Secrets Providers \u00b6 Each Secret is associated with a secrets provider (not to be confused with a circuit provider), which provides the functionality needed to retrieve a specific value from a particular source of secrets. Each secrets provider also defines the set of parameters that a given Secret must specify in order to retrieve a secret value from this provider. Nautobot includes the following built-in secrets providers: Environment Variable - for retrieving a secret value defined in an environment variable; Secrets using this provider must specify the variable name to retrieve. Text File - for retrieving a secret value stored in a text file; Secrets using this provider must specify the absolute path of the file to retrieve. Changed in version 1.4.3 When using the Text File secrets provider, any leading and trailing whitespace or newlines will be stripped. When defining a new Secret, you will need to select the desired secrets provider and then fill in the specific parameters required by that provider in order to have a completely specified, usable Secret record. Tip Nautobot plugins can also implement and register additional secrets providers as desired to support other sources such as Hashicorp Vault or AWS Secrets Manager. Templated Secret Parameters \u00b6 In some cases you may have a collection of closely related secrets values that all follow a similar retrieval pattern. For example you might have a directory of text files each containing the unique password for a specific device, or have defined a set of environment variables providing authentication tokens for each different Git repository. In this case, to reduce the need for repeated data entry, Nautobot provides an option to use Jinja2 templates to dynamically alter the provider parameters of a given Secret based on the requesting object. The relevant object is passed to Jinja2 as obj . Thus, for example: A \"Device Password\" secret could use the Text File provider and specify the file path as \"/opt/nautobot/device_passwords/{{ obj.site.slug }}/{{ obj.name }}.txt\" , so that a device csr1 at site nyc would be able to retrieve its password value from /opt/nautobot/device_passwords/nyc/csr1.txt . A \"Git Token\" secret could use the Environment Variable provider and specify the variable name as \"GIT_TOKEN_{{ obj.slug | replace('-', '_') | upper }}\" , so that a Git repository golden-config would be able to retrieve its token value from $GIT_TOKEN_GOLDEN_CONFIG . Secrets and Security \u00b6 Secrets are of course closely linked to security, and as such they pose a number of unique concerns that are worth discussing. Leakage of Secret Values \u00b6 By design, the UI, REST API, and GraphQL do not provide access to retrieve or report the actual value of any given Secret, as these values are only meant for use within Nautobot itself. Tip If you need to use a secret value for some other purpose (such as to manually log into a device, or query an authenticated REST API endpoint yourself), you should be retrieving the value directly from the appropriate secrets provider rather than trying to relay it through Nautobot. However, code is power, and with power comes responsibility. Warning Any user or process that has the ability to execute code within Nautobot has the potential to access the value of any Secret, and a user or process that has the ability to execute arbitrary code absolutely can access Secrets. What does this mean in practice? Any plugin can potentially access your Secrets, including displaying their values onscreen or even forwarding them to an external system, so only install plugins that you trust. Any Job can potentially access your Secrets, and can trivially log a Secret value to its JobResult, where it may be visible to users, so only install Jobs that you trust, carefully limit which users are able to execute jobs and view job results, and be aware of the potential for privilege escalation resulting from careless or malicious logging. Any Git repository can add new Jobs to your system, so be careful about which users you grant the ability to create/edit GitRepository records. Any user with access to nautobot-server nbshell can execute arbitrary code, including accessing Secrets, and will be able to bypass any Nautobot permissions restrictions as well. Any user with access to modify Secrets can take advantage of a leak of one Secret's information through any of the above vectors to additionally leak other secret values (except as restricted with object permissions , see below). For example, if a Job erroneously logs a username obtained from a Secret as a part of its output, a user could modify the corresponding Secret definition to make the Job log any other secret value they have access to. Using Object Permissions with Secrets \u00b6 Tip In practice you will likely want to carefully restrict which users are allowed to define and edit Secrets, and may want to use object permissions to further restrict which specific Secrets they are allowed to utilize. The two default Secrets providers potentially allow a user to define and use a Secret corresponding to any environment variable in the Nautobot execution context and/or any file readable by the nautobot user. For many users and use cases, you will not want to grant this much power to define and access arbitrary secrets; fortunately Nautobot's built-in permissions model is granular enough to allow for more specifically tailored access grants. For example, to restrict a specific user to only be able to work with Secrets that use the environment-variable Secrets provider, and specifically only to access those environment variables whose names begin with NAPALM_ , you could define a Permission with a specific constraint like: { \"provider\" : \"environment-variable\" , \"parameters__variable__startswith\" : \"NAPALM_\" } Or for a Permission to work with Secrets that use text-file , but only files located in /opt/nautobot/secrets/ , you could use the following constraint: { \"provider\" : \"text-file\" , \"parameters__path__startswith\" : \"/opt/nautobot/secrets/\" } Secrets Groups \u00b6 Added in version 1.2.0 A Secrets Group provides a way to collect and assign a purpose to one or more Secrets. The Secrets Group can then be attached to any object that needs to reference and make use of these Secrets, such as a Git repository needing a username/token to authenticate to a private GitHub repository, or a device using a group of Secrets to drive its NAPALM integration. When creating or editing a Secrets Group, you can assign any number of defined Secrets to this group, assigning each secret an access type and a secret type that are unique within the context of this group. Some examples of how a Secrets Group might be populated for use by a given feature: Feature Access Type Secrets Type(s) Git private repository HTTP(S) Token , possibly also Username Device NAPALM integration Generic Username , Password , possibly an enable Secret A Secrets Group is not limited to containing secrets of a single access type either - for example, a plugin that supports both NETCONF and gNMI protocols to interact with a device could be able to make use of a Secrets Group containing distinct secrets for each protocol. Accessing Secrets in Code \u00b6 Accessing a Secret's value from code is as simple as calling its get_value() method. Providing an obj parameter for context is recommended so as to allow for proper handling of templated secret parameters: >>> secret = Secret . objects . get ( slug = \"napalm-username\" ) >>> secret . get_value () 'user' >>> secret = Secret . objects . get ( slug = \"napalm-password\" ) >>> secret . get_value ( obj = device1 ) 'secret-device1-password' In the case where a secret's value cannot be retrieved successfully, Nautobot will raise a SecretError or one of its subclasses: >>> from nautobot.extras.secrets.exceptions import SecretError >>> try : ... Secret . objects . get ( slug = \"napalm-secret\" ) . get_value () ... except SecretError as exc : ... print ( exc ) ... SecretValueNotFoundError : Secret \"NAPALM Secret\" ( provider \"EnvironmentVariableSecretsProvider\" ): Undefined environment variable \"NAPALM_SECRET\" ! In many cases, rather than accessing a specific Secret directly, you will be working with a Secrets Group instead. To retrieve the value of a specific secret within a group, use the group's get_secret_value() method, again with the option of providing an obj for template context: >>> secrets_group = SecretsGroup . objects . get ( slug = \"netconf-credentials\" ) >>> from nautobot.extras.choices import SecretsGroupAccessTypeChoices , SecretsGroupSecretTypeChoices >>> secrets_group . get_secret_value ( ... access_type = SecretsGroupAccessTypeChoices . TYPE_NETCONF , ... secret_type = SecretsGroupSecretTypeChoices . TYPE_USERNAME , ... obj = device1 , ... ) \"user-device1\"","title":"Secrets"},{"location":"core-functionality/secrets.html#secrets","text":"","title":"Secrets"},{"location":"core-functionality/secrets.html#secrets_1","text":"Added in version 1.2.0 For security reasons, Nautobot generally does not store sensitive secrets (device access credentials, systems-integration API tokens, etc.) in its own database. There are other approaches and systems better suited to this purpose, ranging from simple solutions such as process-specific environment variables or restricted-access files on disk, all the way through to dedicated systems such as Hashicorp Vault or AWS Secrets Manager. However, any number of Nautobot features (including, but not limited to, device access via NAPALM, Git repository access, custom Jobs, and various plugins seeking to integrate with third-party systems) do need the ability to retrieve and make use of such secrets. Towards that end, Nautobot provides a Secret database model. This model does not store the secret value itself, but instead defines how Nautobot can retrieve the secret value as and when it is needed. By using this model as an abstraction of the underlying secrets storage implementation, this makes it possible for any Nautobot feature to make use of secret values without needing to know or care where or how the secret is actually stored. Secrets can be grouped and assigned a specific purpose as members of a Secrets Group, which can then be attached to a Git repository, device, or other data model as needed for a given purpose.","title":"Secrets"},{"location":"core-functionality/secrets.html#secrets-providers","text":"Each Secret is associated with a secrets provider (not to be confused with a circuit provider), which provides the functionality needed to retrieve a specific value from a particular source of secrets. Each secrets provider also defines the set of parameters that a given Secret must specify in order to retrieve a secret value from this provider. Nautobot includes the following built-in secrets providers: Environment Variable - for retrieving a secret value defined in an environment variable; Secrets using this provider must specify the variable name to retrieve. Text File - for retrieving a secret value stored in a text file; Secrets using this provider must specify the absolute path of the file to retrieve. Changed in version 1.4.3 When using the Text File secrets provider, any leading and trailing whitespace or newlines will be stripped. When defining a new Secret, you will need to select the desired secrets provider and then fill in the specific parameters required by that provider in order to have a completely specified, usable Secret record. Tip Nautobot plugins can also implement and register additional secrets providers as desired to support other sources such as Hashicorp Vault or AWS Secrets Manager.","title":"Secrets Providers"},{"location":"core-functionality/secrets.html#templated-secret-parameters","text":"In some cases you may have a collection of closely related secrets values that all follow a similar retrieval pattern. For example you might have a directory of text files each containing the unique password for a specific device, or have defined a set of environment variables providing authentication tokens for each different Git repository. In this case, to reduce the need for repeated data entry, Nautobot provides an option to use Jinja2 templates to dynamically alter the provider parameters of a given Secret based on the requesting object. The relevant object is passed to Jinja2 as obj . Thus, for example: A \"Device Password\" secret could use the Text File provider and specify the file path as \"/opt/nautobot/device_passwords/{{ obj.site.slug }}/{{ obj.name }}.txt\" , so that a device csr1 at site nyc would be able to retrieve its password value from /opt/nautobot/device_passwords/nyc/csr1.txt . A \"Git Token\" secret could use the Environment Variable provider and specify the variable name as \"GIT_TOKEN_{{ obj.slug | replace('-', '_') | upper }}\" , so that a Git repository golden-config would be able to retrieve its token value from $GIT_TOKEN_GOLDEN_CONFIG .","title":"Templated Secret Parameters"},{"location":"core-functionality/secrets.html#secrets-and-security","text":"Secrets are of course closely linked to security, and as such they pose a number of unique concerns that are worth discussing.","title":"Secrets and Security"},{"location":"core-functionality/secrets.html#leakage-of-secret-values","text":"By design, the UI, REST API, and GraphQL do not provide access to retrieve or report the actual value of any given Secret, as these values are only meant for use within Nautobot itself. Tip If you need to use a secret value for some other purpose (such as to manually log into a device, or query an authenticated REST API endpoint yourself), you should be retrieving the value directly from the appropriate secrets provider rather than trying to relay it through Nautobot. However, code is power, and with power comes responsibility. Warning Any user or process that has the ability to execute code within Nautobot has the potential to access the value of any Secret, and a user or process that has the ability to execute arbitrary code absolutely can access Secrets. What does this mean in practice? Any plugin can potentially access your Secrets, including displaying their values onscreen or even forwarding them to an external system, so only install plugins that you trust. Any Job can potentially access your Secrets, and can trivially log a Secret value to its JobResult, where it may be visible to users, so only install Jobs that you trust, carefully limit which users are able to execute jobs and view job results, and be aware of the potential for privilege escalation resulting from careless or malicious logging. Any Git repository can add new Jobs to your system, so be careful about which users you grant the ability to create/edit GitRepository records. Any user with access to nautobot-server nbshell can execute arbitrary code, including accessing Secrets, and will be able to bypass any Nautobot permissions restrictions as well. Any user with access to modify Secrets can take advantage of a leak of one Secret's information through any of the above vectors to additionally leak other secret values (except as restricted with object permissions , see below). For example, if a Job erroneously logs a username obtained from a Secret as a part of its output, a user could modify the corresponding Secret definition to make the Job log any other secret value they have access to.","title":"Leakage of Secret Values"},{"location":"core-functionality/secrets.html#using-object-permissions-with-secrets","text":"Tip In practice you will likely want to carefully restrict which users are allowed to define and edit Secrets, and may want to use object permissions to further restrict which specific Secrets they are allowed to utilize. The two default Secrets providers potentially allow a user to define and use a Secret corresponding to any environment variable in the Nautobot execution context and/or any file readable by the nautobot user. For many users and use cases, you will not want to grant this much power to define and access arbitrary secrets; fortunately Nautobot's built-in permissions model is granular enough to allow for more specifically tailored access grants. For example, to restrict a specific user to only be able to work with Secrets that use the environment-variable Secrets provider, and specifically only to access those environment variables whose names begin with NAPALM_ , you could define a Permission with a specific constraint like: { \"provider\" : \"environment-variable\" , \"parameters__variable__startswith\" : \"NAPALM_\" } Or for a Permission to work with Secrets that use text-file , but only files located in /opt/nautobot/secrets/ , you could use the following constraint: { \"provider\" : \"text-file\" , \"parameters__path__startswith\" : \"/opt/nautobot/secrets/\" }","title":"Using Object Permissions with Secrets"},{"location":"core-functionality/secrets.html#secrets-groups","text":"Added in version 1.2.0 A Secrets Group provides a way to collect and assign a purpose to one or more Secrets. The Secrets Group can then be attached to any object that needs to reference and make use of these Secrets, such as a Git repository needing a username/token to authenticate to a private GitHub repository, or a device using a group of Secrets to drive its NAPALM integration. When creating or editing a Secrets Group, you can assign any number of defined Secrets to this group, assigning each secret an access type and a secret type that are unique within the context of this group. Some examples of how a Secrets Group might be populated for use by a given feature: Feature Access Type Secrets Type(s) Git private repository HTTP(S) Token , possibly also Username Device NAPALM integration Generic Username , Password , possibly an enable Secret A Secrets Group is not limited to containing secrets of a single access type either - for example, a plugin that supports both NETCONF and gNMI protocols to interact with a device could be able to make use of a Secrets Group containing distinct secrets for each protocol.","title":"Secrets Groups"},{"location":"core-functionality/secrets.html#accessing-secrets-in-code","text":"Accessing a Secret's value from code is as simple as calling its get_value() method. Providing an obj parameter for context is recommended so as to allow for proper handling of templated secret parameters: >>> secret = Secret . objects . get ( slug = \"napalm-username\" ) >>> secret . get_value () 'user' >>> secret = Secret . objects . get ( slug = \"napalm-password\" ) >>> secret . get_value ( obj = device1 ) 'secret-device1-password' In the case where a secret's value cannot be retrieved successfully, Nautobot will raise a SecretError or one of its subclasses: >>> from nautobot.extras.secrets.exceptions import SecretError >>> try : ... Secret . objects . get ( slug = \"napalm-secret\" ) . get_value () ... except SecretError as exc : ... print ( exc ) ... SecretValueNotFoundError : Secret \"NAPALM Secret\" ( provider \"EnvironmentVariableSecretsProvider\" ): Undefined environment variable \"NAPALM_SECRET\" ! In many cases, rather than accessing a specific Secret directly, you will be working with a Secrets Group instead. To retrieve the value of a specific secret within a group, use the group's get_secret_value() method, again with the option of providing an obj for template context: >>> secrets_group = SecretsGroup . objects . get ( slug = \"netconf-credentials\" ) >>> from nautobot.extras.choices import SecretsGroupAccessTypeChoices , SecretsGroupSecretTypeChoices >>> secrets_group . get_secret_value ( ... access_type = SecretsGroupAccessTypeChoices . TYPE_NETCONF , ... secret_type = SecretsGroupSecretTypeChoices . TYPE_USERNAME , ... obj = device1 , ... ) \"user-device1\"","title":"Accessing Secrets in Code"},{"location":"core-functionality/services.html","text":"Service Mapping \u00b6 Services \u00b6 A service represents a layer four TCP or UDP service available on a device or virtual machine. For example, you might want to document that an HTTP service is running on a device. Each service includes a name, protocol, and port number; for example, \"SSH (TCP/22)\" or \"DNS (UDP/53).\" A service may optionally be bound to one or more specific IP addresses belonging to its parent device or VM. (If no IP addresses are bound, the service is assumed to be reachable via any assigned IP address.)","title":"Service Mapping"},{"location":"core-functionality/services.html#service-mapping","text":"","title":"Service Mapping"},{"location":"core-functionality/services.html#services","text":"A service represents a layer four TCP or UDP service available on a device or virtual machine. For example, you might want to document that an HTTP service is running on a device. Each service includes a name, protocol, and port number; for example, \"SSH (TCP/22)\" or \"DNS (UDP/53).\" A service may optionally be bound to one or more specific IP addresses belonging to its parent device or VM. (If no IP addresses are bound, the service is assumed to be reachable via any assigned IP address.)","title":"Services"},{"location":"core-functionality/sites-and-racks.html","text":"Sites, Locations, and Racks \u00b6 Sites \u00b6 How you choose to employ sites when modeling your network may vary depending on the nature of your organization, but generally a site will equate to a building or campus. For example, a chain of banks might create a site to represent each of its branches, a site for its corporate headquarters, and two additional sites for its presence in two co-location facilities. Each site must be assigned a unique name and operational status and may optionally be assigned to a region and/or tenant. The following operational statuses are available by default: Planned Staging Active Decommissioning Retired The site model also provides a facility ID field which can be used to annotate a facility ID (such as a data center name) associated with the site. Each site may also have an autonomous system (AS) number and time zone associated with it. (Time zones are provided by the pytz package.) The site model also includes several fields for storing contact and address information as well as geo-location data (GPS coordinates). Info In a future Nautobot release, sites may become just another Location Type, and the Site model may be collapsed into the Location model. Regions \u00b6 Sites can be arranged geographically using regions. A region might represent a continent, country, city, campus, or other area depending on your use case. Regions can be nested recursively to construct a hierarchy. For example, you might define several country regions, and within each of those several state or city regions to which sites are assigned. Info In a future Nautobot release, regions may become another Location Type, and the Region model may be collapsed into the Location model. Location Types \u00b6 Added in version 1.4.0 Before defining individual Locations, you must first define the hierarchy of Location Types that you wish to use for the organization of your network. An example hierarchy might be Building \u2190 Floor \u2190 Room , but you might have more or fewer distinct types depending on your specific organizational requirements. Each Location Type can define a set of \"content types\" that are permitted to associate to Locations of this type. For example, you might permit assigning Prefixes and VLAN Groups to an entire Building or Floor, but only allow Devices and Racks to be assigned to Rooms, never to a more abstract location. Doing so can help ensure consistency of your data. Tip Although it is possible to define a \"tree\" of Location Types with multiple \"branches\", in the majority of cases doing so adds more unnecessary complexity than it's worth. Consider the following hypothetical Location Type tree: Branch Office \u21b3 Branch Floor \u21b3 Branch Floor Room \u21b3 Branch Basement \u21b3 Branch Basement Room Headquarters \u21b3 Headquarters Floor \u21b3 Headquarters Floor Room \u21b3 Headquarters Basement \u21b3 Headquarters Basement Room This would complicate your life significantly when constructing queries, filters, and so forth to actually work with your data - for example, if you wanted a list of all Prefixes that are mapped to floors rather than individual rooms, you would now need to construct a query for Prefixes that are mapped to (a Branch Floor OR a Headquarters Floor OR a Branch Basement OR a Headquarters Basement ). In most cases you would be better served with a far simpler \"linear\" sequence of Location Types, such as Building \u2190 Floor \u2190 Room ; you could then use tags or custom fields to distinguish whether a given Building is a Branch Office or a Headquarters, if that distinction is even important to your network model. Locations \u00b6 Added in version 1.4.0 To locate network information more precisely than a Site defines, you can define a hierarchy of Locations within each Site. Data objects such as devices, prefixes, VLAN groups, etc. can thus be mapped or assigned to a specific building, wing, floor, room, etc. as appropriate to your needs. Once you have defined the hierarchy of Location Types that you wish to use, you can then define Locations. Any \"top-level\" Locations (those whose Location Type has no parent) belong directly to a Site, while \"child\" Locations belong to their immediate parent Location, rather than to the Site as a whole. Info At present, Locations fill the conceptual space between the more abstract Region and Site models and the more concrete Rack Group model. In a future Nautobot release, some or all of these other models may be collapsed into Locations. That is to say, in the future you might not deal with Regions and Sites as distinct models, but instead your Location Type hierarchy might include these higher-level categories, becoming something like Country \u2190 City \u2190 Site \u2190 Building \u2190 Floor \u2190 Room. Much like Sites, each Location must be assigned a name and operational status . The same default operational statuses are defined for Locations as for Sites, but as always, you can customize these to suit your needs. Locations can also be assigned to a tenant. Racks \u00b6 The rack model represents a physical two- or four-post equipment rack in which devices can be installed. Each rack must be assigned to a site, and may optionally be assigned to a location, rack group, and/or tenant. Racks can also be organized by user-defined functional roles. Rack height is measured in rack units (U); racks are commonly between 42U and 48U tall, but Nautobot allows you to define racks of arbitrary height. A toggle is provided to indicate whether rack units are in ascending (from the ground up) or descending order. Each rack is assigned a name and (optionally) a separate facility ID. This is helpful when leasing space in a data center your organization does not own: The facility will often assign a seemingly arbitrary ID to a rack (for example, \"M204.313\") whereas internally you refer to is simply as \"R113.\" A unique serial number and asset tag may also be associated with each rack. A rack must be designated as one of the following types: 2-post frame 4-post frame 4-post cabinet Wall-mounted frame Wall-mounted cabinet Similarly, each rack must be assigned an operational status . The following statuses are available by default: Reserved Available Planned Active Deprecated Each rack has two faces (front and rear) on which devices can be mounted. Rail-to-rail width may be 10, 19, 21, or 23 inches. The outer width and depth of a rack or cabinet can also be annotated in millimeters or inches. Rack Groups \u00b6 Racks can be organized into groups, which can be nested into themselves similar to regions. As with sites, how you choose to designate rack groups will depend on the nature of your organization. Each rack group must be assigned to a parent site (and optionally also a more specific location within that site). Rack groups may optionally be nested within one another to model a multi-level hierarchy. The name and facility ID of each rack within a group must be unique. (Racks not assigned to the same rack group may have identical names and/or facility IDs.) Rack Roles \u00b6 Each rack can optionally be assigned a user-defined functional role. For example, you might designate a rack for compute or storage resources, or to house co-located customer devices. Rack roles are fully customizable and may be color-coded. Rack Reservations \u00b6 Users can reserve specific units within a rack for future use. An arbitrary set of units within a rack can be associated with a single reservation, but reservations cannot span multiple racks. A description is required for each reservation, reservations may optionally be associated with a specific tenant.","title":"Sites, Locations, and Racks"},{"location":"core-functionality/sites-and-racks.html#sites-locations-and-racks","text":"","title":"Sites, Locations, and Racks"},{"location":"core-functionality/sites-and-racks.html#sites","text":"How you choose to employ sites when modeling your network may vary depending on the nature of your organization, but generally a site will equate to a building or campus. For example, a chain of banks might create a site to represent each of its branches, a site for its corporate headquarters, and two additional sites for its presence in two co-location facilities. Each site must be assigned a unique name and operational status and may optionally be assigned to a region and/or tenant. The following operational statuses are available by default: Planned Staging Active Decommissioning Retired The site model also provides a facility ID field which can be used to annotate a facility ID (such as a data center name) associated with the site. Each site may also have an autonomous system (AS) number and time zone associated with it. (Time zones are provided by the pytz package.) The site model also includes several fields for storing contact and address information as well as geo-location data (GPS coordinates). Info In a future Nautobot release, sites may become just another Location Type, and the Site model may be collapsed into the Location model.","title":"Sites"},{"location":"core-functionality/sites-and-racks.html#regions","text":"Sites can be arranged geographically using regions. A region might represent a continent, country, city, campus, or other area depending on your use case. Regions can be nested recursively to construct a hierarchy. For example, you might define several country regions, and within each of those several state or city regions to which sites are assigned. Info In a future Nautobot release, regions may become another Location Type, and the Region model may be collapsed into the Location model.","title":"Regions"},{"location":"core-functionality/sites-and-racks.html#location-types","text":"Added in version 1.4.0 Before defining individual Locations, you must first define the hierarchy of Location Types that you wish to use for the organization of your network. An example hierarchy might be Building \u2190 Floor \u2190 Room , but you might have more or fewer distinct types depending on your specific organizational requirements. Each Location Type can define a set of \"content types\" that are permitted to associate to Locations of this type. For example, you might permit assigning Prefixes and VLAN Groups to an entire Building or Floor, but only allow Devices and Racks to be assigned to Rooms, never to a more abstract location. Doing so can help ensure consistency of your data. Tip Although it is possible to define a \"tree\" of Location Types with multiple \"branches\", in the majority of cases doing so adds more unnecessary complexity than it's worth. Consider the following hypothetical Location Type tree: Branch Office \u21b3 Branch Floor \u21b3 Branch Floor Room \u21b3 Branch Basement \u21b3 Branch Basement Room Headquarters \u21b3 Headquarters Floor \u21b3 Headquarters Floor Room \u21b3 Headquarters Basement \u21b3 Headquarters Basement Room This would complicate your life significantly when constructing queries, filters, and so forth to actually work with your data - for example, if you wanted a list of all Prefixes that are mapped to floors rather than individual rooms, you would now need to construct a query for Prefixes that are mapped to (a Branch Floor OR a Headquarters Floor OR a Branch Basement OR a Headquarters Basement ). In most cases you would be better served with a far simpler \"linear\" sequence of Location Types, such as Building \u2190 Floor \u2190 Room ; you could then use tags or custom fields to distinguish whether a given Building is a Branch Office or a Headquarters, if that distinction is even important to your network model.","title":"Location Types"},{"location":"core-functionality/sites-and-racks.html#locations","text":"Added in version 1.4.0 To locate network information more precisely than a Site defines, you can define a hierarchy of Locations within each Site. Data objects such as devices, prefixes, VLAN groups, etc. can thus be mapped or assigned to a specific building, wing, floor, room, etc. as appropriate to your needs. Once you have defined the hierarchy of Location Types that you wish to use, you can then define Locations. Any \"top-level\" Locations (those whose Location Type has no parent) belong directly to a Site, while \"child\" Locations belong to their immediate parent Location, rather than to the Site as a whole. Info At present, Locations fill the conceptual space between the more abstract Region and Site models and the more concrete Rack Group model. In a future Nautobot release, some or all of these other models may be collapsed into Locations. That is to say, in the future you might not deal with Regions and Sites as distinct models, but instead your Location Type hierarchy might include these higher-level categories, becoming something like Country \u2190 City \u2190 Site \u2190 Building \u2190 Floor \u2190 Room. Much like Sites, each Location must be assigned a name and operational status . The same default operational statuses are defined for Locations as for Sites, but as always, you can customize these to suit your needs. Locations can also be assigned to a tenant.","title":"Locations"},{"location":"core-functionality/sites-and-racks.html#racks","text":"The rack model represents a physical two- or four-post equipment rack in which devices can be installed. Each rack must be assigned to a site, and may optionally be assigned to a location, rack group, and/or tenant. Racks can also be organized by user-defined functional roles. Rack height is measured in rack units (U); racks are commonly between 42U and 48U tall, but Nautobot allows you to define racks of arbitrary height. A toggle is provided to indicate whether rack units are in ascending (from the ground up) or descending order. Each rack is assigned a name and (optionally) a separate facility ID. This is helpful when leasing space in a data center your organization does not own: The facility will often assign a seemingly arbitrary ID to a rack (for example, \"M204.313\") whereas internally you refer to is simply as \"R113.\" A unique serial number and asset tag may also be associated with each rack. A rack must be designated as one of the following types: 2-post frame 4-post frame 4-post cabinet Wall-mounted frame Wall-mounted cabinet Similarly, each rack must be assigned an operational status . The following statuses are available by default: Reserved Available Planned Active Deprecated Each rack has two faces (front and rear) on which devices can be mounted. Rail-to-rail width may be 10, 19, 21, or 23 inches. The outer width and depth of a rack or cabinet can also be annotated in millimeters or inches.","title":"Racks"},{"location":"core-functionality/sites-and-racks.html#rack-groups","text":"Racks can be organized into groups, which can be nested into themselves similar to regions. As with sites, how you choose to designate rack groups will depend on the nature of your organization. Each rack group must be assigned to a parent site (and optionally also a more specific location within that site). Rack groups may optionally be nested within one another to model a multi-level hierarchy. The name and facility ID of each rack within a group must be unique. (Racks not assigned to the same rack group may have identical names and/or facility IDs.)","title":"Rack Groups"},{"location":"core-functionality/sites-and-racks.html#rack-roles","text":"Each rack can optionally be assigned a user-defined functional role. For example, you might designate a rack for compute or storage resources, or to house co-located customer devices. Rack roles are fully customizable and may be color-coded.","title":"Rack Roles"},{"location":"core-functionality/sites-and-racks.html#rack-reservations","text":"Users can reserve specific units within a rack for future use. An arbitrary set of units within a rack can be associated with a single reservation, but reservations cannot span multiple racks. A description is required for each reservation, reservations may optionally be associated with a specific tenant.","title":"Rack Reservations"},{"location":"core-functionality/tenancy.html","text":"Tenancy Assignment \u00b6 Tenants \u00b6 A tenant represents a discrete grouping of resources used for administrative purposes. Typically, tenants are used to represent individual customers or internal departments within an organization. The following objects can be assigned to tenants: Sites Racks Rack reservations Devices VRFs Prefixes IP addresses VLANs Circuits Clusters Virtual machines Tenant assignment is used to signify the ownership of an object in Nautobot. As such, each object may only be owned by a single tenant. For example, if you have a firewall dedicated to a particular customer, you would assign it to the tenant which represents that customer. However, if the firewall serves multiple customers, it doesn't belong to any particular customer, so tenant assignment would not be appropriate. Tenant Groups \u00b6 Tenants can be organized by custom groups. For instance, you might create one group called \"Customers\" and one called \"Departments.\" The assignment of a tenant to a group is optional. Tenant groups may be nested recursively to achieve a multi-level hierarchy. For example, you might have a group called \"Customers\" containing subgroups of individual tenants grouped by product or account team.","title":"Tenancy"},{"location":"core-functionality/tenancy.html#tenancy-assignment","text":"","title":"Tenancy Assignment"},{"location":"core-functionality/tenancy.html#tenants","text":"A tenant represents a discrete grouping of resources used for administrative purposes. Typically, tenants are used to represent individual customers or internal departments within an organization. The following objects can be assigned to tenants: Sites Racks Rack reservations Devices VRFs Prefixes IP addresses VLANs Circuits Clusters Virtual machines Tenant assignment is used to signify the ownership of an object in Nautobot. As such, each object may only be owned by a single tenant. For example, if you have a firewall dedicated to a particular customer, you would assign it to the tenant which represents that customer. However, if the firewall serves multiple customers, it doesn't belong to any particular customer, so tenant assignment would not be appropriate.","title":"Tenants"},{"location":"core-functionality/tenancy.html#tenant-groups","text":"Tenants can be organized by custom groups. For instance, you might create one group called \"Customers\" and one called \"Departments.\" The assignment of a tenant to a group is optional. Tenant groups may be nested recursively to achieve a multi-level hierarchy. For example, you might have a group called \"Customers\" containing subgroups of individual tenants grouped by product or account team.","title":"Tenant Groups"},{"location":"core-functionality/virtualization.html","text":"Virtualization \u00b6 Clusters \u00b6 A cluster is a logical grouping of physical resources within which virtual machines run. A cluster must be assigned a type (technological classification), and may optionally be assigned to a cluster group, site, location, and/or tenant. Physical devices may be associated with clusters as hosts. This allows users to track on which host(s) a particular virtual machine may reside. However, Nautobot does not support pinning a specific VM within a cluster to a particular host device. Cluster Types \u00b6 A cluster type represents a technology or mechanism by which a cluster is formed. For example, you might create a cluster type named \"VMware vSphere\" for a locally hosted cluster or \"DigitalOcean NYC3\" for one hosted by a cloud provider. Cluster Groups \u00b6 Cluster groups may be created for the purpose of organizing clusters. The arrangement of clusters into groups is optional. Virtual Machines \u00b6 A virtual machine represents a virtual compute instance hosted within a cluster. Each VM must be assigned to exactly one cluster. Like devices, each VM can be assigned a platform and/or functional role, and an operational status . The following statuses are available by default: Active Offline Planned Staged Failed Decommissioning Additional fields are available for annotating the vCPU count, memory (GB), and disk (GB) allocated to each VM. Each VM may optionally be assigned to a tenant. Virtual machines may have virtual interfaces assigned to them, but do not support any physical component. Interfaces \u00b6 Virtual machine interfaces behave similarly to device interfaces, and can be assigned IP addresses, VLANs, an operational status and services. However, given their virtual nature, they lack properties pertaining to physical attributes. For example, VM interfaces do not have a physical type and cannot have cables attached to them. The following operational statuses are available by default: Planned Maintenance Active Decommissioning Failed Added in version 1.4.0 Added bridge field. Added parent_interface field. Added status field.","title":"Virtualization"},{"location":"core-functionality/virtualization.html#virtualization","text":"","title":"Virtualization"},{"location":"core-functionality/virtualization.html#clusters","text":"A cluster is a logical grouping of physical resources within which virtual machines run. A cluster must be assigned a type (technological classification), and may optionally be assigned to a cluster group, site, location, and/or tenant. Physical devices may be associated with clusters as hosts. This allows users to track on which host(s) a particular virtual machine may reside. However, Nautobot does not support pinning a specific VM within a cluster to a particular host device.","title":"Clusters"},{"location":"core-functionality/virtualization.html#cluster-types","text":"A cluster type represents a technology or mechanism by which a cluster is formed. For example, you might create a cluster type named \"VMware vSphere\" for a locally hosted cluster or \"DigitalOcean NYC3\" for one hosted by a cloud provider.","title":"Cluster Types"},{"location":"core-functionality/virtualization.html#cluster-groups","text":"Cluster groups may be created for the purpose of organizing clusters. The arrangement of clusters into groups is optional.","title":"Cluster Groups"},{"location":"core-functionality/virtualization.html#virtual-machines","text":"A virtual machine represents a virtual compute instance hosted within a cluster. Each VM must be assigned to exactly one cluster. Like devices, each VM can be assigned a platform and/or functional role, and an operational status . The following statuses are available by default: Active Offline Planned Staged Failed Decommissioning Additional fields are available for annotating the vCPU count, memory (GB), and disk (GB) allocated to each VM. Each VM may optionally be assigned to a tenant. Virtual machines may have virtual interfaces assigned to them, but do not support any physical component.","title":"Virtual Machines"},{"location":"core-functionality/virtualization.html#interfaces","text":"Virtual machine interfaces behave similarly to device interfaces, and can be assigned IP addresses, VLANs, an operational status and services. However, given their virtual nature, they lack properties pertaining to physical attributes. For example, VM interfaces do not have a physical type and cannot have cables attached to them. The following operational statuses are available by default: Planned Maintenance Active Decommissioning Failed Added in version 1.4.0 Added bridge field. Added parent_interface field. Added status field.","title":"Interfaces"},{"location":"core-functionality/vlans.html","text":"VLAN Management \u00b6 VLANs \u00b6 A VLAN represents an isolated layer two domain, identified by a name and a numeric ID (1-4094) as defined in IEEE 802.1Q . Each VLAN may be assigned to a site, location, tenant, and/or VLAN group. Each VLAN must be assigned a status . The following statuses are available by default: Active Reserved Deprecated As with prefixes, each VLAN may also be assigned a functional role. Prefixes and VLANs share the same set of customizable roles. VLAN Groups \u00b6 VLAN groups can be used to organize VLANs within Nautobot. Each group may optionally be assigned to a specific site or a location within a site, but a group cannot belong to multiple sites. Groups can also be used to enforce uniqueness: Each VLAN within a group must have a unique ID and name. VLANs which are not assigned to a group may have overlapping names and IDs (including VLANs which belong to a common site). For example, you can create two VLANs with ID 123, but they cannot both be assigned to the same group.","title":"VLAN Management"},{"location":"core-functionality/vlans.html#vlan-management","text":"","title":"VLAN Management"},{"location":"core-functionality/vlans.html#vlans","text":"A VLAN represents an isolated layer two domain, identified by a name and a numeric ID (1-4094) as defined in IEEE 802.1Q . Each VLAN may be assigned to a site, location, tenant, and/or VLAN group. Each VLAN must be assigned a status . The following statuses are available by default: Active Reserved Deprecated As with prefixes, each VLAN may also be assigned a functional role. Prefixes and VLANs share the same set of customizable roles.","title":"VLANs"},{"location":"core-functionality/vlans.html#vlan-groups","text":"VLAN groups can be used to organize VLANs within Nautobot. Each group may optionally be assigned to a specific site or a location within a site, but a group cannot belong to multiple sites. Groups can also be used to enforce uniqueness: Each VLAN within a group must have a unique ID and name. VLANs which are not assigned to a group may have overlapping names and IDs (including VLANs which belong to a common site). For example, you can create two VLANs with ID 123, but they cannot both be assigned to the same group.","title":"VLAN Groups"},{"location":"development/index.html","text":"Nautobot Development \u00b6 Nautobot is maintained as a GitHub project under the Apache 2 license. Users are encouraged to submit GitHub issues for feature requests and bug reports. Governance \u00b6 Nautobot is a community-based Free Open Source Software (FOSS) project sponsored by Network to Code (NTC) . As a network automation solution provider, Network to Code works with its clients around the world to craft and build network automation strategies and solutions, often tightly integrated with Nautobot serving as a Source of Truth and Network Automation Platform. The direction of this project will be shaped by the community as well as by input from NTC customers; independent of where requests come from, contributors will need to follow the Contributing Guidelines. The Nautobot Core Team is responsible for the direction and execution of the code that gets committed to the project. Project Structure \u00b6 Nautobot components are arranged into functional subsections called apps (a carryover from Django vernacular). Each app holds the models, views, and templates relevant to a particular function: circuits : Communications circuits and providers (not to be confused with power circuits) dcim : Datacenter infrastructure management (sites, racks, and devices) extras : Additional features not considered part of the core data model ipam : IP address management (VRFs, prefixes, IP addresses, and VLANs) tenancy : Tenants (such as customers) to which Nautobot objects may be assigned users : Authentication and user preferences utilities : Resources which are not user-facing (extendable classes, etc.) virtualization : Virtual machines and clusters Release Management \u00b6 Roadmap \u00b6 In order to best understand how to contribute and where to open an issue or discussion, you should understand how work moves from idea to feature and how the roadmap is structured. There are three major \"buckets\" of work to be aware of within the lifecycle of getting contributions committed and released: Current - Work that is planned for the release currently being developed. Near Term - Work that is planned for one of the next two releases after the one currently being developed. Future - Work that needs more discussion and/or will be planned for a version three or more releases later. The following provides more detail on these. Current \u00b6 Current tickets (GitHub issues) that are being worked on for the current release or bugs that are found and will be fixed in the current release. Uses current label on GitHub. The GitHub Release Milestone will track items for the current release. Near Term \u00b6 Current tickets (GitHub issues) that are estimated to complete in one of the next two releases, e.g. 3-6 months to get into core, if accepted. GitHub discussions are used to create one or more GitHub issues when and if something moves from Future to Near Term . Uses near-term label on GitHub. Future \u00b6 Work that is for 3+ releases away or work that needs more free form discussions and brainstorming to better scope future bodies of work. Estimated 7+ months to get into core, if accepted. GitHub Discussions are used for collaborating on future work. If a GitHub issue is opened and is deemed that it is out of scope for Current or Near Term , it will be converted into a GitHub Discussion. GitHub Discussions will be closed when the topic/feature moves from Future to Near Term . Over time, the process of moving work from Future to Near Term to Current will continue to get further refined. Please read through the Nautobot Roadmap so you can understand the current backlog and roadmap and which items are already in Current, Near Term, and Future . Release Schedule \u00b6 Here is what you need to know about Nautobot releases: The initial launch of Nautobot is version 1.0.0beta1 (major.minor.patch) released on February 24, 2021. The core team estimates quarterly releases with the majority of them being minor releases. It is an aspirational goal that there will be no more than one major release per year as major releases do indicate a break in backwards compatibility. For information pertaining to patch releases, which will be released on a schedule, please see the Patch Releases section below. Given the core team is estimating quarterly releases, there will not be firm dates for releases. In order to provide more visibility into the development and release schedule of Nautobot, there will be structured notifications as follows: At the start of a release cycle, the estimated timeframe for release will be a 4-6 week window. Halfway through the release cycle (~6 weeks), the estimated timeframe for release will be narrowed to a 3-4 week window After 8-9 weeks within the development cycle, the estimated timeframe for release will be narrowed further to a 2 week window. The final notification will be provided 3-5 days before the release drops. The dates and notifications will occur by updating the GitHub Release Milestone and on Slack. Patch Releases \u00b6 Specifics around patch releases: Will be released every two weeks on Mondays, starting March 7th, 2022. Will be focused on bugs, security vulnerabilities, backports, and other issues as they arise. Will not introduce significant new functionality except as needed to address a bug. Will not be released if there have been no new merges to develop since the previous release. Should not be considered a mandatory upgrade: If it does not fix a bug or issue you are experiencing, you should not feel the need to upgrade right away. May happen a day or two after the Monday schedule if Monday is an observed holiday. We would like to ensure full team availability post-release. Should a patch release contain a fix for security vulnerability(s) (i.e. CVE(s)), data-loss bug(s), or other critical issue(s), we will: Release a new patch release as soon as the fix has been identified, implemented, and thoroughly tested. No waiting for the next regularly scheduled release date. Bring special attention to these releases in our notification channels. Strongly urge you to upgrade to address these more serious issues as soon as possible. Not adjust any subsequent release dates (the next scheduled release will still occur as scheduled). Long Term Support (LTS) \u00b6 The core team is currently evaluating the possibility of publishing a Long Term Support (LTS) version of Nautobot. At this time there is no formal target for this initial release. Our goal is to collect feedback from users of Nautobot to help identify a maintainable and reliable LTS model. If you have interest in deploying an LTS version of Nautobot, or useful information to help inform the final LTS model, please contribute to the GitHub Discussion thread around LTS. Deprecation Policy \u00b6 The deprecation policy will be such that there will be at least one release that makes users aware of a feature that will be deprecated in the next release. Versioning \u00b6 Semantic Versioning ( SemVer ) is used for Nautobot versioning. Communication \u00b6 Communication among the contributors should always occur via public channels. The following outlines the best ways to communicate and engage on all things Nautobot. Slack \u00b6 #nautobot on Network to Code Slack - Good for quick chats. Avoid any discussion that might need to be referenced later on, as the chat history is not retained long. GitHub \u00b6 GitHub issues - All feature requests, bug reports, and other substantial changes should be documented in an issue. GitHub discussions - The preferred forum for general discussion and support issues. Ideal for shaping a feature request prior to submitting an issue. GitHub's discussions are the best place to get help or propose rough ideas for new functionality. Their integration with GitHub allows for easily cross- referencing and converting posts to issues as needed. There are several categories for discussions: General - General community discussion. Ideas - Ideas for new functionality that isn't yet ready for a formal feature request. These ideas are what will be in scope to review when moving work from Future to Near Term as stated in the previous section. Q&A - Request help with installing or using Nautobot. Contributing \u00b6 We welcome many forms of contributions to Nautobot. While we understand most contributions will commonly come from developers, we encourage others to contribute in the form of docs, tutorials, and user guides. If you have other ideas for contributing, don't hesitate to open an issue or have a discussion in one of the forums above. Please also take a chance to look at our Wiki on GitHub to review any specifics as to how we define and scope work as a community. Reporting Bugs \u00b6 First, ensure that you're running the latest stable version of Nautobot. If you're running an older version, it's possible that the bug has already been fixed. Next, check the GitHub issues list to see if the bug you've found has already been reported. If you think you may be experiencing a reported issue that hasn't already been resolved, please click \"add a reaction\" in the top right corner of the issue and add a thumbs up (+1). You might also want to add a comment describing how it's affecting your installation. This will allow us to prioritize bugs based on how many users are affected. When submitting an issue, please be as descriptive as possible. Be sure to provide all information request in the issue template, including: The environment in which Nautobot is running The exact steps that can be taken to reproduce the issue Expected and observed behavior Any error messages generated Screenshots (if applicable) Please avoid prepending any sort of tag (e.g. \"[Bug]\") to the issue title. The issue will be reviewed by a maintainer after submission and the appropriate labels will be applied for categorization. Keep in mind that bugs are prioritized based on their severity and how much work is required to resolve them. It may take some time for someone to address your issue. Bugs will follow our published workflow from inbound triage to ultimate terminal state, whether accepted or closed: Opening Feature Requests \u00b6 First, check the GitHub issues list and Discussions to see if the feature you're requesting is already listed. You can greater visibility on the committed by looking at the Nautobot Roadmap (Be sure to search closed issues as well, since some feature requests have not have been accepted.) If the feature you'd like to see has already been requested and is open, click \"add a reaction\" in the top right corner of the issue and add a thumbs up (+1). This ensures that the issue has a better chance of receiving attention. Also feel free to add a comment with any additional justification for the feature. (However, note that comments with no substance other than a \"+1\" will be deleted. Please use GitHub's reactions feature to indicate your support.) Before filing a new feature request, consider starting with a GitHub Discussion. Feedback you receive there will help validate and shape the proposed feature before filing a formal issue. If the feature request does not get accepted into the current or near term backlog, it will get converted to a Discussion anyway. Good feature requests are very narrowly defined. Be sure to thoroughly describe the functionality and data model(s) being proposed. The more effort you put into writing a feature request, the better its chance is of being implemented. Overly broad feature requests will be closed. When submitting a feature request on GitHub, be sure to include all information requested by the issue template, including: A detailed description of the proposed functionality A use case for the feature; who would use it and what value it would add to Nautobot A rough description of changes necessary to the database schema (if applicable) Any third-party libraries or other resources which would be involved Please avoid prepending any sort of tag (e.g. \"[Feature]\") to the issue title. The issue will be reviewed by a moderator after submission and the appropriate labels will be applied for categorization. Feature requests will follow our published workflow from inbound triage to ultimate terminal state, whether accepted or closed: Submitting Pull Requests \u00b6 If you're interested in contributing to Nautobot, be sure to check out our getting started documentation for tips on setting up your development environment. It is recommended to open an issue before starting work on a pull request, and discuss your idea with the Nautobot maintainers before beginning work. This will help prevent wasting time on something that we might not be able to implement. When suggesting a new feature, also make sure it won't conflict with any work that's already in progress. Once you've opened or identified an issue you'd like to work on, ask that it be assigned to you so that others are aware it's being worked on. A maintainer will then mark the issue as \"accepted.\" If you followed the project guidelines, have ample tests, code quality, you will first be acknowledged for your work. So, thank you in advance! After that, the PR will be quickly reviewed to ensure that it makes sense as a contribution to the project, and to gauge the work effort or issues with merging into current . If the effort required by the core team isn\u2019t trivial, it\u2019ll likely still be a few weeks before it gets thoroughly reviewed and merged, thus it won't be uncommon to move it to near term with a near-term label. It will just depend on the current backlog. All code submissions should meet the following criteria (CI will enforce these checks): Python syntax is valid All unit tests pass successfully PEP 8 compliance is enforced, with the exception that lines may be greater than 80 characters in length At least one changelog fragment has been included in the feature branch Creating Changelog Fragments \u00b6 All pull requests to next or develop must include a changelog fragment file in the ./changes directory. To create a fragment, use your github issue number and fragment type as the filename. For example, 2362.added . Valid fragment types are added , changed , deprecated , fixed , removed , and security . The change summary is added to the file in plain text. Change summaries should be complete sentences, starting with a capital letter and ending with a period, and be in past tense. Each line of the change fragment will generate a single change entry in the release notes. Use multiple lines in the same file if your change needs to generate multiple release notes in the same category. If the change needs to create multiple entries in separate categories, create multiple files. Example Wrong changes/1234.fixed fix critical bug in documentation Right changes/1234.fixed Fixed critical bug in documentation. Multiple Entry Example This will generate 2 entries in the fixed category and one entry in the changed category. changes/1234.fixed Fixed critical bug in documentation. Fixed release notes generation. changes/1234.changed Changed release notes generation.","title":"Introduction"},{"location":"development/index.html#nautobot-development","text":"Nautobot is maintained as a GitHub project under the Apache 2 license. Users are encouraged to submit GitHub issues for feature requests and bug reports.","title":"Nautobot Development"},{"location":"development/index.html#governance","text":"Nautobot is a community-based Free Open Source Software (FOSS) project sponsored by Network to Code (NTC) . As a network automation solution provider, Network to Code works with its clients around the world to craft and build network automation strategies and solutions, often tightly integrated with Nautobot serving as a Source of Truth and Network Automation Platform. The direction of this project will be shaped by the community as well as by input from NTC customers; independent of where requests come from, contributors will need to follow the Contributing Guidelines. The Nautobot Core Team is responsible for the direction and execution of the code that gets committed to the project.","title":"Governance"},{"location":"development/index.html#project-structure","text":"Nautobot components are arranged into functional subsections called apps (a carryover from Django vernacular). Each app holds the models, views, and templates relevant to a particular function: circuits : Communications circuits and providers (not to be confused with power circuits) dcim : Datacenter infrastructure management (sites, racks, and devices) extras : Additional features not considered part of the core data model ipam : IP address management (VRFs, prefixes, IP addresses, and VLANs) tenancy : Tenants (such as customers) to which Nautobot objects may be assigned users : Authentication and user preferences utilities : Resources which are not user-facing (extendable classes, etc.) virtualization : Virtual machines and clusters","title":"Project Structure"},{"location":"development/index.html#release-management","text":"","title":"Release Management"},{"location":"development/index.html#roadmap","text":"In order to best understand how to contribute and where to open an issue or discussion, you should understand how work moves from idea to feature and how the roadmap is structured. There are three major \"buckets\" of work to be aware of within the lifecycle of getting contributions committed and released: Current - Work that is planned for the release currently being developed. Near Term - Work that is planned for one of the next two releases after the one currently being developed. Future - Work that needs more discussion and/or will be planned for a version three or more releases later. The following provides more detail on these.","title":"Roadmap"},{"location":"development/index.html#current","text":"Current tickets (GitHub issues) that are being worked on for the current release or bugs that are found and will be fixed in the current release. Uses current label on GitHub. The GitHub Release Milestone will track items for the current release.","title":"Current"},{"location":"development/index.html#near-term","text":"Current tickets (GitHub issues) that are estimated to complete in one of the next two releases, e.g. 3-6 months to get into core, if accepted. GitHub discussions are used to create one or more GitHub issues when and if something moves from Future to Near Term . Uses near-term label on GitHub.","title":"Near Term"},{"location":"development/index.html#future","text":"Work that is for 3+ releases away or work that needs more free form discussions and brainstorming to better scope future bodies of work. Estimated 7+ months to get into core, if accepted. GitHub Discussions are used for collaborating on future work. If a GitHub issue is opened and is deemed that it is out of scope for Current or Near Term , it will be converted into a GitHub Discussion. GitHub Discussions will be closed when the topic/feature moves from Future to Near Term . Over time, the process of moving work from Future to Near Term to Current will continue to get further refined. Please read through the Nautobot Roadmap so you can understand the current backlog and roadmap and which items are already in Current, Near Term, and Future .","title":"Future"},{"location":"development/index.html#release-schedule","text":"Here is what you need to know about Nautobot releases: The initial launch of Nautobot is version 1.0.0beta1 (major.minor.patch) released on February 24, 2021. The core team estimates quarterly releases with the majority of them being minor releases. It is an aspirational goal that there will be no more than one major release per year as major releases do indicate a break in backwards compatibility. For information pertaining to patch releases, which will be released on a schedule, please see the Patch Releases section below. Given the core team is estimating quarterly releases, there will not be firm dates for releases. In order to provide more visibility into the development and release schedule of Nautobot, there will be structured notifications as follows: At the start of a release cycle, the estimated timeframe for release will be a 4-6 week window. Halfway through the release cycle (~6 weeks), the estimated timeframe for release will be narrowed to a 3-4 week window After 8-9 weeks within the development cycle, the estimated timeframe for release will be narrowed further to a 2 week window. The final notification will be provided 3-5 days before the release drops. The dates and notifications will occur by updating the GitHub Release Milestone and on Slack.","title":"Release Schedule"},{"location":"development/index.html#patch-releases","text":"Specifics around patch releases: Will be released every two weeks on Mondays, starting March 7th, 2022. Will be focused on bugs, security vulnerabilities, backports, and other issues as they arise. Will not introduce significant new functionality except as needed to address a bug. Will not be released if there have been no new merges to develop since the previous release. Should not be considered a mandatory upgrade: If it does not fix a bug or issue you are experiencing, you should not feel the need to upgrade right away. May happen a day or two after the Monday schedule if Monday is an observed holiday. We would like to ensure full team availability post-release. Should a patch release contain a fix for security vulnerability(s) (i.e. CVE(s)), data-loss bug(s), or other critical issue(s), we will: Release a new patch release as soon as the fix has been identified, implemented, and thoroughly tested. No waiting for the next regularly scheduled release date. Bring special attention to these releases in our notification channels. Strongly urge you to upgrade to address these more serious issues as soon as possible. Not adjust any subsequent release dates (the next scheduled release will still occur as scheduled).","title":"Patch Releases"},{"location":"development/index.html#long-term-support-lts","text":"The core team is currently evaluating the possibility of publishing a Long Term Support (LTS) version of Nautobot. At this time there is no formal target for this initial release. Our goal is to collect feedback from users of Nautobot to help identify a maintainable and reliable LTS model. If you have interest in deploying an LTS version of Nautobot, or useful information to help inform the final LTS model, please contribute to the GitHub Discussion thread around LTS.","title":"Long Term Support (LTS)"},{"location":"development/index.html#deprecation-policy","text":"The deprecation policy will be such that there will be at least one release that makes users aware of a feature that will be deprecated in the next release.","title":"Deprecation Policy"},{"location":"development/index.html#versioning","text":"Semantic Versioning ( SemVer ) is used for Nautobot versioning.","title":"Versioning"},{"location":"development/index.html#communication","text":"Communication among the contributors should always occur via public channels. The following outlines the best ways to communicate and engage on all things Nautobot.","title":"Communication"},{"location":"development/index.html#slack","text":"#nautobot on Network to Code Slack - Good for quick chats. Avoid any discussion that might need to be referenced later on, as the chat history is not retained long.","title":"Slack"},{"location":"development/index.html#github","text":"GitHub issues - All feature requests, bug reports, and other substantial changes should be documented in an issue. GitHub discussions - The preferred forum for general discussion and support issues. Ideal for shaping a feature request prior to submitting an issue. GitHub's discussions are the best place to get help or propose rough ideas for new functionality. Their integration with GitHub allows for easily cross- referencing and converting posts to issues as needed. There are several categories for discussions: General - General community discussion. Ideas - Ideas for new functionality that isn't yet ready for a formal feature request. These ideas are what will be in scope to review when moving work from Future to Near Term as stated in the previous section. Q&A - Request help with installing or using Nautobot.","title":"GitHub"},{"location":"development/index.html#contributing","text":"We welcome many forms of contributions to Nautobot. While we understand most contributions will commonly come from developers, we encourage others to contribute in the form of docs, tutorials, and user guides. If you have other ideas for contributing, don't hesitate to open an issue or have a discussion in one of the forums above. Please also take a chance to look at our Wiki on GitHub to review any specifics as to how we define and scope work as a community.","title":"Contributing"},{"location":"development/index.html#reporting-bugs","text":"First, ensure that you're running the latest stable version of Nautobot. If you're running an older version, it's possible that the bug has already been fixed. Next, check the GitHub issues list to see if the bug you've found has already been reported. If you think you may be experiencing a reported issue that hasn't already been resolved, please click \"add a reaction\" in the top right corner of the issue and add a thumbs up (+1). You might also want to add a comment describing how it's affecting your installation. This will allow us to prioritize bugs based on how many users are affected. When submitting an issue, please be as descriptive as possible. Be sure to provide all information request in the issue template, including: The environment in which Nautobot is running The exact steps that can be taken to reproduce the issue Expected and observed behavior Any error messages generated Screenshots (if applicable) Please avoid prepending any sort of tag (e.g. \"[Bug]\") to the issue title. The issue will be reviewed by a maintainer after submission and the appropriate labels will be applied for categorization. Keep in mind that bugs are prioritized based on their severity and how much work is required to resolve them. It may take some time for someone to address your issue. Bugs will follow our published workflow from inbound triage to ultimate terminal state, whether accepted or closed:","title":"Reporting Bugs"},{"location":"development/index.html#opening-feature-requests","text":"First, check the GitHub issues list and Discussions to see if the feature you're requesting is already listed. You can greater visibility on the committed by looking at the Nautobot Roadmap (Be sure to search closed issues as well, since some feature requests have not have been accepted.) If the feature you'd like to see has already been requested and is open, click \"add a reaction\" in the top right corner of the issue and add a thumbs up (+1). This ensures that the issue has a better chance of receiving attention. Also feel free to add a comment with any additional justification for the feature. (However, note that comments with no substance other than a \"+1\" will be deleted. Please use GitHub's reactions feature to indicate your support.) Before filing a new feature request, consider starting with a GitHub Discussion. Feedback you receive there will help validate and shape the proposed feature before filing a formal issue. If the feature request does not get accepted into the current or near term backlog, it will get converted to a Discussion anyway. Good feature requests are very narrowly defined. Be sure to thoroughly describe the functionality and data model(s) being proposed. The more effort you put into writing a feature request, the better its chance is of being implemented. Overly broad feature requests will be closed. When submitting a feature request on GitHub, be sure to include all information requested by the issue template, including: A detailed description of the proposed functionality A use case for the feature; who would use it and what value it would add to Nautobot A rough description of changes necessary to the database schema (if applicable) Any third-party libraries or other resources which would be involved Please avoid prepending any sort of tag (e.g. \"[Feature]\") to the issue title. The issue will be reviewed by a moderator after submission and the appropriate labels will be applied for categorization. Feature requests will follow our published workflow from inbound triage to ultimate terminal state, whether accepted or closed:","title":"Opening Feature Requests"},{"location":"development/index.html#submitting-pull-requests","text":"If you're interested in contributing to Nautobot, be sure to check out our getting started documentation for tips on setting up your development environment. It is recommended to open an issue before starting work on a pull request, and discuss your idea with the Nautobot maintainers before beginning work. This will help prevent wasting time on something that we might not be able to implement. When suggesting a new feature, also make sure it won't conflict with any work that's already in progress. Once you've opened or identified an issue you'd like to work on, ask that it be assigned to you so that others are aware it's being worked on. A maintainer will then mark the issue as \"accepted.\" If you followed the project guidelines, have ample tests, code quality, you will first be acknowledged for your work. So, thank you in advance! After that, the PR will be quickly reviewed to ensure that it makes sense as a contribution to the project, and to gauge the work effort or issues with merging into current . If the effort required by the core team isn\u2019t trivial, it\u2019ll likely still be a few weeks before it gets thoroughly reviewed and merged, thus it won't be uncommon to move it to near term with a near-term label. It will just depend on the current backlog. All code submissions should meet the following criteria (CI will enforce these checks): Python syntax is valid All unit tests pass successfully PEP 8 compliance is enforced, with the exception that lines may be greater than 80 characters in length At least one changelog fragment has been included in the feature branch","title":"Submitting Pull Requests"},{"location":"development/index.html#creating-changelog-fragments","text":"All pull requests to next or develop must include a changelog fragment file in the ./changes directory. To create a fragment, use your github issue number and fragment type as the filename. For example, 2362.added . Valid fragment types are added , changed , deprecated , fixed , removed , and security . The change summary is added to the file in plain text. Change summaries should be complete sentences, starting with a capital letter and ending with a period, and be in past tense. Each line of the change fragment will generate a single change entry in the release notes. Use multiple lines in the same file if your change needs to generate multiple release notes in the same category. If the change needs to create multiple entries in separate categories, create multiple files. Example Wrong changes/1234.fixed fix critical bug in documentation Right changes/1234.fixed Fixed critical bug in documentation. Multiple Entry Example This will generate 2 entries in the fixed category and one entry in the changed category. changes/1234.fixed Fixed critical bug in documentation. Fixed release notes generation. changes/1234.changed Changed release notes generation.","title":"Creating Changelog Fragments"},{"location":"development/application-registry.html","text":"Application Registry \u00b6 The registry is an in-memory data structure which houses various application-wide parameters, such as the list of enabled plugins. It is not exposed to the user and is not intended to be modified by any code outside of Nautobot core. The registry behaves essentially like a Python dictionary, with the notable exception that once a store (key) has been declared, it cannot be deleted or overwritten. The value of a store can, however, be modified; e.g. by appending a value to a list. Store values generally do not change once the application has been initialized. The registry can be inspected by importing registry from nautobot.extras.registry . Page templates that need access to the registry can use the registry template tag to load it into the template context, for example: <!-- Load the \"registry\" template tag library --> {% load registry %} <!-- Load the registry into the template context as variable \"registry\"--> {% registry %} <!-- Use the registry variable in the template --> {{ registry.datasource_contents }} Stores \u00b6 datasource_contents \u00b6 Definition of data types that can be provided by data source models (such as Git repositories ). Implemented as a dictionary mapping the data source model name to a list of the types of data that it may contain and callback functions associated with those data types. The default mapping in Nautobot is currently: { \"extras.gitrepository\" : [ DatasourceContent ( name = 'config contexts' , content_identifier = 'extras.configcontext' , icon = 'mdi-code-json' , callback = extras . datasources . git . refresh_git_config_contexts , ), DatasourceContent ( name = 'jobs' , content_identifier = 'extras.job' , icon = 'mdi-script-text' , callback = extras . datasources . git . refresh_git_jobs , ), DatasourceContent ( name = 'export templates' , content_identifier = 'extras.exporttemplate' , icon = 'mdi-database-export' , callback = extras . datasources . git . refresh_git_export_templates , ), ] } Plugins may extend this dictionary with additional data sources and/or data types by calling extras.registry.register_datasource_contents() as desired. homepage_layout \u00b6 Added in version 1.2.0 A dictionary holding information about the layout of Nautobot's homepage. Each app may register homepage panels and items using objects from the generic app class. Each object has a weight attribute allowing the developer to define the position of the object. { \"panels\" : { \"Panel 1\" { \"weight\" : 100 , \"items\" : { \"Item 1\" : { \"description\" : \"This is item 1\" , \"link\" : \"example.link_1\" \"model\" : Example , \"permissions\" : \"example.view_link_1\" , \"weight\" : 100 , }, \"Item 2\" : { \"description\" : \"This is item 2\" , \"link\" : \"example.link_2\" \"model\" : Example , \"permissions\" : \"example.view_link_2\" , \"weight\" : 200 , } } } \"Panel 2\" : { \"weight\" : 200 , \"custom_template\" : \"panel_example.html\" , \"custom_data\" : { \"example\" : example_callback_function , }, } } } model_features \u00b6 A dictionary of particular features (e.g. custom fields) mapped to the Nautobot models which support them, arranged by app. For example: { 'custom_fields' : { 'circuits' : [ 'provider' , 'circuit' ], 'dcim' : [ 'site' , 'rack' , 'devicetype' , ... ], ... }, 'webhooks' : { ... }, ... } nav_menu \u00b6 Added in version 1.1.0 Navigation menu items provided by Nautobot applications. Each app may register its navbar configuration inside of the nav_menu dictionary using navigation.py . Tabs are stored in the top level moving down to groups, items and buttons. Tabs, groups and items can be modified by using the key values inside other application and plugins. The nav_menu dict should never be modified directly. Example: { \"tabs\" : { \"tab_1\" : { \"weight\" : 100 , \"permissions\" : [], \"groups\" : { \"group_1\" :{ \"weight\" : 100 , \"permissions\" : [], \"items\" : { \"item_link_1\" : { \"link_text\" : \"Item 1\" , \"weight\" : 100 , \"permissions\" : [], \"buttons\" : { \"button_1\" : { \"button_class\" : \"success\" , \"icon_class\" : \"mdi-plus-thick\" , \"link\" : \"button_link_1\" , \"weight\" : 100 , \"permissions\" : [], }, \"button_2\" : { \"button_class\" : \"success\" , \"icon_class\" : \"mdi-plus-thick\" , \"link\" : \"button_link_2\" , \"weight\" : 200 , \"permissions\" : [], } } }, \"item_link_2\" : { \"link_text\" : \"Item 2\" , \"weight\" : 200 , \"permissions\" : [], \"buttons\" : { \"button_1\" : { \"button_class\" : \"success\" , \"icon_class\" : \"mdi-plus-thick\" , \"link\" : \"button_link_1\" , \"weight\" : 100 , \"permissions\" : [], }, \"button_2\" : { \"button_class\" : \"success\" , \"icon_class\" : \"mdi-plus-thick\" , \"link\" : \"button_link_2\" , \"weight\" : 200 , \"permissions\" : [], } } }, } } } } } } plugin_custom_validators \u00b6 Plugin custom validator classes that provide additional data model validation logic. Implemented as a dictionary mapping data model names to a list of PluginCustomValidator subclasses, for example: { 'circuits.circuit' : [ CircuitMustHaveDescriptionValidator ], 'dcim.site' : [ SiteMustHaveRegionValidator , SiteNameMustIncludeCountryCodeValidator ], } plugin_graphql_types \u00b6 List of GraphQL Type objects that will be added to the GraphQL schema. GraphQL objects that are defined in a plugin will be automatically registered into this registry. An example: [ < DjangoObjectType > , < DjangoObjectType > , < OptimizedDjangoObjectType > ] plugin_jobs \u00b6 Jobs provided by plugins. A list of Job classes, for example: [ demo_data_plugin . jobs . CreateDemoData , demo_data_plugin . jobs . DestroyDemoData , branch_creation_plugin . jobs . CreateNewSmallBranch , branch_creation_plugin . jobs . CreateNewMediumBranch , branch_creation_plugin . jobs . CreateNewLargeBranch , ] plugin_template_extensions \u00b6 Plugin content that gets embedded into core Nautobot templates. The store comprises Nautobot models registered as dictionary keys, each pointing to a list of applicable template extension classes that exist. An example: { 'dcim.site' : [ < TemplateExtension > , < TemplateExtension > , < TemplateExtension > , ], 'dcim.rack' : [ < TemplateExtension > , < TemplateExtension > , ], }","title":"Application Registry"},{"location":"development/application-registry.html#application-registry","text":"The registry is an in-memory data structure which houses various application-wide parameters, such as the list of enabled plugins. It is not exposed to the user and is not intended to be modified by any code outside of Nautobot core. The registry behaves essentially like a Python dictionary, with the notable exception that once a store (key) has been declared, it cannot be deleted or overwritten. The value of a store can, however, be modified; e.g. by appending a value to a list. Store values generally do not change once the application has been initialized. The registry can be inspected by importing registry from nautobot.extras.registry . Page templates that need access to the registry can use the registry template tag to load it into the template context, for example: <!-- Load the \"registry\" template tag library --> {% load registry %} <!-- Load the registry into the template context as variable \"registry\"--> {% registry %} <!-- Use the registry variable in the template --> {{ registry.datasource_contents }}","title":"Application Registry"},{"location":"development/application-registry.html#stores","text":"","title":"Stores"},{"location":"development/application-registry.html#datasource_contents","text":"Definition of data types that can be provided by data source models (such as Git repositories ). Implemented as a dictionary mapping the data source model name to a list of the types of data that it may contain and callback functions associated with those data types. The default mapping in Nautobot is currently: { \"extras.gitrepository\" : [ DatasourceContent ( name = 'config contexts' , content_identifier = 'extras.configcontext' , icon = 'mdi-code-json' , callback = extras . datasources . git . refresh_git_config_contexts , ), DatasourceContent ( name = 'jobs' , content_identifier = 'extras.job' , icon = 'mdi-script-text' , callback = extras . datasources . git . refresh_git_jobs , ), DatasourceContent ( name = 'export templates' , content_identifier = 'extras.exporttemplate' , icon = 'mdi-database-export' , callback = extras . datasources . git . refresh_git_export_templates , ), ] } Plugins may extend this dictionary with additional data sources and/or data types by calling extras.registry.register_datasource_contents() as desired.","title":"datasource_contents"},{"location":"development/application-registry.html#homepage_layout","text":"Added in version 1.2.0 A dictionary holding information about the layout of Nautobot's homepage. Each app may register homepage panels and items using objects from the generic app class. Each object has a weight attribute allowing the developer to define the position of the object. { \"panels\" : { \"Panel 1\" { \"weight\" : 100 , \"items\" : { \"Item 1\" : { \"description\" : \"This is item 1\" , \"link\" : \"example.link_1\" \"model\" : Example , \"permissions\" : \"example.view_link_1\" , \"weight\" : 100 , }, \"Item 2\" : { \"description\" : \"This is item 2\" , \"link\" : \"example.link_2\" \"model\" : Example , \"permissions\" : \"example.view_link_2\" , \"weight\" : 200 , } } } \"Panel 2\" : { \"weight\" : 200 , \"custom_template\" : \"panel_example.html\" , \"custom_data\" : { \"example\" : example_callback_function , }, } } }","title":"homepage_layout"},{"location":"development/application-registry.html#model_features","text":"A dictionary of particular features (e.g. custom fields) mapped to the Nautobot models which support them, arranged by app. For example: { 'custom_fields' : { 'circuits' : [ 'provider' , 'circuit' ], 'dcim' : [ 'site' , 'rack' , 'devicetype' , ... ], ... }, 'webhooks' : { ... }, ... }","title":"model_features"},{"location":"development/application-registry.html#nav_menu","text":"Added in version 1.1.0 Navigation menu items provided by Nautobot applications. Each app may register its navbar configuration inside of the nav_menu dictionary using navigation.py . Tabs are stored in the top level moving down to groups, items and buttons. Tabs, groups and items can be modified by using the key values inside other application and plugins. The nav_menu dict should never be modified directly. Example: { \"tabs\" : { \"tab_1\" : { \"weight\" : 100 , \"permissions\" : [], \"groups\" : { \"group_1\" :{ \"weight\" : 100 , \"permissions\" : [], \"items\" : { \"item_link_1\" : { \"link_text\" : \"Item 1\" , \"weight\" : 100 , \"permissions\" : [], \"buttons\" : { \"button_1\" : { \"button_class\" : \"success\" , \"icon_class\" : \"mdi-plus-thick\" , \"link\" : \"button_link_1\" , \"weight\" : 100 , \"permissions\" : [], }, \"button_2\" : { \"button_class\" : \"success\" , \"icon_class\" : \"mdi-plus-thick\" , \"link\" : \"button_link_2\" , \"weight\" : 200 , \"permissions\" : [], } } }, \"item_link_2\" : { \"link_text\" : \"Item 2\" , \"weight\" : 200 , \"permissions\" : [], \"buttons\" : { \"button_1\" : { \"button_class\" : \"success\" , \"icon_class\" : \"mdi-plus-thick\" , \"link\" : \"button_link_1\" , \"weight\" : 100 , \"permissions\" : [], }, \"button_2\" : { \"button_class\" : \"success\" , \"icon_class\" : \"mdi-plus-thick\" , \"link\" : \"button_link_2\" , \"weight\" : 200 , \"permissions\" : [], } } }, } } } } } }","title":"nav_menu"},{"location":"development/application-registry.html#plugin_custom_validators","text":"Plugin custom validator classes that provide additional data model validation logic. Implemented as a dictionary mapping data model names to a list of PluginCustomValidator subclasses, for example: { 'circuits.circuit' : [ CircuitMustHaveDescriptionValidator ], 'dcim.site' : [ SiteMustHaveRegionValidator , SiteNameMustIncludeCountryCodeValidator ], }","title":"plugin_custom_validators"},{"location":"development/application-registry.html#plugin_graphql_types","text":"List of GraphQL Type objects that will be added to the GraphQL schema. GraphQL objects that are defined in a plugin will be automatically registered into this registry. An example: [ < DjangoObjectType > , < DjangoObjectType > , < OptimizedDjangoObjectType > ]","title":"plugin_graphql_types"},{"location":"development/application-registry.html#plugin_jobs","text":"Jobs provided by plugins. A list of Job classes, for example: [ demo_data_plugin . jobs . CreateDemoData , demo_data_plugin . jobs . DestroyDemoData , branch_creation_plugin . jobs . CreateNewSmallBranch , branch_creation_plugin . jobs . CreateNewMediumBranch , branch_creation_plugin . jobs . CreateNewLargeBranch , ]","title":"plugin_jobs"},{"location":"development/application-registry.html#plugin_template_extensions","text":"Plugin content that gets embedded into core Nautobot templates. The store comprises Nautobot models registered as dictionary keys, each pointing to a list of applicable template extension classes that exist. An example: { 'dcim.site' : [ < TemplateExtension > , < TemplateExtension > , < TemplateExtension > , ], 'dcim.rack' : [ < TemplateExtension > , < TemplateExtension > , ], }","title":"plugin_template_extensions"},{"location":"development/best-practices.html","text":"Best Practices \u00b6 While there are many different development interfaces in Nautobot that each expose unique functionality, there are a common set of a best practices that have broad applicability to users and developers alike. This includes elements of writing Jobs, Plugins, and scripts for execution through the nbshell . Base Classes \u00b6 For models that support change-logging, custom fields, and relationships (which includes all subclasses of OrganizationalModel and PrimaryModel ), the \"Full-featured models\" base classes below should always be used. For less full-featured models, refer to the \"Minimal models\" column instead. Feature Full-featured models Minimal models FilterSets NautobotFilterSet BaseFilterSet Object create/edit forms NautobotModelForm BootstrapMixin Object bulk-edit forms NautobotBulkEditForm BootstrapMixin Table filter forms NautobotFilterForm BootstrapMixin Read-only serializers BaseModelSerializer BaseModelSerializer Nested serializers WritableNestedSerializer WritableNestedSerializer All other serializers NautobotModelSerializer ValidatedModelSerializer API View Sets NautobotModelViewSet ModelViewSet Model Existence in the Database \u00b6 A common Django pattern is to check whether a model instance's primary key ( pk ) field is set as a proxy for whether the instance has been written to the database or whether it exists only in memory. Because of the way Nautobot's UUID primary keys are implemented, this check will not work as expected because model instances are assigned a UUID in memory at instance creation time , not at the time they are written to the database (when the model's save() method is called). Instead, for any model which inherits from nautobot.core.models.BaseModel , you should check an instance's present_in_database property which will be either True or False . Instead of: if instance . pk : # Are we working with an existing instance in the database? # Actually, the above check doesn't tell us one way or the other! ... else : # Will never be reached! ... Use: if instance . present_in_database : # We're working with an existing instance in the database! ... else : # We're working with a newly created instance not yet written to the database! ... Note There is one case where a model instance will have a null primary key, and that is the case where it has been removed from the database and is in the process of being deleted. For most purposes, this is not the case you are intending to check! Model Validation \u00b6 Django offers several places and mechanism in which to exert data and model validation. All model specific validation should occur within the model's clean() method or field specific validators. This ensures the validation logic runs and is consistent through the various Nautobot interfaces (Web UI, REST API, ORM, etc). Consuming Model Validation \u00b6 Django places specific separation between validation and the saving of an instance and this means it is a common Django pattern to make explicit calls first to a model instance's clean() / full_clean() methods and then the save() method. Calling only the save() method does not automatically enforce validation and may lead to data integrity issues. Nautobot provides a convenience method that both enforces model validation and saves the instance in a single call to validated_save() . Any model which inherits from nautobot.core.models.BaseModel has this method available. This includes all core models and it is recommended that all new Nautobot models and plugin-provided models also inherit from BaseModel or one of its descendants such as nautobot.core.models.generics.OrganizationalModel or nautobot.core.models.generics.PrimaryModel . The intended audience for the validated_save() convenience method is Job authors and anyone writing scripts for, or interacting with the ORM directly through the nbshell command. It is generally not recommended however, to use validated_save() as a blanket replacement for the save() method in the core of Nautobot. During execution, should model validation fail, validated_save() will raise django.core.exceptions.ValidationError in the normal Django fashion. Slug Field \u00b6 Moving forward in Nautobot, all models should have a slug field. This field can be safely/correctly used in URL patterns, dictionary keys, GraphQL and REST API. Nautobot has provided the AutoSlugField to handle automatically populating the slug field from another field(s). Generally speaking model slugs should be populated from the name field. Below is an example on defining the slug field. from django.db import models from nautobot.core.fields import AutoSlugField from nautobot.core.models.generics import PrimaryModel class ExampleModel ( PrimaryModel ): name = models . CharField ( max_length = 100 , unique = True ) slug = AutoSlugField ( populate_from = 'name' ) Getting URL Routes \u00b6 When developing new models a need often arises to retrieve a reversible route for a model to access it in either the web UI or the REST API. When this time comes, you must use nautobot.utilities.utils.get_route_for_model . You must not write your own logic to construct route names. from nautobot.utilities.utils import get_route_for_model This utility function supports both UI and API views for both Nautobot core apps and Nautobot plugins. Added in version 1.4.3 Support for generating API routes was added to get_route_for_model() by passing the argument api=True . UI Routes \u00b6 Instead of: route = f \" { model . _meta . app_label } : { model . _meta . model_name } _list\" if model . _meta . app_label in settings . PLUGINS : route = f \"plugins: { route } \" Use: route = get_route_for_model ( model , \"list\" ) REST API Routes \u00b6 Instead of: api_route = f \" { model . _meta . app_label } -api: { model . _meta . model_name } -list\" if model . _meta . app_label in settings . PLUGINS : api_route = f \"plugins-api: { api_route } \" Use: api_route = get_route_for_model ( model , \"list\" , api = True ) Examples \u00b6 Core models: >>> get_route_for_model ( Device , \"list\" ) \"dcim:device_list\" >>> get_route_for_model ( Device , \"list\" , api = True ) \"dcim-api:device-list\" Plugin models: >>> get_route_for_model ( ExampleModel , \"list\" ) \"plugins:example_plugin:examplemodel_list\" >>> get_route_for_model ( ExampleModel , \"list\" , api = True ) \"plugins-api:example_plugin-api:examplemodel-list\" Tip The first argument may also optionally be an instance of a model, or a string using the dotted notation of {app_label}.{model} (e.g. dcim.device ). Using an instance: >>> instance = Device . objects . first () >>> get_route_for_model ( instance , \"list\" ) \"dcim:device_list\" Using dotted notation: >>> get_route_for_model ( \"dcim.device\" , \"list\" ) \"dcim:device_list\" Filtering Models with FilterSets \u00b6 The following best practices must be considered when establishing new FilterSet classes for model classes. Mapping Model Fields to Filters \u00b6 Filtersets must inherit from nautobot.extras.filters.NautobotFilterSet (which inherits from nautobot.utilities.filters.BaseFilterSet ) This affords that automatically generated lookup expressions ( ic , nic , iew , niew , etc.) are always included This also asserts that the correct underlying Form class that maps the generated form field types and widgets will be included FIltersets must publish all model fields from a model, including related fields. All fields should be provided using Meta.fields = \"__all__\" and this would be preferable for the first and common case as it requires the least maintanence and overhead and asserts parity between the model fields and the filterset filters. In some cases simply excluding certain fields would be the next most preferable e.g. Meta.exclude = [\"unwanted_field\", \"other_unwanted_field\"] Finally, the last resort should be explicitly declaring the desired fields using Meta.fields = . This should be avoided because it incurs the highest technical debt in maintaining alignment between model fields and filters. In the event that fields do need to be customized to extend lookup expressions, a dictionary of field names mapped to a list of lookups may be used, however, this pattern is only compatible with explicitly declaring all fields, which should also be avoided for the common case. For example: class UserFilter ( NautobotFilterSet ): class Meta : model = User fields = { 'username' : [ 'exact' , 'contains' ], 'last_login' : [ 'exact' , 'year__gt' ], } It is acceptable that default filter mappings may need to be overridden with custom filter declarations, but filter_overrides (see below) should be used as a first resort. Custom filter definitions must not shadow the name of an existing model field if it is also changing the type. For example DeviceFilterSet.interfaces is a BooleanFilter that is shadowing the Device.interfaces related manager. This introduces problems with automatic introspection of the filterset and this pattern must be avoided. For foreign-key related fields, on existing core models in the v1.3 release train : The field should be shadowed, replacing the PK filter with a lookup-based on a more human-readable value (typically slug , if available). A PK-based filter should be made available as well, generally with a name suffixed by _id . For example: provider = django_filters . ModelMultipleChoiceFilter ( field_name = \"provider__slug\" , queryset = Provider . objects . all (), to_field_name = \"slug\" , label = \"Provider (slug)\" , ) provider_id = django_filters . ModelMultipleChoiceFilter ( queryset = Provider . objects . all (), label = \"Provider (ID)\" , ) For foreign-key related fields on new core models for v1.4 or later: The field must be shadowed utilizing a hybrid NaturalKeyOrPKMultipleChoiceFilter which will automatically try to lookup by UUID or slug depending on the value of the incoming argument (e.g. UUID string vs. slug string). Fields that use name instead of slug can set the natural_key argument on NaturalKeyOrPKMultipleChoiceFilter . In default settings for filtersets, when not using NaturalKeyOrPKMultipleChoiceFilter , provider would be a pk (UUID) field, whereas using NaturalKeyOrPKMultipleChoiceFilter will automatically support both input values for slug or pk . New filtersets should follow this direction vs. propagating the need to continue to overload the default foreign-key filter and define an additional _id filter on each new filterset. We know that most existing FilterSets aren't following this pattern, and we plan to change that in a major release. Using the previous field ( provider ) as an example, it would look something like this: from nautobot.utilities.filters import NaturalKeyOrPKMultipleChoiceFilter provider = NaturalKeyOrPKMultipleChoiceFilter ( queryset = Provider . objects . all (), label = \"Provider (slug or ID)\" , ) # optionally use the to_field_name argument to set the field to name instead of slug provider = NaturalKeyOrPKMultipleChoiceFilter ( to_field_name = \"name\" , queryset = Provider . objects . all (), label = \"Provider (name or ID)\" , ) Filter Naming and Definition \u00b6 Boolean filters for membership must be named with has_{related_name} (e.g. has_interfaces ) Boolean filters for identity must be named with is_{name} (e.g. is_virtual_chassis ) although this is semantically identical to has_ filters, there may be occasions where naming the filter is_ would be more intuitive. Filters must declare field_name when they have a different name than the underlying model field they are referencing. Where possible the suffix component of the filter name must map directly to the underlying field name. For example, DeviceFilterSet.has_console_ports could be better named, to assert that the filter name following the has_ prefix is a one-to-one mapping to the underlying model's related field name ( consoleports ) therefore field_name must point to the field name as defined on the model: has_consoleports = BooleanFilter ( field_name = \"consoleports\" ) Filters must be declared using the appropriate lookup expression ( lookup_expr ) if any other expression than exact (the default) is required. For example: has_consoleports = BooleanFilter ( field_name = \"consoleports\" , lookup_expr = \"isnull\" ) Filters must be declared using exclude=True if a queryset .exclude() is required to be called vs. queryset .filter() which is the default when the filter default exclude=False is passed through. If you require Foo.objects.exclude() , you must pass exclude=True instead of defining a filterset method to explicitly hard-code such a query. For example: has_consoleports = BooleanFilter ( field_name = \"consoleports\" , lookup_expr = \"isnull\" , exclude = True ) Filters must be declared using disinct=True if a queryset .distinct() is required to be called on the queryset Filters must not be set to be required using required=True Filter methods defined using the method= keyword argument may only be used as a last resort (see below) when correct usage of field_name , lookup_expr , exclude , or other filter keyword arguments do not suffice. In other words: filter methods should used as the exception and not the rule. Use of filter_overrides must be considered in cases where more-specific class-local overrides. The need may ocassionally arise to change certain filter-level arguments used for filter generation, such such as changing a filter class, or customizing a UI widget. Any extra arguments are sent to the filter as keyword arguments at instance creation time. (Hint: extra must be a callable) For example: class ProductFilter ( NautobotFilterSet ): class Meta : model = Interface fields = \"__all__\" filter_overrides = { # This would change the default to all CharFields to use lookup_expr=\"icontains\". It # would also pass in the custom `choices` generated by the `generate_choices()` # function. models . CharField : { \"filter_class\" : filters . MultiValueCharFilter , \"extra\" : lambda f : { \"lookup_expr\" : \"icontains\" , \"choices\" : generate_choices (), }, }, # This would make BooleanFields use a radio select widget vs. the default of checkbox models . BooleanField : { \"extra\" : lambda f : { \"widget\" : forms . RadioSelect , }, }, } Warning Existing features of filtersets and filters must be exhausted first using keyword arguments before resorting to customizing, re-declaring/overloading, or defining filter methods. Filter Methods \u00b6 Filters on a filterset can reference a method (either a callable, or the name of a method on the filterset) to perform custom business logic for that filter field. However, many uses of filter methods in Nautobot are problematic because they break the ability for such filter fields to be properly reversible. Consider this example from nautobot.dcim.filters.DeviceFilterSet.pass_through_ports : # Filter field definition is a BooleanFilter, for which an \"isnull\" lookup_expr # is the only valid filter expression pass_through_ports = django_filters . BooleanFilter ( method = \"_pass_through_ports\" , label = \"Has pass-through ports\" , ) # Method definition loses context and further the field's lookup_expr # falls back to the default of \"exact\" and the `name` value is irrelevant here. def _pass_through_ports ( self , queryset , name , value ): breakpoint () # This was added to illustrate debugging with pdb below return queryset . exclude ( frontports__isnull = value , rearports__isnull = value ) The default lookup_expr unless otherwise specified is \u201cexact\u201d, as seen in django_filters.conf : 'DEFAULT_LOOKUP_EXPR' : 'exact' , When this method is called, the internal state is default, making reverse introspection impossible, because the lookup_expr is defaulting to \u201cexact\u201d: ( Pdb ) field = self . filters [ name ] ( Pdb ) field . exclude False ( Pdb ) field . lookup_expr 'exact' This means that the arguments for the field are being completely ignored and the hard-coded queryset queryset.exclude(frontports__isnull=value, rearports__isnull=value) is all that is being run when this method is called. Additionally, name variable that gets passed to the method cannot be used here because there are two field names at play ( frontports and rearports ). This hard-coding is impossible to introspect and therefore impossible to reverse. So while this filter definition coudl be improved like so, there is still no way to know what is going on in the method body: pass_through_ports = django_filters . BooleanFilter ( method = \"_pass_through_ports\" , # The method that is called exclude = True , # Perform an `.exclude()` vs. `.filter()`` lookup_expr = \"isnull\" , # Perform `isnull` vs. `exact`` label = \"Has pass-through ports\" , ) For illustration, if we use another breakpoint, you can see that the filter field now has the correct attributes that can be used to help reverse this query: ( Pdb ) field = self . filters [ name ] ( Pdb ) field . exclude True ( Pdb ) field . lookup_expr 'isnull' Except that it stops there becuse of the method body. Here are the problems: There's no way to identify either of the field names required here The name that is incoming to the method is the filter name as defined ( pass_through_ports in this case) does not map to an actual model field So the filter can be introspected for lookup_expr value using self.filters[name].lookup_expr , but it would have to be assumed that applies to both fields. Same with exclude ( self.filters[name].exclude ) It would be better to just eliminate pass_through_ports=True entirely in exchange for front_ports=True&rear_ports=True (current) or has_frontports=True&has_rearports=True (future). Generating Reversible Q Objects \u00b6 With consistent and proper use of filter field arguments when defining them on a fitlerset, a query could be constructed using the field_name and lookup_expr values. For example: def generate_query ( self , field , value ): query = Q () predicate = { f \" { field . field_name } __ { field . lookup_expr } \" : value } if field . exclude : query |= ~ Q ( ** predicate ) else : query |= Q ( ** predicate ) return query ## Somewhere else in business logic: field = filterset . filters [ name ] value = filterset . data [ name ] query = generate_query ( field , value ) filterset . qs . filter ( query ) . count () # 339 Summary \u00b6 For the vast majority of cases where we have method filters, it\u2019s for Boolean filters For the common case method filters are unnecessary technical debt and should be eliminated where better suited by proper use of filter field arguments Reversibility may not always necessarily be required, but by properly defining field_name , lookup_expr , and exclude on filter fields, introspection becomes deterministic and reversible queries can be reliably generated as needed. For exceptions such as DeviceFilterSet.has_primary_ip where it checks for both Device.primary_ip4 OR Device.primary_ip6 , method filters may still be necessary, however, they would be the exception and not the norm. The good news is that in the core there are not that many of these filter methods defined, but we also don\u2019t want to see them continue to proliferate. Using NautobotUIViewSet for Plugin Development \u00b6 Added in version 1.4.0 Using NautobotUIViewSet for plugin development is strongly recommended.","title":"Best Practices"},{"location":"development/best-practices.html#best-practices","text":"While there are many different development interfaces in Nautobot that each expose unique functionality, there are a common set of a best practices that have broad applicability to users and developers alike. This includes elements of writing Jobs, Plugins, and scripts for execution through the nbshell .","title":"Best Practices"},{"location":"development/best-practices.html#base-classes","text":"For models that support change-logging, custom fields, and relationships (which includes all subclasses of OrganizationalModel and PrimaryModel ), the \"Full-featured models\" base classes below should always be used. For less full-featured models, refer to the \"Minimal models\" column instead. Feature Full-featured models Minimal models FilterSets NautobotFilterSet BaseFilterSet Object create/edit forms NautobotModelForm BootstrapMixin Object bulk-edit forms NautobotBulkEditForm BootstrapMixin Table filter forms NautobotFilterForm BootstrapMixin Read-only serializers BaseModelSerializer BaseModelSerializer Nested serializers WritableNestedSerializer WritableNestedSerializer All other serializers NautobotModelSerializer ValidatedModelSerializer API View Sets NautobotModelViewSet ModelViewSet","title":"Base Classes"},{"location":"development/best-practices.html#model-existence-in-the-database","text":"A common Django pattern is to check whether a model instance's primary key ( pk ) field is set as a proxy for whether the instance has been written to the database or whether it exists only in memory. Because of the way Nautobot's UUID primary keys are implemented, this check will not work as expected because model instances are assigned a UUID in memory at instance creation time , not at the time they are written to the database (when the model's save() method is called). Instead, for any model which inherits from nautobot.core.models.BaseModel , you should check an instance's present_in_database property which will be either True or False . Instead of: if instance . pk : # Are we working with an existing instance in the database? # Actually, the above check doesn't tell us one way or the other! ... else : # Will never be reached! ... Use: if instance . present_in_database : # We're working with an existing instance in the database! ... else : # We're working with a newly created instance not yet written to the database! ... Note There is one case where a model instance will have a null primary key, and that is the case where it has been removed from the database and is in the process of being deleted. For most purposes, this is not the case you are intending to check!","title":"Model Existence in the Database"},{"location":"development/best-practices.html#model-validation","text":"Django offers several places and mechanism in which to exert data and model validation. All model specific validation should occur within the model's clean() method or field specific validators. This ensures the validation logic runs and is consistent through the various Nautobot interfaces (Web UI, REST API, ORM, etc).","title":"Model Validation"},{"location":"development/best-practices.html#consuming-model-validation","text":"Django places specific separation between validation and the saving of an instance and this means it is a common Django pattern to make explicit calls first to a model instance's clean() / full_clean() methods and then the save() method. Calling only the save() method does not automatically enforce validation and may lead to data integrity issues. Nautobot provides a convenience method that both enforces model validation and saves the instance in a single call to validated_save() . Any model which inherits from nautobot.core.models.BaseModel has this method available. This includes all core models and it is recommended that all new Nautobot models and plugin-provided models also inherit from BaseModel or one of its descendants such as nautobot.core.models.generics.OrganizationalModel or nautobot.core.models.generics.PrimaryModel . The intended audience for the validated_save() convenience method is Job authors and anyone writing scripts for, or interacting with the ORM directly through the nbshell command. It is generally not recommended however, to use validated_save() as a blanket replacement for the save() method in the core of Nautobot. During execution, should model validation fail, validated_save() will raise django.core.exceptions.ValidationError in the normal Django fashion.","title":"Consuming Model Validation"},{"location":"development/best-practices.html#slug-field","text":"Moving forward in Nautobot, all models should have a slug field. This field can be safely/correctly used in URL patterns, dictionary keys, GraphQL and REST API. Nautobot has provided the AutoSlugField to handle automatically populating the slug field from another field(s). Generally speaking model slugs should be populated from the name field. Below is an example on defining the slug field. from django.db import models from nautobot.core.fields import AutoSlugField from nautobot.core.models.generics import PrimaryModel class ExampleModel ( PrimaryModel ): name = models . CharField ( max_length = 100 , unique = True ) slug = AutoSlugField ( populate_from = 'name' )","title":"Slug Field"},{"location":"development/best-practices.html#getting-url-routes","text":"When developing new models a need often arises to retrieve a reversible route for a model to access it in either the web UI or the REST API. When this time comes, you must use nautobot.utilities.utils.get_route_for_model . You must not write your own logic to construct route names. from nautobot.utilities.utils import get_route_for_model This utility function supports both UI and API views for both Nautobot core apps and Nautobot plugins. Added in version 1.4.3 Support for generating API routes was added to get_route_for_model() by passing the argument api=True .","title":"Getting URL Routes"},{"location":"development/best-practices.html#ui-routes","text":"Instead of: route = f \" { model . _meta . app_label } : { model . _meta . model_name } _list\" if model . _meta . app_label in settings . PLUGINS : route = f \"plugins: { route } \" Use: route = get_route_for_model ( model , \"list\" )","title":"UI Routes"},{"location":"development/best-practices.html#rest-api-routes","text":"Instead of: api_route = f \" { model . _meta . app_label } -api: { model . _meta . model_name } -list\" if model . _meta . app_label in settings . PLUGINS : api_route = f \"plugins-api: { api_route } \" Use: api_route = get_route_for_model ( model , \"list\" , api = True )","title":"REST API Routes"},{"location":"development/best-practices.html#examples","text":"Core models: >>> get_route_for_model ( Device , \"list\" ) \"dcim:device_list\" >>> get_route_for_model ( Device , \"list\" , api = True ) \"dcim-api:device-list\" Plugin models: >>> get_route_for_model ( ExampleModel , \"list\" ) \"plugins:example_plugin:examplemodel_list\" >>> get_route_for_model ( ExampleModel , \"list\" , api = True ) \"plugins-api:example_plugin-api:examplemodel-list\" Tip The first argument may also optionally be an instance of a model, or a string using the dotted notation of {app_label}.{model} (e.g. dcim.device ). Using an instance: >>> instance = Device . objects . first () >>> get_route_for_model ( instance , \"list\" ) \"dcim:device_list\" Using dotted notation: >>> get_route_for_model ( \"dcim.device\" , \"list\" ) \"dcim:device_list\"","title":"Examples"},{"location":"development/best-practices.html#filtering-models-with-filtersets","text":"The following best practices must be considered when establishing new FilterSet classes for model classes.","title":"Filtering Models with FilterSets"},{"location":"development/best-practices.html#mapping-model-fields-to-filters","text":"Filtersets must inherit from nautobot.extras.filters.NautobotFilterSet (which inherits from nautobot.utilities.filters.BaseFilterSet ) This affords that automatically generated lookup expressions ( ic , nic , iew , niew , etc.) are always included This also asserts that the correct underlying Form class that maps the generated form field types and widgets will be included FIltersets must publish all model fields from a model, including related fields. All fields should be provided using Meta.fields = \"__all__\" and this would be preferable for the first and common case as it requires the least maintanence and overhead and asserts parity between the model fields and the filterset filters. In some cases simply excluding certain fields would be the next most preferable e.g. Meta.exclude = [\"unwanted_field\", \"other_unwanted_field\"] Finally, the last resort should be explicitly declaring the desired fields using Meta.fields = . This should be avoided because it incurs the highest technical debt in maintaining alignment between model fields and filters. In the event that fields do need to be customized to extend lookup expressions, a dictionary of field names mapped to a list of lookups may be used, however, this pattern is only compatible with explicitly declaring all fields, which should also be avoided for the common case. For example: class UserFilter ( NautobotFilterSet ): class Meta : model = User fields = { 'username' : [ 'exact' , 'contains' ], 'last_login' : [ 'exact' , 'year__gt' ], } It is acceptable that default filter mappings may need to be overridden with custom filter declarations, but filter_overrides (see below) should be used as a first resort. Custom filter definitions must not shadow the name of an existing model field if it is also changing the type. For example DeviceFilterSet.interfaces is a BooleanFilter that is shadowing the Device.interfaces related manager. This introduces problems with automatic introspection of the filterset and this pattern must be avoided. For foreign-key related fields, on existing core models in the v1.3 release train : The field should be shadowed, replacing the PK filter with a lookup-based on a more human-readable value (typically slug , if available). A PK-based filter should be made available as well, generally with a name suffixed by _id . For example: provider = django_filters . ModelMultipleChoiceFilter ( field_name = \"provider__slug\" , queryset = Provider . objects . all (), to_field_name = \"slug\" , label = \"Provider (slug)\" , ) provider_id = django_filters . ModelMultipleChoiceFilter ( queryset = Provider . objects . all (), label = \"Provider (ID)\" , ) For foreign-key related fields on new core models for v1.4 or later: The field must be shadowed utilizing a hybrid NaturalKeyOrPKMultipleChoiceFilter which will automatically try to lookup by UUID or slug depending on the value of the incoming argument (e.g. UUID string vs. slug string). Fields that use name instead of slug can set the natural_key argument on NaturalKeyOrPKMultipleChoiceFilter . In default settings for filtersets, when not using NaturalKeyOrPKMultipleChoiceFilter , provider would be a pk (UUID) field, whereas using NaturalKeyOrPKMultipleChoiceFilter will automatically support both input values for slug or pk . New filtersets should follow this direction vs. propagating the need to continue to overload the default foreign-key filter and define an additional _id filter on each new filterset. We know that most existing FilterSets aren't following this pattern, and we plan to change that in a major release. Using the previous field ( provider ) as an example, it would look something like this: from nautobot.utilities.filters import NaturalKeyOrPKMultipleChoiceFilter provider = NaturalKeyOrPKMultipleChoiceFilter ( queryset = Provider . objects . all (), label = \"Provider (slug or ID)\" , ) # optionally use the to_field_name argument to set the field to name instead of slug provider = NaturalKeyOrPKMultipleChoiceFilter ( to_field_name = \"name\" , queryset = Provider . objects . all (), label = \"Provider (name or ID)\" , )","title":"Mapping Model Fields to Filters"},{"location":"development/best-practices.html#filter-naming-and-definition","text":"Boolean filters for membership must be named with has_{related_name} (e.g. has_interfaces ) Boolean filters for identity must be named with is_{name} (e.g. is_virtual_chassis ) although this is semantically identical to has_ filters, there may be occasions where naming the filter is_ would be more intuitive. Filters must declare field_name when they have a different name than the underlying model field they are referencing. Where possible the suffix component of the filter name must map directly to the underlying field name. For example, DeviceFilterSet.has_console_ports could be better named, to assert that the filter name following the has_ prefix is a one-to-one mapping to the underlying model's related field name ( consoleports ) therefore field_name must point to the field name as defined on the model: has_consoleports = BooleanFilter ( field_name = \"consoleports\" ) Filters must be declared using the appropriate lookup expression ( lookup_expr ) if any other expression than exact (the default) is required. For example: has_consoleports = BooleanFilter ( field_name = \"consoleports\" , lookup_expr = \"isnull\" ) Filters must be declared using exclude=True if a queryset .exclude() is required to be called vs. queryset .filter() which is the default when the filter default exclude=False is passed through. If you require Foo.objects.exclude() , you must pass exclude=True instead of defining a filterset method to explicitly hard-code such a query. For example: has_consoleports = BooleanFilter ( field_name = \"consoleports\" , lookup_expr = \"isnull\" , exclude = True ) Filters must be declared using disinct=True if a queryset .distinct() is required to be called on the queryset Filters must not be set to be required using required=True Filter methods defined using the method= keyword argument may only be used as a last resort (see below) when correct usage of field_name , lookup_expr , exclude , or other filter keyword arguments do not suffice. In other words: filter methods should used as the exception and not the rule. Use of filter_overrides must be considered in cases where more-specific class-local overrides. The need may ocassionally arise to change certain filter-level arguments used for filter generation, such such as changing a filter class, or customizing a UI widget. Any extra arguments are sent to the filter as keyword arguments at instance creation time. (Hint: extra must be a callable) For example: class ProductFilter ( NautobotFilterSet ): class Meta : model = Interface fields = \"__all__\" filter_overrides = { # This would change the default to all CharFields to use lookup_expr=\"icontains\". It # would also pass in the custom `choices` generated by the `generate_choices()` # function. models . CharField : { \"filter_class\" : filters . MultiValueCharFilter , \"extra\" : lambda f : { \"lookup_expr\" : \"icontains\" , \"choices\" : generate_choices (), }, }, # This would make BooleanFields use a radio select widget vs. the default of checkbox models . BooleanField : { \"extra\" : lambda f : { \"widget\" : forms . RadioSelect , }, }, } Warning Existing features of filtersets and filters must be exhausted first using keyword arguments before resorting to customizing, re-declaring/overloading, or defining filter methods.","title":"Filter Naming and Definition"},{"location":"development/best-practices.html#filter-methods","text":"Filters on a filterset can reference a method (either a callable, or the name of a method on the filterset) to perform custom business logic for that filter field. However, many uses of filter methods in Nautobot are problematic because they break the ability for such filter fields to be properly reversible. Consider this example from nautobot.dcim.filters.DeviceFilterSet.pass_through_ports : # Filter field definition is a BooleanFilter, for which an \"isnull\" lookup_expr # is the only valid filter expression pass_through_ports = django_filters . BooleanFilter ( method = \"_pass_through_ports\" , label = \"Has pass-through ports\" , ) # Method definition loses context and further the field's lookup_expr # falls back to the default of \"exact\" and the `name` value is irrelevant here. def _pass_through_ports ( self , queryset , name , value ): breakpoint () # This was added to illustrate debugging with pdb below return queryset . exclude ( frontports__isnull = value , rearports__isnull = value ) The default lookup_expr unless otherwise specified is \u201cexact\u201d, as seen in django_filters.conf : 'DEFAULT_LOOKUP_EXPR' : 'exact' , When this method is called, the internal state is default, making reverse introspection impossible, because the lookup_expr is defaulting to \u201cexact\u201d: ( Pdb ) field = self . filters [ name ] ( Pdb ) field . exclude False ( Pdb ) field . lookup_expr 'exact' This means that the arguments for the field are being completely ignored and the hard-coded queryset queryset.exclude(frontports__isnull=value, rearports__isnull=value) is all that is being run when this method is called. Additionally, name variable that gets passed to the method cannot be used here because there are two field names at play ( frontports and rearports ). This hard-coding is impossible to introspect and therefore impossible to reverse. So while this filter definition coudl be improved like so, there is still no way to know what is going on in the method body: pass_through_ports = django_filters . BooleanFilter ( method = \"_pass_through_ports\" , # The method that is called exclude = True , # Perform an `.exclude()` vs. `.filter()`` lookup_expr = \"isnull\" , # Perform `isnull` vs. `exact`` label = \"Has pass-through ports\" , ) For illustration, if we use another breakpoint, you can see that the filter field now has the correct attributes that can be used to help reverse this query: ( Pdb ) field = self . filters [ name ] ( Pdb ) field . exclude True ( Pdb ) field . lookup_expr 'isnull' Except that it stops there becuse of the method body. Here are the problems: There's no way to identify either of the field names required here The name that is incoming to the method is the filter name as defined ( pass_through_ports in this case) does not map to an actual model field So the filter can be introspected for lookup_expr value using self.filters[name].lookup_expr , but it would have to be assumed that applies to both fields. Same with exclude ( self.filters[name].exclude ) It would be better to just eliminate pass_through_ports=True entirely in exchange for front_ports=True&rear_ports=True (current) or has_frontports=True&has_rearports=True (future).","title":"Filter Methods"},{"location":"development/best-practices.html#generating-reversible-q-objects","text":"With consistent and proper use of filter field arguments when defining them on a fitlerset, a query could be constructed using the field_name and lookup_expr values. For example: def generate_query ( self , field , value ): query = Q () predicate = { f \" { field . field_name } __ { field . lookup_expr } \" : value } if field . exclude : query |= ~ Q ( ** predicate ) else : query |= Q ( ** predicate ) return query ## Somewhere else in business logic: field = filterset . filters [ name ] value = filterset . data [ name ] query = generate_query ( field , value ) filterset . qs . filter ( query ) . count () # 339","title":"Generating Reversible Q Objects"},{"location":"development/best-practices.html#summary","text":"For the vast majority of cases where we have method filters, it\u2019s for Boolean filters For the common case method filters are unnecessary technical debt and should be eliminated where better suited by proper use of filter field arguments Reversibility may not always necessarily be required, but by properly defining field_name , lookup_expr , and exclude on filter fields, introspection becomes deterministic and reversible queries can be reliably generated as needed. For exceptions such as DeviceFilterSet.has_primary_ip where it checks for both Device.primary_ip4 OR Device.primary_ip6 , method filters may still be necessary, however, they would be the exception and not the norm. The good news is that in the core there are not that many of these filter methods defined, but we also don\u2019t want to see them continue to proliferate.","title":"Summary"},{"location":"development/best-practices.html#using-nautobotuiviewset-for-plugin-development","text":"Added in version 1.4.0 Using NautobotUIViewSet for plugin development is strongly recommended.","title":"Using NautobotUIViewSet for Plugin Development"},{"location":"development/docker-compose-advanced-use-cases.html","text":"Docker Compose Advanced Use Cases \u00b6 This section describes some of the more advanced use cases for the Docker Compose development workflow . Invoke Configuration \u00b6 The Invoke tasks have some default configuration which you may want to override. Configuration properties include: project_name : The name that all Docker containers will be grouped together under (default: nautobot , resulting in containers named nautobot_nautobot_1 , nautobot_redis_1 , etc.) python_ver : the Python version which is used to build the Docker container (default: 3.7 ) local : run the commands in the local environment vs the Docker container (default: False ) compose_dir : the full path to the directory containing the Docker Compose YAML files (default: \"<nautobot source directory>/development\" ) compose_files : the Docker Compose YAML file(s) to use (default: [\"docker-compose.yml\", \"docker-compose.postgres.yml\", \"docker-compose.dev.yml\"] ) docker_image_names_main and docker_image_names_develop : Used when building Docker images for publication ; you shouldn't generally need to change these. These setting may be overridden several different ways (from highest to lowest precedence): Command line argument on the individual commands (see invoke $command --help ) if available Using environment variables such as INVOKE_NAUTOBOT_PYTHON_VER ; the variables are prefixed with INVOKE_NAUTOBOT_ and must be uppercase; note that Invoke does not presently support environment variable overriding of list properties such as compose_files . Using an invoke.yml file (see invoke.yml.example ) Working with Docker Compose \u00b6 The files related to the Docker development environment can be found inside of the development directory at the root of the project. In this directory you'll find the following core files: docker-compose.yml - Docker service containers and their relationships to the Nautobot container docker-compose.build.yml - Docker compose override file used to start/build the production docker images for local testing. docker-compose.debug.yml - Docker compose override file used to start the Nautobot container for use with Visual Studio Code's dev container integration . docker-compose.dev.yml - Docker compose override file used to mount the Nautobot source code inside the container at /source and the nautobot_config.py from the same directory as /opt/nautobot/nautobot_config.py for the active configuration. docker-compose.mysql.yml - Docker compose override file used to add a MySQL container as the database backend for Nautobot. docker-compose.postgres.yml - Docker compose override file used to add a Postgres container as the database backend for Nautobot. dev.env - Environment variables used to setup the container services nautobot_config.py - Nautobot configuration file In addition to the development directory, additional non-development-specific Docker-related files are located in the docker directory at the root of the project. In the docker directory you will find the following files: Dockerfile - Docker container definition for Nautobot containers docker-entrypoint.sh - Commands and operations ran once Nautobot container is started including database migrations and optionally creating a superuser uwsgi.ini - The uWSGI ini file used in the production docker container Docker-Compose Overrides \u00b6 If you require changing any of the defaults found in docker-compose.yml , create a file inside the development directory called docker-compose.override.yml and add this file to the compose_files setting in your invoke.yml file, for example: --- nautobot : compose_files : - \"docker-compose.yml\" - \"docker-compose.postgres.yml\" - \"docker-compose.dev.yml\" - \"docker-compose.override.yml\" This file will override any configuration in the main docker-compose.yml file, without making changes to the repository. Please see the official documentation on extending Docker Compose for more information. Automatically Creating a Superuser \u00b6 There may be times where you want to bootstrap Nautobot with a superuser account and API token already created for quick access or for running within a CI/CD pipeline. By using a custom invoke.yml as described above, in combination with custom docker-compose.override.yml and override.env files, you can automatically bootstrap Nautobot with a user and token. Create invoke.yml as described above, then create development/docker-compose.override.yml with the following contents: --- services : nautobot : env_file : - \"override.env\" The docker-entrypoint.sh script will run any migrations and then look for specific variables set to create the superuser. The docker-entrypoint.sh script is copied in during the Docker image build and will read from the default dev.env as the env_file until you override it as seen above. Any variables defined in this file will override the defaults. The override.env should be located in the development/ directory, and should look like the following: # Superuser information. NAUTOBOT_CREATE_SUPERUSER defaults to false. NAUTOBOT_CREATE_SUPERUSER = true NAUTOBOT_SUPERUSER_NAME = admin NAUTOBOT_SUPERUSER_EMAIL = admin@example.com NAUTOBOT_SUPERUSER_PASSWORD = admin NAUTOBOT_SUPERUSER_API_TOKEN = 0123456789abcdef0123456789abcdef01234567 The variables defined above within override.env will signal the docker-entrypoint.sh script to create the superuser with the specified username, email, password, and API token. After these two files are created, you can use the invoke tasks to manage the development containers. Using MySQL instead of PostgreSQL \u00b6 By default the Docker development environment is configured to use a PostgreSQL container as the database backend. For development or testing purposes, you might optionally choose to use MySQL instead. In order to do so, you need to make the following changes to your environment: Set up invoke.yml as described above and use it to override the postgres docker-compose file: --- nautobot : compose_files : - \"docker-compose.yml\" - \"docker-compose.mysql.yml\" - \"docker-compose.dev.yml\" Then invoke stop (if you previously had the docker environment running with Postgres) and invoke start and you should now be running with MySQL. Running an RQ worker \u00b6 By default the Docker development environment no longer includes an RQ worker container, as RQ support in Nautobot is deprecated and will be removed entirely in a future release. If you need to run an RQ worker, you can set up invoke.yml as described above with the following docker-compose.override.yml : --- services : rq_worker : image : \"networktocode/nautobot-dev-py${PYTHON_VER}:local\" entrypoint : \"nautobot-server rqworker\" healthcheck : interval : 60s timeout : 30s start_period : 5s retries : 3 test : [ \"CMD\" , \"nautobot-server\" , \"health_check\" ] depends_on : - nautobot env_file : - ./dev.env tty : true volumes : - ./nautobot_config.py:/opt/nautobot/nautobot_config.py - ../:/source Microsoft Visual Studio Code Integration \u00b6 For users of Microsoft Visual Studio Code, several files are included to ease development and integrate with the VS Code Remote - Containers extension . The following related files are found relative to the root of the project: .devcontainers/devcontainer.json - Dev. container definition nautobot.code-workspace - VS Code workspace configuration for Nautobot development/docker-compose.debug.yml - Docker Compose file with debug configuration for VS Code After opening the project directory in VS Code in a supported environment, you will be prompted by VS Code to Reopen in Container and Open Workspace . Select Reopen in Container to build and start the development containers. Once your window is connected to the container, you can open the workspace which enables support for Run/Debug. To start Nautobot, select Run Without Debugging or Start Debugging from the Run menu. Once Nautobot has started, you will be prompted to open a browser to connect to Nautobot. Note You can run tests with nautobot-server --config=nautobot/core/tests/nautobot_config.py test nautobot while inside the Container. Special Workflow for Containers on Remote Servers \u00b6 A slightly different workflow is needed when your development container is running on a remotely-connected server (such as with SSH). VS Code will not offer the Reopen in Container option on a remote server. To work with remote containers, after invoke build use docker-compose as follows to start the containers. This prevents the HTTP service from automatically starting inside the container: $ cd development $ docker-compose -f docker-compose.yml -f docker-compose.debug.yml up Now open the VS Code Docker extension. In the CONTAINERS/development section, right click on a running container and select the Attach Visual Studio Code menu item. The Select the container to attach VS Code input field provides a list of running containers. Click on development_nautobot_1 to use VS Code inside the container. The devcontainer will startup now. As a last step open the folder /opt/nautobot in VS Code.","title":"Advanced Docker Compose Usage"},{"location":"development/docker-compose-advanced-use-cases.html#docker-compose-advanced-use-cases","text":"This section describes some of the more advanced use cases for the Docker Compose development workflow .","title":"Docker Compose Advanced Use Cases"},{"location":"development/docker-compose-advanced-use-cases.html#invoke-configuration","text":"The Invoke tasks have some default configuration which you may want to override. Configuration properties include: project_name : The name that all Docker containers will be grouped together under (default: nautobot , resulting in containers named nautobot_nautobot_1 , nautobot_redis_1 , etc.) python_ver : the Python version which is used to build the Docker container (default: 3.7 ) local : run the commands in the local environment vs the Docker container (default: False ) compose_dir : the full path to the directory containing the Docker Compose YAML files (default: \"<nautobot source directory>/development\" ) compose_files : the Docker Compose YAML file(s) to use (default: [\"docker-compose.yml\", \"docker-compose.postgres.yml\", \"docker-compose.dev.yml\"] ) docker_image_names_main and docker_image_names_develop : Used when building Docker images for publication ; you shouldn't generally need to change these. These setting may be overridden several different ways (from highest to lowest precedence): Command line argument on the individual commands (see invoke $command --help ) if available Using environment variables such as INVOKE_NAUTOBOT_PYTHON_VER ; the variables are prefixed with INVOKE_NAUTOBOT_ and must be uppercase; note that Invoke does not presently support environment variable overriding of list properties such as compose_files . Using an invoke.yml file (see invoke.yml.example )","title":"Invoke Configuration"},{"location":"development/docker-compose-advanced-use-cases.html#working-with-docker-compose","text":"The files related to the Docker development environment can be found inside of the development directory at the root of the project. In this directory you'll find the following core files: docker-compose.yml - Docker service containers and their relationships to the Nautobot container docker-compose.build.yml - Docker compose override file used to start/build the production docker images for local testing. docker-compose.debug.yml - Docker compose override file used to start the Nautobot container for use with Visual Studio Code's dev container integration . docker-compose.dev.yml - Docker compose override file used to mount the Nautobot source code inside the container at /source and the nautobot_config.py from the same directory as /opt/nautobot/nautobot_config.py for the active configuration. docker-compose.mysql.yml - Docker compose override file used to add a MySQL container as the database backend for Nautobot. docker-compose.postgres.yml - Docker compose override file used to add a Postgres container as the database backend for Nautobot. dev.env - Environment variables used to setup the container services nautobot_config.py - Nautobot configuration file In addition to the development directory, additional non-development-specific Docker-related files are located in the docker directory at the root of the project. In the docker directory you will find the following files: Dockerfile - Docker container definition for Nautobot containers docker-entrypoint.sh - Commands and operations ran once Nautobot container is started including database migrations and optionally creating a superuser uwsgi.ini - The uWSGI ini file used in the production docker container","title":"Working with Docker Compose"},{"location":"development/docker-compose-advanced-use-cases.html#docker-compose-overrides","text":"If you require changing any of the defaults found in docker-compose.yml , create a file inside the development directory called docker-compose.override.yml and add this file to the compose_files setting in your invoke.yml file, for example: --- nautobot : compose_files : - \"docker-compose.yml\" - \"docker-compose.postgres.yml\" - \"docker-compose.dev.yml\" - \"docker-compose.override.yml\" This file will override any configuration in the main docker-compose.yml file, without making changes to the repository. Please see the official documentation on extending Docker Compose for more information.","title":"Docker-Compose Overrides"},{"location":"development/docker-compose-advanced-use-cases.html#automatically-creating-a-superuser","text":"There may be times where you want to bootstrap Nautobot with a superuser account and API token already created for quick access or for running within a CI/CD pipeline. By using a custom invoke.yml as described above, in combination with custom docker-compose.override.yml and override.env files, you can automatically bootstrap Nautobot with a user and token. Create invoke.yml as described above, then create development/docker-compose.override.yml with the following contents: --- services : nautobot : env_file : - \"override.env\" The docker-entrypoint.sh script will run any migrations and then look for specific variables set to create the superuser. The docker-entrypoint.sh script is copied in during the Docker image build and will read from the default dev.env as the env_file until you override it as seen above. Any variables defined in this file will override the defaults. The override.env should be located in the development/ directory, and should look like the following: # Superuser information. NAUTOBOT_CREATE_SUPERUSER defaults to false. NAUTOBOT_CREATE_SUPERUSER = true NAUTOBOT_SUPERUSER_NAME = admin NAUTOBOT_SUPERUSER_EMAIL = admin@example.com NAUTOBOT_SUPERUSER_PASSWORD = admin NAUTOBOT_SUPERUSER_API_TOKEN = 0123456789abcdef0123456789abcdef01234567 The variables defined above within override.env will signal the docker-entrypoint.sh script to create the superuser with the specified username, email, password, and API token. After these two files are created, you can use the invoke tasks to manage the development containers.","title":"Automatically Creating a Superuser"},{"location":"development/docker-compose-advanced-use-cases.html#using-mysql-instead-of-postgresql","text":"By default the Docker development environment is configured to use a PostgreSQL container as the database backend. For development or testing purposes, you might optionally choose to use MySQL instead. In order to do so, you need to make the following changes to your environment: Set up invoke.yml as described above and use it to override the postgres docker-compose file: --- nautobot : compose_files : - \"docker-compose.yml\" - \"docker-compose.mysql.yml\" - \"docker-compose.dev.yml\" Then invoke stop (if you previously had the docker environment running with Postgres) and invoke start and you should now be running with MySQL.","title":"Using MySQL instead of PostgreSQL"},{"location":"development/docker-compose-advanced-use-cases.html#running-an-rq-worker","text":"By default the Docker development environment no longer includes an RQ worker container, as RQ support in Nautobot is deprecated and will be removed entirely in a future release. If you need to run an RQ worker, you can set up invoke.yml as described above with the following docker-compose.override.yml : --- services : rq_worker : image : \"networktocode/nautobot-dev-py${PYTHON_VER}:local\" entrypoint : \"nautobot-server rqworker\" healthcheck : interval : 60s timeout : 30s start_period : 5s retries : 3 test : [ \"CMD\" , \"nautobot-server\" , \"health_check\" ] depends_on : - nautobot env_file : - ./dev.env tty : true volumes : - ./nautobot_config.py:/opt/nautobot/nautobot_config.py - ../:/source","title":"Running an RQ worker"},{"location":"development/docker-compose-advanced-use-cases.html#microsoft-visual-studio-code-integration","text":"For users of Microsoft Visual Studio Code, several files are included to ease development and integrate with the VS Code Remote - Containers extension . The following related files are found relative to the root of the project: .devcontainers/devcontainer.json - Dev. container definition nautobot.code-workspace - VS Code workspace configuration for Nautobot development/docker-compose.debug.yml - Docker Compose file with debug configuration for VS Code After opening the project directory in VS Code in a supported environment, you will be prompted by VS Code to Reopen in Container and Open Workspace . Select Reopen in Container to build and start the development containers. Once your window is connected to the container, you can open the workspace which enables support for Run/Debug. To start Nautobot, select Run Without Debugging or Start Debugging from the Run menu. Once Nautobot has started, you will be prompted to open a browser to connect to Nautobot. Note You can run tests with nautobot-server --config=nautobot/core/tests/nautobot_config.py test nautobot while inside the Container.","title":"Microsoft Visual Studio Code Integration"},{"location":"development/docker-compose-advanced-use-cases.html#special-workflow-for-containers-on-remote-servers","text":"A slightly different workflow is needed when your development container is running on a remotely-connected server (such as with SSH). VS Code will not offer the Reopen in Container option on a remote server. To work with remote containers, after invoke build use docker-compose as follows to start the containers. This prevents the HTTP service from automatically starting inside the container: $ cd development $ docker-compose -f docker-compose.yml -f docker-compose.debug.yml up Now open the VS Code Docker extension. In the CONTAINERS/development section, right click on a running container and select the Attach Visual Studio Code menu item. The Select the container to attach VS Code input field provides a list of running containers. Click on development_nautobot_1 to use VS Code inside the container. The devcontainer will startup now. As a last step open the folder /opt/nautobot in VS Code.","title":"Special Workflow for Containers on Remote Servers"},{"location":"development/extending-models.html","text":"Extending Models \u00b6 Below is a list of tasks to consider when adding a new field to a core model. 1. Generate and run database migrations \u00b6 Django migrations are used to express changes to the database schema. In most cases, Django can generate these automatically, however very complex changes may require manual intervention. Always remember to specify a short but descriptive name when generating a new migration. Warning Assert that you have installed Nautobot in your development environment using poetry install so that changes you make to migrations will apply to the source tree! $ nautobot-server makemigrations <app> -n <name> $ nautobot-server migrate Where possible, try to merge related changes into a single migration. For example, if three new fields are being added to different models within an app, these can be expressed in the same migration. You can merge a new migration with an existing one by combining their operations lists. Note Migrations can only be merged within a release. Once a new release has been published, its migrations cannot be altered (other than for the purpose of correcting a bug). 2. Add validation logic to clean() \u00b6 If the new field introduces additional validation requirements (beyond what's included with the field itself), implement them in the model's clean() method. Remember to call the model's original method using super() before or after your custom validation as appropriate: class Foo ( models . Model ): def clean ( self ): super () . clean () # Custom validation goes here if self . bar is None : raise ValidationError () 3. Add CSV helpers \u00b6 Add the name of the new field to csv_headers and included a CSV-friendly representation of its data in the model's to_csv() method. These will be used when exporting objects in CSV format. 4. Update relevant querysets \u00b6 If you're adding a relational field (e.g. ForeignKey ) and intend to include the data when retrieving a list of objects, be sure to include the field using prefetch_related() as appropriate. This will optimize the view and avoid extraneous database queries. 5. Update API serializer \u00b6 Extend the model's API serializer in <app>.api.serializers to include the new field. In most cases, it will not be necessary to also extend the nested serializer, which produces a minimal representation of the model. 6. Add field to forms \u00b6 Extend any forms to include the new field as appropriate. Common forms include: Credit/edit - Manipulating a single object Bulk edit - Performing a change on many objects at once CSV import - The form used when bulk importing objects in CSV format Filter - Displays the options available for filtering a list of objects (both UI and API) 7. Extend object filter set \u00b6 If the new field should be filterable, add it to the FilterSet for the model. If the field should be searchable, remember to reference it in the FilterSet's SearchFilter definition. All filtersets should inherit from BaseFilterSet or NautobotFilterSet as appropriate to the scope of the model's functionality. 8. Add column to object table \u00b6 If the new field will be included in the object list view, add a column to the model's table. For simple fields, adding the field name to Meta.fields will be sufficient. More complex fields may require declaring a custom column. 9. Update the UI templates \u00b6 Edit the object's view template to display the new field. There may also be a custom add/edit form template that needs to be updated. 10. Validate the new field in GraphQL \u00b6 If the model is already part of the GraphQL schema, the new field will be included automatically. Validate that the new field is rendering properly in GraphQL. If the field is not compatible with GraphQL or shouldn't be included in GraphQL it's possible to exclude a specific field in the GraphQL Type Object associated with this specific model. You can refer to the graphene-django documentation for additional information. Note that if you're using the convert_django_field registry to override the default GraphQL representation for any field type, this override must be registered within the relevant app's ready() function, rather than at module import time. 11. Create/extend test cases \u00b6 Create or extend the relevant test cases to verify that the new field and any accompanying validation logic perform as expected. This is especially important for relational fields. Nautobot incorporates various test suites, including: API serializer/view tests Filter tests Form tests Model tests View tests Be diligent to ensure all of the relevant test suites are adapted or extended as necessary to test any new functionality. 12. Update the model's documentation \u00b6 Each model has a dedicated page in the documentation, at models/<app>/<model>.md . Update this file to include any relevant information about the new field. Note that this documentation page will be accessible through the web UI via a \"question mark\" icon on the corresponding model create/edit page. Warning Due to a limitation in how the documentation is rendered in the web UI, cross-reference hyperlinks between Nautobot documentation pages will not work in the web UI (they will be broken links), so avoid using them in a model documentation page. External hyperlinks (to the Django documentation, for example) will work correctly and may be used as needed.","title":"Extending Models"},{"location":"development/extending-models.html#extending-models","text":"Below is a list of tasks to consider when adding a new field to a core model.","title":"Extending Models"},{"location":"development/extending-models.html#1-generate-and-run-database-migrations","text":"Django migrations are used to express changes to the database schema. In most cases, Django can generate these automatically, however very complex changes may require manual intervention. Always remember to specify a short but descriptive name when generating a new migration. Warning Assert that you have installed Nautobot in your development environment using poetry install so that changes you make to migrations will apply to the source tree! $ nautobot-server makemigrations <app> -n <name> $ nautobot-server migrate Where possible, try to merge related changes into a single migration. For example, if three new fields are being added to different models within an app, these can be expressed in the same migration. You can merge a new migration with an existing one by combining their operations lists. Note Migrations can only be merged within a release. Once a new release has been published, its migrations cannot be altered (other than for the purpose of correcting a bug).","title":"1. Generate and run database migrations"},{"location":"development/extending-models.html#2-add-validation-logic-to-clean","text":"If the new field introduces additional validation requirements (beyond what's included with the field itself), implement them in the model's clean() method. Remember to call the model's original method using super() before or after your custom validation as appropriate: class Foo ( models . Model ): def clean ( self ): super () . clean () # Custom validation goes here if self . bar is None : raise ValidationError ()","title":"2. Add validation logic to clean()"},{"location":"development/extending-models.html#3-add-csv-helpers","text":"Add the name of the new field to csv_headers and included a CSV-friendly representation of its data in the model's to_csv() method. These will be used when exporting objects in CSV format.","title":"3. Add CSV helpers"},{"location":"development/extending-models.html#4-update-relevant-querysets","text":"If you're adding a relational field (e.g. ForeignKey ) and intend to include the data when retrieving a list of objects, be sure to include the field using prefetch_related() as appropriate. This will optimize the view and avoid extraneous database queries.","title":"4. Update relevant querysets"},{"location":"development/extending-models.html#5-update-api-serializer","text":"Extend the model's API serializer in <app>.api.serializers to include the new field. In most cases, it will not be necessary to also extend the nested serializer, which produces a minimal representation of the model.","title":"5. Update API serializer"},{"location":"development/extending-models.html#6-add-field-to-forms","text":"Extend any forms to include the new field as appropriate. Common forms include: Credit/edit - Manipulating a single object Bulk edit - Performing a change on many objects at once CSV import - The form used when bulk importing objects in CSV format Filter - Displays the options available for filtering a list of objects (both UI and API)","title":"6. Add field to forms"},{"location":"development/extending-models.html#7-extend-object-filter-set","text":"If the new field should be filterable, add it to the FilterSet for the model. If the field should be searchable, remember to reference it in the FilterSet's SearchFilter definition. All filtersets should inherit from BaseFilterSet or NautobotFilterSet as appropriate to the scope of the model's functionality.","title":"7. Extend object filter set"},{"location":"development/extending-models.html#8-add-column-to-object-table","text":"If the new field will be included in the object list view, add a column to the model's table. For simple fields, adding the field name to Meta.fields will be sufficient. More complex fields may require declaring a custom column.","title":"8. Add column to object table"},{"location":"development/extending-models.html#9-update-the-ui-templates","text":"Edit the object's view template to display the new field. There may also be a custom add/edit form template that needs to be updated.","title":"9. Update the UI templates"},{"location":"development/extending-models.html#10-validate-the-new-field-in-graphql","text":"If the model is already part of the GraphQL schema, the new field will be included automatically. Validate that the new field is rendering properly in GraphQL. If the field is not compatible with GraphQL or shouldn't be included in GraphQL it's possible to exclude a specific field in the GraphQL Type Object associated with this specific model. You can refer to the graphene-django documentation for additional information. Note that if you're using the convert_django_field registry to override the default GraphQL representation for any field type, this override must be registered within the relevant app's ready() function, rather than at module import time.","title":"10. Validate the new field in GraphQL"},{"location":"development/extending-models.html#11-createextend-test-cases","text":"Create or extend the relevant test cases to verify that the new field and any accompanying validation logic perform as expected. This is especially important for relational fields. Nautobot incorporates various test suites, including: API serializer/view tests Filter tests Form tests Model tests View tests Be diligent to ensure all of the relevant test suites are adapted or extended as necessary to test any new functionality.","title":"11. Create/extend test cases"},{"location":"development/extending-models.html#12-update-the-models-documentation","text":"Each model has a dedicated page in the documentation, at models/<app>/<model>.md . Update this file to include any relevant information about the new field. Note that this documentation page will be accessible through the web UI via a \"question mark\" icon on the corresponding model create/edit page. Warning Due to a limitation in how the documentation is rendered in the web UI, cross-reference hyperlinks between Nautobot documentation pages will not work in the web UI (they will be broken links), so avoid using them in a model documentation page. External hyperlinks (to the Django documentation, for example) will work correctly and may be used as needed.","title":"12. Update the model's documentation"},{"location":"development/generic-views.html","text":"Generic Views \u00b6 ObjectView - Retrieve a single object for display. ObjectListView - List a series of objects. ObjectEditView - Create or edit a single object. ObjectDeleteView - Delete a single object. BulkCreateView - Create new objects in bulk. BulkDeleteView - Delete objects in bulk. BulkEditView - Edit objects in bulk. BulkImportView - Import objects in bulk from CSV. Once you define a view by subclassing any of the above generic classes, you must register it in your urls.py as usual. There are a few things to be aware of here: Reverse URL naming needs to follow a template of {modelname}_{method} where the model name is lowercased model class name from models.py and method is the purpose of the view. E.g. _list , _add , _edit . The default rendering context for the ObjectListView includes some standard action_buttons for interacting with the listed model. By default this view defines action_buttons = (\"add\", \"import\", \"export\") . The export action is handled automatically by ObjectListView , but the add and import actions need corresponding views in order to work. In other words, if you implement an ObjectListView and do not override its action_buttons , you must also implement and register the corresponding ObjectEditView and BulkImportView subclasses as well. Warning If you're missing any of the aforementioned URLs/Views, when accessing your list view it will result in a error Reverse for 'None' not found. 'None' is not a valid view function or pattern name. If you do not need ObjectEditView and/or BulkImportView for your particular model, as an alternative you can simply update your ObjectListView subclass to overload the action buttons. For example, action_buttons = (\"add\",) or if none are required action_buttons = () . To demonstrate these concepts we can look at the example_plugin included in the Nautobot repository. The example plugin has a simple model called ExampleModel : class ExampleModel ( OrganizationalModel ): name = models . CharField ( max_length = 20 , help_text = \"The name of this Example.\" ) number = models . IntegerField ( default = 100 , help_text = \"The number of this Example.\" ) csv_headers = [ \"name\" , \"number\" ] class Meta : ordering = [ \"name\" ] The list view for this model subclasses generic.ObjectListView and does not overload the action_buttons : class ExampleModelListView ( generic . ObjectListView ): \"\"\"List `ExampleModel` objects.\"\"\" queryset = ExampleModel . objects . all () filterset = filters . ExampleModelFilterSet filterset_form = forms . ExampleModelFilterForm table = tables . ExampleModelTable Info Since action_buttons was not overloaded, action_buttons = (\"add\", \"import\", \"export\") is inherited. In order for this to work properly we expect to see urls.py have each of the required URLs/Views implemented with the template mentioned above. urlpatterns = [ ... path ( \"models/\" , views . ExampleModelListView . as_view (), name = \"examplemodel_list\" ), path ( \"models/add/\" , views . ExampleModelEditView . as_view (), name = \"examplemodel_add\" ), ... path ( \"models/import/\" , views . ExampleModelBulkImportView . as_view (), name = \"examplemodel_import\" , ), ... ]","title":"Generic Views"},{"location":"development/generic-views.html#generic-views","text":"ObjectView - Retrieve a single object for display. ObjectListView - List a series of objects. ObjectEditView - Create or edit a single object. ObjectDeleteView - Delete a single object. BulkCreateView - Create new objects in bulk. BulkDeleteView - Delete objects in bulk. BulkEditView - Edit objects in bulk. BulkImportView - Import objects in bulk from CSV. Once you define a view by subclassing any of the above generic classes, you must register it in your urls.py as usual. There are a few things to be aware of here: Reverse URL naming needs to follow a template of {modelname}_{method} where the model name is lowercased model class name from models.py and method is the purpose of the view. E.g. _list , _add , _edit . The default rendering context for the ObjectListView includes some standard action_buttons for interacting with the listed model. By default this view defines action_buttons = (\"add\", \"import\", \"export\") . The export action is handled automatically by ObjectListView , but the add and import actions need corresponding views in order to work. In other words, if you implement an ObjectListView and do not override its action_buttons , you must also implement and register the corresponding ObjectEditView and BulkImportView subclasses as well. Warning If you're missing any of the aforementioned URLs/Views, when accessing your list view it will result in a error Reverse for 'None' not found. 'None' is not a valid view function or pattern name. If you do not need ObjectEditView and/or BulkImportView for your particular model, as an alternative you can simply update your ObjectListView subclass to overload the action buttons. For example, action_buttons = (\"add\",) or if none are required action_buttons = () . To demonstrate these concepts we can look at the example_plugin included in the Nautobot repository. The example plugin has a simple model called ExampleModel : class ExampleModel ( OrganizationalModel ): name = models . CharField ( max_length = 20 , help_text = \"The name of this Example.\" ) number = models . IntegerField ( default = 100 , help_text = \"The number of this Example.\" ) csv_headers = [ \"name\" , \"number\" ] class Meta : ordering = [ \"name\" ] The list view for this model subclasses generic.ObjectListView and does not overload the action_buttons : class ExampleModelListView ( generic . ObjectListView ): \"\"\"List `ExampleModel` objects.\"\"\" queryset = ExampleModel . objects . all () filterset = filters . ExampleModelFilterSet filterset_form = forms . ExampleModelFilterForm table = tables . ExampleModelTable Info Since action_buttons was not overloaded, action_buttons = (\"add\", \"import\", \"export\") is inherited. In order for this to work properly we expect to see urls.py have each of the required URLs/Views implemented with the template mentioned above. urlpatterns = [ ... path ( \"models/\" , views . ExampleModelListView . as_view (), name = \"examplemodel_list\" ), path ( \"models/add/\" , views . ExampleModelEditView . as_view (), name = \"examplemodel_add\" ), ... path ( \"models/import/\" , views . ExampleModelBulkImportView . as_view (), name = \"examplemodel_import\" , ), ... ]","title":"Generic Views"},{"location":"development/getting-started.html","text":"Getting Started \u00b6 Git Branches \u00b6 The Nautobot project follows a branching model based on Git-flow . As such, there are three persistent git branches: main - Serves as a snapshot of the current stable release develop - All bug fixes and minor feature development on the upcoming stable release occurs here next - All major new feature development for the next feature release occurs here. You will always base pull requests off of either the develop branch, for fixes and minor features, or next , if you're working on a feature targeted for a later release. Never target fix or feature pull requests into the main branch, which receives merges only from the develop branch and only for new stable releases of Nautobot. Forking the Repo \u00b6 When developing Nautobot, you'll be working on your own fork, so your first step will be to fork the official GitHub repository . You will then clone your GitHub fork locally for development. Note It is highly recommended that you use SSH with GitHub. If you haven't already, make sure that you setup Git and add an SSH key to your GitHub account before proceeding. In this guide, SSH will be used to interact with Git. $ git clone git@github.com:yourusername/nautobot.git Cloning into 'nautobot'... remote: Enumerating objects: 231, done. remote: Counting objects: 100% (231/231), done. remote: Compressing objects: 100% (147/147), done. remote: Total 56705 (delta 134), reused 145 (delta 84), pack-reused 56474 Receiving objects: 100% (56705/56705), 27.96 MiB | 34.92 MiB/s, done. Resolving deltas: 100% (44177/44177), done. $ ls nautobot/ CHANGELOG.md README.md docs nautobot.code-workspace site CONTRIBUTING.md contrib manage.py poetry.lock tasks.py LICENSE.txt development mkdocs.yml pyproject.toml upgrade.sh NOTICE dist nautobot scripts About Remote Repos \u00b6 Git refers to remote repositories as remotes . When you make your initial clone of your fork, Git defaults to naming this remote origin . Throughout this documentation, the following remote names will be used: origin - The default remote name used to refer to your fork of Nautobot upstream - The main remote used to refer to the official Nautobot repository Setting up your Remotes \u00b6 Remote repos are managed using the git remote command. Upon cloning Nautobot for the first time, you will have only a single remote: $ git remote -v origin git@github.com:yourusername/nautobot.git (fetch) origin git@github.com:yourusername/nautobot.git (push) Add the official Nautobot repo as a the upstream remote: $ git remote add upstream git@github.com:nautobot/nautobot.git View your remotes again to confirm you've got both origin pointing to your fork and upstream pointing to the official repo: $ git remote -v origin git@github.com:yourusername/nautobot.git (fetch) origin git@github.com:yourusername/nautobot.git (push) upstream git@github.com:nautobot/nautobot.git (fetch) upstream git@github.com:nautobot/nautobot.git (push) You're now ready to proceed to the next steps. Hint You will always push changes to origin (your fork) and pull changes from upstream (official repo). Creating a Branch \u00b6 Before you make any changes, always create a new branch. Again, for bug fixes and minor features, you'll want to create your branches from the develop branch, while for major new features, you'll branch from next instead. Before you ever create a new branch, always checkout the appropriate branch and make sure you you've got the latest changes from upstream : $ git checkout develop $ git pull upstream develop Warning If you do not do this, you run the risk of having merge conflicts in your branch, and that's never fun to deal with. Trust us on this one. Now that you've got the latest upstream changes, create your branch. It's convention to always prefix your branch name with your GitHub username or your initials, and suffix it with the issue number if appropriate, separated by hyphens. For example: $ git checkout -b yourusername-myfeature-1234 Enabling Pre-Commit Hooks \u00b6 Nautobot ships with a Git pre-commit hook script that automatically checks for style compliance and missing database migrations prior to committing changes. This helps avoid erroneous commits that result in CI test failures. Note This pre-commit hook currently only supports the Python Virtual Environment Workflow. You are encouraged to enable it by creating a link to scripts/git-hooks/pre-commit : $ cd .git/hooks/ $ ln -s ../../scripts/git-hooks/pre-commit Setting up your Development Environment \u00b6 Getting started with Nautobot development is pretty straightforward, and should feel very familiar to anyone with Django development experience. We can recommend either a Docker Compose workflow (if you don't want to install dependencies such as PostgreSQL and Redis directly onto your system) or a Python virtual environment workflow . Windows Development \u00b6 Local development on Windows Subsystem for Linux (WSL) is not currently supported. When developing locally on Windows, we recommend using a virtual machine running an officially supported operating system . Docker Compose Workflow \u00b6 This workflow uses Docker and Docker Compose and assumes that you have them installed. For the Docker Compose workflow, Nautobot uses Invoke as a replacement for Make. Invoke was chosen because it is less arcane than make. Instead of a Makefile , Invoke reads the tasks.py in the project root. Note Although the Docker Compose workflow uses containers, it is important to note that the containers are running the local repository code on your machine. Changes you make to your local code will be picked up and executed by the containers. Install Invoke \u00b6 Because it is used to execute all common Docker workflow tasks, Invoke must be installed for your user environment. On most systems, if you're installing without root/superuser permissions, the default will install into your local user environment. $ pip3 install invoke If you run into issues, you may also deliberately tell pip3 to install into your user environment by adding the --user flag: $ pip3 install --user invoke Please see the official documentation on Pip user installs for more information. List Invoke Tasks \u00b6 Now that you have an invoke command, list the tasks defined in tasks.py : $ invoke --list Available tasks: black Check Python code style with Black. build Build Nautobot docker image. build-and-check-docs Build docs for use within Nautobot. buildx Build Nautobot docker image using the experimental buildx docker functionality (multi-arch capablility). check-migrations Check for missing migrations. check-schema Render the REST API schema and check for problems. cli Launch a bash shell inside the running Nautobot (or other) Docker container. createsuperuser Create a new Nautobot superuser account (default: \"admin\"), will prompt for password. debug Start Nautobot and its dependencies in debug mode. destroy Destroy all containers and volumes. docker-push Tags and pushes docker images to the appropriate repos, intended for release use only. dumpdata Dump data from database to db_output file. flake8 Check for PEP8 compliance and other style issues. hadolint Check Dockerfile for hadolint compliance and other style issues. integration-test Run Nautobot integration tests. loaddata Load data from file. makemigrations Perform makemigrations operation in Django. markdownlint Lint Markdown files. migrate Perform migrate operation in Django. nbshell Launch an interactive nbshell session. post-upgrade Performs Nautobot common post-upgrade operations using a single entrypoint. pylint Perform static analysis of Nautobot code. restart Gracefully restart containers. start Start Nautobot and its dependencies in detached mode. stop Stop Nautobot and its dependencies. tests Run all linters and unit tests. unittest Run Nautobot unit tests. unittest-coverage Report on code test coverage as measured by 'invoke unittest'. vscode Launch Visual Studio Code with the appropriate Environment variables to run in a container. Using Docker with Invoke \u00b6 A development environment can be easily started up from the root of the project using the following commands: invoke build - Builds Nautobot docker images invoke migrate - Performs database migration operation in Django invoke createsuperuser - Creates a superuser account for the Nautobot application invoke debug - Starts Docker containers for Nautobot, PostgreSQL, Redis, Celery, and the RQ worker in debug mode and attaches their output to the terminal in the foreground. You may enter Control-C to stop the containers Additional useful commands for the development environment: invoke start [-s servicename] - Starts Docker containers for Nautobot, PostgreSQL, Redis, Celery, and the RQ worker (or a specific container/service, such as invoke start -s redis ) to run in the background with debug disabled invoke cli [-s servicename] - Launch a bash shell inside the specified service container (if none is specified, defaults to the Nautobot container) invoke stop [-s servicename] - Stops all containers (or a specific container/service) created by invoke start Note The mkdocs container must be started manually with invoke start -s mkdocs . It will not start automatically with the invoke start or invoke debug commands. Tip To learn about advanced use cases within the Docker Compose workflow, see the Docker Compose Advanced Use Cases page. Proceed to the Working in your Development Environment section Python Virtual Environment Workflow \u00b6 This workflow uses Python and Poetry to work with your development environment locally. It requires that you install the required system dependencies on your system yourself. There are a few things you'll need: A Linux system or environment A MySQL or PostgreSQL server, which can be installed locally per the documentation A Redis server, which can also be installed locally A supported version of Python A recent version of Poetry Install Poetry \u00b6 Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update/remove) them for you. It will also manage virtual environments automatically, and allow for publishing packages to the Python Package Index . You may install Poetry in your user environment by running: $ curl -sSL https://install.python-poetry.org | python3 - For detailed installation instructions, please see the official Poetry installation guide . Install Hadolint \u00b6 Hadolint is a tool used to validate and lint Dockerfiles to ensure we are following best practices. On macOS with Homebrew you can install Hadolint by running: $ brew install hadolint Install markdownlint-cli \u00b6 markdownlint-cli is a tool used to validate and lint Markdown files, such as Nautobot's documentation, to ensure that they are correctly constructed. On macOS with Homebrew you can install markdownlint-cli by running: $ brew install markdownlint-cli Creating a Python Virtual Environment \u00b6 A Python virtual environment (or virtualenv ) is like a container for a set of Python packages. A virtualenv allow you to build environments suited to specific projects without interfering with system packages or other projects. When installed per the documentation, Nautobot uses a virtual environment in production. For Nautobot development, we have selected Poetry, which will transparently create a virtualenv for you, automatically install all dependencies required for Nautobot to operate, and will also install the nautobot-server CLI command that you will utilize to interact with Nautobot from here on out. Bootstrap your virtual environment using poetry install : $ poetry install Hint If you are doing development or testing using MySQL, you may quickly install the mysqlclient library along with Nautobot by running poetry install --extras mysql . This will create automatically create a virtualenv in your home directory, which houses a virtual copy of the Python executable and its related libraries and tooling. When running Nautobot for development, it will be run using the Python binary at found within the virtualenv. Once created, you may activate the virtual environment using poetry shell : $ poetry shell Spawning shell within /home/example/.cache/pypoetry/virtualenvs/nautobot-Ams_xyDt-py3.8 $ . /home/example/.cache/pypoetry/virtualenvs/nautobot-Ams_xyDt-py3.8/bin/activate (nautobot-Ams_xyDt-py3.8) $ Notice that the console prompt changes to indicate the active environment. This updates the necessary system environment variables to ensure that any Python scripts are run within the virtual environment. Observe also that the python interpreter is bound within the virtualenv: (nautobot-Ams_xyDt-py3.8) $ which python /home/example/.cache/pypoetry/virtualenvs/nautobot-Ams_xyDt-py3.8/bin/python To exit the virtual shell, use exit : (nautobot-Ams_xyDt-py3.8) $ exit $ Working with Poetry \u00b6 Poetry automatically installs your dependencies. However, if you need to install any additional dependencies this can be done with pip . For example, if you really like using ipython for development: (nautobot-Ams_xyDt-py3.8) $ pip3 install ipython Collecting ipython Using cached ipython-7.20.0-py3-none-any.whl (784 kB) ... It may not always be convenient to enter into the virtual shell just to run programs. You may also execute a given command ad hoc within the project's virtual shell by using poetry run : $ poetry run mkdocs serve Check out the Poetry usage guide for more tips. Configuring Nautobot \u00b6 Note Unless otherwise noted, all following commands should be executed inside the virtualenv. Hint Use poetry shell to enter the virtualenv. Nautobot's configuration file is nautobot_config.py . Initializing a Config \u00b6 You may also initialize a new configuration using nautobot-server init : $ nautobot-server init Configuration file created at '/home/example/.nautobot/nautobot_config.py' You may also specify alternate file locations. Please refer to Configuring Nautobot for how to do that. Using the Development Config \u00b6 A nautobot_config.py suitable for development purposes can be found at development/nautobot_config.py . You may customize the values there or utilize environment variables to override the default values. If you want to use this file, initialize a config first, then copy this file to the default location Nautobot expects to find its config: $ cp development/nautobot_config.py ~/.nautobot/nautobot_config.py Required Settings \u00b6 A newly created configuration includes sane defaults. If you need to customize them, edit your nautobot_config.py and update the following settings as required: ALLOWED_HOSTS : This can be set to [\"*\"] for development purposes and must be set if DEBUG=False DATABASES : Database connection parameters, if different from the defaults Redis settings : Redis configuration requires multiple settings including CACHEOPS_REDIS and RQ_QUEUES . The defaults should be fine for development. DEBUG : Set to True to enable verbose exception logging and, if installed, the Django debug toolbar EXTRA_INSTALLED_APPS : Optionally provide a list of extra Django apps/plugins you may desire to use for development Working in your Development Environment \u00b6 Below are common commands for working your development environment. Creating a Superuser \u00b6 You'll need to create a administrative superuser account to be able to log into the Nautobot Web UI for the first time. Specifying an email address for the user is not required, but be sure to use a very strong password. Docker Compose Workflow Virtual Environment Workflow invoke createsuperuser nautobot-server createsuperuser Starting the Development Server \u00b6 Django provides a lightweight HTTP/WSGI server for development use. The development server automatically reloads Python code for each request, as needed. You don\u2019t need to restart the server for code changes to take effect. However, some actions like adding files don\u2019t trigger a restart, so you\u2019ll have to restart the server in these cases. Danger DO NOT USE THIS SERVER IN A PRODUCTION SETTING. The development server is for development and testing purposes only. It is neither performant nor secure enough for production use. You can start the Nautobot development server with the invoke start command (if using Docker), or the nautobot-server runserver management command: Docker Compose Workflow Virtual Environment Workflow invoke start nautobot-server runserver For example: $ nautobot-server runserver Performing system checks... System check identified no issues (0 silenced). November 18, 2020 - 15:52:31 Django version 3.1, using settings 'nautobot.core.settings' Starting development server at http://127.0.0.1:8080/ Quit the server with CONTROL-C. Warning Do not use poetry run nautobot-server runserver as it will crash unless you also pass the --noreload flag, which somewhat defeats the purpose of using the development server. It is recommended to use nautobot-server runserver from within an active virtualenv (e.g. poetry shell ). This is a known issue with Django and Poetry . Please see the official Django documentation on runserver for more information. You can then log into the development server at localhost:8080 with the superuser you created. Starting the Interactive Shell \u00b6 Nautobot provides an interactive Python shell that sets up the server environment and gives you direct access to the database models for debugging. Nautobot extends this slightly to automatically import models and other utilities. Run the Nautobot interactive shell with invoke nbshell (Docker) or the nautobot-server nbshell management command: Docker Compose Workflow Virtual Environment Workflow invoke nbshell nautobot-server nbshell For example: $ nautobot-server nbshell ### Nautobot interactive shell (localhost) ### Python 3.9.1 | Django 3.1.3 | Nautobot 1.0.0b1 ### lsmodels() will show available models. Use help(<model>) for more info. >>> Post-upgrade Operations \u00b6 There will be times where you're working with the bleeding edge of Nautobot from the develop branch or feature branches and will need to pull in database changes or run server operations. Get into the habit of running nautobot-server post_upgrade (or invoke post-upgrade when using Docker) after you pull in a major set of changes from Nautobot, which performs a handful of common operations (such as migrate ) from a single command: Docker Compose Workflow Virtual Environment Workflow invoke post-upgrade nautobot-server post_upgrade Please see the documentation on the nautobot-server post_upgrade command for more information. Reinstalling Nautobot \u00b6 Note This mostly applies to working with Nautobot in a virtualenv, since Docker containers are typically rebuilt when the code changes. Sometimes when files are renamed, moved, or deleted and you've been working in the same environment for a while, you can encounter weird behavior. If this happens, don't panic and nuke your environment. First, use pip3 to explicitly uninstall the Nautobot package from the environment: $ pip3 uninstall -y nautobot Found existing installation: nautobot 1.0.0b2 Uninstalling nautobot-1.0.0b2: Successfully uninstalled nautobot-1.0.0b2 Then try to just have Poetry do the right thing by telling it to install again: $ poetry install Installing dependencies from lock file No dependencies to install or update Installing the current project: nautobot (1.0.0-beta.2) Running Tests \u00b6 Throughout the course of development, it's a good idea to occasionally run Nautobot's test suite to catch any potential errors. Tests come in two primary flavors: Unit tests and integration tests. Unit Tests \u00b6 Unit tests are automated tests written and run to ensure that a section of the Nautobot application (known as the \"unit\") meets its design and behaves as intended and expected. Most commonly as a developer of or contributor to Nautobot you will be writing unit tests to exercise the code you have written. Unit tests are not meant to test how the application behaves, only the individual blocks of code, therefore use of mock data and phony connections is common in unit test code. As a guiding principle, unit tests should be fast, because they will be executed quite often. By Nautobot convention, unit tests must be tagged with unit . The base test case class nautobot.utilities.testing.TestCase has this tag, therefore any test cases inheriting from that class do not need to be explicitly tagged. All existing view and API test cases in the Nautobot test suite utilities inherit from this class. Warning New unit tests must always inherit from nautobot.utilities.testing.TestCase . Do not use django.test.TestCase . Wrong: from django.test import TestCase class MyTestCase ( TestCase ): ... Right: from nautobot.utilities.testing import TestCase class MyTestCase ( TestCase ): ... Unit tests are run using the invoke unittest command (if using the Docker development environment) or the nautobot-server test command: Docker Compose Workflow Virtual Environment Workflow invoke unittest nautobot-server --config=nautobot/core/tests/nautobot_config.py test nautobot Info By default invoke unittest will start and run the unit tests inside the Docker development container; this ensures that PostgreSQL and Redis servers are available during the test. However, if you have your environment configured such that nautobot-server can run locally, outside of the Docker environment, you may wish to set the environment variable INVOKE_NAUTOBOT_LOCAL=True to execute these tests in your local environment instead. See the Invoke configuration for more information. In cases where you haven't made any changes to the database (which is most of the time), you can append the --keepdb argument to this command to reuse the test database between runs. This cuts down on the time it takes to run the test suite since the database doesn't have to be rebuilt each time. Docker Compose Workflow Virtual Environment Workflow invoke unittest --keepdb nautobot-server --config=nautobot/core/tests/nautobot_config.py test --keepdb nautobot Note Using the --keepdb argument will raise errors if you've modified any model fields since the previous test run. Warning In some cases when tests fail and exit uncleanly it may leave the test database in an inconsistent state. If you encounter errors about missing objects, remove --keepdb and run the tests again. Integration Tests \u00b6 Integration tests are automated tests written and run to ensure that the Nautobot application behaves as expected when being used as it would be in practice. By contrast to unit tests, where individual units of code are being tested, integration tests rely upon the server code actually running, and web UI clients or API clients to make real connections to the service to exercise actual workflows, such as navigating to the login page, filling out the username/passwords fields, and clicking the \"Log In\" button. Integration testing is much more involved, and builds on top of the foundation laid by unit testing. As a guiding principle, integration tests should be comprehensive, because they are the last mile to asserting that Nautobot does what it is advertised to do. Without integration testing, we have to do it all manually, and that's no fun for anyone! Running integrations tests requires the use of Docker at this time. They can be directly invoked using nautobot-server test just as unit tests can, however, a headless Firefox browser provided by Selenium is required. Because Selenium installation and setup is complicated, we have included a configuration for this to work out of the box using Docker. The Selenium container is running a standalone, headless Firefox \"web driver\" browser that can be remotely controlled by Nautobot for use in integration testing. Before running integration tests, the selenium container must be running. If you are using the Docker Compose workflow, it is automatically started for you. For the Virtual Environment workflow, you must start it manually. Docker Compose Workflow Virtual Environment Workflow (automatic) invoke start --service selenium By Nautobot convention, integration tests must be tagged with integration . The base test case class nautobot.utilities.testing.integration.SeleniumTestCase has this tag, therefore any test cases inheriting from that class do not need to be explicitly tagged. All existing integration test cases in the Nautobot test suite utilities inherit from this class. Warning New integration tests must always inherit from nautobot.utilities.testing.integration.SeleniumTestCase and added in the integration directory in the tests directory of an inner Nautobot application. Do not use any other base class for integration tests. We never want to risk running the unit tests and integration tests at the same time. The isolation from each other is critical to a clean and manageable continuous development cycle. Wrong: from django.contrib.staticfiles.testing import StaticLiveServerTestCase class MyIntegrationTestCase ( StaticLiveServerTestCase ): ... Right: from nautobot.utilities.testing.integration import SeleniumTestCase class MyIntegrationTestCase ( SeleniumTestCase ): ... Integration tests are run using the invoke integration-test command. All integration tests must inherit from nautobot.utilities.testing.integration.SeleniumTestCase , which itself is tagged with integration . A custom test runner has been implemented to automatically skip any test case tagged with integration by default, so normal unit tests run without any concern. To run the integration tests the --tag integration argument must be passed to nautobot-server test . Docker Compose Workflow Virtual Environment Workflow invoke integration-test nautobot-server --config=nautobot/core/tests/nautobot_config.py test --tag integration nautobot Info The same arguments supported by invoke unittest are supported by invoke integration-test . The key difference being the dependency upon the Selenium container, and inclusion of the integration tag. Tip You may also use invoke integration-test in the Virtual Environment workflow given that the selenium container is running, and that the INVOKE_NAUTOBOT_LOCAL=True environment variable has been set. Customizing Integration Test Executions \u00b6 The following environment variables can be provided when running tests to customize where Nautobot looks for Selenium and where Selenium looks for Nautobot. If using the default setup documented above, there is no need to customize these. NAUTOBOT_SELENIUM_URL - The URL used by the Nautobot test runner to remotely control the headless Selenium Firefox node. You can provide your own, but it must be a Remote WebDriver . (Default: http://localhost:4444/wd/hub ; for Docker: http://selenium:4444/wd/hub ) NAUTOBOT_SELENIUM_HOST - The hostname used by the Selenium WebDriver to access Nautobot using Firefox. (Default: host.docker.internal ; for Docker: nautobot ) Verifying the REST API Schema \u00b6 If you make changes to the REST API, you should verify that the REST API OpenAPI schema renders correctly without errors. To verify that there are no errors, you can run the invoke check-schema command (if using the Docker development environment) or the nautobot-server spectacular command. In the latter case you should run the command for each supported REST API version that Nautobot provides (e.g. \"1.2\", \"1.3\") Docker Compose Workflow Virtual Environment Workflow invoke check-schema nautobot-server spectacular --api-version 1.2 --validate --fail-on-warn --file /dev/null Verifying Code Style and Static Analysis \u00b6 To enforce best practices around consistent coding style , Nautobot uses Flake8 and Black . Additionally, static analysis of Nautobot code is performed by Pylint . You should run all of these commands and ensure that they pass fully with regard to your code changes before opening a pull request upstream. Docker Compose Workflow Virtual Environment Workflow invoke flake8 flake8 invoke black black invoke pylint nautobot-server pylint nautobot tasks.py && nautobot-server pylint --recursive development/ examples/ Handling Migrations \u00b6 If you're unsure whether a database schema migration is needed based on your changes, you can run the following command: Docker Compose Workflow Virtual Environment Workflow invoke check-migrations nautobot-server --config=nautobot/core/tests/nautobot_config.py makemigrations --dry-run --check If your branch modifies a Django model (and as a result requires a database schema modification), please be sure to provide a meaningful name to the migration before pushing. If you have yet to run invoke makemigrations , you can pass in a name for the migration with the -n option, example invoke makemigrations -n provider_increase_account_length . If you have already run invoke makemigrations , rename the generated migration files, for example 0004_provider_increase_account_length instead of 0004_auto_20211220_2104 . You\u2019ll also want to run black against the generated migration file as the autogenerated code doesn\u2019t follow our style guide by default. When modifying model field attributes, modify the test data in the tests too to reflect these changes and also any forms which refer to the model. Working on Documentation \u00b6 Some features require documentation updates or new documentation to be written. The documentation files can be found in the docs directory. To preview these changes locally, you can use mkdocs . For substantial changes to the code (including new features, removal of existing features, or significant changes in behavior) you should always make corresponding documentation updates. Nautobot's documentation pipeline includes a custom plugin for mkdocs that adds a few useful macros for annotating such changes: +++ 1.4.3 , on a line by itself, is a shorthand for !!! version-added \"Added in version 1.4.3\" +/- 1.4.3 , on a line by itself, is a shorthand for !!! version-changed \"Changed in version 1.4.3\" --- 1.4.3 , on a line by itself, is a shorthand for !!! version-removed \"Removed in version 1.4.3\" These admonitions in turn appear in the rendered documentation as follows: Added in version 1.4.3 Changed in version 1.4.3 Removed in version 1.4.3 You can also add text to any of these admonitions for further clarity, for example: +++ 1.4.3 The custom `mkdocs` plugin was added. will render as: Added in version 1.4.3 The custom mkdocs plugin was added. Caution While you can use the version-added / version-changed / version-removed admonitions directly to add a custom title to a specific admonition, in general, you should use the macros for consistency across the documentation. Writing Documentation \u00b6 You can preview the documentation using the server built into mkdocs, which should start a web server at http://localhost:8001 . Docker Compose Workflow Virtual Environment Workflow invoke start -s mkdocs mkdocs serve Documentation is written in Markdown. If you need to add additional pages or sections to the documentation, you can add them to mkdocs.yml at the root of the repository. Verifying Documentation \u00b6 Nautobot uses markdownlint-cli to verify correctness of the documentation. You should run this command and ensure that it passes fully with regard to your documentation changes before opening a pull request upstream. Docker Compose Workflow Virtual Environment Workflow invoke markdownlint markdownlint --ignore nautobot/project-static --config .markdownlint.yml nautobot examples *.md Submitting Pull Requests \u00b6 Once you're happy with your work and have verified that all tests pass, commit your changes and push it upstream to your fork. Always provide descriptive (but not excessively verbose) commit messages. When working on a specific issue, be sure to reference it. $ git commit -m \"Closes #1234: Add IPv5 support\" $ git push origin Once your fork has the new commit, submit a pull request to the Nautobot repo to propose the changes. Be sure to provide a detailed accounting of the changes being made and the reasons for doing so. Once submitted, a maintainer will review your pull request and either merge it or request changes. If changes are needed, you can make them via new commits to your fork: The pull request will update automatically. Note Remember, pull requests are entertained only for accepted issues. If an issue you want to work on hasn't been approved by a maintainer yet, it's best to avoid risking your time and effort on a change that might not be accepted. Troubleshooting \u00b6 Below are common issues you might encounter in your development environment and how to address them. FATAL: sorry, too many clients already \u00b6 When using nautobot-server runserver to do development you might run into a traceback that looks something like this: Exception Type: OperationalError at /extras/tags/ Exception Value: FATAL: sorry, too many clients already The runserver development server is multi-threaded by default, which means that every request is creating its own connection. If you are doing some local testing or development that is resulting in a lot of connections to the database, pass --nothreading to the runserver command to disable threading: $ nautobot-server runserver --nothreading","title":"Getting Started"},{"location":"development/getting-started.html#getting-started","text":"","title":"Getting Started"},{"location":"development/getting-started.html#git-branches","text":"The Nautobot project follows a branching model based on Git-flow . As such, there are three persistent git branches: main - Serves as a snapshot of the current stable release develop - All bug fixes and minor feature development on the upcoming stable release occurs here next - All major new feature development for the next feature release occurs here. You will always base pull requests off of either the develop branch, for fixes and minor features, or next , if you're working on a feature targeted for a later release. Never target fix or feature pull requests into the main branch, which receives merges only from the develop branch and only for new stable releases of Nautobot.","title":"Git Branches"},{"location":"development/getting-started.html#forking-the-repo","text":"When developing Nautobot, you'll be working on your own fork, so your first step will be to fork the official GitHub repository . You will then clone your GitHub fork locally for development. Note It is highly recommended that you use SSH with GitHub. If you haven't already, make sure that you setup Git and add an SSH key to your GitHub account before proceeding. In this guide, SSH will be used to interact with Git. $ git clone git@github.com:yourusername/nautobot.git Cloning into 'nautobot'... remote: Enumerating objects: 231, done. remote: Counting objects: 100% (231/231), done. remote: Compressing objects: 100% (147/147), done. remote: Total 56705 (delta 134), reused 145 (delta 84), pack-reused 56474 Receiving objects: 100% (56705/56705), 27.96 MiB | 34.92 MiB/s, done. Resolving deltas: 100% (44177/44177), done. $ ls nautobot/ CHANGELOG.md README.md docs nautobot.code-workspace site CONTRIBUTING.md contrib manage.py poetry.lock tasks.py LICENSE.txt development mkdocs.yml pyproject.toml upgrade.sh NOTICE dist nautobot scripts","title":"Forking the Repo"},{"location":"development/getting-started.html#about-remote-repos","text":"Git refers to remote repositories as remotes . When you make your initial clone of your fork, Git defaults to naming this remote origin . Throughout this documentation, the following remote names will be used: origin - The default remote name used to refer to your fork of Nautobot upstream - The main remote used to refer to the official Nautobot repository","title":"About Remote Repos"},{"location":"development/getting-started.html#setting-up-your-remotes","text":"Remote repos are managed using the git remote command. Upon cloning Nautobot for the first time, you will have only a single remote: $ git remote -v origin git@github.com:yourusername/nautobot.git (fetch) origin git@github.com:yourusername/nautobot.git (push) Add the official Nautobot repo as a the upstream remote: $ git remote add upstream git@github.com:nautobot/nautobot.git View your remotes again to confirm you've got both origin pointing to your fork and upstream pointing to the official repo: $ git remote -v origin git@github.com:yourusername/nautobot.git (fetch) origin git@github.com:yourusername/nautobot.git (push) upstream git@github.com:nautobot/nautobot.git (fetch) upstream git@github.com:nautobot/nautobot.git (push) You're now ready to proceed to the next steps. Hint You will always push changes to origin (your fork) and pull changes from upstream (official repo).","title":"Setting up your Remotes"},{"location":"development/getting-started.html#creating-a-branch","text":"Before you make any changes, always create a new branch. Again, for bug fixes and minor features, you'll want to create your branches from the develop branch, while for major new features, you'll branch from next instead. Before you ever create a new branch, always checkout the appropriate branch and make sure you you've got the latest changes from upstream : $ git checkout develop $ git pull upstream develop Warning If you do not do this, you run the risk of having merge conflicts in your branch, and that's never fun to deal with. Trust us on this one. Now that you've got the latest upstream changes, create your branch. It's convention to always prefix your branch name with your GitHub username or your initials, and suffix it with the issue number if appropriate, separated by hyphens. For example: $ git checkout -b yourusername-myfeature-1234","title":"Creating a Branch"},{"location":"development/getting-started.html#enabling-pre-commit-hooks","text":"Nautobot ships with a Git pre-commit hook script that automatically checks for style compliance and missing database migrations prior to committing changes. This helps avoid erroneous commits that result in CI test failures. Note This pre-commit hook currently only supports the Python Virtual Environment Workflow. You are encouraged to enable it by creating a link to scripts/git-hooks/pre-commit : $ cd .git/hooks/ $ ln -s ../../scripts/git-hooks/pre-commit","title":"Enabling Pre-Commit Hooks"},{"location":"development/getting-started.html#setting-up-your-development-environment","text":"Getting started with Nautobot development is pretty straightforward, and should feel very familiar to anyone with Django development experience. We can recommend either a Docker Compose workflow (if you don't want to install dependencies such as PostgreSQL and Redis directly onto your system) or a Python virtual environment workflow .","title":"Setting up your Development Environment"},{"location":"development/getting-started.html#windows-development","text":"Local development on Windows Subsystem for Linux (WSL) is not currently supported. When developing locally on Windows, we recommend using a virtual machine running an officially supported operating system .","title":"Windows Development"},{"location":"development/getting-started.html#docker-compose-workflow","text":"This workflow uses Docker and Docker Compose and assumes that you have them installed. For the Docker Compose workflow, Nautobot uses Invoke as a replacement for Make. Invoke was chosen because it is less arcane than make. Instead of a Makefile , Invoke reads the tasks.py in the project root. Note Although the Docker Compose workflow uses containers, it is important to note that the containers are running the local repository code on your machine. Changes you make to your local code will be picked up and executed by the containers.","title":"Docker Compose Workflow"},{"location":"development/getting-started.html#install-invoke","text":"Because it is used to execute all common Docker workflow tasks, Invoke must be installed for your user environment. On most systems, if you're installing without root/superuser permissions, the default will install into your local user environment. $ pip3 install invoke If you run into issues, you may also deliberately tell pip3 to install into your user environment by adding the --user flag: $ pip3 install --user invoke Please see the official documentation on Pip user installs for more information.","title":"Install Invoke"},{"location":"development/getting-started.html#list-invoke-tasks","text":"Now that you have an invoke command, list the tasks defined in tasks.py : $ invoke --list Available tasks: black Check Python code style with Black. build Build Nautobot docker image. build-and-check-docs Build docs for use within Nautobot. buildx Build Nautobot docker image using the experimental buildx docker functionality (multi-arch capablility). check-migrations Check for missing migrations. check-schema Render the REST API schema and check for problems. cli Launch a bash shell inside the running Nautobot (or other) Docker container. createsuperuser Create a new Nautobot superuser account (default: \"admin\"), will prompt for password. debug Start Nautobot and its dependencies in debug mode. destroy Destroy all containers and volumes. docker-push Tags and pushes docker images to the appropriate repos, intended for release use only. dumpdata Dump data from database to db_output file. flake8 Check for PEP8 compliance and other style issues. hadolint Check Dockerfile for hadolint compliance and other style issues. integration-test Run Nautobot integration tests. loaddata Load data from file. makemigrations Perform makemigrations operation in Django. markdownlint Lint Markdown files. migrate Perform migrate operation in Django. nbshell Launch an interactive nbshell session. post-upgrade Performs Nautobot common post-upgrade operations using a single entrypoint. pylint Perform static analysis of Nautobot code. restart Gracefully restart containers. start Start Nautobot and its dependencies in detached mode. stop Stop Nautobot and its dependencies. tests Run all linters and unit tests. unittest Run Nautobot unit tests. unittest-coverage Report on code test coverage as measured by 'invoke unittest'. vscode Launch Visual Studio Code with the appropriate Environment variables to run in a container.","title":"List Invoke Tasks"},{"location":"development/getting-started.html#using-docker-with-invoke","text":"A development environment can be easily started up from the root of the project using the following commands: invoke build - Builds Nautobot docker images invoke migrate - Performs database migration operation in Django invoke createsuperuser - Creates a superuser account for the Nautobot application invoke debug - Starts Docker containers for Nautobot, PostgreSQL, Redis, Celery, and the RQ worker in debug mode and attaches their output to the terminal in the foreground. You may enter Control-C to stop the containers Additional useful commands for the development environment: invoke start [-s servicename] - Starts Docker containers for Nautobot, PostgreSQL, Redis, Celery, and the RQ worker (or a specific container/service, such as invoke start -s redis ) to run in the background with debug disabled invoke cli [-s servicename] - Launch a bash shell inside the specified service container (if none is specified, defaults to the Nautobot container) invoke stop [-s servicename] - Stops all containers (or a specific container/service) created by invoke start Note The mkdocs container must be started manually with invoke start -s mkdocs . It will not start automatically with the invoke start or invoke debug commands. Tip To learn about advanced use cases within the Docker Compose workflow, see the Docker Compose Advanced Use Cases page. Proceed to the Working in your Development Environment section","title":"Using Docker with Invoke"},{"location":"development/getting-started.html#python-virtual-environment-workflow","text":"This workflow uses Python and Poetry to work with your development environment locally. It requires that you install the required system dependencies on your system yourself. There are a few things you'll need: A Linux system or environment A MySQL or PostgreSQL server, which can be installed locally per the documentation A Redis server, which can also be installed locally A supported version of Python A recent version of Poetry","title":"Python Virtual Environment Workflow"},{"location":"development/getting-started.html#install-poetry","text":"Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update/remove) them for you. It will also manage virtual environments automatically, and allow for publishing packages to the Python Package Index . You may install Poetry in your user environment by running: $ curl -sSL https://install.python-poetry.org | python3 - For detailed installation instructions, please see the official Poetry installation guide .","title":"Install Poetry"},{"location":"development/getting-started.html#install-hadolint","text":"Hadolint is a tool used to validate and lint Dockerfiles to ensure we are following best practices. On macOS with Homebrew you can install Hadolint by running: $ brew install hadolint","title":"Install Hadolint"},{"location":"development/getting-started.html#install-markdownlint-cli","text":"markdownlint-cli is a tool used to validate and lint Markdown files, such as Nautobot's documentation, to ensure that they are correctly constructed. On macOS with Homebrew you can install markdownlint-cli by running: $ brew install markdownlint-cli","title":"Install markdownlint-cli"},{"location":"development/getting-started.html#creating-a-python-virtual-environment","text":"A Python virtual environment (or virtualenv ) is like a container for a set of Python packages. A virtualenv allow you to build environments suited to specific projects without interfering with system packages or other projects. When installed per the documentation, Nautobot uses a virtual environment in production. For Nautobot development, we have selected Poetry, which will transparently create a virtualenv for you, automatically install all dependencies required for Nautobot to operate, and will also install the nautobot-server CLI command that you will utilize to interact with Nautobot from here on out. Bootstrap your virtual environment using poetry install : $ poetry install Hint If you are doing development or testing using MySQL, you may quickly install the mysqlclient library along with Nautobot by running poetry install --extras mysql . This will create automatically create a virtualenv in your home directory, which houses a virtual copy of the Python executable and its related libraries and tooling. When running Nautobot for development, it will be run using the Python binary at found within the virtualenv. Once created, you may activate the virtual environment using poetry shell : $ poetry shell Spawning shell within /home/example/.cache/pypoetry/virtualenvs/nautobot-Ams_xyDt-py3.8 $ . /home/example/.cache/pypoetry/virtualenvs/nautobot-Ams_xyDt-py3.8/bin/activate (nautobot-Ams_xyDt-py3.8) $ Notice that the console prompt changes to indicate the active environment. This updates the necessary system environment variables to ensure that any Python scripts are run within the virtual environment. Observe also that the python interpreter is bound within the virtualenv: (nautobot-Ams_xyDt-py3.8) $ which python /home/example/.cache/pypoetry/virtualenvs/nautobot-Ams_xyDt-py3.8/bin/python To exit the virtual shell, use exit : (nautobot-Ams_xyDt-py3.8) $ exit $","title":"Creating a Python Virtual Environment"},{"location":"development/getting-started.html#working-with-poetry","text":"Poetry automatically installs your dependencies. However, if you need to install any additional dependencies this can be done with pip . For example, if you really like using ipython for development: (nautobot-Ams_xyDt-py3.8) $ pip3 install ipython Collecting ipython Using cached ipython-7.20.0-py3-none-any.whl (784 kB) ... It may not always be convenient to enter into the virtual shell just to run programs. You may also execute a given command ad hoc within the project's virtual shell by using poetry run : $ poetry run mkdocs serve Check out the Poetry usage guide for more tips.","title":"Working with Poetry"},{"location":"development/getting-started.html#configuring-nautobot","text":"Note Unless otherwise noted, all following commands should be executed inside the virtualenv. Hint Use poetry shell to enter the virtualenv. Nautobot's configuration file is nautobot_config.py .","title":"Configuring Nautobot"},{"location":"development/getting-started.html#initializing-a-config","text":"You may also initialize a new configuration using nautobot-server init : $ nautobot-server init Configuration file created at '/home/example/.nautobot/nautobot_config.py' You may also specify alternate file locations. Please refer to Configuring Nautobot for how to do that.","title":"Initializing a Config"},{"location":"development/getting-started.html#using-the-development-config","text":"A nautobot_config.py suitable for development purposes can be found at development/nautobot_config.py . You may customize the values there or utilize environment variables to override the default values. If you want to use this file, initialize a config first, then copy this file to the default location Nautobot expects to find its config: $ cp development/nautobot_config.py ~/.nautobot/nautobot_config.py","title":"Using the Development Config"},{"location":"development/getting-started.html#required-settings","text":"A newly created configuration includes sane defaults. If you need to customize them, edit your nautobot_config.py and update the following settings as required: ALLOWED_HOSTS : This can be set to [\"*\"] for development purposes and must be set if DEBUG=False DATABASES : Database connection parameters, if different from the defaults Redis settings : Redis configuration requires multiple settings including CACHEOPS_REDIS and RQ_QUEUES . The defaults should be fine for development. DEBUG : Set to True to enable verbose exception logging and, if installed, the Django debug toolbar EXTRA_INSTALLED_APPS : Optionally provide a list of extra Django apps/plugins you may desire to use for development","title":"Required Settings"},{"location":"development/getting-started.html#working-in-your-development-environment","text":"Below are common commands for working your development environment.","title":"Working in your Development Environment"},{"location":"development/getting-started.html#creating-a-superuser","text":"You'll need to create a administrative superuser account to be able to log into the Nautobot Web UI for the first time. Specifying an email address for the user is not required, but be sure to use a very strong password. Docker Compose Workflow Virtual Environment Workflow invoke createsuperuser nautobot-server createsuperuser","title":"Creating a Superuser"},{"location":"development/getting-started.html#starting-the-development-server","text":"Django provides a lightweight HTTP/WSGI server for development use. The development server automatically reloads Python code for each request, as needed. You don\u2019t need to restart the server for code changes to take effect. However, some actions like adding files don\u2019t trigger a restart, so you\u2019ll have to restart the server in these cases. Danger DO NOT USE THIS SERVER IN A PRODUCTION SETTING. The development server is for development and testing purposes only. It is neither performant nor secure enough for production use. You can start the Nautobot development server with the invoke start command (if using Docker), or the nautobot-server runserver management command: Docker Compose Workflow Virtual Environment Workflow invoke start nautobot-server runserver For example: $ nautobot-server runserver Performing system checks... System check identified no issues (0 silenced). November 18, 2020 - 15:52:31 Django version 3.1, using settings 'nautobot.core.settings' Starting development server at http://127.0.0.1:8080/ Quit the server with CONTROL-C. Warning Do not use poetry run nautobot-server runserver as it will crash unless you also pass the --noreload flag, which somewhat defeats the purpose of using the development server. It is recommended to use nautobot-server runserver from within an active virtualenv (e.g. poetry shell ). This is a known issue with Django and Poetry . Please see the official Django documentation on runserver for more information. You can then log into the development server at localhost:8080 with the superuser you created.","title":"Starting the Development Server"},{"location":"development/getting-started.html#starting-the-interactive-shell","text":"Nautobot provides an interactive Python shell that sets up the server environment and gives you direct access to the database models for debugging. Nautobot extends this slightly to automatically import models and other utilities. Run the Nautobot interactive shell with invoke nbshell (Docker) or the nautobot-server nbshell management command: Docker Compose Workflow Virtual Environment Workflow invoke nbshell nautobot-server nbshell For example: $ nautobot-server nbshell ### Nautobot interactive shell (localhost) ### Python 3.9.1 | Django 3.1.3 | Nautobot 1.0.0b1 ### lsmodels() will show available models. Use help(<model>) for more info. >>>","title":"Starting the Interactive Shell"},{"location":"development/getting-started.html#post-upgrade-operations","text":"There will be times where you're working with the bleeding edge of Nautobot from the develop branch or feature branches and will need to pull in database changes or run server operations. Get into the habit of running nautobot-server post_upgrade (or invoke post-upgrade when using Docker) after you pull in a major set of changes from Nautobot, which performs a handful of common operations (such as migrate ) from a single command: Docker Compose Workflow Virtual Environment Workflow invoke post-upgrade nautobot-server post_upgrade Please see the documentation on the nautobot-server post_upgrade command for more information.","title":"Post-upgrade Operations"},{"location":"development/getting-started.html#reinstalling-nautobot","text":"Note This mostly applies to working with Nautobot in a virtualenv, since Docker containers are typically rebuilt when the code changes. Sometimes when files are renamed, moved, or deleted and you've been working in the same environment for a while, you can encounter weird behavior. If this happens, don't panic and nuke your environment. First, use pip3 to explicitly uninstall the Nautobot package from the environment: $ pip3 uninstall -y nautobot Found existing installation: nautobot 1.0.0b2 Uninstalling nautobot-1.0.0b2: Successfully uninstalled nautobot-1.0.0b2 Then try to just have Poetry do the right thing by telling it to install again: $ poetry install Installing dependencies from lock file No dependencies to install or update Installing the current project: nautobot (1.0.0-beta.2)","title":"Reinstalling Nautobot"},{"location":"development/getting-started.html#running-tests","text":"Throughout the course of development, it's a good idea to occasionally run Nautobot's test suite to catch any potential errors. Tests come in two primary flavors: Unit tests and integration tests.","title":"Running Tests"},{"location":"development/getting-started.html#unit-tests","text":"Unit tests are automated tests written and run to ensure that a section of the Nautobot application (known as the \"unit\") meets its design and behaves as intended and expected. Most commonly as a developer of or contributor to Nautobot you will be writing unit tests to exercise the code you have written. Unit tests are not meant to test how the application behaves, only the individual blocks of code, therefore use of mock data and phony connections is common in unit test code. As a guiding principle, unit tests should be fast, because they will be executed quite often. By Nautobot convention, unit tests must be tagged with unit . The base test case class nautobot.utilities.testing.TestCase has this tag, therefore any test cases inheriting from that class do not need to be explicitly tagged. All existing view and API test cases in the Nautobot test suite utilities inherit from this class. Warning New unit tests must always inherit from nautobot.utilities.testing.TestCase . Do not use django.test.TestCase . Wrong: from django.test import TestCase class MyTestCase ( TestCase ): ... Right: from nautobot.utilities.testing import TestCase class MyTestCase ( TestCase ): ... Unit tests are run using the invoke unittest command (if using the Docker development environment) or the nautobot-server test command: Docker Compose Workflow Virtual Environment Workflow invoke unittest nautobot-server --config=nautobot/core/tests/nautobot_config.py test nautobot Info By default invoke unittest will start and run the unit tests inside the Docker development container; this ensures that PostgreSQL and Redis servers are available during the test. However, if you have your environment configured such that nautobot-server can run locally, outside of the Docker environment, you may wish to set the environment variable INVOKE_NAUTOBOT_LOCAL=True to execute these tests in your local environment instead. See the Invoke configuration for more information. In cases where you haven't made any changes to the database (which is most of the time), you can append the --keepdb argument to this command to reuse the test database between runs. This cuts down on the time it takes to run the test suite since the database doesn't have to be rebuilt each time. Docker Compose Workflow Virtual Environment Workflow invoke unittest --keepdb nautobot-server --config=nautobot/core/tests/nautobot_config.py test --keepdb nautobot Note Using the --keepdb argument will raise errors if you've modified any model fields since the previous test run. Warning In some cases when tests fail and exit uncleanly it may leave the test database in an inconsistent state. If you encounter errors about missing objects, remove --keepdb and run the tests again.","title":"Unit Tests"},{"location":"development/getting-started.html#integration-tests","text":"Integration tests are automated tests written and run to ensure that the Nautobot application behaves as expected when being used as it would be in practice. By contrast to unit tests, where individual units of code are being tested, integration tests rely upon the server code actually running, and web UI clients or API clients to make real connections to the service to exercise actual workflows, such as navigating to the login page, filling out the username/passwords fields, and clicking the \"Log In\" button. Integration testing is much more involved, and builds on top of the foundation laid by unit testing. As a guiding principle, integration tests should be comprehensive, because they are the last mile to asserting that Nautobot does what it is advertised to do. Without integration testing, we have to do it all manually, and that's no fun for anyone! Running integrations tests requires the use of Docker at this time. They can be directly invoked using nautobot-server test just as unit tests can, however, a headless Firefox browser provided by Selenium is required. Because Selenium installation and setup is complicated, we have included a configuration for this to work out of the box using Docker. The Selenium container is running a standalone, headless Firefox \"web driver\" browser that can be remotely controlled by Nautobot for use in integration testing. Before running integration tests, the selenium container must be running. If you are using the Docker Compose workflow, it is automatically started for you. For the Virtual Environment workflow, you must start it manually. Docker Compose Workflow Virtual Environment Workflow (automatic) invoke start --service selenium By Nautobot convention, integration tests must be tagged with integration . The base test case class nautobot.utilities.testing.integration.SeleniumTestCase has this tag, therefore any test cases inheriting from that class do not need to be explicitly tagged. All existing integration test cases in the Nautobot test suite utilities inherit from this class. Warning New integration tests must always inherit from nautobot.utilities.testing.integration.SeleniumTestCase and added in the integration directory in the tests directory of an inner Nautobot application. Do not use any other base class for integration tests. We never want to risk running the unit tests and integration tests at the same time. The isolation from each other is critical to a clean and manageable continuous development cycle. Wrong: from django.contrib.staticfiles.testing import StaticLiveServerTestCase class MyIntegrationTestCase ( StaticLiveServerTestCase ): ... Right: from nautobot.utilities.testing.integration import SeleniumTestCase class MyIntegrationTestCase ( SeleniumTestCase ): ... Integration tests are run using the invoke integration-test command. All integration tests must inherit from nautobot.utilities.testing.integration.SeleniumTestCase , which itself is tagged with integration . A custom test runner has been implemented to automatically skip any test case tagged with integration by default, so normal unit tests run without any concern. To run the integration tests the --tag integration argument must be passed to nautobot-server test . Docker Compose Workflow Virtual Environment Workflow invoke integration-test nautobot-server --config=nautobot/core/tests/nautobot_config.py test --tag integration nautobot Info The same arguments supported by invoke unittest are supported by invoke integration-test . The key difference being the dependency upon the Selenium container, and inclusion of the integration tag. Tip You may also use invoke integration-test in the Virtual Environment workflow given that the selenium container is running, and that the INVOKE_NAUTOBOT_LOCAL=True environment variable has been set.","title":"Integration Tests"},{"location":"development/getting-started.html#customizing-integration-test-executions","text":"The following environment variables can be provided when running tests to customize where Nautobot looks for Selenium and where Selenium looks for Nautobot. If using the default setup documented above, there is no need to customize these. NAUTOBOT_SELENIUM_URL - The URL used by the Nautobot test runner to remotely control the headless Selenium Firefox node. You can provide your own, but it must be a Remote WebDriver . (Default: http://localhost:4444/wd/hub ; for Docker: http://selenium:4444/wd/hub ) NAUTOBOT_SELENIUM_HOST - The hostname used by the Selenium WebDriver to access Nautobot using Firefox. (Default: host.docker.internal ; for Docker: nautobot )","title":"Customizing Integration Test Executions"},{"location":"development/getting-started.html#verifying-the-rest-api-schema","text":"If you make changes to the REST API, you should verify that the REST API OpenAPI schema renders correctly without errors. To verify that there are no errors, you can run the invoke check-schema command (if using the Docker development environment) or the nautobot-server spectacular command. In the latter case you should run the command for each supported REST API version that Nautobot provides (e.g. \"1.2\", \"1.3\") Docker Compose Workflow Virtual Environment Workflow invoke check-schema nautobot-server spectacular --api-version 1.2 --validate --fail-on-warn --file /dev/null","title":"Verifying the REST API Schema"},{"location":"development/getting-started.html#verifying-code-style-and-static-analysis","text":"To enforce best practices around consistent coding style , Nautobot uses Flake8 and Black . Additionally, static analysis of Nautobot code is performed by Pylint . You should run all of these commands and ensure that they pass fully with regard to your code changes before opening a pull request upstream. Docker Compose Workflow Virtual Environment Workflow invoke flake8 flake8 invoke black black invoke pylint nautobot-server pylint nautobot tasks.py && nautobot-server pylint --recursive development/ examples/","title":"Verifying Code Style and Static Analysis"},{"location":"development/getting-started.html#handling-migrations","text":"If you're unsure whether a database schema migration is needed based on your changes, you can run the following command: Docker Compose Workflow Virtual Environment Workflow invoke check-migrations nautobot-server --config=nautobot/core/tests/nautobot_config.py makemigrations --dry-run --check If your branch modifies a Django model (and as a result requires a database schema modification), please be sure to provide a meaningful name to the migration before pushing. If you have yet to run invoke makemigrations , you can pass in a name for the migration with the -n option, example invoke makemigrations -n provider_increase_account_length . If you have already run invoke makemigrations , rename the generated migration files, for example 0004_provider_increase_account_length instead of 0004_auto_20211220_2104 . You\u2019ll also want to run black against the generated migration file as the autogenerated code doesn\u2019t follow our style guide by default. When modifying model field attributes, modify the test data in the tests too to reflect these changes and also any forms which refer to the model.","title":"Handling Migrations"},{"location":"development/getting-started.html#working-on-documentation","text":"Some features require documentation updates or new documentation to be written. The documentation files can be found in the docs directory. To preview these changes locally, you can use mkdocs . For substantial changes to the code (including new features, removal of existing features, or significant changes in behavior) you should always make corresponding documentation updates. Nautobot's documentation pipeline includes a custom plugin for mkdocs that adds a few useful macros for annotating such changes: +++ 1.4.3 , on a line by itself, is a shorthand for !!! version-added \"Added in version 1.4.3\" +/- 1.4.3 , on a line by itself, is a shorthand for !!! version-changed \"Changed in version 1.4.3\" --- 1.4.3 , on a line by itself, is a shorthand for !!! version-removed \"Removed in version 1.4.3\" These admonitions in turn appear in the rendered documentation as follows: Added in version 1.4.3 Changed in version 1.4.3 Removed in version 1.4.3 You can also add text to any of these admonitions for further clarity, for example: +++ 1.4.3 The custom `mkdocs` plugin was added. will render as: Added in version 1.4.3 The custom mkdocs plugin was added. Caution While you can use the version-added / version-changed / version-removed admonitions directly to add a custom title to a specific admonition, in general, you should use the macros for consistency across the documentation.","title":"Working on Documentation"},{"location":"development/getting-started.html#writing-documentation","text":"You can preview the documentation using the server built into mkdocs, which should start a web server at http://localhost:8001 . Docker Compose Workflow Virtual Environment Workflow invoke start -s mkdocs mkdocs serve Documentation is written in Markdown. If you need to add additional pages or sections to the documentation, you can add them to mkdocs.yml at the root of the repository.","title":"Writing Documentation"},{"location":"development/getting-started.html#verifying-documentation","text":"Nautobot uses markdownlint-cli to verify correctness of the documentation. You should run this command and ensure that it passes fully with regard to your documentation changes before opening a pull request upstream. Docker Compose Workflow Virtual Environment Workflow invoke markdownlint markdownlint --ignore nautobot/project-static --config .markdownlint.yml nautobot examples *.md","title":"Verifying Documentation"},{"location":"development/getting-started.html#submitting-pull-requests","text":"Once you're happy with your work and have verified that all tests pass, commit your changes and push it upstream to your fork. Always provide descriptive (but not excessively verbose) commit messages. When working on a specific issue, be sure to reference it. $ git commit -m \"Closes #1234: Add IPv5 support\" $ git push origin Once your fork has the new commit, submit a pull request to the Nautobot repo to propose the changes. Be sure to provide a detailed accounting of the changes being made and the reasons for doing so. Once submitted, a maintainer will review your pull request and either merge it or request changes. If changes are needed, you can make them via new commits to your fork: The pull request will update automatically. Note Remember, pull requests are entertained only for accepted issues. If an issue you want to work on hasn't been approved by a maintainer yet, it's best to avoid risking your time and effort on a change that might not be accepted.","title":"Submitting Pull Requests"},{"location":"development/getting-started.html#troubleshooting","text":"Below are common issues you might encounter in your development environment and how to address them.","title":"Troubleshooting"},{"location":"development/getting-started.html#fatal-sorry-too-many-clients-already","text":"When using nautobot-server runserver to do development you might run into a traceback that looks something like this: Exception Type: OperationalError at /extras/tags/ Exception Value: FATAL: sorry, too many clients already The runserver development server is multi-threaded by default, which means that every request is creating its own connection. If you are doing some local testing or development that is resulting in a lot of connections to the database, pass --nothreading to the runserver command to disable threading: $ nautobot-server runserver --nothreading","title":"FATAL: sorry, too many clients already"},{"location":"development/homepage.html","text":"Populating the Home Page \u00b6 Added in version 1.2.0 Both core applications and plugins can contribute items to the Nautobot home page by defining layout inside of their app's homepage.py . Using a key and weight system, a developer can integrate amongst existing home page panels or can create entirely new panels as desired. Adding a new Home Page Panel \u00b6 Each panel on the home page is defined by a HomePagePanel object. A HomePagePanel may contain either or both of HomePageItem and/or HomePageGroup objects, or may define custom content via a referenced Django template. A HomePageGroup may itself contain HomePageItem objects as well, and individual HomePageItem objects may also reference custom Django templates. Some examples: This is a single HomePagePanel (defined in nautobot/dcim/homepage.py ) containing four HomePageItem and one HomePageGroup (the Connections group, which in turn contains four more HomePageItem ). Using these objects together allows you to create panels that match the visual style of most other panels on the Nautobot home page. This is a HomePagePanel (defined in nautobot/extras/homepage.py ) that uses a custom template to render content that doesn't fit into the HomePageGroup / HomePageItem pattern. The position of a panel in the home page is defined by its weight . The lower the weight the closer to the start (top/left) of the home page the object will be. All core objects have weights in multiples of 100, meaning there is plenty of space around the objects for plugins to customize. In the below code example, you can see that the Example Plugin panel has a weight value of 150 . This means it will appear between Organization (weight 100 ) and DCIM (weight 200 ). Tip Weights for already existing items can be found in the Nautobot source code (in nautobot/<app>/homepage.py ) or with a web session open to your Nautobot instance, you can inspect an element of the home page using the developer tools. Example of custom code being used in a panel can be seen in the Custom Example Plugin panel below. The attribute custom_template is used to refer to the filename of a template. Templates need to be stored in the templates inc folder for the plugin ( /example_plugin/templates/example_plugin/inc/ ). If additional data is needed to render the custom template, callback functions can be used to collect this data. In the below example, the Custom Example Plugin panel is using the callback get_example_data() to dynamically populate the key example_data into the rendering context of this panel. from nautobot.core.apps import HomePageItem , HomePagePanel from .models import ExampleModel def get_example_data ( request ): return ExampleModel . objects . all () layout = ( HomePagePanel ( name = \"Example Plugin\" , weight = 150 , items = ( HomePageItem ( name = \"Example Models\" , link = \"plugins:example_plugin:examplemodel_list\" , description = \"List example plugin models.\" , permissions = [ \"example_plugin.view_examplemodel\" ], weight = 100 , ), ), ), HomePagePanel ( name = \"Custom Example Plugin\" , custom_template = \"panel_example_example.html\" , custom_data = { \"example_data\" : get_example_data }, permissions = [ \"example_plugin.view_examplemodel\" ], weight = 350 , ), )","title":"Home Page Panels"},{"location":"development/homepage.html#populating-the-home-page","text":"Added in version 1.2.0 Both core applications and plugins can contribute items to the Nautobot home page by defining layout inside of their app's homepage.py . Using a key and weight system, a developer can integrate amongst existing home page panels or can create entirely new panels as desired.","title":"Populating the Home Page"},{"location":"development/homepage.html#adding-a-new-home-page-panel","text":"Each panel on the home page is defined by a HomePagePanel object. A HomePagePanel may contain either or both of HomePageItem and/or HomePageGroup objects, or may define custom content via a referenced Django template. A HomePageGroup may itself contain HomePageItem objects as well, and individual HomePageItem objects may also reference custom Django templates. Some examples: This is a single HomePagePanel (defined in nautobot/dcim/homepage.py ) containing four HomePageItem and one HomePageGroup (the Connections group, which in turn contains four more HomePageItem ). Using these objects together allows you to create panels that match the visual style of most other panels on the Nautobot home page. This is a HomePagePanel (defined in nautobot/extras/homepage.py ) that uses a custom template to render content that doesn't fit into the HomePageGroup / HomePageItem pattern. The position of a panel in the home page is defined by its weight . The lower the weight the closer to the start (top/left) of the home page the object will be. All core objects have weights in multiples of 100, meaning there is plenty of space around the objects for plugins to customize. In the below code example, you can see that the Example Plugin panel has a weight value of 150 . This means it will appear between Organization (weight 100 ) and DCIM (weight 200 ). Tip Weights for already existing items can be found in the Nautobot source code (in nautobot/<app>/homepage.py ) or with a web session open to your Nautobot instance, you can inspect an element of the home page using the developer tools. Example of custom code being used in a panel can be seen in the Custom Example Plugin panel below. The attribute custom_template is used to refer to the filename of a template. Templates need to be stored in the templates inc folder for the plugin ( /example_plugin/templates/example_plugin/inc/ ). If additional data is needed to render the custom template, callback functions can be used to collect this data. In the below example, the Custom Example Plugin panel is using the callback get_example_data() to dynamically populate the key example_data into the rendering context of this panel. from nautobot.core.apps import HomePageItem , HomePagePanel from .models import ExampleModel def get_example_data ( request ): return ExampleModel . objects . all () layout = ( HomePagePanel ( name = \"Example Plugin\" , weight = 150 , items = ( HomePageItem ( name = \"Example Models\" , link = \"plugins:example_plugin:examplemodel_list\" , description = \"List example plugin models.\" , permissions = [ \"example_plugin.view_examplemodel\" ], weight = 100 , ), ), ), HomePagePanel ( name = \"Custom Example Plugin\" , custom_template = \"panel_example_example.html\" , custom_data = { \"example_data\" : get_example_data }, permissions = [ \"example_plugin.view_examplemodel\" ], weight = 350 , ), )","title":"Adding a new Home Page Panel"},{"location":"development/navigation-menu.html","text":"Populating the Navigation Menu \u00b6 Both core applications and plugins can contribute items to the navigation menu by defining menu_items inside of their app's navigation.py . Using the key and weight system, a developer can integrate amongst existing menu tabs, groups, items and buttons and/or create entirely new menus as desired. Modifying Existing Menu \u00b6 By defining an object with the same identifier, a developer can modify existing objects. The example below shows modifying an existing tab to have a new group. A tab object is being created with the same identifier as an existing object using the name attribute. Then a group is being created with a weight of 150 , which means it will appear between the already defined Circuits and Provider groups. Tip Weights for already existing items can be found in the nautobot source code (in navigation.py ) or with a web session open to your nautobot instance, you can inspect an element of the navbar using the developer tools. Each type of element will have an attribute data-{type}-weight . The type can be tab , group , item or button . This pattern works for modifying all objects in the tree. New items can be added to existing groups and new buttons can be added to existing items. menu_tabs = ( NavMenuTab ( name = \"Circuits\" , groups = ( NavMenuGroup ( name = \"Example Circuit Group\" , weight = 150 , items = ( NavMenuItem ( link = \"plugins:example_plugin:examplemodel_list\" , name = \"Example Model\" , permissions = [ \"example_plugin.view_examplemodel\" ], buttons = ( NavMenuAddButton ( link = \"plugins:example_plugin:examplemodel_add\" , permissions = [ \"example_plugin.add_examplemodel\" , ], ), NavMenuImportButton ( link = \"plugins:example_plugin:examplemodel_import\" , permissions = [ \"example_plugin.add_examplemodel\" ], ), ), ), ), ), ), ), ) Adding a New Menu \u00b6 The code below shows how to add a new tab to the navbar. A tab is defined by a NavMenuTab object. Similarly a group is defined using NavMenuGroup . Both of these objects are used as containers for actual items. The position in the navigation menu is defined by the weight. The lower the weight the closer to the start of the menus the object will be. All core objects have weights in multiples of 100, meaning there is plenty of space around the objects for plugins to customize. Below you can see Example Tab has a weight value of 150 . This means the tab will appear between Organization and Devices . from nautobot.core.apps import NavMenuAddButton , NavMenuGroup , NavMenuItem , NavMenuImportButton , NavMenuTab menu_items = ( NavMenuTab ( name = \"Example Tab\" , weight = 150 , groups = ( NavMenuGroup ( name = \"Example Group 1\" , weight = 100 , items = ( NavMenuItem ( link = \"plugins:example_plugin:examplemodel_list\" , link_text = \"Example Model\" , permissions = [ \"example_plugin.view_examplemodel\" ], buttons = ( NavMenuAddButton ( link = \"plugins:example_plugin:examplemodel_add\" , permissions = [ \"example_plugin.add_examplemodel\" , ], ), NavMenuImportButton ( link = \"plugins:example_plugin:examplemodel_import\" , permissions = [ \"example_plugin.add_examplemodel\" ], ), ), ), ), ), ), ), ) Classes and Attributes \u00b6 Note For the NavMenuTab , NavMenuGroup , and NavMenuItem objects to be hidden when the user does not have permissions, set HIDE_RESTRICTED_UI = True in the nautobot_config.py . Please refer to HIDE_RESTRICTED_UI A NavMenuTab has the following attributes: name - Display name to be shown in navigation menu weight - Defines the position the object should be displayed at (optional) permissions - A list of permissions required to display this link (optional) groups - List or tuple of NavMenuGroup A NavMenuGroup has the following attributes: name - Display name to be shown in navigation menu weight - Defines the position the object should be displayed at (optional) permissions - A list of permissions required to display this link (optional) items - List or tuple of NavMenuItem A NavMenuItem has the following attributes: link - The name of the URL path to which this menu item links name - The text presented to the user weight - Defines the position the object should be displayed at (optional) permissions - A list of permissions required to display this link (optional) buttons - An iterable of NavMenuButton (or subclasses of NavMenuButton) instances to display (optional) Note Any buttons associated within a menu item will be hidden if the user does not have permission to access the menu item, regardless of what permissions are set on the buttons. A NavMenuButton has the following attributes: title - The tooltip text (displayed when the mouse hovers over the button) link - The name of the URL path to which this button links weight - Defines the position the object should be displayed at (optional) icon_class - Button icon CSS classes (Nautobot currently supports Material Design Icons or one of the choices provided by ButtonActionIconChoices ) button_class - One of the choices provided by ButtonActionColorChoices (optional) permissions - A list of permissions required to display this button (optional) Note NavMenuAddButton and NavMenuImportButton are subclasses of NavMenuButton that can be used to provide the commonly used \"Add\" and \"Import\" buttons.","title":"Navigation Menu"},{"location":"development/navigation-menu.html#populating-the-navigation-menu","text":"Both core applications and plugins can contribute items to the navigation menu by defining menu_items inside of their app's navigation.py . Using the key and weight system, a developer can integrate amongst existing menu tabs, groups, items and buttons and/or create entirely new menus as desired.","title":"Populating the Navigation Menu"},{"location":"development/navigation-menu.html#modifying-existing-menu","text":"By defining an object with the same identifier, a developer can modify existing objects. The example below shows modifying an existing tab to have a new group. A tab object is being created with the same identifier as an existing object using the name attribute. Then a group is being created with a weight of 150 , which means it will appear between the already defined Circuits and Provider groups. Tip Weights for already existing items can be found in the nautobot source code (in navigation.py ) or with a web session open to your nautobot instance, you can inspect an element of the navbar using the developer tools. Each type of element will have an attribute data-{type}-weight . The type can be tab , group , item or button . This pattern works for modifying all objects in the tree. New items can be added to existing groups and new buttons can be added to existing items. menu_tabs = ( NavMenuTab ( name = \"Circuits\" , groups = ( NavMenuGroup ( name = \"Example Circuit Group\" , weight = 150 , items = ( NavMenuItem ( link = \"plugins:example_plugin:examplemodel_list\" , name = \"Example Model\" , permissions = [ \"example_plugin.view_examplemodel\" ], buttons = ( NavMenuAddButton ( link = \"plugins:example_plugin:examplemodel_add\" , permissions = [ \"example_plugin.add_examplemodel\" , ], ), NavMenuImportButton ( link = \"plugins:example_plugin:examplemodel_import\" , permissions = [ \"example_plugin.add_examplemodel\" ], ), ), ), ), ), ), ), )","title":"Modifying Existing Menu"},{"location":"development/navigation-menu.html#adding-a-new-menu","text":"The code below shows how to add a new tab to the navbar. A tab is defined by a NavMenuTab object. Similarly a group is defined using NavMenuGroup . Both of these objects are used as containers for actual items. The position in the navigation menu is defined by the weight. The lower the weight the closer to the start of the menus the object will be. All core objects have weights in multiples of 100, meaning there is plenty of space around the objects for plugins to customize. Below you can see Example Tab has a weight value of 150 . This means the tab will appear between Organization and Devices . from nautobot.core.apps import NavMenuAddButton , NavMenuGroup , NavMenuItem , NavMenuImportButton , NavMenuTab menu_items = ( NavMenuTab ( name = \"Example Tab\" , weight = 150 , groups = ( NavMenuGroup ( name = \"Example Group 1\" , weight = 100 , items = ( NavMenuItem ( link = \"plugins:example_plugin:examplemodel_list\" , link_text = \"Example Model\" , permissions = [ \"example_plugin.view_examplemodel\" ], buttons = ( NavMenuAddButton ( link = \"plugins:example_plugin:examplemodel_add\" , permissions = [ \"example_plugin.add_examplemodel\" , ], ), NavMenuImportButton ( link = \"plugins:example_plugin:examplemodel_import\" , permissions = [ \"example_plugin.add_examplemodel\" ], ), ), ), ), ), ), ), )","title":"Adding a New Menu"},{"location":"development/navigation-menu.html#classes-and-attributes","text":"Note For the NavMenuTab , NavMenuGroup , and NavMenuItem objects to be hidden when the user does not have permissions, set HIDE_RESTRICTED_UI = True in the nautobot_config.py . Please refer to HIDE_RESTRICTED_UI A NavMenuTab has the following attributes: name - Display name to be shown in navigation menu weight - Defines the position the object should be displayed at (optional) permissions - A list of permissions required to display this link (optional) groups - List or tuple of NavMenuGroup A NavMenuGroup has the following attributes: name - Display name to be shown in navigation menu weight - Defines the position the object should be displayed at (optional) permissions - A list of permissions required to display this link (optional) items - List or tuple of NavMenuItem A NavMenuItem has the following attributes: link - The name of the URL path to which this menu item links name - The text presented to the user weight - Defines the position the object should be displayed at (optional) permissions - A list of permissions required to display this link (optional) buttons - An iterable of NavMenuButton (or subclasses of NavMenuButton) instances to display (optional) Note Any buttons associated within a menu item will be hidden if the user does not have permission to access the menu item, regardless of what permissions are set on the buttons. A NavMenuButton has the following attributes: title - The tooltip text (displayed when the mouse hovers over the button) link - The name of the URL path to which this button links weight - Defines the position the object should be displayed at (optional) icon_class - Button icon CSS classes (Nautobot currently supports Material Design Icons or one of the choices provided by ButtonActionIconChoices ) button_class - One of the choices provided by ButtonActionColorChoices (optional) permissions - A list of permissions required to display this button (optional) Note NavMenuAddButton and NavMenuImportButton are subclasses of NavMenuButton that can be used to provide the commonly used \"Add\" and \"Import\" buttons.","title":"Classes and Attributes"},{"location":"development/release-checklist.html","text":"Release Checklist \u00b6 This document is intended for Nautobot maintainers and covers the steps to perform when releasing new versions. Minor Version Bumps \u00b6 Update Requirements \u00b6 Required Python packages are maintained in two files: pyproject.toml and poetry.lock . The pyproject.toml file \u00b6 Python packages are defined inside of pyproject.toml . The [tool.poetry.dependencies] section of this file contains a list of all the packages required by Nautobot. Where possible, we use tilde requirements to specify a minimal version with some ability to update, for example: # REST API framework djangorestframework = \"~3.12.2\" This would allow Poetry to install djangorestframework versions >=3.12.2 but <3.13.0 . The poetry.lock file \u00b6 The other file is poetry.lock , which is managed by Poetry and contains package names, versions, and other metadata. Each of the required packages pinned to its current stable version. When Nautobot is installed, this file is used to resolve and install all dependencies listed in pyproject.toml , but Poetry will use the exact versions found in poetry.lock to ensure that a new release of a dependency doesn't break Nautobot. Warning You must never directly edit this file. You will use poetry update commands to manage it. Run poetry update \u00b6 Every minor version release should refresh poetry.lock , so that it lists the most recent stable release of each package. To do this: Review each requirement's release notes for any breaking or otherwise noteworthy changes. Run poetry update <package> to update the package versions in poetry.lock as appropriate. If a required package requires updating to a new release not covered in the version constraints for a package as defined in pyproject.toml , (e.g. Django ~3.1.7 would never install Django >=4.0.0 ), update it manually in pyproject.toml . Run poetry install to install the refreshed versions of all required packages. Run all tests and check that the UI and API function as expected. Hint You may use poetry update --dry-run to have Poetry automatically tell you what package updates are available and the versions it would upgrade. Update Static Libraries \u00b6 Update the following static libraries to their most recent stable release: Bootstrap 3 Material Design Icons Select2 jQuery jQuery UI Link to the Release Notes Page \u00b6 Add the release notes ( docs/release-notes/X.Y.md ) to the table of contents within mkdocs.yml , and point index.md to the new file. Verify and Revise the Install Documentation \u00b6 Follow the install instructions to perform a new production installation of Nautobot. The goal of this step is to walk through the entire install process as documented to make sure nothing there needs to be changed or updated, to catch any errors or omissions in the documentation, and to ensure that it is current with each release. Tip Fire up mkdocs serve in your development environment to start the documentation server! This allows you to view the documentation locally and automatically rebuilds the documents as you make changes. Commit any necessary changes to the documentation before proceeding with the release. Close the Release Milestone \u00b6 Close the release milestone on GitHub after ensuring there are no remaining open issues associated with it. All Releases \u00b6 Verify CI Build Status \u00b6 Ensure that continuous integration testing on the develop branch is completing successfully. Bump the Version \u00b6 Update the package version using poetry version . This command shows the current version of the project or bumps the version of the project and writes the new version back to pyproject.toml if a valid bump rule is provided. The new version should ideally be a valid semver string or a valid bump rule: patch , minor , major , prepatch , preminor , premajor , prerelease . Always try to use a bump rule when you can. Display the current version with no arguments: $ poetry version nautobot 1.0.0-beta.2 Bump pre-release versions using prerelease : $ poetry version prerelease Bumping version from 1.0.0-beta.2 to 1.0.0-beta.3 For major versions, use major : $ poetry version major Bumping version from 1.0.0-beta.2 to 1.0.0 For patch versions, use minor : $ poetry version minor Bumping version from 1.0.0 to 1.1.0 And lastly, for patch versions, you guessed it, use patch : $ poetry version patch Bumping version from 1.1.0 to 1.1.1 Please see the official Poetry documentation on version for more information. Update the Changelog \u00b6 Create a release branch off of develop ( git checkout -b release-1.4.3 develop ) Generate release notes with towncrier build --version 1.4.3 and answer yes to the prompt Is it okay if I remove those files? [Y/n]: . This will update the release notes in nautobot/docs/release-notes/version-1.4.md , stage that file in git, and git rm all of the fragments that have now been incorporated into the release notes. Run invoke markdownlint to make sure the generated release notes pass the linter checks. Check the git diff to verify the changes are correct ( git diff --cached ). Commit and push the staged changes. Important The changelog must adhere to the Keep a Changelog style guide. Submit Pull Requests \u00b6 Submit a pull request to merge your release branch into develop . Once merged, submit another pull request titled \"Release vX.Y.Z\" to merge the develop branch into main . Copy the documented release notes into the pull request's body. Once CI has completed on the PR, merge it. Create a New Release \u00b6 Draft a new release with the following parameters. Tag: Current version (e.g. v1.0.0 ) Target: main Title: Version and date (e.g. v1.0.0 - 2021-06-01 ) Copy the description from the pull request to the release. Publish to PyPI \u00b6 Now that there is a tagged release, the final step is to upload the package to the Python Package Index. First, you'll need to render the documentation. poetry run mkdocs build --no-directory-urls --strict Second, you'll need to build the Python package distributions (which will include the rendered documentation): $ poetry build Finally, publish to PyPI using the username __token__ and the Nautobot PyPI API token as the password. The API token can be found in the Nautobot maintainers vault (if you're a maintainer, you'll have access to this vault): $ poetry publish --username __token__ --password <api_token> Publish Docker Images \u00b6 Build the images locally: for ver in 3.7 3.8 3.9 3.10; do export INVOKE_NAUTOBOT_PYTHON_VER=$ver invoke buildx --target final --tag networktocode/nautobot-py${INVOKE_NAUTOBOT_PYTHON_VER}:local invoke buildx --target final-dev --tag networktocode/nautobot-dev-py${INVOKE_NAUTOBOT_PYTHON_VER}:local done Test the images locally - to do this you need to set the following in your invoke.yml : --- nautobot : compose_files : - \"docker-compose.yml\" - \"docker-compose.build.yml\" Warning You should not include docker-compose.dev.yml in this test scenario! for ver in 3.7 3.8 3.9 3.10; do export INVOKE_NAUTOBOT_PYTHON_VER=$ver invoke stop invoke integration-tests done Push the images to GitHub Container Registry and Docker Hub docker login docker login ghcr.io for ver in 3.7 3.8 3.9 3.10; do export INVOKE_NAUTOBOT_PYTHON_VER=$ver invoke docker-push main done Bump the Development Version \u00b6 Use poetry version prepatch to bump the version to the next release and commit it to the develop branch. For example, if you just released v1.1.0 : $ poetry version prepatch Bumping version from 1.1.0 to 1.1.1-alpha.0","title":"Release Checklist"},{"location":"development/release-checklist.html#release-checklist","text":"This document is intended for Nautobot maintainers and covers the steps to perform when releasing new versions.","title":"Release Checklist"},{"location":"development/release-checklist.html#minor-version-bumps","text":"","title":"Minor Version Bumps"},{"location":"development/release-checklist.html#update-requirements","text":"Required Python packages are maintained in two files: pyproject.toml and poetry.lock .","title":"Update Requirements"},{"location":"development/release-checklist.html#the-pyprojecttoml-file","text":"Python packages are defined inside of pyproject.toml . The [tool.poetry.dependencies] section of this file contains a list of all the packages required by Nautobot. Where possible, we use tilde requirements to specify a minimal version with some ability to update, for example: # REST API framework djangorestframework = \"~3.12.2\" This would allow Poetry to install djangorestframework versions >=3.12.2 but <3.13.0 .","title":"The pyproject.toml file"},{"location":"development/release-checklist.html#the-poetrylock-file","text":"The other file is poetry.lock , which is managed by Poetry and contains package names, versions, and other metadata. Each of the required packages pinned to its current stable version. When Nautobot is installed, this file is used to resolve and install all dependencies listed in pyproject.toml , but Poetry will use the exact versions found in poetry.lock to ensure that a new release of a dependency doesn't break Nautobot. Warning You must never directly edit this file. You will use poetry update commands to manage it.","title":"The poetry.lock file"},{"location":"development/release-checklist.html#run-poetry-update","text":"Every minor version release should refresh poetry.lock , so that it lists the most recent stable release of each package. To do this: Review each requirement's release notes for any breaking or otherwise noteworthy changes. Run poetry update <package> to update the package versions in poetry.lock as appropriate. If a required package requires updating to a new release not covered in the version constraints for a package as defined in pyproject.toml , (e.g. Django ~3.1.7 would never install Django >=4.0.0 ), update it manually in pyproject.toml . Run poetry install to install the refreshed versions of all required packages. Run all tests and check that the UI and API function as expected. Hint You may use poetry update --dry-run to have Poetry automatically tell you what package updates are available and the versions it would upgrade.","title":"Run poetry update"},{"location":"development/release-checklist.html#update-static-libraries","text":"Update the following static libraries to their most recent stable release: Bootstrap 3 Material Design Icons Select2 jQuery jQuery UI","title":"Update Static Libraries"},{"location":"development/release-checklist.html#link-to-the-release-notes-page","text":"Add the release notes ( docs/release-notes/X.Y.md ) to the table of contents within mkdocs.yml , and point index.md to the new file.","title":"Link to the Release Notes Page"},{"location":"development/release-checklist.html#verify-and-revise-the-install-documentation","text":"Follow the install instructions to perform a new production installation of Nautobot. The goal of this step is to walk through the entire install process as documented to make sure nothing there needs to be changed or updated, to catch any errors or omissions in the documentation, and to ensure that it is current with each release. Tip Fire up mkdocs serve in your development environment to start the documentation server! This allows you to view the documentation locally and automatically rebuilds the documents as you make changes. Commit any necessary changes to the documentation before proceeding with the release.","title":"Verify and Revise the Install Documentation"},{"location":"development/release-checklist.html#close-the-release-milestone","text":"Close the release milestone on GitHub after ensuring there are no remaining open issues associated with it.","title":"Close the Release Milestone"},{"location":"development/release-checklist.html#all-releases","text":"","title":"All Releases"},{"location":"development/release-checklist.html#verify-ci-build-status","text":"Ensure that continuous integration testing on the develop branch is completing successfully.","title":"Verify CI Build Status"},{"location":"development/release-checklist.html#bump-the-version","text":"Update the package version using poetry version . This command shows the current version of the project or bumps the version of the project and writes the new version back to pyproject.toml if a valid bump rule is provided. The new version should ideally be a valid semver string or a valid bump rule: patch , minor , major , prepatch , preminor , premajor , prerelease . Always try to use a bump rule when you can. Display the current version with no arguments: $ poetry version nautobot 1.0.0-beta.2 Bump pre-release versions using prerelease : $ poetry version prerelease Bumping version from 1.0.0-beta.2 to 1.0.0-beta.3 For major versions, use major : $ poetry version major Bumping version from 1.0.0-beta.2 to 1.0.0 For patch versions, use minor : $ poetry version minor Bumping version from 1.0.0 to 1.1.0 And lastly, for patch versions, you guessed it, use patch : $ poetry version patch Bumping version from 1.1.0 to 1.1.1 Please see the official Poetry documentation on version for more information.","title":"Bump the Version"},{"location":"development/release-checklist.html#update-the-changelog","text":"Create a release branch off of develop ( git checkout -b release-1.4.3 develop ) Generate release notes with towncrier build --version 1.4.3 and answer yes to the prompt Is it okay if I remove those files? [Y/n]: . This will update the release notes in nautobot/docs/release-notes/version-1.4.md , stage that file in git, and git rm all of the fragments that have now been incorporated into the release notes. Run invoke markdownlint to make sure the generated release notes pass the linter checks. Check the git diff to verify the changes are correct ( git diff --cached ). Commit and push the staged changes. Important The changelog must adhere to the Keep a Changelog style guide.","title":"Update the Changelog"},{"location":"development/release-checklist.html#submit-pull-requests","text":"Submit a pull request to merge your release branch into develop . Once merged, submit another pull request titled \"Release vX.Y.Z\" to merge the develop branch into main . Copy the documented release notes into the pull request's body. Once CI has completed on the PR, merge it.","title":"Submit Pull Requests"},{"location":"development/release-checklist.html#create-a-new-release","text":"Draft a new release with the following parameters. Tag: Current version (e.g. v1.0.0 ) Target: main Title: Version and date (e.g. v1.0.0 - 2021-06-01 ) Copy the description from the pull request to the release.","title":"Create a New Release"},{"location":"development/release-checklist.html#publish-to-pypi","text":"Now that there is a tagged release, the final step is to upload the package to the Python Package Index. First, you'll need to render the documentation. poetry run mkdocs build --no-directory-urls --strict Second, you'll need to build the Python package distributions (which will include the rendered documentation): $ poetry build Finally, publish to PyPI using the username __token__ and the Nautobot PyPI API token as the password. The API token can be found in the Nautobot maintainers vault (if you're a maintainer, you'll have access to this vault): $ poetry publish --username __token__ --password <api_token>","title":"Publish to PyPI"},{"location":"development/release-checklist.html#publish-docker-images","text":"Build the images locally: for ver in 3.7 3.8 3.9 3.10; do export INVOKE_NAUTOBOT_PYTHON_VER=$ver invoke buildx --target final --tag networktocode/nautobot-py${INVOKE_NAUTOBOT_PYTHON_VER}:local invoke buildx --target final-dev --tag networktocode/nautobot-dev-py${INVOKE_NAUTOBOT_PYTHON_VER}:local done Test the images locally - to do this you need to set the following in your invoke.yml : --- nautobot : compose_files : - \"docker-compose.yml\" - \"docker-compose.build.yml\" Warning You should not include docker-compose.dev.yml in this test scenario! for ver in 3.7 3.8 3.9 3.10; do export INVOKE_NAUTOBOT_PYTHON_VER=$ver invoke stop invoke integration-tests done Push the images to GitHub Container Registry and Docker Hub docker login docker login ghcr.io for ver in 3.7 3.8 3.9 3.10; do export INVOKE_NAUTOBOT_PYTHON_VER=$ver invoke docker-push main done","title":"Publish Docker Images"},{"location":"development/release-checklist.html#bump-the-development-version","text":"Use poetry version prepatch to bump the version to the next release and commit it to the develop branch. For example, if you just released v1.1.0 : $ poetry version prepatch Bumping version from 1.1.0 to 1.1.1-alpha.0","title":"Bump the Development Version"},{"location":"development/style-guide.html","text":"Style Guide \u00b6 Nautobot generally follows the Django style guide , which is itself based on PEP 8 . The following tools are used to enforce coding style and best practices: Flake8 is used to validate code style. Black is used to enforce code formatting conventions. Pylint is used for Python static code analysis. Hadolint is used to lint and validate Docker best practices in the Dockerfile. MarkdownLint-cli is used to lint and validate Markdown (documentation) files. Nautobot-specific configuration of these tools is maintained in the files .flake8 , .markdownlint.yml , or pyproject.toml as appropriate to the individual tool. It is strongly recommended to include all of the above tools as part of your commit process before opening any pull request. A Git commit hook is provided in the source at scripts/git-hooks/pre-commit . Linking to this script from .git/hooks/ will invoke these tools prior to every commit attempt and abort if the validation fails. $ cd .git/hooks/ $ ln -s ../../scripts/git-hooks/pre-commit You can also invoke these utilities manually against the development Docker containers by running: invoke flake8 invoke black invoke check-migrations invoke hadolint invoke markdownlint invoke pylint Introducing New Dependencies \u00b6 The introduction of a new dependency is best avoided unless it is absolutely necessary. For small features, it's generally preferable to replicate functionality within the Nautobot code base rather than to introduce reliance on an external project. This reduces both the burden of tracking new releases and our exposure to outside bugs and attacks. If there's a strong case for introducing a new dependency, it must meet the following criteria: Its complete source code must be published and freely accessible without registration. Its license must be conducive to inclusion in an open source project. It must be actively maintained, with no longer than one year between releases. It must be available via the Python Package Index (PyPI). New dependencies can be added to the project via the poetry add command. This will correctly add the dependency to pyproject.toml as well as the poetry.lock file. You should then update the pyproject.toml with a comment providing a short description of the package and/or how Nautobot is making use of it. General Guidance \u00b6 When in doubt, remain consistent: It is better to be consistently incorrect than inconsistently correct. If you notice in the course of unrelated work a pattern that should be corrected, continue to follow the pattern for now and open a bug so that the entire code base can be evaluated at a later point. Prioritize readability over concision. Python is a very flexible language that typically offers several options for expressing a given piece of logic, but some may be more friendly to the reader than others. (List comprehensions are particularly vulnerable to over-optimization.) Always remain considerate of the future reader who may need to interpret your code without the benefit of the context within which you are writing it. No easter eggs. While they can be fun, Nautobot must be considered as a business-critical tool. The potential, however minor, for introducing a bug caused by unnecessary logic is best avoided entirely. Constants (variables which generally do not change) should be declared in constants.py within each app. Wildcard imports from the file are acceptable. Every model should have a docstring. Every custom method should include an explanation of its function. Nested API serializers generate minimal representations of an object. These are stored separately from the primary serializers to avoid circular dependencies. Always import nested serializers from other apps directly. For example, from within the DCIM app you would write from nautobot.ipam.api.nested_serializers import NestedIPAddressSerializer . The combination of nautobot.utilities.filters.BaseFilterSet , nautobot.extras.filters.CreatedUpdatedFilterSet , nautobot.extras.filters.CustomFieldModelFilterSet , and nautobot.extras.filters.RelationshipModelFilterSet is such a common use case throughout the code base that they have a helper class which combines all of these at nautobot.extras.NautobotFilterSet . Use this helper class if you need the functionality from these classes. The combination of nautobot.utilities.forms.BootstrapMixin , nautobot.extras.forms.CustomFieldModelFormMixin , nautobot.extras.forms.RelationshipModelFormMixin and nautobot.extras.forms.NoteModelFormMixin is such a common use case throughout the code base that they have a helper class which combines all of these at nautobot.extras.forms.NautobotModelForm . Use this helper class if you need the functionality from these classes. Added in version 1.4.0 Similarly, for filter forms, nautobot.extras.forms.NautobotFilterForm combines nautobot.utilities.forms.BootstrapMixin , nautobot.extras.forms.CustomFieldModelFilterFormMixin , and nautobot.extras.forms.RelationshipModelFilterFormMixin , and should be used where appropriate. Similarly, for bulk-edit forms, nautobot.extras.forms.NautobotBulkEditForm combines nautobot.utilities.forms.BulkEditForm and nautobot.utilities.forms.BootstrapMixin with nautobot.extras.forms.CustomFieldModelBulkEditFormMixin , nautobot.extras.forms.RelationshipModelBulkEditFormMixin and nautobot.extras.forms.NoteModelBulkEditFormMixin , and should be used where appropriate. API serializers for most models should inherit from nautobot.extras.api.serializers.NautobotModelSerializer and any appropriate mixins. Only use more abstract base classes such as ValidatedModelSerializer where absolutely required. NautobotModelSerializer will automatically add serializer fields for id , created / last_updated (if applicable), custom_fields , computed_fields , and relationships , so there's generally no need to explicitly declare these fields in .Meta.fields of each serializer class. Similarly, TaggedObjectSerializer and StatusModelSerializerMixin will automatically add the tags and status fields when included in a serializer class. API Views for most models should inherit from nautobot.extras.api.views.NautobotModelViewSet . Only use more abstract base classes such as ModelViewSet where absolutely required. Branding \u00b6 When referring to Nautobot in writing, use the proper form \"Nautobot,\" with the letter N. The lowercase form \"nautobot\" should be used in code, filenames, etc. There is an SVG form of the Nautobot logo at nautobot/docs/nautobot_logo.svg . It is preferred to use this logo for all purposes as it scales to arbitrary sizes without loss of resolution. If a raster image is required, the SVG logo should be converted to a PNG image of the prescribed size.","title":"Style Guide"},{"location":"development/style-guide.html#style-guide","text":"Nautobot generally follows the Django style guide , which is itself based on PEP 8 . The following tools are used to enforce coding style and best practices: Flake8 is used to validate code style. Black is used to enforce code formatting conventions. Pylint is used for Python static code analysis. Hadolint is used to lint and validate Docker best practices in the Dockerfile. MarkdownLint-cli is used to lint and validate Markdown (documentation) files. Nautobot-specific configuration of these tools is maintained in the files .flake8 , .markdownlint.yml , or pyproject.toml as appropriate to the individual tool. It is strongly recommended to include all of the above tools as part of your commit process before opening any pull request. A Git commit hook is provided in the source at scripts/git-hooks/pre-commit . Linking to this script from .git/hooks/ will invoke these tools prior to every commit attempt and abort if the validation fails. $ cd .git/hooks/ $ ln -s ../../scripts/git-hooks/pre-commit You can also invoke these utilities manually against the development Docker containers by running: invoke flake8 invoke black invoke check-migrations invoke hadolint invoke markdownlint invoke pylint","title":"Style Guide"},{"location":"development/style-guide.html#introducing-new-dependencies","text":"The introduction of a new dependency is best avoided unless it is absolutely necessary. For small features, it's generally preferable to replicate functionality within the Nautobot code base rather than to introduce reliance on an external project. This reduces both the burden of tracking new releases and our exposure to outside bugs and attacks. If there's a strong case for introducing a new dependency, it must meet the following criteria: Its complete source code must be published and freely accessible without registration. Its license must be conducive to inclusion in an open source project. It must be actively maintained, with no longer than one year between releases. It must be available via the Python Package Index (PyPI). New dependencies can be added to the project via the poetry add command. This will correctly add the dependency to pyproject.toml as well as the poetry.lock file. You should then update the pyproject.toml with a comment providing a short description of the package and/or how Nautobot is making use of it.","title":"Introducing New Dependencies"},{"location":"development/style-guide.html#general-guidance","text":"When in doubt, remain consistent: It is better to be consistently incorrect than inconsistently correct. If you notice in the course of unrelated work a pattern that should be corrected, continue to follow the pattern for now and open a bug so that the entire code base can be evaluated at a later point. Prioritize readability over concision. Python is a very flexible language that typically offers several options for expressing a given piece of logic, but some may be more friendly to the reader than others. (List comprehensions are particularly vulnerable to over-optimization.) Always remain considerate of the future reader who may need to interpret your code without the benefit of the context within which you are writing it. No easter eggs. While they can be fun, Nautobot must be considered as a business-critical tool. The potential, however minor, for introducing a bug caused by unnecessary logic is best avoided entirely. Constants (variables which generally do not change) should be declared in constants.py within each app. Wildcard imports from the file are acceptable. Every model should have a docstring. Every custom method should include an explanation of its function. Nested API serializers generate minimal representations of an object. These are stored separately from the primary serializers to avoid circular dependencies. Always import nested serializers from other apps directly. For example, from within the DCIM app you would write from nautobot.ipam.api.nested_serializers import NestedIPAddressSerializer . The combination of nautobot.utilities.filters.BaseFilterSet , nautobot.extras.filters.CreatedUpdatedFilterSet , nautobot.extras.filters.CustomFieldModelFilterSet , and nautobot.extras.filters.RelationshipModelFilterSet is such a common use case throughout the code base that they have a helper class which combines all of these at nautobot.extras.NautobotFilterSet . Use this helper class if you need the functionality from these classes. The combination of nautobot.utilities.forms.BootstrapMixin , nautobot.extras.forms.CustomFieldModelFormMixin , nautobot.extras.forms.RelationshipModelFormMixin and nautobot.extras.forms.NoteModelFormMixin is such a common use case throughout the code base that they have a helper class which combines all of these at nautobot.extras.forms.NautobotModelForm . Use this helper class if you need the functionality from these classes. Added in version 1.4.0 Similarly, for filter forms, nautobot.extras.forms.NautobotFilterForm combines nautobot.utilities.forms.BootstrapMixin , nautobot.extras.forms.CustomFieldModelFilterFormMixin , and nautobot.extras.forms.RelationshipModelFilterFormMixin , and should be used where appropriate. Similarly, for bulk-edit forms, nautobot.extras.forms.NautobotBulkEditForm combines nautobot.utilities.forms.BulkEditForm and nautobot.utilities.forms.BootstrapMixin with nautobot.extras.forms.CustomFieldModelBulkEditFormMixin , nautobot.extras.forms.RelationshipModelBulkEditFormMixin and nautobot.extras.forms.NoteModelBulkEditFormMixin , and should be used where appropriate. API serializers for most models should inherit from nautobot.extras.api.serializers.NautobotModelSerializer and any appropriate mixins. Only use more abstract base classes such as ValidatedModelSerializer where absolutely required. NautobotModelSerializer will automatically add serializer fields for id , created / last_updated (if applicable), custom_fields , computed_fields , and relationships , so there's generally no need to explicitly declare these fields in .Meta.fields of each serializer class. Similarly, TaggedObjectSerializer and StatusModelSerializerMixin will automatically add the tags and status fields when included in a serializer class. API Views for most models should inherit from nautobot.extras.api.views.NautobotModelViewSet . Only use more abstract base classes such as ModelViewSet where absolutely required.","title":"General Guidance"},{"location":"development/style-guide.html#branding","text":"When referring to Nautobot in writing, use the proper form \"Nautobot,\" with the letter N. The lowercase form \"nautobot\" should be used in code, filenames, etc. There is an SVG form of the Nautobot logo at nautobot/docs/nautobot_logo.svg . It is preferred to use this logo for all purposes as it scales to arbitrary sizes without loss of resolution. If a raster image is required, the SVG logo should be converted to a PNG image of the prescribed size.","title":"Branding"},{"location":"development/templates.html","text":"Page Templates \u00b6 Nautobot comes with a variety of page templates that allow for a lot of flexibility while keeping the page style consistent with the rest of the application. This document presents these templates and their features. You can use these templates as the basis for your templates by calling {% extends '<template_name>' %} at the top of your template file. Object Detail \u00b6 Added in version 1.2.0 The most customizable template is generic/object_detail.html , as object detail views have a wide range of specific requirements to be accommodated. It provides the following blocks: header : overloading this block allows for changing the entire top row of the page, including the title, breadcrumbs, search field, and tabs. breadcrumbs : overloading this block allows for changing the entire breadcrumbs block. extra_breadcrumbs : this enables extending the breadcrumbs block just before the model without having to redefine the entire block. buttons : overloading this block allows redefining the entire button section on the right of the page. extra_buttons : this block enables extending the buttons block without losing the predefined buttons. Custom buttons will appear between the plugin buttons and clone/edit/delete actions. masthead : is the block that contains the title. Overloading it enables to change anything about the title block. title : is the block contained by masthead and wrapped in a heading block. Overloading it makes it possible to change the heading text as well as the page title shown in the browser. nav_tabs : are the navigation tabs. If overloaded, custom tabs can be rendered instead of the default. extra_nav_tabs : this block allows to add new tabs without having to override the default ones. content : is the entire content of the page below the header . content_left_page : is a half-width column on the left. Multiple panels can be rendered in a single block. content_right_page : is a half-width column on the right. content_full_width_page : is a full-width column. Object List \u00b6 The base template for listing objects is generic/object_list.html , with the following blocks: buttons : may provide a set of buttons at the top right of the page, to the left of the table configuration button. sidebar : may implement a sidebar below the search form on the right. bulk_buttons : may be a set of buttons at the bottom of the table, to the left of potential bulk edit or delete buttons. Object Edit \u00b6 The base template for object addition or change is generic/object_edit.html , with the following blocks: form : is the block in which the form gets rendered. This can be overridden to provide a custom UI or UX for form views beyond what render_form provides. Object Import \u00b6 The base template for object import is generic/object_import.html , with the following blocks: tabs : may provide tabs at the top of the page. The default import view is not tabbed. Object Deletion \u00b6 The base template for object deletion is generic/object_delete.html , with the following blocks: message : is the confirmation message for deletion, which can be overridden. message_extra : provides a way to add to the default message without overriding it. Bulk Edit \u00b6 The base template for bulk object change is generic/object_bulk_edit.html . It does not provide any blocks for customizing the user experience. Bulk Import \u00b6 The base template for bulk object import is generic/object_bulk_import.html , with the following blocks: tabs : may provide tabs at the top of the page. The default import view is not tabbed. Bulk Deletion \u00b6 The base template for bulk object deletion is generic/object_bulk_delete.html , with the following blocks: message_extra : provides a way to add to the default message. Note : contrary to the deletion of a single object, this template does not provide a way to completely override the deletion message. Bulk Renaming \u00b6 The base template for renaming objects in bulk is generic/object_bulk_rename.html . It does not provide any blocks for customizing the user experience.","title":"Page Templates"},{"location":"development/templates.html#page-templates","text":"Nautobot comes with a variety of page templates that allow for a lot of flexibility while keeping the page style consistent with the rest of the application. This document presents these templates and their features. You can use these templates as the basis for your templates by calling {% extends '<template_name>' %} at the top of your template file.","title":"Page Templates"},{"location":"development/templates.html#object-detail","text":"Added in version 1.2.0 The most customizable template is generic/object_detail.html , as object detail views have a wide range of specific requirements to be accommodated. It provides the following blocks: header : overloading this block allows for changing the entire top row of the page, including the title, breadcrumbs, search field, and tabs. breadcrumbs : overloading this block allows for changing the entire breadcrumbs block. extra_breadcrumbs : this enables extending the breadcrumbs block just before the model without having to redefine the entire block. buttons : overloading this block allows redefining the entire button section on the right of the page. extra_buttons : this block enables extending the buttons block without losing the predefined buttons. Custom buttons will appear between the plugin buttons and clone/edit/delete actions. masthead : is the block that contains the title. Overloading it enables to change anything about the title block. title : is the block contained by masthead and wrapped in a heading block. Overloading it makes it possible to change the heading text as well as the page title shown in the browser. nav_tabs : are the navigation tabs. If overloaded, custom tabs can be rendered instead of the default. extra_nav_tabs : this block allows to add new tabs without having to override the default ones. content : is the entire content of the page below the header . content_left_page : is a half-width column on the left. Multiple panels can be rendered in a single block. content_right_page : is a half-width column on the right. content_full_width_page : is a full-width column.","title":"Object Detail"},{"location":"development/templates.html#object-list","text":"The base template for listing objects is generic/object_list.html , with the following blocks: buttons : may provide a set of buttons at the top right of the page, to the left of the table configuration button. sidebar : may implement a sidebar below the search form on the right. bulk_buttons : may be a set of buttons at the bottom of the table, to the left of potential bulk edit or delete buttons.","title":"Object List"},{"location":"development/templates.html#object-edit","text":"The base template for object addition or change is generic/object_edit.html , with the following blocks: form : is the block in which the form gets rendered. This can be overridden to provide a custom UI or UX for form views beyond what render_form provides.","title":"Object Edit"},{"location":"development/templates.html#object-import","text":"The base template for object import is generic/object_import.html , with the following blocks: tabs : may provide tabs at the top of the page. The default import view is not tabbed.","title":"Object Import"},{"location":"development/templates.html#object-deletion","text":"The base template for object deletion is generic/object_delete.html , with the following blocks: message : is the confirmation message for deletion, which can be overridden. message_extra : provides a way to add to the default message without overriding it.","title":"Object Deletion"},{"location":"development/templates.html#bulk-edit","text":"The base template for bulk object change is generic/object_bulk_edit.html . It does not provide any blocks for customizing the user experience.","title":"Bulk Edit"},{"location":"development/templates.html#bulk-import","text":"The base template for bulk object import is generic/object_bulk_import.html , with the following blocks: tabs : may provide tabs at the top of the page. The default import view is not tabbed.","title":"Bulk Import"},{"location":"development/templates.html#bulk-deletion","text":"The base template for bulk object deletion is generic/object_bulk_delete.html , with the following blocks: message_extra : provides a way to add to the default message. Note : contrary to the deletion of a single object, this template does not provide a way to completely override the deletion message.","title":"Bulk Deletion"},{"location":"development/templates.html#bulk-renaming","text":"The base template for renaming objects in bulk is generic/object_bulk_rename.html . It does not provide any blocks for customizing the user experience.","title":"Bulk Renaming"},{"location":"development/user-preferences.html","text":"User Preferences \u00b6 The users.User model holds individual preferences for each user in the form of JSON data in the config_data field. This page serves as a manifest of all recognized user preferences in Nautobot. Available Preferences \u00b6 Name Description extras.configcontext.format Preferred format when rendering config context data (JSON or YAML) pagination.per_page The number of items to display per page of a paginated table tables.TABLE_NAME.columns The ordered list of columns to display when viewing the table","title":"User Preferences"},{"location":"development/user-preferences.html#user-preferences","text":"The users.User model holds individual preferences for each user in the form of JSON data in the config_data field. This page serves as a manifest of all recognized user preferences in Nautobot.","title":"User Preferences"},{"location":"development/user-preferences.html#available-preferences","text":"Name Description extras.configcontext.format Preferred format when rendering config context data (JSON or YAML) pagination.per_page The number of items to display per page of a paginated table tables.TABLE_NAME.columns The ordered list of columns to display when viewing the table","title":"Available Preferences"},{"location":"docker/index.html","text":"Nautobot Docker Images \u00b6 Nautobot is packaged as a Docker image for use in a production environment. The published image is based on the python:3.7-slim image to maintain the most compatibility with Nautobot deployments. The Docker image and deployment strategies are being actively developed, check back here or join the #nautobot channel on Network to Code's Slack community for the most up to date information. Platforms \u00b6 Nautobot docker images are currently provided for both linux/amd64 and linux/arm64 architectures. Please note ARM64 support is untested by our automated tests and should be considered in an alpha state. Tags \u00b6 A set of Docker images are built for each Nautobot release and published to both Docker Hub and the GitHub Container Registry . Additionally, GitHub Actions are used to automatically build images corresponding to each commit to the develop and next branches; these images are only published to the GitHub Container Registry. To get a specific tagged image from Docker Hub or the GitHub Container Registry run: docker image pull networktocode/nautobot:${TAG} or docker pull ghcr.io/nautobot/nautobot:${TAG} The following tags are available on both Docker Hub and the GitHub Container Registry: Tag Nautobot Version Python Version Example ${NAUTOBOT_VER} As specified 3.7 1.2.7 ${NAUTOBOT_VER}-py${PYTHON_VER} As specified As specified 1.2.7-py3.8 ${NAUTOBOT_MAJOR_VER}.${NAUTOBOT_MINOR_VER} As specified 3.7 1.2 ${NAUTOBOT_MAJOR_VER}.${NAUTOBOT_MINOR_VER}-py${PYTHON_VER} As specified As specified 1.2-py3.8 stable Latest stable release 3.7 stable stable-py${PYTHON_VER} Latest stable release As specified stable-py3.8 The following additional tags are only available from the GitHub Container Registry: Tag Nautobot Branch Python Version latest develop , the latest commit 3.7 latest-py${PYTHON_VER} develop , the latest commit As specified develop develop , the latest commit 3.7 develop-py${PYTHON_VER} develop , the latest commit As specified develop-${GIT_SHA:0:7}-$(date +%s) develop , a specific commit 3.7 develop-${GIT_SHA:0:7}-$(date +%s)-py${PYTHON_VER} develop , a specific commit As specified next next , the latest commit 3.7 next-py${PYTHON_VER} next , the latest commit As specified next-${GIT_SHA:0:7}-$(date +%s) next , a specific commit 3.7 next-${GIT_SHA:0:7}-$(date +%s)-py${PYTHON_VER} next , a specific commit As specified Currently images are pushed for the following python versions: 3.7 3.8 3.9 3.10 Info Developer images networktocode/nautobot-dev:${TAG} and ghcr.io/nautobot/nautobot-dev:${TAG} are also provided with the same tags as above. These images provide the development dependencies needed to build Nautobot; they can be used as a base for development to develop additional Nautobot plugins but should NOT be used in production. Getting Started \u00b6 Nautobot requires a MySQL or PostgreSQL database and Redis instance before it will start. Because of this the quickest and easiest way to get Nautobot running is with docker-compose , which will install and configure PostgreSQL and Redis containers for you automatically. Configuration \u00b6 Most configuration parameters are available via environment variables which can be passed to the container. If you desire you can inject your own nautobot_config.py by overriding /opt/nautobot/nautobot_config.py using docker volumes by adding -v /local/path/to/custom/nautobot_config.py:/opt/nautobot/nautobot_config.py to your docker run command, for example: $ docker run --name nautobot -v /local/path/to/custom/nautobot_config.py:/opt/nautobot/nautobot_config.py networktocode/nautobot Or if you are using docker-compose: services : nautobot : image : \"networktocode/nautobot\" volumes : - /local/path/to/custom/nautobot_config.py:/opt/nautobot/nautobot_config.py:ro Docker only configuration \u00b6 The entry point for the Docker container has some additional features that can be configured via additional environment variables. The following are all optional variables: NAUTOBOT_CREATE_SUPERUSER \u00b6 Default: unset Enables creation of a super user specified by NAUTOBOT_SUPERUSER_NAME , NAUTOBOT_SUPERUSER_EMAIL , NAUTOBOT_SUPERUSER_PASSWORD , and NAUTOBOT_SUPERUSER_API_TOKEN . NAUTOBOT_DOCKER_SKIP_INIT \u00b6 Default: unset When starting, the container attempts to connect to the database and run database migrations and upgrade steps necessary when upgrading versions. In normal operation this is harmless to run on every startup and validates the database is operating correctly. However, in certain circumstances such as database maintenance when the database is in a read-only mode it may make sense to start Nautobot but skip these steps. Setting this variable to true will start Nautobot without running these initial steps. Note Setting this value to anything other than \"false\" (case-insensitive) will prevent migrations from occurring. NAUTOBOT_SUPERUSER_API_TOKEN \u00b6 Default: unset If NAUTOBOT_CREATE_SUPERUSER is true, NAUTOBOT_SUPERUSER_API_TOKEN specifies the API token of the super user to be created; alternatively the /run/secrets/superuser_api_token file contents are read for the token. Either the variable or the file is required if NAUTOBOT_CREATE_SUPERUSER is true. NAUTOBOT_SUPERUSER_EMAIL \u00b6 Default: admin@example.com If NAUTOBOT_CREATE_SUPERUSER is true, NAUTOBOT_SUPERUSER_EMAIL specifies the email address of the super user to be created. NAUTOBOT_SUPERUSER_NAME \u00b6 Default: admin If NAUTOBOT_CREATE_SUPERUSER is true, NAUTOBOT_SUPERUSER_NAME specifies the username of the super user to be created. NAUTOBOT_SUPERUSER_PASSWORD \u00b6 Default: unset If NAUTOBOT_CREATE_SUPERUSER is true, NAUTOBOT_SUPERUSER_PASSWORD specifies the password of the super user to be created; alternatively the /run/secrets/superuser_password file contents are read for the password. Either the variable or the file is required if NAUTOBOT_CREATE_SUPERUSER is true. uWSGI \u00b6 The docker container uses uWSGI to serve Nautobot. A default configuration is provided , and can be overridden by injecting a new uwsgi.ini file at /opt/nautobot/uwsgi.ini . There are a couple of environment variables provided to override some uWSGI defaults: NAUTOBOT_UWSGI_BUFFER_SIZE \u00b6 Added in version 1.3.9 Default: 4096 Max: 65535 The max size of non-body request payload, roughly the size of request headers for uWSGI. Request headers that might contain lengthy query parameters, for example GraphQL or Relationship filtered lookups, might go well over the default limit. Increasing this limit will have an impact on running memory usage. Please see the uWSGI documentation for more information. This can also be overridden by appending -b DESIRED_BUFFER_SIZE , ex: -b 8192 , to the entry command in all Nautobot containers running uWSGI if you are on a release before 1.3.9 . NAUTOBOT_UWSGI_LISTEN \u00b6 Default: 128 The socket listen queue size of uWSGI. In production environments it is recommended to increase this value to 1024 or higher, however depending on your platform, this may require additional kernel parameter settings, please see the uWSGI documentation for more information. Please see the official uWSGI documentation on listen for more information. NAUTOBOT_UWSGI_PROCESSES \u00b6 Default: 3 The number of worker processes uWSGI will spawn. Please see the official uWSGI documentation on processes for more information. SSL \u00b6 Self signed SSL certificates are included by default with the container. For a production deployment you should utilize your own signed certificates, these can be injected into the container at runtime using docker volumes . The public certificate should be placed at /opt/nautobot/nautobot.crt and the private key should be at /opt/nautobot/nautobot.key . Using a docker run these can be injected using the -v parameter: $ docker run --name nautobot -v /local/path/to/custom/nautobot.crt:/opt/nautobot/nautobot.crt -v /local/path/to/custom/nautobot.key:/opt/nautobot/nautobot.key networktocode/nautobot Or if you are using docker-compose : services : nautobot : image : \"networktocode/nautobot\" volumes : - /local/path/to/custom/nautobot.crt:/opt/nautobot/nautobot.crt:ro - /local/path/to/custom/nautobot.key:/opt/nautobot/nautobot.key:ro Nautobot Plugins \u00b6 At this time adding Nautobot plugins to the existing Docker image is not supported, however, you can use the Nautobot image as the base within your Dockerfile to install your own plugins, here is an example dockerfile: FROM networktocode/nautobot RUN pip install nautobot-chatops COPY nautobot_config.py /opt/nautobot/nautobot_config.py Building the Image \u00b6 If you have a development environment you can use invoke to build the docker images. By default invoke build will build the development containers: $ invoke build ... $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE networktocode/nautobot-dev local 25487d93fc1f 16 seconds ago 630MB If you need to build or test the final image, you must set your invoke.yml to use docker-compose.build.yml in place of docker-compose.dev.yml : --- nautobot : compose_files : - \"docker-compose.yml\" - \"docker-compose.build.yml\" Then you can re-run the invoke build command: $ invoke build ... $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE networktocode/nautobot local 0a24d68da987 55 seconds ago 337MB If you do not have a development environment created you can also build the container using the regular docker build command: $ docker build -t networktocode/nautobot -f ./docker/Dockerfile --build-arg PYTHON_VER=3.7 . Docker Compose \u00b6 An example library for using Docker Compose to build out all of the components for Nautobot can be found within the Nautobot community. Please see https://github.com/nautobot/nautobot-docker-compose/ for examples on the base application, LDAP integration, and using plugins.","title":"Nautobot Docker Images"},{"location":"docker/index.html#nautobot-docker-images","text":"Nautobot is packaged as a Docker image for use in a production environment. The published image is based on the python:3.7-slim image to maintain the most compatibility with Nautobot deployments. The Docker image and deployment strategies are being actively developed, check back here or join the #nautobot channel on Network to Code's Slack community for the most up to date information.","title":"Nautobot Docker Images"},{"location":"docker/index.html#platforms","text":"Nautobot docker images are currently provided for both linux/amd64 and linux/arm64 architectures. Please note ARM64 support is untested by our automated tests and should be considered in an alpha state.","title":"Platforms"},{"location":"docker/index.html#tags","text":"A set of Docker images are built for each Nautobot release and published to both Docker Hub and the GitHub Container Registry . Additionally, GitHub Actions are used to automatically build images corresponding to each commit to the develop and next branches; these images are only published to the GitHub Container Registry. To get a specific tagged image from Docker Hub or the GitHub Container Registry run: docker image pull networktocode/nautobot:${TAG} or docker pull ghcr.io/nautobot/nautobot:${TAG} The following tags are available on both Docker Hub and the GitHub Container Registry: Tag Nautobot Version Python Version Example ${NAUTOBOT_VER} As specified 3.7 1.2.7 ${NAUTOBOT_VER}-py${PYTHON_VER} As specified As specified 1.2.7-py3.8 ${NAUTOBOT_MAJOR_VER}.${NAUTOBOT_MINOR_VER} As specified 3.7 1.2 ${NAUTOBOT_MAJOR_VER}.${NAUTOBOT_MINOR_VER}-py${PYTHON_VER} As specified As specified 1.2-py3.8 stable Latest stable release 3.7 stable stable-py${PYTHON_VER} Latest stable release As specified stable-py3.8 The following additional tags are only available from the GitHub Container Registry: Tag Nautobot Branch Python Version latest develop , the latest commit 3.7 latest-py${PYTHON_VER} develop , the latest commit As specified develop develop , the latest commit 3.7 develop-py${PYTHON_VER} develop , the latest commit As specified develop-${GIT_SHA:0:7}-$(date +%s) develop , a specific commit 3.7 develop-${GIT_SHA:0:7}-$(date +%s)-py${PYTHON_VER} develop , a specific commit As specified next next , the latest commit 3.7 next-py${PYTHON_VER} next , the latest commit As specified next-${GIT_SHA:0:7}-$(date +%s) next , a specific commit 3.7 next-${GIT_SHA:0:7}-$(date +%s)-py${PYTHON_VER} next , a specific commit As specified Currently images are pushed for the following python versions: 3.7 3.8 3.9 3.10 Info Developer images networktocode/nautobot-dev:${TAG} and ghcr.io/nautobot/nautobot-dev:${TAG} are also provided with the same tags as above. These images provide the development dependencies needed to build Nautobot; they can be used as a base for development to develop additional Nautobot plugins but should NOT be used in production.","title":"Tags"},{"location":"docker/index.html#getting-started","text":"Nautobot requires a MySQL or PostgreSQL database and Redis instance before it will start. Because of this the quickest and easiest way to get Nautobot running is with docker-compose , which will install and configure PostgreSQL and Redis containers for you automatically.","title":"Getting Started"},{"location":"docker/index.html#configuration","text":"Most configuration parameters are available via environment variables which can be passed to the container. If you desire you can inject your own nautobot_config.py by overriding /opt/nautobot/nautobot_config.py using docker volumes by adding -v /local/path/to/custom/nautobot_config.py:/opt/nautobot/nautobot_config.py to your docker run command, for example: $ docker run --name nautobot -v /local/path/to/custom/nautobot_config.py:/opt/nautobot/nautobot_config.py networktocode/nautobot Or if you are using docker-compose: services : nautobot : image : \"networktocode/nautobot\" volumes : - /local/path/to/custom/nautobot_config.py:/opt/nautobot/nautobot_config.py:ro","title":"Configuration"},{"location":"docker/index.html#docker-only-configuration","text":"The entry point for the Docker container has some additional features that can be configured via additional environment variables. The following are all optional variables:","title":"Docker only configuration"},{"location":"docker/index.html#nautobot_create_superuser","text":"Default: unset Enables creation of a super user specified by NAUTOBOT_SUPERUSER_NAME , NAUTOBOT_SUPERUSER_EMAIL , NAUTOBOT_SUPERUSER_PASSWORD , and NAUTOBOT_SUPERUSER_API_TOKEN .","title":"NAUTOBOT_CREATE_SUPERUSER"},{"location":"docker/index.html#nautobot_docker_skip_init","text":"Default: unset When starting, the container attempts to connect to the database and run database migrations and upgrade steps necessary when upgrading versions. In normal operation this is harmless to run on every startup and validates the database is operating correctly. However, in certain circumstances such as database maintenance when the database is in a read-only mode it may make sense to start Nautobot but skip these steps. Setting this variable to true will start Nautobot without running these initial steps. Note Setting this value to anything other than \"false\" (case-insensitive) will prevent migrations from occurring.","title":"NAUTOBOT_DOCKER_SKIP_INIT"},{"location":"docker/index.html#nautobot_superuser_api_token","text":"Default: unset If NAUTOBOT_CREATE_SUPERUSER is true, NAUTOBOT_SUPERUSER_API_TOKEN specifies the API token of the super user to be created; alternatively the /run/secrets/superuser_api_token file contents are read for the token. Either the variable or the file is required if NAUTOBOT_CREATE_SUPERUSER is true.","title":"NAUTOBOT_SUPERUSER_API_TOKEN"},{"location":"docker/index.html#nautobot_superuser_email","text":"Default: admin@example.com If NAUTOBOT_CREATE_SUPERUSER is true, NAUTOBOT_SUPERUSER_EMAIL specifies the email address of the super user to be created.","title":"NAUTOBOT_SUPERUSER_EMAIL"},{"location":"docker/index.html#nautobot_superuser_name","text":"Default: admin If NAUTOBOT_CREATE_SUPERUSER is true, NAUTOBOT_SUPERUSER_NAME specifies the username of the super user to be created.","title":"NAUTOBOT_SUPERUSER_NAME"},{"location":"docker/index.html#nautobot_superuser_password","text":"Default: unset If NAUTOBOT_CREATE_SUPERUSER is true, NAUTOBOT_SUPERUSER_PASSWORD specifies the password of the super user to be created; alternatively the /run/secrets/superuser_password file contents are read for the password. Either the variable or the file is required if NAUTOBOT_CREATE_SUPERUSER is true.","title":"NAUTOBOT_SUPERUSER_PASSWORD"},{"location":"docker/index.html#uwsgi","text":"The docker container uses uWSGI to serve Nautobot. A default configuration is provided , and can be overridden by injecting a new uwsgi.ini file at /opt/nautobot/uwsgi.ini . There are a couple of environment variables provided to override some uWSGI defaults:","title":"uWSGI"},{"location":"docker/index.html#nautobot_uwsgi_buffer_size","text":"Added in version 1.3.9 Default: 4096 Max: 65535 The max size of non-body request payload, roughly the size of request headers for uWSGI. Request headers that might contain lengthy query parameters, for example GraphQL or Relationship filtered lookups, might go well over the default limit. Increasing this limit will have an impact on running memory usage. Please see the uWSGI documentation for more information. This can also be overridden by appending -b DESIRED_BUFFER_SIZE , ex: -b 8192 , to the entry command in all Nautobot containers running uWSGI if you are on a release before 1.3.9 .","title":"NAUTOBOT_UWSGI_BUFFER_SIZE"},{"location":"docker/index.html#nautobot_uwsgi_listen","text":"Default: 128 The socket listen queue size of uWSGI. In production environments it is recommended to increase this value to 1024 or higher, however depending on your platform, this may require additional kernel parameter settings, please see the uWSGI documentation for more information. Please see the official uWSGI documentation on listen for more information.","title":"NAUTOBOT_UWSGI_LISTEN"},{"location":"docker/index.html#nautobot_uwsgi_processes","text":"Default: 3 The number of worker processes uWSGI will spawn. Please see the official uWSGI documentation on processes for more information.","title":"NAUTOBOT_UWSGI_PROCESSES"},{"location":"docker/index.html#ssl","text":"Self signed SSL certificates are included by default with the container. For a production deployment you should utilize your own signed certificates, these can be injected into the container at runtime using docker volumes . The public certificate should be placed at /opt/nautobot/nautobot.crt and the private key should be at /opt/nautobot/nautobot.key . Using a docker run these can be injected using the -v parameter: $ docker run --name nautobot -v /local/path/to/custom/nautobot.crt:/opt/nautobot/nautobot.crt -v /local/path/to/custom/nautobot.key:/opt/nautobot/nautobot.key networktocode/nautobot Or if you are using docker-compose : services : nautobot : image : \"networktocode/nautobot\" volumes : - /local/path/to/custom/nautobot.crt:/opt/nautobot/nautobot.crt:ro - /local/path/to/custom/nautobot.key:/opt/nautobot/nautobot.key:ro","title":"SSL"},{"location":"docker/index.html#nautobot-plugins","text":"At this time adding Nautobot plugins to the existing Docker image is not supported, however, you can use the Nautobot image as the base within your Dockerfile to install your own plugins, here is an example dockerfile: FROM networktocode/nautobot RUN pip install nautobot-chatops COPY nautobot_config.py /opt/nautobot/nautobot_config.py","title":"Nautobot Plugins"},{"location":"docker/index.html#building-the-image","text":"If you have a development environment you can use invoke to build the docker images. By default invoke build will build the development containers: $ invoke build ... $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE networktocode/nautobot-dev local 25487d93fc1f 16 seconds ago 630MB If you need to build or test the final image, you must set your invoke.yml to use docker-compose.build.yml in place of docker-compose.dev.yml : --- nautobot : compose_files : - \"docker-compose.yml\" - \"docker-compose.build.yml\" Then you can re-run the invoke build command: $ invoke build ... $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE networktocode/nautobot local 0a24d68da987 55 seconds ago 337MB If you do not have a development environment created you can also build the container using the regular docker build command: $ docker build -t networktocode/nautobot -f ./docker/Dockerfile --build-arg PYTHON_VER=3.7 .","title":"Building the Image"},{"location":"docker/index.html#docker-compose","text":"An example library for using Docker Compose to build out all of the components for Nautobot can be found within the Nautobot community. Please see https://github.com/nautobot/nautobot-docker-compose/ for examples on the base application, LDAP integration, and using plugins.","title":"Docker Compose"},{"location":"installation/index.html","text":"Installation \u00b6 This set of documents will help you get Nautobot up and running. As an alternative, you can also run Nautobot in Docker . About Dependencies \u00b6 This section describes the system dependencies required for Nautobot. They can be all installed on a single system, or distributed across your environment. That will be up to you. Our install instructions assume a single system install, and that is good for most use cases. More advanced configurations are also possible, but are not covered here. The installation instructions below will guide you through a fresh installation. Mandatory dependencies \u00b6 The following minimum versions are required for Nautobot to operate: Dependency Role Minimum Version Python Application 3.7 PostgreSQL Database 9.6 MySQL Database 8.0 Redis Cache, Queue 4.0 Note Either PostgreSQL or MySQL must be selected, but not both. Added in version 1.1.0 MySQL support was added. Added in version 1.3.0 Python 3.10 support was added. Removed in version 1.3.0 Python 3.6 support was removed. Nautobot will not work without these dependencies. Python \u00b6 Nautobot is written in the Python programming language . The official Python package installer is called Pip , and you will see the pip command referenced often to install or update Python packages. All Nautobot plugins and library dependencies will be written using Python. Database \u00b6 Nautobot uses a relational database to store its data. Both MySQL and PostgreSQL are officially supported. MySQL \u00b6 MySQL is an open-source relational database management system that\u2019s relatively easy to set up and manage, fast, reliable, and well-understood. PostgreSQL \u00b6 PostgreSQL is a powerful, feature-rich open source relational database server that can handle complex queries and massive databases. Redis \u00b6 Redis is an open source, in-memory data store which Nautobot employs for caching and queuing. Optional dependencies \u00b6 Nautobot will still operate without these optional dependencies, but would likely not be ready for use in a production environment without them. The installation and configuration of these dependencies are covered in the detailed guides which follow. For production deployment we recommend the following: uWSGI WSGI server NGINX HTTP server External authentication service for SSO such as SAML, OAuth2, or LDAP, or an authenticating proxy For additional features: NAPALM support for retrieving operational data from network devices Prometheus metrics for exporting application performance and telemetry data Installing Nautobot Dependencies \u00b6 Nautobot was designed to be a cross-platform application that can run on nearly any system that is able to run the required dependencies. Only the operating system platforms listed below are officially supported at this time . Nautobot has been tested and confirmed to work on the following platforms. Detailed install and deployment instructions can be found by following the link to each. Installing Nautobot Dependencies on CentOS/RHEL \u00b6 Red Hat flavors of Linux including CentOS 8.2+ or Red Hat Enterprise Linux (RHEL) 8.2+ are supported. The same installation instructions can be used on either. Installing Nautobot Dependencies on CentOS/RHEL Installing Nautobot Dependencies on Ubuntu \u00b6 Ubuntu 20.04 or later is supported. Installing Nautobot Dependencies on Ubuntu Installing on Other Systems \u00b6 Nautobot should work on any POSIX-compliant system including practically any flavor of Linux, BSD, or even macOS, but those are not officially supported at this time. Running Nautobot in Docker \u00b6 Nautobot docker images are available for use in a containerized deployment for an easier installation, see the Docker overview for more information. Upgrading \u00b6 If you are upgrading from an existing installation, please consult the upgrading guide .","title":"Installing Prerequisites"},{"location":"installation/index.html#installation","text":"This set of documents will help you get Nautobot up and running. As an alternative, you can also run Nautobot in Docker .","title":"Installation"},{"location":"installation/index.html#about-dependencies","text":"This section describes the system dependencies required for Nautobot. They can be all installed on a single system, or distributed across your environment. That will be up to you. Our install instructions assume a single system install, and that is good for most use cases. More advanced configurations are also possible, but are not covered here. The installation instructions below will guide you through a fresh installation.","title":"About Dependencies"},{"location":"installation/index.html#mandatory-dependencies","text":"The following minimum versions are required for Nautobot to operate: Dependency Role Minimum Version Python Application 3.7 PostgreSQL Database 9.6 MySQL Database 8.0 Redis Cache, Queue 4.0 Note Either PostgreSQL or MySQL must be selected, but not both. Added in version 1.1.0 MySQL support was added. Added in version 1.3.0 Python 3.10 support was added. Removed in version 1.3.0 Python 3.6 support was removed. Nautobot will not work without these dependencies.","title":"Mandatory dependencies"},{"location":"installation/index.html#python","text":"Nautobot is written in the Python programming language . The official Python package installer is called Pip , and you will see the pip command referenced often to install or update Python packages. All Nautobot plugins and library dependencies will be written using Python.","title":"Python"},{"location":"installation/index.html#database","text":"Nautobot uses a relational database to store its data. Both MySQL and PostgreSQL are officially supported.","title":"Database"},{"location":"installation/index.html#mysql","text":"MySQL is an open-source relational database management system that\u2019s relatively easy to set up and manage, fast, reliable, and well-understood.","title":"MySQL"},{"location":"installation/index.html#postgresql","text":"PostgreSQL is a powerful, feature-rich open source relational database server that can handle complex queries and massive databases.","title":"PostgreSQL"},{"location":"installation/index.html#redis","text":"Redis is an open source, in-memory data store which Nautobot employs for caching and queuing.","title":"Redis"},{"location":"installation/index.html#optional-dependencies","text":"Nautobot will still operate without these optional dependencies, but would likely not be ready for use in a production environment without them. The installation and configuration of these dependencies are covered in the detailed guides which follow. For production deployment we recommend the following: uWSGI WSGI server NGINX HTTP server External authentication service for SSO such as SAML, OAuth2, or LDAP, or an authenticating proxy For additional features: NAPALM support for retrieving operational data from network devices Prometheus metrics for exporting application performance and telemetry data","title":"Optional dependencies"},{"location":"installation/index.html#installing-nautobot-dependencies","text":"Nautobot was designed to be a cross-platform application that can run on nearly any system that is able to run the required dependencies. Only the operating system platforms listed below are officially supported at this time . Nautobot has been tested and confirmed to work on the following platforms. Detailed install and deployment instructions can be found by following the link to each.","title":"Installing Nautobot Dependencies"},{"location":"installation/index.html#installing-nautobot-dependencies-on-centosrhel","text":"Red Hat flavors of Linux including CentOS 8.2+ or Red Hat Enterprise Linux (RHEL) 8.2+ are supported. The same installation instructions can be used on either. Installing Nautobot Dependencies on CentOS/RHEL","title":"Installing Nautobot Dependencies on CentOS/RHEL"},{"location":"installation/index.html#installing-nautobot-dependencies-on-ubuntu","text":"Ubuntu 20.04 or later is supported. Installing Nautobot Dependencies on Ubuntu","title":"Installing Nautobot Dependencies on Ubuntu"},{"location":"installation/index.html#installing-on-other-systems","text":"Nautobot should work on any POSIX-compliant system including practically any flavor of Linux, BSD, or even macOS, but those are not officially supported at this time.","title":"Installing on Other Systems"},{"location":"installation/index.html#running-nautobot-in-docker","text":"Nautobot docker images are available for use in a containerized deployment for an easier installation, see the Docker overview for more information.","title":"Running Nautobot in Docker"},{"location":"installation/index.html#upgrading","text":"If you are upgrading from an existing installation, please consult the upgrading guide .","title":"Upgrading"},{"location":"installation/centos.html","text":"Installing Nautobot Dependencies on CentOS/RHEL \u00b6 This installation guide assumes that you are running CentOS or RHEL version 8.2+ on your system. Install System Packages \u00b6 Install the prerequisite system libraries and utilities. This will install: Git Python 3 Pip Redis server and client $ sudo dnf check-update $ sudo dnf install -y git python38 python38-devel python38-pip redis Database Setup \u00b6 In this step you'll set up your database server, create a database and database user for use by Nautobot, and verify your connection to the database. You must select either MySQL or PostgreSQL. PostgreSQL is used by default with Nautobot, so if you just want to get started or don't have a preference, please stick with PostgreSQL. Please follow the steps for your selected database backend below. PostgreSQL Setup \u00b6 Install PostgreSQL \u00b6 This will install the PostgreSQL database server and client. $ sudo dnf install -y postgresql-server Initialize PostgreSQL \u00b6 CentOS/RHEL requires a manual step to generate the initial configurations required by PostgreSQL. $ sudo postgresql-setup --initdb Configure Authentication \u00b6 CentOS/RHEL configures PostgreSQL to use ident host-based authentication by default. Because Nautobot will need to authenticate using a username and password, we must update pg_hba.conf to support md5 password authentication. As root, edit /var/lib/pgsql/data/pg_hba.conf and change ident to md5 for the lines below. Before: # IPv4 local connections: host all all 127.0.0.1/32 ident # IPv6 local connections: host all all ::1/128 ident After: # IPv4 local connections: host all all 127.0.0.1/32 md5 # IPv6 local connections: host all all ::1/128 md5 Start PostgreSQL \u00b6 Start the service and enable it to run at system startup: $ sudo systemctl enable --now postgresql Create a PostgreSQL Database \u00b6 At a minimum, we need to create a database for Nautobot and assign it a username and password for authentication. This is done with the following commands. Danger Do not use the password from the example. Choose a strong, random password to ensure secure database authentication for your Nautobot installation. $ sudo -u postgres psql psql (10.15) Type \"help\" for help. postgres=# CREATE DATABASE nautobot; CREATE DATABASE postgres=# CREATE USER nautobot WITH PASSWORD 'insecure_password'; CREATE ROLE postgres=# GRANT ALL PRIVILEGES ON DATABASE nautobot TO nautobot; GRANT postgres=# \\q Verify PostgreSQL Service Status \u00b6 You can verify that authentication works issuing the following command and providing the configured password. (Replace localhost with your database server if using a remote database.) If successful, you will enter a nautobot prompt. Type \\conninfo to confirm your connection, or type \\q to exit. $ psql --username nautobot --password --host localhost nautobot Password for user nautobot: psql (10.15) Type \"help\" for help. nautobot=> \\conninfo You are connected to database \"nautobot\" as user \"nautobot\" on host \"localhost\" (address \"127.0.0.1\") at port \"5432\". nautobot=> \\q MySQL Setup \u00b6 Install MySQL \u00b6 This will install the MySQL database server and client. Additionally, MySQL requires that gcc and the MySQL development libraries are installed so that we may compile the Python mysqlclient library during the Nautobot installation steps. sudo dnf install -y gcc mysql-server mysql-devel Start MySQL \u00b6 Start the service and enable it to run at system startup: $ sudo systemctl enable --now mysql Create a MySQL Database \u00b6 At a minimum, we need to create a database for Nautobot and assign it a username and password for authentication. This is done with the following commands. Note Replace localhost below with your database server if using a remote database. Danger Do not use the password from the example. Choose a strong, random password to ensure secure database authentication for your Nautobot installation. $ sudo -u root mysql Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 8 Server version: 8.0.21 Source distribution Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> CREATE DATABASE nautobot; Query OK, 1 row affected (0.00 sec) mysql> CREATE USER 'nautobot'@'localhost' IDENTIFIED BY 'insecure_password'; Query OK, 0 rows affected (0.00 sec) mysql> GRANT ALL ON nautobot.* TO 'nautobot'@'localhost'; Query OK, 0 rows affected (0.00 sec) mysql> \\q Bye Verify MySQL Service Status \u00b6 You can verify that authentication works issuing the following command and providing the configured password. If successful, you will enter a mysql> prompt. Type status to confirm your connection, or type \\q to exit. Note Replace localhost below with your database server if using a remote database. $ mysql --user nautobot --password --host localhost nautobot Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 10 Server version: 8.0.21 Source distribution Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> status -------------- mysql Ver 8.0.21 for Linux on x86_64 (Source distribution) Connection id: 10 Current database: nautobot Current user: nautobot@localhost SSL: Not in use Current pager: stdout Using outfile: '' Using delimiter: ; Server version: 8.0.21 Source distribution Protocol version: 10 Connection: Localhost via UNIX socket Server characterset: utf8mb4 Db characterset: utf8mb4 Client characterset: utf8mb4 Conn. characterset: utf8mb4 UNIX socket: /var/lib/mysql/mysql.sock Binary data as: Hexadecimal Uptime: 4 min 12 sec Threads: 2 Questions: 12 Slow queries: 0 Opens: 151 Flush tables: 3 Open tables: 69 Queries per second avg: 0.047 -------------- mysql> \\q Bye Redis Setup \u00b6 Start Redis \u00b6 Start the service and enable it to run at system startup: $ sudo systemctl enable --now redis Verify Redis Service Status \u00b6 Use the redis-cli utility to ensure the Redis service is functional: $ redis-cli ping PONG Deploy Nautobot \u00b6 Now that Nautobot dependencies are installed and configured, you're ready to Install Nautobot !","title":"Installing Nautobot Dependencies on CentOS/RHEL"},{"location":"installation/centos.html#installing-nautobot-dependencies-on-centosrhel","text":"This installation guide assumes that you are running CentOS or RHEL version 8.2+ on your system.","title":"Installing Nautobot Dependencies on CentOS/RHEL"},{"location":"installation/centos.html#install-system-packages","text":"Install the prerequisite system libraries and utilities. This will install: Git Python 3 Pip Redis server and client $ sudo dnf check-update $ sudo dnf install -y git python38 python38-devel python38-pip redis","title":"Install System Packages"},{"location":"installation/centos.html#database-setup","text":"In this step you'll set up your database server, create a database and database user for use by Nautobot, and verify your connection to the database. You must select either MySQL or PostgreSQL. PostgreSQL is used by default with Nautobot, so if you just want to get started or don't have a preference, please stick with PostgreSQL. Please follow the steps for your selected database backend below.","title":"Database Setup"},{"location":"installation/centos.html#postgresql-setup","text":"","title":"PostgreSQL Setup"},{"location":"installation/centos.html#install-postgresql","text":"This will install the PostgreSQL database server and client. $ sudo dnf install -y postgresql-server","title":"Install PostgreSQL"},{"location":"installation/centos.html#initialize-postgresql","text":"CentOS/RHEL requires a manual step to generate the initial configurations required by PostgreSQL. $ sudo postgresql-setup --initdb","title":"Initialize PostgreSQL"},{"location":"installation/centos.html#configure-authentication","text":"CentOS/RHEL configures PostgreSQL to use ident host-based authentication by default. Because Nautobot will need to authenticate using a username and password, we must update pg_hba.conf to support md5 password authentication. As root, edit /var/lib/pgsql/data/pg_hba.conf and change ident to md5 for the lines below. Before: # IPv4 local connections: host all all 127.0.0.1/32 ident # IPv6 local connections: host all all ::1/128 ident After: # IPv4 local connections: host all all 127.0.0.1/32 md5 # IPv6 local connections: host all all ::1/128 md5","title":"Configure Authentication"},{"location":"installation/centos.html#start-postgresql","text":"Start the service and enable it to run at system startup: $ sudo systemctl enable --now postgresql","title":"Start PostgreSQL"},{"location":"installation/centos.html#create-a-postgresql-database","text":"At a minimum, we need to create a database for Nautobot and assign it a username and password for authentication. This is done with the following commands. Danger Do not use the password from the example. Choose a strong, random password to ensure secure database authentication for your Nautobot installation. $ sudo -u postgres psql psql (10.15) Type \"help\" for help. postgres=# CREATE DATABASE nautobot; CREATE DATABASE postgres=# CREATE USER nautobot WITH PASSWORD 'insecure_password'; CREATE ROLE postgres=# GRANT ALL PRIVILEGES ON DATABASE nautobot TO nautobot; GRANT postgres=# \\q","title":"Create a PostgreSQL Database"},{"location":"installation/centos.html#verify-postgresql-service-status","text":"You can verify that authentication works issuing the following command and providing the configured password. (Replace localhost with your database server if using a remote database.) If successful, you will enter a nautobot prompt. Type \\conninfo to confirm your connection, or type \\q to exit. $ psql --username nautobot --password --host localhost nautobot Password for user nautobot: psql (10.15) Type \"help\" for help. nautobot=> \\conninfo You are connected to database \"nautobot\" as user \"nautobot\" on host \"localhost\" (address \"127.0.0.1\") at port \"5432\". nautobot=> \\q","title":"Verify PostgreSQL Service Status"},{"location":"installation/centos.html#mysql-setup","text":"","title":"MySQL Setup"},{"location":"installation/centos.html#install-mysql","text":"This will install the MySQL database server and client. Additionally, MySQL requires that gcc and the MySQL development libraries are installed so that we may compile the Python mysqlclient library during the Nautobot installation steps. sudo dnf install -y gcc mysql-server mysql-devel","title":"Install MySQL"},{"location":"installation/centos.html#start-mysql","text":"Start the service and enable it to run at system startup: $ sudo systemctl enable --now mysql","title":"Start MySQL"},{"location":"installation/centos.html#create-a-mysql-database","text":"At a minimum, we need to create a database for Nautobot and assign it a username and password for authentication. This is done with the following commands. Note Replace localhost below with your database server if using a remote database. Danger Do not use the password from the example. Choose a strong, random password to ensure secure database authentication for your Nautobot installation. $ sudo -u root mysql Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 8 Server version: 8.0.21 Source distribution Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> CREATE DATABASE nautobot; Query OK, 1 row affected (0.00 sec) mysql> CREATE USER 'nautobot'@'localhost' IDENTIFIED BY 'insecure_password'; Query OK, 0 rows affected (0.00 sec) mysql> GRANT ALL ON nautobot.* TO 'nautobot'@'localhost'; Query OK, 0 rows affected (0.00 sec) mysql> \\q Bye","title":"Create a MySQL Database"},{"location":"installation/centos.html#verify-mysql-service-status","text":"You can verify that authentication works issuing the following command and providing the configured password. If successful, you will enter a mysql> prompt. Type status to confirm your connection, or type \\q to exit. Note Replace localhost below with your database server if using a remote database. $ mysql --user nautobot --password --host localhost nautobot Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 10 Server version: 8.0.21 Source distribution Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> status -------------- mysql Ver 8.0.21 for Linux on x86_64 (Source distribution) Connection id: 10 Current database: nautobot Current user: nautobot@localhost SSL: Not in use Current pager: stdout Using outfile: '' Using delimiter: ; Server version: 8.0.21 Source distribution Protocol version: 10 Connection: Localhost via UNIX socket Server characterset: utf8mb4 Db characterset: utf8mb4 Client characterset: utf8mb4 Conn. characterset: utf8mb4 UNIX socket: /var/lib/mysql/mysql.sock Binary data as: Hexadecimal Uptime: 4 min 12 sec Threads: 2 Questions: 12 Slow queries: 0 Opens: 151 Flush tables: 3 Open tables: 69 Queries per second avg: 0.047 -------------- mysql> \\q Bye","title":"Verify MySQL Service Status"},{"location":"installation/centos.html#redis-setup","text":"","title":"Redis Setup"},{"location":"installation/centos.html#start-redis","text":"Start the service and enable it to run at system startup: $ sudo systemctl enable --now redis","title":"Start Redis"},{"location":"installation/centos.html#verify-redis-service-status","text":"Use the redis-cli utility to ensure the Redis service is functional: $ redis-cli ping PONG","title":"Verify Redis Service Status"},{"location":"installation/centos.html#deploy-nautobot","text":"Now that Nautobot dependencies are installed and configured, you're ready to Install Nautobot !","title":"Deploy Nautobot"},{"location":"installation/external-authentication.html","text":"External Authentication \u00b6 This guide explains how to implement authentication using an external server. User authentication will fall back to built-in Django users in the event of a failure. Supported External Authentication Backends \u00b6 LDAP Authentication Remote User Authentication SSO Authentication","title":"External Authentication (Optional)"},{"location":"installation/external-authentication.html#external-authentication","text":"This guide explains how to implement authentication using an external server. User authentication will fall back to built-in Django users in the event of a failure.","title":"External Authentication"},{"location":"installation/external-authentication.html#supported-external-authentication-backends","text":"LDAP Authentication Remote User Authentication SSO Authentication","title":"Supported External Authentication Backends"},{"location":"installation/http-server.html","text":"Configuring an HTTP Server \u00b6 This documentation provides example configurations for NGINX though any HTTP server which supports WSGI should be compatible. Obtain an SSL Certificate \u00b6 To enable HTTPS access to Nautobot, you'll need a valid SSL certificate. You can purchase one from a trusted commercial provider, obtain one for free from Let's Encrypt , or generate your own (although self-signed certificates are generally untrusted). Both the public certificate and private key files need to be installed on your Nautobot server in a secure location that is readable only by the root user. Warning The command below can be used to generate a self-signed certificate for testing purposes, however it is strongly recommended to use a certificate from a trusted authority in production. Two files will be created: the public certificate ( nautobot.crt ) and the private key ( nautobot.key ). The certificate is published to the world, whereas the private key must be kept secret at all times. Info Some Linux installations, including CentOS, have changed the location for SSL certificates from /etc/ssl/ to /etc/pki/tls/ . The command below may need to be changed to reflect the certificate location. The following command will prompt you for additional details of the certificate; all of which are optional. $ sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout /etc/ssl/private/nautobot.key \\ -out /etc/ssl/certs/nautobot.crt HTTP Server Installation \u00b6 Any HTTP server of your choosing is supported. For your convenience, setup instructions for NGINX are provided here. Warning The following steps must be performed with root permissions. NGINX \u00b6 NGINX is a free, open source, high-performance HTTP server and reverse proxy and is by far the most popular choice. Install NGINX \u00b6 Begin by installing NGINX: On Ubuntu: $ sudo apt install -y nginx On CentOS/RHEL: $ sudo dnf install -y nginx Configure NGINX \u00b6 Once NGINX is installed, copy and paste the following NGINX configuration into /etc/nginx/sites-available/nautobot.conf for Ubuntu or /etc/nginx/conf.d/nautobot.conf for CentOS/RHEL: Note If the file location of SSL certificates had to be changed in the Obtain an SSL Certificate step above, then the location will need to be changed in the NGINX configuration below. server { listen 443 ssl http2 default_server; listen [::]:443 ssl http2 default_server; server_name _; ssl_certificate /etc/ssl/certs/nautobot.crt; ssl_certificate_key /etc/ssl/private/nautobot.key; client_max_body_size 25m; location /static/ { alias /opt/nautobot/static/; } # For subdirectory hosting, you'll want to toggle this (e.g. `/nautobot/`). # Don't forget to set `FORCE_SCRIPT_NAME` in your `nautobot_config.py` to match. # location /nautobot/ { location / { include uwsgi_params; uwsgi_pass 127.0.0.1:8001; uwsgi_param Host $host; uwsgi_param X-Real-IP $remote_addr; uwsgi_param X-Forwarded-For $proxy_add_x_forwarded_for; uwsgi_param X-Forwarded-Proto $http_x_forwarded_proto; # If you want subdirectory hosting, uncomment this. The path must match # the path of this location block (e.g. `/nautobot`). For NGINX the path # MUST NOT end with a trailing \"/\". # uwsgi_param SCRIPT_NAME /nautobot; } } server { # Redirect HTTP traffic to HTTPS listen 80 default_server; listen [::]:80 default_server; server_name _; return 301 https://$host$request_uri; } Enable Nautobot \u00b6 On Ubuntu: To enable the Nautobot site, you'll need to delete /etc/nginx/sites-enabled/default and create a symbolic link in the sites-enabled directory to the configuration file you just created: $ sudo rm -f /etc/nginx/sites-enabled/default $ sudo ln -s /etc/nginx/sites-available/nautobot.conf /etc/nginx/sites-enabled/nautobot.conf On CentOS: Run the following command to disable the default site that comes with the nginx package: $ sudo sed -i 's@ default_server@@' /etc/nginx/nginx.conf Restart NGINX \u00b6 Finally, restart the nginx service to use the new configuration. $ sudo systemctl restart nginx Info If the restart fails, and you changed the default key location, check to make sure the nautobot.conf file you pasted has the updated key location. For example, CentOS requires keys to be in /etc/pki/tls/ instead of /etc/ssl/ . Confirm Permissions for NAUTOBOT_ROOT \u00b6 Ensure that the NAUTOBOT_ROOT permissions are set to 755 . If permissions need to be changed, as the nautobot user run: $ chmod 755 $NAUTOBOT_ROOT Confirm Connectivity \u00b6 At this point, you should be able to connect to the HTTPS service at the server name or IP address you provided. If you used a self-signed certificate, you will likely need to explicitly allow connectivity in your browser. Info Please keep in mind that the configurations provided here are bare minimums required to get Nautobot up and running. You may want to make adjustments to better suit your production environment. Warning Certain components of Nautobot (such as the display of rack elevation diagrams) rely on the use of embedded objects. Ensure that your HTTP server configuration does not override the X-Frame-Options response header set by Nautobot. Troubleshooting \u00b6 Unable to Connect \u00b6 If you are unable to connect to the HTTP server, check that: NGINX is running and configured to listen on the correct port. Access is not being blocked by a firewall somewhere along the path. (Try connecting locally from the server itself.) Static Media Failure \u00b6 If you get a Static Media Failure; The following static media file failed to load: css/base.css , verify the permissions on the $NAUTOBOT_ROOT directory are 755 . Example of correct permissions: [root@localhost ~]# ls -l /opt/ total 4 drwxr-xr-x. 11 nautobot nautobot 4096 Apr 5 11:24 nautobot [root@localhost ~]# If the permissions are not correct, modify them accordingly. Example of modifying the permissions: [nautobot@localhost ~]$ ls -l /opt/ total 4 drwx------. 11 nautobot nautobot 4096 Apr 5 10:00 nautobot [nautobot@localhost ~]$ chmod 755 $NAUTOBOT [nautobot@localhost ~]$ ls -l /opt/ total 4 drwxr-xr-x. 11 nautobot nautobot 4096 Apr 5 11:24 nautobot 502 Bad Gateway \u00b6 If you are able to connect but receive a 502 (bad gateway) error, check the following: The uWSGI worker processes are running ( systemctl status nautobot should show a status of active (running) ) NGINX is configured to connect to the port on which uWSGI is listening (default is 8001 ). SELinux may be preventing the reverse proxy connection. You may need to allow HTTP network connections with the command setsebool -P httpd_can_network_connect 1 . For further information, view the SELinux troubleshooting guide.","title":"Configuring an HTTP Server"},{"location":"installation/http-server.html#configuring-an-http-server","text":"This documentation provides example configurations for NGINX though any HTTP server which supports WSGI should be compatible.","title":"Configuring an HTTP Server"},{"location":"installation/http-server.html#obtain-an-ssl-certificate","text":"To enable HTTPS access to Nautobot, you'll need a valid SSL certificate. You can purchase one from a trusted commercial provider, obtain one for free from Let's Encrypt , or generate your own (although self-signed certificates are generally untrusted). Both the public certificate and private key files need to be installed on your Nautobot server in a secure location that is readable only by the root user. Warning The command below can be used to generate a self-signed certificate for testing purposes, however it is strongly recommended to use a certificate from a trusted authority in production. Two files will be created: the public certificate ( nautobot.crt ) and the private key ( nautobot.key ). The certificate is published to the world, whereas the private key must be kept secret at all times. Info Some Linux installations, including CentOS, have changed the location for SSL certificates from /etc/ssl/ to /etc/pki/tls/ . The command below may need to be changed to reflect the certificate location. The following command will prompt you for additional details of the certificate; all of which are optional. $ sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout /etc/ssl/private/nautobot.key \\ -out /etc/ssl/certs/nautobot.crt","title":"Obtain an SSL Certificate"},{"location":"installation/http-server.html#http-server-installation","text":"Any HTTP server of your choosing is supported. For your convenience, setup instructions for NGINX are provided here. Warning The following steps must be performed with root permissions.","title":"HTTP Server Installation"},{"location":"installation/http-server.html#nginx","text":"NGINX is a free, open source, high-performance HTTP server and reverse proxy and is by far the most popular choice.","title":"NGINX"},{"location":"installation/http-server.html#install-nginx","text":"Begin by installing NGINX: On Ubuntu: $ sudo apt install -y nginx On CentOS/RHEL: $ sudo dnf install -y nginx","title":"Install NGINX"},{"location":"installation/http-server.html#configure-nginx","text":"Once NGINX is installed, copy and paste the following NGINX configuration into /etc/nginx/sites-available/nautobot.conf for Ubuntu or /etc/nginx/conf.d/nautobot.conf for CentOS/RHEL: Note If the file location of SSL certificates had to be changed in the Obtain an SSL Certificate step above, then the location will need to be changed in the NGINX configuration below. server { listen 443 ssl http2 default_server; listen [::]:443 ssl http2 default_server; server_name _; ssl_certificate /etc/ssl/certs/nautobot.crt; ssl_certificate_key /etc/ssl/private/nautobot.key; client_max_body_size 25m; location /static/ { alias /opt/nautobot/static/; } # For subdirectory hosting, you'll want to toggle this (e.g. `/nautobot/`). # Don't forget to set `FORCE_SCRIPT_NAME` in your `nautobot_config.py` to match. # location /nautobot/ { location / { include uwsgi_params; uwsgi_pass 127.0.0.1:8001; uwsgi_param Host $host; uwsgi_param X-Real-IP $remote_addr; uwsgi_param X-Forwarded-For $proxy_add_x_forwarded_for; uwsgi_param X-Forwarded-Proto $http_x_forwarded_proto; # If you want subdirectory hosting, uncomment this. The path must match # the path of this location block (e.g. `/nautobot`). For NGINX the path # MUST NOT end with a trailing \"/\". # uwsgi_param SCRIPT_NAME /nautobot; } } server { # Redirect HTTP traffic to HTTPS listen 80 default_server; listen [::]:80 default_server; server_name _; return 301 https://$host$request_uri; }","title":"Configure NGINX"},{"location":"installation/http-server.html#enable-nautobot","text":"On Ubuntu: To enable the Nautobot site, you'll need to delete /etc/nginx/sites-enabled/default and create a symbolic link in the sites-enabled directory to the configuration file you just created: $ sudo rm -f /etc/nginx/sites-enabled/default $ sudo ln -s /etc/nginx/sites-available/nautobot.conf /etc/nginx/sites-enabled/nautobot.conf On CentOS: Run the following command to disable the default site that comes with the nginx package: $ sudo sed -i 's@ default_server@@' /etc/nginx/nginx.conf","title":"Enable Nautobot"},{"location":"installation/http-server.html#restart-nginx","text":"Finally, restart the nginx service to use the new configuration. $ sudo systemctl restart nginx Info If the restart fails, and you changed the default key location, check to make sure the nautobot.conf file you pasted has the updated key location. For example, CentOS requires keys to be in /etc/pki/tls/ instead of /etc/ssl/ .","title":"Restart NGINX"},{"location":"installation/http-server.html#confirm-permissions-for-nautobot_root","text":"Ensure that the NAUTOBOT_ROOT permissions are set to 755 . If permissions need to be changed, as the nautobot user run: $ chmod 755 $NAUTOBOT_ROOT","title":"Confirm Permissions for NAUTOBOT_ROOT"},{"location":"installation/http-server.html#confirm-connectivity","text":"At this point, you should be able to connect to the HTTPS service at the server name or IP address you provided. If you used a self-signed certificate, you will likely need to explicitly allow connectivity in your browser. Info Please keep in mind that the configurations provided here are bare minimums required to get Nautobot up and running. You may want to make adjustments to better suit your production environment. Warning Certain components of Nautobot (such as the display of rack elevation diagrams) rely on the use of embedded objects. Ensure that your HTTP server configuration does not override the X-Frame-Options response header set by Nautobot.","title":"Confirm Connectivity"},{"location":"installation/http-server.html#troubleshooting","text":"","title":"Troubleshooting"},{"location":"installation/http-server.html#unable-to-connect","text":"If you are unable to connect to the HTTP server, check that: NGINX is running and configured to listen on the correct port. Access is not being blocked by a firewall somewhere along the path. (Try connecting locally from the server itself.)","title":"Unable to Connect"},{"location":"installation/http-server.html#static-media-failure","text":"If you get a Static Media Failure; The following static media file failed to load: css/base.css , verify the permissions on the $NAUTOBOT_ROOT directory are 755 . Example of correct permissions: [root@localhost ~]# ls -l /opt/ total 4 drwxr-xr-x. 11 nautobot nautobot 4096 Apr 5 11:24 nautobot [root@localhost ~]# If the permissions are not correct, modify them accordingly. Example of modifying the permissions: [nautobot@localhost ~]$ ls -l /opt/ total 4 drwx------. 11 nautobot nautobot 4096 Apr 5 10:00 nautobot [nautobot@localhost ~]$ chmod 755 $NAUTOBOT [nautobot@localhost ~]$ ls -l /opt/ total 4 drwxr-xr-x. 11 nautobot nautobot 4096 Apr 5 11:24 nautobot","title":"Static Media Failure"},{"location":"installation/http-server.html#502-bad-gateway","text":"If you are able to connect but receive a 502 (bad gateway) error, check the following: The uWSGI worker processes are running ( systemctl status nautobot should show a status of active (running) ) NGINX is configured to connect to the port on which uWSGI is listening (default is 8001 ). SELinux may be preventing the reverse proxy connection. You may need to allow HTTP network connections with the command setsebool -P httpd_can_network_connect 1 . For further information, view the SELinux troubleshooting guide.","title":"502 Bad Gateway"},{"location":"installation/migrating-from-netbox.html","text":"Migrating to Nautobot from NetBox \u00b6 Review the Release Notes \u00b6 Be sure to carefully review all release notes that have been published. In particular, the Nautobot 1.0 release notes include an overview of key changes between NetBox 2.10 and Nautobot 1.0, while later release notes highlight incremental changes between Nautobot versions. Install Nautobot \u00b6 Install Nautobot as described in the documentation . Configure Nautobot \u00b6 Although Nautobot will run perfectly well with a default configuration (such as generated by nautobot-server init , you may want to replicate aspects of your previous NetBox configuration to Nautobot. Refer to the configuration documentation for details on the available options. Migrate Database Contents Using nautobot-netbox-importer \u00b6 Due to a number of significant infrastructural changes between the applications, you cannot simply point Nautobot at your existing NetBox PostgreSQL database and have it automatically load your data. Fortunately, Network to Code (NTC) and collaborators have developed a Nautobot plugin, nautobot-netbox-importer , that can be used to import a NetBox database dump file into Nautobot. For full details, refer to the plugin's own documentation , but here is a brief overview: Export your NetBox database to a JSON file. Install the importer plugin. Enable the importer plugin. Run the plugin's import command to import the data. Connect to Nautobot and verify that your data has been successfully imported. Migrate Files from NetBox to Nautobot \u00b6 Uploaded media (device images, etc.) are stored on the filesystem rather than in the database and hence need to be migrated separately. The same is true for custom scripts and reports that you may wish to import. Copy Uploaded Media \u00b6 The exact command will depend on where your MEDIA_ROOT is configured in NetBox as well as where it's configured in Nautobot, but in general it will be: cp -pr $NETBOX_MEDIA_ROOT/* $NAUTOBOT_MEDIA_ROOT/* Copy Custom Scripts and Reports \u00b6 Similarly, the exact commands depend on your SCRIPTS_ROOT and REPORTS_ROOT settings in NetBox and your JOBS_ROOT in Nautobot, but in general they will be: cp -pr $NETBOX_SCRIPTS_ROOT/* $NAUTOBOT_JOBS_ROOT/ cp -pr $NETBOX_REPORTS_ROOT/* $NAUTOBOT_JOBS_ROOT/ Update Scripts, Reports, and Plugins for Nautobot compatibility \u00b6 Depending on the complexity of your scripts, reports, or plugins, and how tightly integrated with NetBox they were, it may be simple or complex to port them to be compatible with Nautobot, and we cannot possibly provide a generalized step-by-step guide that would cover all possibilities. One change that you will certainly have to make to even begin this process, however, is updating the Python module names for any modules that were being imported from NetBox: circuits.* -> nautobot.circuits.* dcim.* -> nautobot.dcim.* extras.* -> nautobot.extras.* ipam.* -> nautobot.ipam.* netbox.* -> nautobot.core.* tenancy.* -> nautobot.tenancy.* utilities.* -> nautobot.utilities.* virtualization.* -> nautobot.virtualization.* Update Your other Integration Code \u00b6 If you have developed any custom integrations or plugins you may need to update some of your calls. Please see the data model changes below for guidance. Data Model Changes \u00b6 The following backwards-incompatible changes have been made to the data model in Nautobot. Status Fields \u00b6 Tip Status names are now lower-cased when setting the status field on CSV imports. The slug value is used for create/update of objects and for filtering in the API. A new Status model has been added to represent the status field for many models. Each status has a human-readable name field (e.g. Active ), and a slug field (e.g. active ). Display name \u00b6 Several models such as device type and VLAN exposed a display_name property, which has now been renamed to display . In fact, there are several other instances, especially in the REST API, where the display_name field was used and as such, all instances have been renamed to display . CSV Imports \u00b6 When using CSV import to define a status field on imported objects, such as when importing Devices or Prefixes, the Status.slug field is used. For example, the built-in Active status has a slug of active , so the active value would be used for import. Default Choices \u00b6 Because status fields are now stored in the database, they cannot have a default value, just like other similar objects like Device Roles or Device Types. In cases where status was not required to be set because it would use the default value, you must now provide a status yourself. Note For consistency in the API, the slug value of a status is used when creating or updating an object. Choices in Code \u00b6 All *StatusChoices enums used for populated status field choices (such as nautobot.dcim.choices.DeviceStatusChoices ) are deprecated. Any code you have that is leveraging these will now result in an error when performing lookups on objects with status fields. Anywhere you have code like this: from dcim.choices import DeviceStatusChoices from dcim.models import Device Device . objects . filter ( status = DeviceStatusChoices . STATUS_PLANNED ) Update it to this: from nautobot.extras.models import Status from nautobot.dcim.models import Device Device . objects . filter ( status = Status . objects . get ( slug = \"planned\" )) UUID Primary Database Keys \u00b6 Tip Primary key (aka ID) fields are no longer auto-incrementing integers and are now randomly-generated UUIDs. Database keys are now defined as randomly-generated Universally Unique Identifiers (UUIDs) instead of integers, protecting against certain classes of data-traversal attacks. Merge of UserConfig data into User model \u00b6 There is no longer a distinct UserConfig model; instead, user configuration and preferences are stored directly on the User model under the key config_data . Custom Fields \u00b6 Tip You can no longer rename or change the type of a custom field. Custom Fields have been overhauled for asserting data integrity and improving user experience. Custom Fields can no longer be renamed or have their type changed after they have been created. Choices for Custom Fields are now stored as discrete CustomFieldChoice database objects. Choices that are in active use cannot be deleted. IPAM Network Field Types \u00b6 Tip Nautobot 1.2 and later supports most of the same filter-based network membership queries as NetBox. See below and the filtering documentation for more details. (Prior to Nautobot 1.2, IPAM network objects only supported model-manager-based methods for network membership filtering.) All IPAM objects with network field types ( ipam.Aggregate , ipam.IPAddress , and ipam.Prefix ) are no longer hard-coded to use PostgreSQL-only inet or cidr field types and are now using a custom implementation leveraging SQL-standard varbinary field types. Technical Details \u00b6 Below is a summary of the underlying technical changes to network fields. These will be explained in more detail in the following sections. For IPAddress , the address field was exploded out to host , broadcast , and prefix_length fields; address was converted into a computed field. For Aggregate and Prefix objects, the prefix field was exploded out to network , broadcast , and prefix_length fields; prefix was converted into a computed field. The host , network , and broadcast fields are now of a varbinary database type, which is represented as a packed binary integer (for example, the host 1.1.1.1 is packed as b\"\\x01\\x01\\x01\\x01\" ) Network membership queries are accomplished by triangulating the \"position\" of an address using the IP, broadcast, and prefix length of the source and target addresses. Note You should never have to worry about the binary nature of how the network fields are stored in the database! The Django database ORM takes care of it all! Changes to IPAddress \u00b6 The following fields have changed when working with ipam.IPAddress objects: address is now a computed field \u00b6 This field is computed from {host}/{prefix_length} and is represented as a netaddr.IPNetwork object. >>> ip = IPAddress ( address = \"1.1.1.1/30\" ) >>> ip . address IPNetwork ( '1.1.1.1/30' ) While this field is now a virtual field, it can still be used in many ways. It can be used to create objects: >>> ip = IPAddress . objects . create ( address = \"1.1.1.1/30\" ) It can be used in .get() and .filter() lookups where address is the primary argument: >>> IPAddress . objects . get ( address = \"1.1.1.1/30\" ) IPNetwork ( '1.1.1.1/30' ) >>> IPAddress . objects . filter ( address = \"1.1.1.1/30\" ) < IPAddressQuerySet [ < IPAddress : 1.1.1.1 / 30 > ] > Note If you use a prefix_length other than /32 (IPv4) or /128 (IPv6) it must be included in your lookups This field cannot be used in nested filter expressions : >>> Device . objects . filter ( primary_ip4__address = \"1.1.1.1\" ) django . core . exceptions . FieldError : Related Field got invalid lookup : address host contains the IP address \u00b6 The IP (host) component of the address is now stored in the host field. >>> ip . host '1.1.1.1' This field can be used in nested filter expressions, for example: >>> Device . objects . filter ( primary_ip4__host = \"1.1.1.1\" ) IPAddress prefix_length contains the prefix length \u00b6 This is an integer, such as 30 for /30 . >>> ip . prefix_length 30 For IP addresses with a prefix length other than a host prefix, you will need to filter using host and prefix_length fields for greater accuracy. For example, if you have multiple IPAddress objects with the same host value but different prefix_length : >>> IPAddress . objects . create ( address = \"1.1.1.1/32\" ) < IPAddress : 1.1.1.1 / 32 > >>> IPAddress . objects . filter ( host = \"1.1.1.1\" ) < IPAddressQuerySet [ < IPAddress : 1.1.1.1 / 30 > , < IPAddress : 1.1.1.1 / 32 > ] > >>> IPAddress . objects . filter ( host = \"1.1.1.1\" , prefix_length = 30 ) < IPAddressQuerySet [ < IPAddress : 1.1.1.1 / 30 > ] > IPAddress broadcast contains the broadcast address \u00b6 If the prefix length is that of a host prefix (e.g. /32 ), broadcast will be the same as the host : >>> IPAddress . objects . get ( address = \"1.1.1.1/32\" ) . broadcast '1.1.1.1' If the prefix length is any larger (e.g. /24 ), broadcast will be that of the containing network for that prefix length (e.g. 1.1.1.255 ): >>> IPAddress . objects . create ( address = \"1.1.1.1/24\" ) . broadcast '1.1.1.255' Note This field is largely for internal use only for facilitating network membership queries and it is not recommend that you use it for filtering. Changes to Aggregate and Prefix \u00b6 The following fields have changed when working with ipam.Aggregate and ipam.Prefix objects. These objects share the same field changes. For these examples we will be using Prefix objects, but they apply just as equally to Aggregate objects. prefix is now a computed field \u00b6 This field is computed from {network}/{prefix_length} and is represented as a netaddr.IPNetwork object. While this field is now a virtual field, it can still be used in many ways. It can be used to create objects: >>> net = Prefix . objects . create ( prefix = \"1.1.1.0/24\" ) It can be used in .get() and .filter() lookups where prefix is the primary argument: >>> Prefix . objects . get ( prefix = \"1.1.1.0/24\" ) < Prefix : 1.1.1.0 / 24 > >>> Prefix . objects . filter ( prefix = \"1.1.1.0/24\" ) < PrefixQuerySet [ < Prefix : 1.1.1.0 / 24 > ] > network contains the network address \u00b6 The network component of the address is now stored in the network field. >>> net . network '1.1.1.0' Aggregate/Prefix prefix_length contains the prefix length \u00b6 This is an integer, such as 24 for /24 . >>> net . prefix_length 24 It's highly likely that you will have multiple objects with the same network address but varying prefix lengths, so you will need to filter using network and prefix_length fields for greater accuracy. For example, if you have multiple Prefix objects with the same network value but different prefix_length : >>> Prefix . objects . create ( prefix = \"1.1.1.0/25\" ) < Prefix : 1.1.1.0 / 25 > >>> Prefix . objects . filter ( network = \"1.1.1.0\" ) < PrefixQuerySet [ < Prefix : 1.1.1.0 / 24 > , < Prefix : 1.1.1.0 / 25 > ] > >>> Prefix . objects . filter ( network = \"1.1.1.0\" , prefix_length = 25 ) < PrefixQuerySet [ < Prefix : 1.1.1.0 / 25 > ] > Aggregate/Prefix broadcast contains the broadcast address \u00b6 The broadcast will be derived from the prefix_length and will be that of the last network address for that prefix length (e.g. 1.1.1.255 ): >>> Prefix . objects . get ( prefix = \"1.1.1.0/24\" ) . broadcast '1.1.1.255' Note This field is largely for internal use only for facilitating network membership queries and it is not recommend that you use it for filtering. Membership Lookups \u00b6 Nautobot 1.0.x and 1.1.x did not support the custom lookup expressions that NetBox supported for membership queries on IPAM objects (such as Prefix.objects.filter(prefix__net_contained=\"10.0.0.0/24\") ), but instead provided an alternate approach using model manager methods (such as Prefix.objects.net_contained(\"10.0.0.0/24\") ). In Nautobot 1.2.0 and later, both model manager methods and custom lookup expressions are supported for this purpose, but the latter are now preferred for most use cases and are generally equivalent to their counterparts in NetBox. Note Nautobot did not mimic the support of non-subnets for the net_in query to avoid mistakes and confusion caused by an IP address being mistaken for a /32 as an example. net_mask_length \u00b6 Returns target addresses matching the source address prefix length. Note The NetBox filter net_mask_length should use the prefix_length field for filtering. NetBox: IPAddress . objects . filter ( address__net_mask_length = value ) # or Prefix . objects . filter ( prefix__net_mask_length = value ) Nautobot: IPAddress . objects . filter ( prefix_length = value ) # or Prefix . objects . filter ( prefix_length = value ) REST API Changes \u00b6 The following backwards-incompatible changes have been made to the REST API in Nautobot. Display field \u00b6 In several endpoints such as device type and VLAN, a display_name field is used to expose a human friendly string value for the object. This field has been renamed to display and has been standardized across all model API endpoints. Custom Field Choices \u00b6 Custom field choices are exposed in Nautobot at a dedicated endpoint: /api/extras/custom-field-choices/ . This replaces the choices field on on the CustomField model and the subsequent endpoint: /api/extras/custom-fields/","title":"Migrating from NetBox"},{"location":"installation/migrating-from-netbox.html#migrating-to-nautobot-from-netbox","text":"","title":"Migrating to Nautobot from NetBox"},{"location":"installation/migrating-from-netbox.html#review-the-release-notes","text":"Be sure to carefully review all release notes that have been published. In particular, the Nautobot 1.0 release notes include an overview of key changes between NetBox 2.10 and Nautobot 1.0, while later release notes highlight incremental changes between Nautobot versions.","title":"Review the Release Notes"},{"location":"installation/migrating-from-netbox.html#install-nautobot","text":"Install Nautobot as described in the documentation .","title":"Install Nautobot"},{"location":"installation/migrating-from-netbox.html#configure-nautobot","text":"Although Nautobot will run perfectly well with a default configuration (such as generated by nautobot-server init , you may want to replicate aspects of your previous NetBox configuration to Nautobot. Refer to the configuration documentation for details on the available options.","title":"Configure Nautobot"},{"location":"installation/migrating-from-netbox.html#migrate-database-contents-using-nautobot-netbox-importer","text":"Due to a number of significant infrastructural changes between the applications, you cannot simply point Nautobot at your existing NetBox PostgreSQL database and have it automatically load your data. Fortunately, Network to Code (NTC) and collaborators have developed a Nautobot plugin, nautobot-netbox-importer , that can be used to import a NetBox database dump file into Nautobot. For full details, refer to the plugin's own documentation , but here is a brief overview: Export your NetBox database to a JSON file. Install the importer plugin. Enable the importer plugin. Run the plugin's import command to import the data. Connect to Nautobot and verify that your data has been successfully imported.","title":"Migrate Database Contents Using nautobot-netbox-importer"},{"location":"installation/migrating-from-netbox.html#migrate-files-from-netbox-to-nautobot","text":"Uploaded media (device images, etc.) are stored on the filesystem rather than in the database and hence need to be migrated separately. The same is true for custom scripts and reports that you may wish to import.","title":"Migrate Files from NetBox to Nautobot"},{"location":"installation/migrating-from-netbox.html#copy-uploaded-media","text":"The exact command will depend on where your MEDIA_ROOT is configured in NetBox as well as where it's configured in Nautobot, but in general it will be: cp -pr $NETBOX_MEDIA_ROOT/* $NAUTOBOT_MEDIA_ROOT/*","title":"Copy Uploaded Media"},{"location":"installation/migrating-from-netbox.html#copy-custom-scripts-and-reports","text":"Similarly, the exact commands depend on your SCRIPTS_ROOT and REPORTS_ROOT settings in NetBox and your JOBS_ROOT in Nautobot, but in general they will be: cp -pr $NETBOX_SCRIPTS_ROOT/* $NAUTOBOT_JOBS_ROOT/ cp -pr $NETBOX_REPORTS_ROOT/* $NAUTOBOT_JOBS_ROOT/","title":"Copy Custom Scripts and Reports"},{"location":"installation/migrating-from-netbox.html#update-scripts-reports-and-plugins-for-nautobot-compatibility","text":"Depending on the complexity of your scripts, reports, or plugins, and how tightly integrated with NetBox they were, it may be simple or complex to port them to be compatible with Nautobot, and we cannot possibly provide a generalized step-by-step guide that would cover all possibilities. One change that you will certainly have to make to even begin this process, however, is updating the Python module names for any modules that were being imported from NetBox: circuits.* -> nautobot.circuits.* dcim.* -> nautobot.dcim.* extras.* -> nautobot.extras.* ipam.* -> nautobot.ipam.* netbox.* -> nautobot.core.* tenancy.* -> nautobot.tenancy.* utilities.* -> nautobot.utilities.* virtualization.* -> nautobot.virtualization.*","title":"Update Scripts, Reports, and Plugins for Nautobot compatibility"},{"location":"installation/migrating-from-netbox.html#update-your-other-integration-code","text":"If you have developed any custom integrations or plugins you may need to update some of your calls. Please see the data model changes below for guidance.","title":"Update Your other Integration Code"},{"location":"installation/migrating-from-netbox.html#data-model-changes","text":"The following backwards-incompatible changes have been made to the data model in Nautobot.","title":"Data Model Changes"},{"location":"installation/migrating-from-netbox.html#status-fields","text":"Tip Status names are now lower-cased when setting the status field on CSV imports. The slug value is used for create/update of objects and for filtering in the API. A new Status model has been added to represent the status field for many models. Each status has a human-readable name field (e.g. Active ), and a slug field (e.g. active ).","title":"Status Fields"},{"location":"installation/migrating-from-netbox.html#display-name","text":"Several models such as device type and VLAN exposed a display_name property, which has now been renamed to display . In fact, there are several other instances, especially in the REST API, where the display_name field was used and as such, all instances have been renamed to display .","title":"Display name"},{"location":"installation/migrating-from-netbox.html#csv-imports","text":"When using CSV import to define a status field on imported objects, such as when importing Devices or Prefixes, the Status.slug field is used. For example, the built-in Active status has a slug of active , so the active value would be used for import.","title":"CSV Imports"},{"location":"installation/migrating-from-netbox.html#default-choices","text":"Because status fields are now stored in the database, they cannot have a default value, just like other similar objects like Device Roles or Device Types. In cases where status was not required to be set because it would use the default value, you must now provide a status yourself. Note For consistency in the API, the slug value of a status is used when creating or updating an object.","title":"Default Choices"},{"location":"installation/migrating-from-netbox.html#choices-in-code","text":"All *StatusChoices enums used for populated status field choices (such as nautobot.dcim.choices.DeviceStatusChoices ) are deprecated. Any code you have that is leveraging these will now result in an error when performing lookups on objects with status fields. Anywhere you have code like this: from dcim.choices import DeviceStatusChoices from dcim.models import Device Device . objects . filter ( status = DeviceStatusChoices . STATUS_PLANNED ) Update it to this: from nautobot.extras.models import Status from nautobot.dcim.models import Device Device . objects . filter ( status = Status . objects . get ( slug = \"planned\" ))","title":"Choices in Code"},{"location":"installation/migrating-from-netbox.html#uuid-primary-database-keys","text":"Tip Primary key (aka ID) fields are no longer auto-incrementing integers and are now randomly-generated UUIDs. Database keys are now defined as randomly-generated Universally Unique Identifiers (UUIDs) instead of integers, protecting against certain classes of data-traversal attacks.","title":"UUID Primary Database Keys"},{"location":"installation/migrating-from-netbox.html#merge-of-userconfig-data-into-user-model","text":"There is no longer a distinct UserConfig model; instead, user configuration and preferences are stored directly on the User model under the key config_data .","title":"Merge of UserConfig data into User model"},{"location":"installation/migrating-from-netbox.html#custom-fields","text":"Tip You can no longer rename or change the type of a custom field. Custom Fields have been overhauled for asserting data integrity and improving user experience. Custom Fields can no longer be renamed or have their type changed after they have been created. Choices for Custom Fields are now stored as discrete CustomFieldChoice database objects. Choices that are in active use cannot be deleted.","title":"Custom Fields"},{"location":"installation/migrating-from-netbox.html#ipam-network-field-types","text":"Tip Nautobot 1.2 and later supports most of the same filter-based network membership queries as NetBox. See below and the filtering documentation for more details. (Prior to Nautobot 1.2, IPAM network objects only supported model-manager-based methods for network membership filtering.) All IPAM objects with network field types ( ipam.Aggregate , ipam.IPAddress , and ipam.Prefix ) are no longer hard-coded to use PostgreSQL-only inet or cidr field types and are now using a custom implementation leveraging SQL-standard varbinary field types.","title":"IPAM Network Field Types"},{"location":"installation/migrating-from-netbox.html#technical-details","text":"Below is a summary of the underlying technical changes to network fields. These will be explained in more detail in the following sections. For IPAddress , the address field was exploded out to host , broadcast , and prefix_length fields; address was converted into a computed field. For Aggregate and Prefix objects, the prefix field was exploded out to network , broadcast , and prefix_length fields; prefix was converted into a computed field. The host , network , and broadcast fields are now of a varbinary database type, which is represented as a packed binary integer (for example, the host 1.1.1.1 is packed as b\"\\x01\\x01\\x01\\x01\" ) Network membership queries are accomplished by triangulating the \"position\" of an address using the IP, broadcast, and prefix length of the source and target addresses. Note You should never have to worry about the binary nature of how the network fields are stored in the database! The Django database ORM takes care of it all!","title":"Technical Details"},{"location":"installation/migrating-from-netbox.html#changes-to-ipaddress","text":"The following fields have changed when working with ipam.IPAddress objects:","title":"Changes to IPAddress"},{"location":"installation/migrating-from-netbox.html#address-is-now-a-computed-field","text":"This field is computed from {host}/{prefix_length} and is represented as a netaddr.IPNetwork object. >>> ip = IPAddress ( address = \"1.1.1.1/30\" ) >>> ip . address IPNetwork ( '1.1.1.1/30' ) While this field is now a virtual field, it can still be used in many ways. It can be used to create objects: >>> ip = IPAddress . objects . create ( address = \"1.1.1.1/30\" ) It can be used in .get() and .filter() lookups where address is the primary argument: >>> IPAddress . objects . get ( address = \"1.1.1.1/30\" ) IPNetwork ( '1.1.1.1/30' ) >>> IPAddress . objects . filter ( address = \"1.1.1.1/30\" ) < IPAddressQuerySet [ < IPAddress : 1.1.1.1 / 30 > ] > Note If you use a prefix_length other than /32 (IPv4) or /128 (IPv6) it must be included in your lookups This field cannot be used in nested filter expressions : >>> Device . objects . filter ( primary_ip4__address = \"1.1.1.1\" ) django . core . exceptions . FieldError : Related Field got invalid lookup : address","title":"address is now a computed field"},{"location":"installation/migrating-from-netbox.html#host-contains-the-ip-address","text":"The IP (host) component of the address is now stored in the host field. >>> ip . host '1.1.1.1' This field can be used in nested filter expressions, for example: >>> Device . objects . filter ( primary_ip4__host = \"1.1.1.1\" )","title":"host contains the IP address"},{"location":"installation/migrating-from-netbox.html#ipaddress-prefix_length-contains-the-prefix-length","text":"This is an integer, such as 30 for /30 . >>> ip . prefix_length 30 For IP addresses with a prefix length other than a host prefix, you will need to filter using host and prefix_length fields for greater accuracy. For example, if you have multiple IPAddress objects with the same host value but different prefix_length : >>> IPAddress . objects . create ( address = \"1.1.1.1/32\" ) < IPAddress : 1.1.1.1 / 32 > >>> IPAddress . objects . filter ( host = \"1.1.1.1\" ) < IPAddressQuerySet [ < IPAddress : 1.1.1.1 / 30 > , < IPAddress : 1.1.1.1 / 32 > ] > >>> IPAddress . objects . filter ( host = \"1.1.1.1\" , prefix_length = 30 ) < IPAddressQuerySet [ < IPAddress : 1.1.1.1 / 30 > ] >","title":"IPAddress prefix_length contains the prefix length"},{"location":"installation/migrating-from-netbox.html#ipaddress-broadcast-contains-the-broadcast-address","text":"If the prefix length is that of a host prefix (e.g. /32 ), broadcast will be the same as the host : >>> IPAddress . objects . get ( address = \"1.1.1.1/32\" ) . broadcast '1.1.1.1' If the prefix length is any larger (e.g. /24 ), broadcast will be that of the containing network for that prefix length (e.g. 1.1.1.255 ): >>> IPAddress . objects . create ( address = \"1.1.1.1/24\" ) . broadcast '1.1.1.255' Note This field is largely for internal use only for facilitating network membership queries and it is not recommend that you use it for filtering.","title":"IPAddress broadcast contains the broadcast address"},{"location":"installation/migrating-from-netbox.html#changes-to-aggregate-and-prefix","text":"The following fields have changed when working with ipam.Aggregate and ipam.Prefix objects. These objects share the same field changes. For these examples we will be using Prefix objects, but they apply just as equally to Aggregate objects.","title":"Changes to Aggregate and Prefix"},{"location":"installation/migrating-from-netbox.html#prefix-is-now-a-computed-field","text":"This field is computed from {network}/{prefix_length} and is represented as a netaddr.IPNetwork object. While this field is now a virtual field, it can still be used in many ways. It can be used to create objects: >>> net = Prefix . objects . create ( prefix = \"1.1.1.0/24\" ) It can be used in .get() and .filter() lookups where prefix is the primary argument: >>> Prefix . objects . get ( prefix = \"1.1.1.0/24\" ) < Prefix : 1.1.1.0 / 24 > >>> Prefix . objects . filter ( prefix = \"1.1.1.0/24\" ) < PrefixQuerySet [ < Prefix : 1.1.1.0 / 24 > ] >","title":"prefix is now a computed field"},{"location":"installation/migrating-from-netbox.html#network-contains-the-network-address","text":"The network component of the address is now stored in the network field. >>> net . network '1.1.1.0'","title":"network contains the network address"},{"location":"installation/migrating-from-netbox.html#aggregateprefix-prefix_length-contains-the-prefix-length","text":"This is an integer, such as 24 for /24 . >>> net . prefix_length 24 It's highly likely that you will have multiple objects with the same network address but varying prefix lengths, so you will need to filter using network and prefix_length fields for greater accuracy. For example, if you have multiple Prefix objects with the same network value but different prefix_length : >>> Prefix . objects . create ( prefix = \"1.1.1.0/25\" ) < Prefix : 1.1.1.0 / 25 > >>> Prefix . objects . filter ( network = \"1.1.1.0\" ) < PrefixQuerySet [ < Prefix : 1.1.1.0 / 24 > , < Prefix : 1.1.1.0 / 25 > ] > >>> Prefix . objects . filter ( network = \"1.1.1.0\" , prefix_length = 25 ) < PrefixQuerySet [ < Prefix : 1.1.1.0 / 25 > ] >","title":"Aggregate/Prefix prefix_length contains the prefix length"},{"location":"installation/migrating-from-netbox.html#aggregateprefix-broadcast-contains-the-broadcast-address","text":"The broadcast will be derived from the prefix_length and will be that of the last network address for that prefix length (e.g. 1.1.1.255 ): >>> Prefix . objects . get ( prefix = \"1.1.1.0/24\" ) . broadcast '1.1.1.255' Note This field is largely for internal use only for facilitating network membership queries and it is not recommend that you use it for filtering.","title":"Aggregate/Prefix broadcast contains the broadcast address"},{"location":"installation/migrating-from-netbox.html#membership-lookups","text":"Nautobot 1.0.x and 1.1.x did not support the custom lookup expressions that NetBox supported for membership queries on IPAM objects (such as Prefix.objects.filter(prefix__net_contained=\"10.0.0.0/24\") ), but instead provided an alternate approach using model manager methods (such as Prefix.objects.net_contained(\"10.0.0.0/24\") ). In Nautobot 1.2.0 and later, both model manager methods and custom lookup expressions are supported for this purpose, but the latter are now preferred for most use cases and are generally equivalent to their counterparts in NetBox. Note Nautobot did not mimic the support of non-subnets for the net_in query to avoid mistakes and confusion caused by an IP address being mistaken for a /32 as an example.","title":"Membership Lookups"},{"location":"installation/migrating-from-netbox.html#net_mask_length","text":"Returns target addresses matching the source address prefix length. Note The NetBox filter net_mask_length should use the prefix_length field for filtering. NetBox: IPAddress . objects . filter ( address__net_mask_length = value ) # or Prefix . objects . filter ( prefix__net_mask_length = value ) Nautobot: IPAddress . objects . filter ( prefix_length = value ) # or Prefix . objects . filter ( prefix_length = value )","title":"net_mask_length"},{"location":"installation/migrating-from-netbox.html#rest-api-changes","text":"The following backwards-incompatible changes have been made to the REST API in Nautobot.","title":"REST API Changes"},{"location":"installation/migrating-from-netbox.html#display-field","text":"In several endpoints such as device type and VLAN, a display_name field is used to expose a human friendly string value for the object. This field has been renamed to display and has been standardized across all model API endpoints.","title":"Display field"},{"location":"installation/migrating-from-netbox.html#custom-field-choices","text":"Custom field choices are exposed in Nautobot at a dedicated endpoint: /api/extras/custom-field-choices/ . This replaces the choices field on on the CustomField model and the subsequent endpoint: /api/extras/custom-fields/","title":"Custom Field Choices"},{"location":"installation/migrating-from-postgresql.html","text":"Migrating from PostgreSQL to MySQL \u00b6 This document explains how to migrate the contents of an existing Nautobot PostgreSQL database to a new MySQL database. Export data from PostgreSQL \u00b6 In your existing installation of Nautobot with PostgreSQL, run the following command to generate a JSON dump of the database contents. This may take several minutes to complete depending on the size of your database. (nautobot-postgres) $ nautobot-server dumpdata \\ --natural-foreign \\ --natural-primary \\ --exclude contenttypes \\ --exclude auth.permission \\ --exclude django_rq \\ --format json \\ --indent 2 \\ --traceback \\ > nautobot_dump.json This will result in a file named nautobot_dump.json . Create the MySQL database \u00b6 Create the MySQL database for Nautobot, ensuring that it is utilizing the default character set ( utf8mb4 ) and default collation ( utf8mb4_0900_ai_ci ) settings for case-insensitivity. It is required that MySQL will be case-insensitive. Because these encodings are the defaults, if your MySQL installation has not been modified, there will be nothing for you to do other than make sure. In very rare cases, there may problems when importing your data from the case-sensitive PostgreSQL database dump that will need to be handled on a case-by-case basis. Please refer to the instructions for CentOS/RHEL or Ubuntu as necessary if you are unsure how to set up MySQL and create the Nautobot database. Confirming database encoding \u00b6 To confirm that your MySQL database has the correct encoding, you may start up a database shell using nautobot-server dbshell and run the following command: (nautobot-mysql) $ nautobot-server dbshell mysql> SELECT @@character_set_database, @@collation_database; +--------------------------+----------------------+ | @@character_set_database | @@collation_database | +--------------------------+----------------------+ | utf8mb4 | utf8mb4_0900_ai_ci | +--------------------------+----------------------+ 1 row in set (0.00 sec) Apply database migrations to the MySQL database \u00b6 With Nautobot pointing to the MySQL database (we recommend creating a new Nautobot installation for this purpose), run nautobot-server migrate to create all of Nautobot's tables in the database: (nautobot-mysql) $ nautobot-server migrate Remove the auto-populated Status records from the MySQL database \u00b6 A side effect of the nautobot-server migrate command is that it will populate the Status table with a number of predefined records. This is normally useful for getting started quickly with Nautobot, but since we're going to be importing data from our other database, these records will likely conflict with the records to be imported. Therefore we need to remove them, using the nautobot-server nbshell command in our MySQL instance of Nautobot: (nautobot-mysql) $ nautobot-server nbshell ### Nautobot interactive shell (32cec46b2b7e) ### Python 3.9.7 | Django 3.1.13 | Nautobot 1.1.3 ### lsmodels() will show available models. Use help(<model>) for more info. >>> Status.objects.all().delete() (67, {'extras.Status_content_types': 48, 'extras.Status': 19}) >>> Press Control-D to exit the nbshell when you are finished. Import the database dump into MySQL \u00b6 Use the nautobot-server loaddata command to import the database dump that you previously created. This may take several minutes to complete depending on the size of your database. (nautobot-mysql) $ nautobot-server loaddata --traceback nautobot_dump.json Assuming that the command ran to completion with no errors, you should now have a fully populated clone of your original database in MySQL.","title":"Migrating from PostgreSQL to MySQL"},{"location":"installation/migrating-from-postgresql.html#migrating-from-postgresql-to-mysql","text":"This document explains how to migrate the contents of an existing Nautobot PostgreSQL database to a new MySQL database.","title":"Migrating from PostgreSQL to MySQL"},{"location":"installation/migrating-from-postgresql.html#export-data-from-postgresql","text":"In your existing installation of Nautobot with PostgreSQL, run the following command to generate a JSON dump of the database contents. This may take several minutes to complete depending on the size of your database. (nautobot-postgres) $ nautobot-server dumpdata \\ --natural-foreign \\ --natural-primary \\ --exclude contenttypes \\ --exclude auth.permission \\ --exclude django_rq \\ --format json \\ --indent 2 \\ --traceback \\ > nautobot_dump.json This will result in a file named nautobot_dump.json .","title":"Export data from PostgreSQL"},{"location":"installation/migrating-from-postgresql.html#create-the-mysql-database","text":"Create the MySQL database for Nautobot, ensuring that it is utilizing the default character set ( utf8mb4 ) and default collation ( utf8mb4_0900_ai_ci ) settings for case-insensitivity. It is required that MySQL will be case-insensitive. Because these encodings are the defaults, if your MySQL installation has not been modified, there will be nothing for you to do other than make sure. In very rare cases, there may problems when importing your data from the case-sensitive PostgreSQL database dump that will need to be handled on a case-by-case basis. Please refer to the instructions for CentOS/RHEL or Ubuntu as necessary if you are unsure how to set up MySQL and create the Nautobot database.","title":"Create the MySQL database"},{"location":"installation/migrating-from-postgresql.html#confirming-database-encoding","text":"To confirm that your MySQL database has the correct encoding, you may start up a database shell using nautobot-server dbshell and run the following command: (nautobot-mysql) $ nautobot-server dbshell mysql> SELECT @@character_set_database, @@collation_database; +--------------------------+----------------------+ | @@character_set_database | @@collation_database | +--------------------------+----------------------+ | utf8mb4 | utf8mb4_0900_ai_ci | +--------------------------+----------------------+ 1 row in set (0.00 sec)","title":"Confirming database encoding"},{"location":"installation/migrating-from-postgresql.html#apply-database-migrations-to-the-mysql-database","text":"With Nautobot pointing to the MySQL database (we recommend creating a new Nautobot installation for this purpose), run nautobot-server migrate to create all of Nautobot's tables in the database: (nautobot-mysql) $ nautobot-server migrate","title":"Apply database migrations to the MySQL database"},{"location":"installation/migrating-from-postgresql.html#remove-the-auto-populated-status-records-from-the-mysql-database","text":"A side effect of the nautobot-server migrate command is that it will populate the Status table with a number of predefined records. This is normally useful for getting started quickly with Nautobot, but since we're going to be importing data from our other database, these records will likely conflict with the records to be imported. Therefore we need to remove them, using the nautobot-server nbshell command in our MySQL instance of Nautobot: (nautobot-mysql) $ nautobot-server nbshell ### Nautobot interactive shell (32cec46b2b7e) ### Python 3.9.7 | Django 3.1.13 | Nautobot 1.1.3 ### lsmodels() will show available models. Use help(<model>) for more info. >>> Status.objects.all().delete() (67, {'extras.Status_content_types': 48, 'extras.Status': 19}) >>> Press Control-D to exit the nbshell when you are finished.","title":"Remove the auto-populated Status records from the MySQL database"},{"location":"installation/migrating-from-postgresql.html#import-the-database-dump-into-mysql","text":"Use the nautobot-server loaddata command to import the database dump that you previously created. This may take several minutes to complete depending on the size of your database. (nautobot-mysql) $ nautobot-server loaddata --traceback nautobot_dump.json Assuming that the command ran to completion with no errors, you should now have a fully populated clone of your original database in MySQL.","title":"Import the database dump into MySQL"},{"location":"installation/nautobot.html","text":"Installing Nautobot \u00b6 This section of the documentation discusses installing and configuring the Nautobot application itself. These instructions will guide you through the following actions: Establish a Nautobot root directory for the application environment Create a nautobot system account Create a Python virtual environment (virtualenv) Install Nautobot and all required Python packages Run the database schema migrations Aggregate static resource files on disk Verify the installation using the development/test server Important Your database server and Redis must have been successfully installed before continuing with deployment steps. If you haven't done that yet, please visit the guide on Installing Nautobot Dependencies Choose your NAUTOBOT_ROOT \u00b6 You need to select a directory path where everything related to Nautobot will be installed. We will use this value across the documentation and it will be referred to as NAUTOBOT_ROOT . We will be using this path as the home directory of the nautobot user. Tip We have selected /opt/nautobot , but you may use any directory you choose. Later on, we will need to set this directory path as the NAUTOBOT_ROOT environment variable to tell Nautobot where to find its files and settings. Create the Nautobot System User \u00b6 Create a system user account named nautobot . This user will own all of the Nautobot files, and the Nautobot web services will be configured to run under this account. The following command also creates the /opt/nautobot directory and sets it as the home directory for the user. $ sudo useradd --system --shell /bin/bash --create-home --home-dir /opt/nautobot nautobot Setup the Virtual Environment \u00b6 A Python virtual environment or virtualenv is like a container for a set of Python packages. A virtualenv allows you to build environments suited to specific projects without interfering with system packages or other projects. When installed per the documentation, Nautobot uses a virtual environment in production. In the following steps, we will have you create the virtualenv within the NAUTOBOT_ROOT you chose in the previous step. This is the same we had you set as the home directory as the nautobot user. Note Instead of deliberately requiring you to activate/deactivate the virtualenv, we are emphasizing on relying on the $PATH to access programs installed within it. We find this to be much more intuitive and natural when working with Nautobot in this way. Create the Virtual Environment \u00b6 As root, we're going to create the virtualenv in our NAUTOBOT_ROOT as the nautobot user to populate the /opt/nautobot directory with a self-contained Python environment including a bin directory for scripts and a lib directory for Python libraries. $ sudo -u nautobot python3 -m venv /opt/nautobot Update the Nautobot .bashrc \u00b6 So what about the NAUTOBOT_ROOT ? We've referenced this environment variable several times. Here is where it finally gets set. We need to set the NAUTOBOT_ROOT environment variable for the nautobot user and make sure that it always set without having to do it manually. Run this command to update ~/.bashrc for nautobot so that anytime you become nautobot , your NAUTOBOT_ROOT will be set automatically. $ echo \"export NAUTOBOT_ROOT=/opt/nautobot\" | sudo tee -a ~nautobot/.bashrc Sudo to nautobot \u00b6 It is critical to install Nautobot as the nautobot user so that we don't have to worry about fixing permissions later. $ sudo -iu nautobot Observe also that you can now echo the value of the NAUTOBOT_ROOT environment variable that is automatically set because we added to .bashrc : $ echo $NAUTOBOT_ROOT /opt/nautobot Warning Unless explicitly stated, all remaining steps requiring the use of pip3 or nautobot-server in this document should be performed as the nautobot user! Understanding the Virtual Environment \u00b6 Because the nautobot user was created with NAUTOBOT_ROOT set as its home directory and we had you set the shell to /bin/bash , the binary path $NAUTOBOT_ROOT/bin is automatically added to the beginning of the $PATH environment variable: In Ubuntu 20.04: $ echo $PATH /opt/nautobot/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin Due to differences between OS, in CentOS $PATH will appear as: $ echo $PATH /opt/nautobot/.local/bin:/opt/nautobot/bin:/opt/nautobot/.local/bin:/opt/nautobot/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/sbin Therefore, any commands executed by the nautobot user will always check $NAUTOBOT_ROOT/bin first. Since NAUTOBOT_ROOT also contains the Python virtualenv for Nautobot, all of the commands you will execute as the nautobot user, will automatically prefer the virtualenv's commands because they come first in the $PATH . As the nautobot user, you may use which pip3 to confirm that you are using the correct version of pip3 . The path should match that of $NAUTOBOT_ROOT/bin . For example: $ which pip3 /opt/nautobot/bin/pip3 This makes sure that the version of Python you're using, as well any dependencies that you install, remain isolated in this environment. Prepare the Virtual Environment \u00b6 Before we install anything into the virtualenv, we want to make sure that Pip is running the latest version. Pip is Python's package installer and is referred interchangeably as pip or pip3 . For the purpose of this document, we'll deliberately be referring to it as pip3 . Many common issues can be solved by running the latest version of Pip. Before continuing with installing Nautobot, upgrade Pip to its latest release. We also want to deliberately install the wheel library which will tell Pip to always try to install wheel packages if they are available. A wheel is a pre-compiled Python package , which is quicker and safer to install because it does not require development libraries or gcc to be installed on your system just so that some more advanced Python libraries can be compiled. $ pip3 install --upgrade pip wheel Install Nautobot \u00b6 Use Pip to install Nautobot: $ pip3 install nautobot Hint If you are using MySQL as your database backend, use pip3 install \"nautobot[mysql]\" to install Nautobot and the mysqlclient library together! Install MySQL client library \u00b6 If you are using MySQL as your database server you must install the mysqlclient database client for Python. Warning If you're using a MySQL database, Nautobot will not work without this client library. You cannot skip this step. $ pip3 install \"nautobot[mysql]\" Great! We have NAUTOBOT_ROOT ready for use by the nautobot user, so let's proceed to verifying the installation. Verify your Nautobot Installation \u00b6 You should now have a fancy nautobot-server command in your environment. This will be your gateway to all things Nautobot! Run it to confirm the installed version of nautobot : $ nautobot-server --version Configuration \u00b6 Before you can use Nautobot, you'll need to configure it by telling it where your database and Redis servers can be found, among other things. This is done with the nautobot_config.py configuration file. Initialize your configuration \u00b6 Initialize a new configuration by running nautobot-server init . You may specify an alternate location and detailed instructions for this are covered in the documentation on Nautobot Configuration . However, because we've set the NAUTOBOT_ROOT , this command will automatically create a new nautobot_config.py at the default location based on this at $NAUTOBOT_ROOT/nautobot_config.py : $ nautobot-server init Configuration file created at '/opt/nautobot/nautobot_config.py' Required Settings \u00b6 Your nautobot_config.py provides sane defaults for all of the configuration settings. You will inevitably need to update the settings for your environment, most notably the DATABASES setting. If you do not wish to modify the config, by default, many of these configuration settings can also be specified by environment variables. Please see Required Settings for further details. Edit $NAUTOBOT_ROOT/nautobot_config.py , and head over to the documentation on Required Settings to tweak your required settings. At a minimum, you'll need to update the following settings: ALLOWED_HOSTS : You must set this value. This can be set to [\"*\"] for a quick start, but this value is not suitable for production deployment. DATABASES : Database connection parameters. If you installed your database server on the same system as Nautobot, you'll need to update the USER and PASSWORD fields here. If you are using MySQL, you'll also need to update the ENGINE field, changing the default database driver suffix from django.db.backends.postgresql to django.db.backends.mysql . Redis settings : Redis configuration requires multiple settings including CACHEOPS_REDIS and RQ_QUEUES , if different from the defaults. If you installed Redis on the same system as Nautobot, you do not need to change these settings. Important You absolutely must update your required settings in your nautobot_config.py or Nautobot will not work. Warning If you are using MySQL as your database backend, you must also update the database ENGINE setting to django.db.backends.mysql . Save your changes to your nautobot_config.py and then proceed to the next step. MySQL Unicode Settings \u00b6 If you are using MySQL as your database backend, and you want to enable support for Unicode emojis, please make sure to add \"OPTIONS\": {\"charset\": \"utf8mb4\"} to your DATABASES setting. Please see the configuration guide on MySQL Unicode settings for more information. Optional Settings \u00b6 All Python packages required by Nautobot will be installed automatically when running pip3 install nautobot . Nautobot also supports the ability to install optional Python packages. If desired, these packages should be listed in local_requirements.txt within the NAUTOBOT_ROOT directory, such as /opt/nautobot/local_requirements.txt . If you decide to use any Nautobot plugins , they should be listed in this file. We will cover two examples of common optional settings below. Configuring NAPALM \u00b6 Nautobot provides built-in support for the NAPALM automation library, which allows Nautobot to fetch live data from devices and return it to a requester via its REST API. The NAPALM_USERNAME and NAPALM_PASSWORD configuration parameters define the credentials to be used when connecting to a device. To use NAPALM, add nautobot[napalm] to your local_requirements.txt so that it can be installed and kept up to date: $ echo \"nautobot[napalm]\" >> $NAUTOBOT_ROOT/local_requirements.txt Remote File Storage \u00b6 By default, Nautobot will use the local filesystem to store uploaded files. To use a remote filesystem, install the django-storages library and configure your desired storage backend in nautobot_config.py . To use remote file storage, add nautobot[remote_storage] to your local_requirements.txt so that it can be installed and kept up to date: $ echo \"nautobot[remote_storage]\" >> $NAUTOBOT_ROOT/local_requirements.txt Prepare the Database \u00b6 Before Nautobot can run, the database migrations must be performed to prepare the database for use. This will populate the database tables and relationships: $ nautobot-server migrate Create a Superuser \u00b6 Nautobot does not come with any predefined user accounts. You'll need to create a administrative superuser account to be able to log into Nautobot for the first time. Specifying an email address for the user is not required, but be sure to use a very strong password. $ nautobot-server createsuperuser Create Static Directories \u00b6 Nautobot relies upon many static files including: git - For storing Git repositories jobs - For storing custom Jobs media - For storing uploaded images and attachments (such as device type images) static - The home for CSS, JavaScript, and images used to serve the web interface Each of these have their own corresponding setting that defined in nautobot_config.py , but by default they will all be placed in NAUTOBOT_ROOT unless you tell Nautobot otherwise by customizing their unique variable. The collectstatic command will create these directories if they do not exist, and in the case of the static files directory, it will also copy the appropriate files: $ nautobot-server collectstatic Install Local Requirements \u00b6 Note If you did not create a local_requirements.txt above, please skip this step. This step is entirely optional. As indicated above, we mentioned that any extra local requirements should go into $NAUTOBOT_ROOT/local_requirements.txt . $ pip3 install -r $NAUTOBOT_ROOT/local_requirements.txt Check your Configuration \u00b6 Nautobot leverages Django's built-in system check framework to validate the configuration to detect common problems and to provide hints for how to fix them. Checks are ran automatically when running a development server using nautobot-server runserver , but not when running in production using WSGI. Hint Get into the habit of running checks before deployments! $ nautobot-server check Test the Application \u00b6 At this point, we should be able to run Nautobot's development server for testing. We can check by starting a development instance: $ nautobot-server runserver 0.0.0.0:8080 --insecure Next, connect to the name or IP of the server (as defined in ALLOWED_HOSTS ) on port 8080; for example, http://127.0.0.1:8080/ . You should be greeted with the Nautobot home page. Danger DO NOT USE THIS SERVER IN A PRODUCTION SETTING. The development server is for development and testing purposes only. It is neither performant nor secure enough for production use. Warning If the test service does not run, or you cannot reach the Nautobot home page, something has gone wrong. Do not proceed with the rest of this guide until the installation has been corrected. Some platforms (such as CentOS) have a firewall enabled by default. If you are unable to connect to the server url on port 8080, verify the firewall policy to allow the appropriate connections, or select an already permitted port. Important Certain Nautobot features (Git repository synchronization, webhooks, jobs, etc.) depend on the presence of Nautobot's background Celery worker process, which is not automatically started by the runserver command. To start it for testing purposes, you can run nautobot-server celery worker (for background tasks) or nautobot-server rqworker (for jobs) separately. For production use, Nautobot and the worker processes should be managed by systemd rather than started manually, as described in the next section of this documentation. Note that the initial user interface will be locked down for non-authenticated users. Try logging in using the superuser account we just created. Once authenticated, you'll be able to access all areas of the UI: Type Ctrl-C to stop the development server. Now you're ready to proceed to starting Nautobot as a system service .","title":"Installing Nautobot"},{"location":"installation/nautobot.html#installing-nautobot","text":"This section of the documentation discusses installing and configuring the Nautobot application itself. These instructions will guide you through the following actions: Establish a Nautobot root directory for the application environment Create a nautobot system account Create a Python virtual environment (virtualenv) Install Nautobot and all required Python packages Run the database schema migrations Aggregate static resource files on disk Verify the installation using the development/test server Important Your database server and Redis must have been successfully installed before continuing with deployment steps. If you haven't done that yet, please visit the guide on Installing Nautobot Dependencies","title":"Installing Nautobot"},{"location":"installation/nautobot.html#choose-your-nautobot_root","text":"You need to select a directory path where everything related to Nautobot will be installed. We will use this value across the documentation and it will be referred to as NAUTOBOT_ROOT . We will be using this path as the home directory of the nautobot user. Tip We have selected /opt/nautobot , but you may use any directory you choose. Later on, we will need to set this directory path as the NAUTOBOT_ROOT environment variable to tell Nautobot where to find its files and settings.","title":"Choose your NAUTOBOT_ROOT"},{"location":"installation/nautobot.html#create-the-nautobot-system-user","text":"Create a system user account named nautobot . This user will own all of the Nautobot files, and the Nautobot web services will be configured to run under this account. The following command also creates the /opt/nautobot directory and sets it as the home directory for the user. $ sudo useradd --system --shell /bin/bash --create-home --home-dir /opt/nautobot nautobot","title":"Create the Nautobot System User"},{"location":"installation/nautobot.html#setup-the-virtual-environment","text":"A Python virtual environment or virtualenv is like a container for a set of Python packages. A virtualenv allows you to build environments suited to specific projects without interfering with system packages or other projects. When installed per the documentation, Nautobot uses a virtual environment in production. In the following steps, we will have you create the virtualenv within the NAUTOBOT_ROOT you chose in the previous step. This is the same we had you set as the home directory as the nautobot user. Note Instead of deliberately requiring you to activate/deactivate the virtualenv, we are emphasizing on relying on the $PATH to access programs installed within it. We find this to be much more intuitive and natural when working with Nautobot in this way.","title":"Setup the Virtual Environment"},{"location":"installation/nautobot.html#create-the-virtual-environment","text":"As root, we're going to create the virtualenv in our NAUTOBOT_ROOT as the nautobot user to populate the /opt/nautobot directory with a self-contained Python environment including a bin directory for scripts and a lib directory for Python libraries. $ sudo -u nautobot python3 -m venv /opt/nautobot","title":"Create the Virtual Environment"},{"location":"installation/nautobot.html#update-the-nautobot-bashrc","text":"So what about the NAUTOBOT_ROOT ? We've referenced this environment variable several times. Here is where it finally gets set. We need to set the NAUTOBOT_ROOT environment variable for the nautobot user and make sure that it always set without having to do it manually. Run this command to update ~/.bashrc for nautobot so that anytime you become nautobot , your NAUTOBOT_ROOT will be set automatically. $ echo \"export NAUTOBOT_ROOT=/opt/nautobot\" | sudo tee -a ~nautobot/.bashrc","title":"Update the Nautobot .bashrc"},{"location":"installation/nautobot.html#sudo-to-nautobot","text":"It is critical to install Nautobot as the nautobot user so that we don't have to worry about fixing permissions later. $ sudo -iu nautobot Observe also that you can now echo the value of the NAUTOBOT_ROOT environment variable that is automatically set because we added to .bashrc : $ echo $NAUTOBOT_ROOT /opt/nautobot Warning Unless explicitly stated, all remaining steps requiring the use of pip3 or nautobot-server in this document should be performed as the nautobot user!","title":"Sudo to nautobot"},{"location":"installation/nautobot.html#understanding-the-virtual-environment","text":"Because the nautobot user was created with NAUTOBOT_ROOT set as its home directory and we had you set the shell to /bin/bash , the binary path $NAUTOBOT_ROOT/bin is automatically added to the beginning of the $PATH environment variable: In Ubuntu 20.04: $ echo $PATH /opt/nautobot/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin Due to differences between OS, in CentOS $PATH will appear as: $ echo $PATH /opt/nautobot/.local/bin:/opt/nautobot/bin:/opt/nautobot/.local/bin:/opt/nautobot/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/sbin Therefore, any commands executed by the nautobot user will always check $NAUTOBOT_ROOT/bin first. Since NAUTOBOT_ROOT also contains the Python virtualenv for Nautobot, all of the commands you will execute as the nautobot user, will automatically prefer the virtualenv's commands because they come first in the $PATH . As the nautobot user, you may use which pip3 to confirm that you are using the correct version of pip3 . The path should match that of $NAUTOBOT_ROOT/bin . For example: $ which pip3 /opt/nautobot/bin/pip3 This makes sure that the version of Python you're using, as well any dependencies that you install, remain isolated in this environment.","title":"Understanding the Virtual Environment"},{"location":"installation/nautobot.html#prepare-the-virtual-environment","text":"Before we install anything into the virtualenv, we want to make sure that Pip is running the latest version. Pip is Python's package installer and is referred interchangeably as pip or pip3 . For the purpose of this document, we'll deliberately be referring to it as pip3 . Many common issues can be solved by running the latest version of Pip. Before continuing with installing Nautobot, upgrade Pip to its latest release. We also want to deliberately install the wheel library which will tell Pip to always try to install wheel packages if they are available. A wheel is a pre-compiled Python package , which is quicker and safer to install because it does not require development libraries or gcc to be installed on your system just so that some more advanced Python libraries can be compiled. $ pip3 install --upgrade pip wheel","title":"Prepare the Virtual Environment"},{"location":"installation/nautobot.html#install-nautobot","text":"Use Pip to install Nautobot: $ pip3 install nautobot Hint If you are using MySQL as your database backend, use pip3 install \"nautobot[mysql]\" to install Nautobot and the mysqlclient library together!","title":"Install Nautobot"},{"location":"installation/nautobot.html#install-mysql-client-library","text":"If you are using MySQL as your database server you must install the mysqlclient database client for Python. Warning If you're using a MySQL database, Nautobot will not work without this client library. You cannot skip this step. $ pip3 install \"nautobot[mysql]\" Great! We have NAUTOBOT_ROOT ready for use by the nautobot user, so let's proceed to verifying the installation.","title":"Install MySQL client library"},{"location":"installation/nautobot.html#verify-your-nautobot-installation","text":"You should now have a fancy nautobot-server command in your environment. This will be your gateway to all things Nautobot! Run it to confirm the installed version of nautobot : $ nautobot-server --version","title":"Verify your Nautobot Installation"},{"location":"installation/nautobot.html#configuration","text":"Before you can use Nautobot, you'll need to configure it by telling it where your database and Redis servers can be found, among other things. This is done with the nautobot_config.py configuration file.","title":"Configuration"},{"location":"installation/nautobot.html#initialize-your-configuration","text":"Initialize a new configuration by running nautobot-server init . You may specify an alternate location and detailed instructions for this are covered in the documentation on Nautobot Configuration . However, because we've set the NAUTOBOT_ROOT , this command will automatically create a new nautobot_config.py at the default location based on this at $NAUTOBOT_ROOT/nautobot_config.py : $ nautobot-server init Configuration file created at '/opt/nautobot/nautobot_config.py'","title":"Initialize your configuration"},{"location":"installation/nautobot.html#required-settings","text":"Your nautobot_config.py provides sane defaults for all of the configuration settings. You will inevitably need to update the settings for your environment, most notably the DATABASES setting. If you do not wish to modify the config, by default, many of these configuration settings can also be specified by environment variables. Please see Required Settings for further details. Edit $NAUTOBOT_ROOT/nautobot_config.py , and head over to the documentation on Required Settings to tweak your required settings. At a minimum, you'll need to update the following settings: ALLOWED_HOSTS : You must set this value. This can be set to [\"*\"] for a quick start, but this value is not suitable for production deployment. DATABASES : Database connection parameters. If you installed your database server on the same system as Nautobot, you'll need to update the USER and PASSWORD fields here. If you are using MySQL, you'll also need to update the ENGINE field, changing the default database driver suffix from django.db.backends.postgresql to django.db.backends.mysql . Redis settings : Redis configuration requires multiple settings including CACHEOPS_REDIS and RQ_QUEUES , if different from the defaults. If you installed Redis on the same system as Nautobot, you do not need to change these settings. Important You absolutely must update your required settings in your nautobot_config.py or Nautobot will not work. Warning If you are using MySQL as your database backend, you must also update the database ENGINE setting to django.db.backends.mysql . Save your changes to your nautobot_config.py and then proceed to the next step.","title":"Required Settings"},{"location":"installation/nautobot.html#mysql-unicode-settings","text":"If you are using MySQL as your database backend, and you want to enable support for Unicode emojis, please make sure to add \"OPTIONS\": {\"charset\": \"utf8mb4\"} to your DATABASES setting. Please see the configuration guide on MySQL Unicode settings for more information.","title":"MySQL Unicode Settings"},{"location":"installation/nautobot.html#optional-settings","text":"All Python packages required by Nautobot will be installed automatically when running pip3 install nautobot . Nautobot also supports the ability to install optional Python packages. If desired, these packages should be listed in local_requirements.txt within the NAUTOBOT_ROOT directory, such as /opt/nautobot/local_requirements.txt . If you decide to use any Nautobot plugins , they should be listed in this file. We will cover two examples of common optional settings below.","title":"Optional Settings"},{"location":"installation/nautobot.html#configuring-napalm","text":"Nautobot provides built-in support for the NAPALM automation library, which allows Nautobot to fetch live data from devices and return it to a requester via its REST API. The NAPALM_USERNAME and NAPALM_PASSWORD configuration parameters define the credentials to be used when connecting to a device. To use NAPALM, add nautobot[napalm] to your local_requirements.txt so that it can be installed and kept up to date: $ echo \"nautobot[napalm]\" >> $NAUTOBOT_ROOT/local_requirements.txt","title":"Configuring NAPALM"},{"location":"installation/nautobot.html#remote-file-storage","text":"By default, Nautobot will use the local filesystem to store uploaded files. To use a remote filesystem, install the django-storages library and configure your desired storage backend in nautobot_config.py . To use remote file storage, add nautobot[remote_storage] to your local_requirements.txt so that it can be installed and kept up to date: $ echo \"nautobot[remote_storage]\" >> $NAUTOBOT_ROOT/local_requirements.txt","title":"Remote File Storage"},{"location":"installation/nautobot.html#prepare-the-database","text":"Before Nautobot can run, the database migrations must be performed to prepare the database for use. This will populate the database tables and relationships: $ nautobot-server migrate","title":"Prepare the Database"},{"location":"installation/nautobot.html#create-a-superuser","text":"Nautobot does not come with any predefined user accounts. You'll need to create a administrative superuser account to be able to log into Nautobot for the first time. Specifying an email address for the user is not required, but be sure to use a very strong password. $ nautobot-server createsuperuser","title":"Create a Superuser"},{"location":"installation/nautobot.html#create-static-directories","text":"Nautobot relies upon many static files including: git - For storing Git repositories jobs - For storing custom Jobs media - For storing uploaded images and attachments (such as device type images) static - The home for CSS, JavaScript, and images used to serve the web interface Each of these have their own corresponding setting that defined in nautobot_config.py , but by default they will all be placed in NAUTOBOT_ROOT unless you tell Nautobot otherwise by customizing their unique variable. The collectstatic command will create these directories if they do not exist, and in the case of the static files directory, it will also copy the appropriate files: $ nautobot-server collectstatic","title":"Create Static Directories"},{"location":"installation/nautobot.html#install-local-requirements","text":"Note If you did not create a local_requirements.txt above, please skip this step. This step is entirely optional. As indicated above, we mentioned that any extra local requirements should go into $NAUTOBOT_ROOT/local_requirements.txt . $ pip3 install -r $NAUTOBOT_ROOT/local_requirements.txt","title":"Install Local Requirements"},{"location":"installation/nautobot.html#check-your-configuration","text":"Nautobot leverages Django's built-in system check framework to validate the configuration to detect common problems and to provide hints for how to fix them. Checks are ran automatically when running a development server using nautobot-server runserver , but not when running in production using WSGI. Hint Get into the habit of running checks before deployments! $ nautobot-server check","title":"Check your Configuration"},{"location":"installation/nautobot.html#test-the-application","text":"At this point, we should be able to run Nautobot's development server for testing. We can check by starting a development instance: $ nautobot-server runserver 0.0.0.0:8080 --insecure Next, connect to the name or IP of the server (as defined in ALLOWED_HOSTS ) on port 8080; for example, http://127.0.0.1:8080/ . You should be greeted with the Nautobot home page. Danger DO NOT USE THIS SERVER IN A PRODUCTION SETTING. The development server is for development and testing purposes only. It is neither performant nor secure enough for production use. Warning If the test service does not run, or you cannot reach the Nautobot home page, something has gone wrong. Do not proceed with the rest of this guide until the installation has been corrected. Some platforms (such as CentOS) have a firewall enabled by default. If you are unable to connect to the server url on port 8080, verify the firewall policy to allow the appropriate connections, or select an already permitted port. Important Certain Nautobot features (Git repository synchronization, webhooks, jobs, etc.) depend on the presence of Nautobot's background Celery worker process, which is not automatically started by the runserver command. To start it for testing purposes, you can run nautobot-server celery worker (for background tasks) or nautobot-server rqworker (for jobs) separately. For production use, Nautobot and the worker processes should be managed by systemd rather than started manually, as described in the next section of this documentation. Note that the initial user interface will be locked down for non-authenticated users. Try logging in using the superuser account we just created. Once authenticated, you'll be able to access all areas of the UI: Type Ctrl-C to stop the development server. Now you're ready to proceed to starting Nautobot as a system service .","title":"Test the Application"},{"location":"installation/selinux-troubleshooting.html","text":"SELinux Troubleshooting \u00b6 When installing Nautobot for the first time on a Redhat-based Linux Distribution, SELinux may prevent the Nautobot stack from working properly. An example is SELinux preventing the HTTP daemon (NGINX, et al.) from communicating to the Django application stack on the backend. Determine if SELinux is the Culprit \u00b6 An example of a broken application can be seen in the Nginx error logs below: $ sudo tail -f /var/log/nginx/error.log 2021/02/26 15:16:55 [crit] 67245#0: *494 connect() to 127.0.0.1:8080 failed (13: Permission denied) while connecting to upstream, client: 47.221.167.40, server: nautobot.example.com, request: \"GET / HTTP/1.1\", upstream: \"http://127.0.0.1:8080/\", host: \"nautobot.example.com\" 2021/02/26 15:16:56 [crit] 67245#0: *494 connect() to 127.0.0.1:8080 failed (13: Permission denied) while connecting to upstream, client: 47.221.167.40, server: nautobot.example.com, request: \"GET /favicon.ico HTTP/1.1\", upstream: \"http://127.0.0.1:8080/favicon.ico\", host: \"nautobot.example.com\", referrer: \"https://nautobot.example.com/\" 2021/02/26 15:16:58 [crit] 67245#0: *544 connect() to 127.0.0.1:8080 failed (13: Permission denied) while connecting to upstream, client: 47.221.167.40, server: nautobot.example.com, request: \"GET / HTTP/1.1\", upstream: \"http://127.0.0.1:8080/\", host: \"nautobot.example.com\" 2021/02/26 15:16:58 [crit] 67245#0: *544 connect() to 127.0.0.1:8080 failed (13: Permission denied) while connecting to upstream, client: 47.221.167.40, server: nautobot.example.com, request: \"GET /favicon.ico HTTP/1.1\", upstream: \"http://127.0.0.1:8080/favicon.ico\", host: \"nautobot.example.com\", referrer: \"https://nautobot.example.com/\" A quick way to verify that SELinux is preventing the application from working is to first, verify that SELinux is indeed enabled and operating in enforcing mode, and second, temporarily put SELinux in permissive mode. With SELinux in permissive mode, the application stack can be tested again. If the application starts working as expected, then SELinux is most likely the culprit. # sestatus | egrep 'SELinux status|Current mode' SELinux status: enabled Current mode: enforcing To put SELinux in permissive mode, execute the setenforce command with the 0 flag. # setenforce 0 # sestatus | egrep 'SELinux status|Current mode' SELinux status: enabled Current mode: permissive With SELinux in permissive mode, test the application stack and ensure everything is working properly. If the application is working, put SELinux back into enforcing mode. This is done by executing the setenforce command with the 1 flag. # setenforce 1 # sestatus | egrep 'SELinux status|Current mode' SELinux status: enabled Current mode: enforcing Troubleshoot SELinux \u00b6 Troubleshooting SELinux in most instances is straightforward. Using the sealert command to parse /var/log/audit/audit.log is the fastest way to pin-point SELinux specific issues. In many cases, sealert will also provide guidance as to how to resolve the issue. # sealert -a /var/log/audit/audit.log 100% done found 1 alerts in /var/log/audit/audit.log -------------------------------------------------------------------------------- SELinux is preventing /usr/sbin/nginx from name_connect access on the tcp_socket port 8080. ***** Plugin connect_ports (85.9 confidence) suggests ********************* If you want to allow /usr/sbin/nginx to connect to network port 8080 Then you need to modify the port type. Do # semanage port -a -t PORT_TYPE -p tcp 8080 where PORT_TYPE is one of the following: dns_port_t, dnssec_port_t, kerberos_port_t, ocsp_port_t. ***** Plugin catchall_boolean (7.33 confidence) suggests ****************** If you want to allow httpd to can network connect Then you must tell SELinux about this by enabling the 'httpd_can_network_connect' boolean. Do setsebool -P httpd_can_network_connect 1 ***** Plugin catchall_boolean (7.33 confidence) suggests ****************** If you want to allow nis to enabled Then you must tell SELinux about this by enabling the 'nis_enabled' boolean. Do setsebool -P nis_enabled 1 ***** Plugin catchall (1.35 confidence) suggests ************************** If you believe that nginx should be allowed name_connect access on the port 8080 tcp_socket by default. Then you should report this as a bug. You can generate a local policy module to allow this access. Do allow this access for now by executing: # ausearch -c 'nginx' --raw | audit2allow -M my-nginx # semodule -X 300 -i my-nginx.pp Additional Information: Source Context system_u:system_r:httpd_t:s0 Target Context system_u:object_r:unreserved_port_t:s0 Target Objects port 8080 [ tcp_socket ] Source nginx Source Path /usr/sbin/nginx Port 8080 Host <Unknown> Source RPM Packages nginx-1.14.1-9.module_el8.0.0+184+e34fea82.x86_64 Target RPM Packages SELinux Policy RPM selinux-policy-targeted-3.14.3-54.el8_3.2.noarch Local Policy RPM selinux-policy-targeted-3.14.3-54.el8_3.2.noarch Selinux Enabled True Policy Type targeted Enforcing Mode Enforcing Host Name nautobot.example.com Platform Linux nautobot.example.com 4.18.0-240.1.1.el8_3.x86_64 #1 SMP Thu Nov 19 17:20:08 UTC 2020 x86_64 x86_64 Alert Count 5 First Seen 2021-02-26 15:16:55 UTC Last Seen 2021-02-26 15:23:12 UTC Local ID b83bb817-85f6-4f5c-b6e0-eee3acc85504 Raw Audit Messages type=AVC msg=audit(1614352992.209:585): avc: denied { name_connect } for pid=67245 comm=\"nginx\" dest=8080 scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:object_r:unreserved_port_t:s0 tclass=tcp_socket permissive=0 type=SYSCALL msg=audit(1614352992.209:585): arch=x86_64 syscall=connect success=no exit=EACCES a0=12 a1=55d061477358 a2=10 a3=7ffc0c62296c items=0 ppid=67243 pid=67245 auid=4294967295 uid=988 gid=985 euid=988 suid=988 fsuid=988 egid=985 sgid=985 fsgid=985 tty=(none) ses=4294967295 comm=nginx exe=/usr/sbin/nginx subj=system_u:system_r:httpd_t:s0 key=(null)ARCH=x86_64 SYSCALL=connect AUID=unset UID=nginx GID=nginx EUID=nginx SUID=nginx FSUID=nginx EGID=nginx SGID=nginx FSGID=nginx Hash: nginx,httpd_t,unreserved_port_t,tcp_socket,name_connect In the first few lines of the audit, sealert details what SELinux is blocking and provides some options to remedy the issue. Since Nginx is communicating with the Nautobot application via HTTP, the second option is the best option. SELinux is preventing /usr/sbin/nginx from name_connect access on the tcp_socket port 8080. ***** Plugin connect_ports (85.9 confidence) suggests ********************* If you want to allow /usr/sbin/nginx to connect to network port 8080 Then you need to modify the port type. Do # semanage port -a -t PORT_TYPE -p tcp 8080 where PORT_TYPE is one of the following: dns_port_t, dnssec_port_t, kerberos_port_t, ocsp_port_t. ***** Plugin catchall_boolean (7.33 confidence) suggests ****************** If you want to allow httpd to can network connect Then you must tell SELinux about this by enabling the 'httpd_can_network_connect' boolean. Do setsebool -P httpd_can_network_connect 1 Executing setsebool -P httpd_can_network_connect 1 should remedy the issue. Verify this by executing the setsebool command, verify that SELinux is enabled and in enforcing mode via the sestatus command, and test the application stack for functionality. The first curl command demonstrates the failure. Nginx responds with a HTTP response code of 502, indicating that it is unable to communicate with the Nautobot application. After executing the setsebool command, curl is used again to verify that Nginx is able to communicate with the Nautobot application. This is verified with the HTTP response code of 200. # curl -ik https://nautobot.example.com HTTP/1.1 502 Bad Gateway Server: nginx/1.14.1 Date: Fri, 26 Feb 2021 15:41:22 GMT Content-Type: text/html Content-Length: 173 Connection: keep-alive # sestatus SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: targeted Current mode: enforcing Mode from config file: enforcing Policy MLS status: enabled Policy deny_unknown status: allowed Memory protection checking: actual (secure) Max kernel policy version: 32 # setsebool -P httpd_can_network_connect 1 # curl -ik https://nautobot.example.com HTTP/1.1 200 OK Server: nginx/1.14.1 Date: Fri, 26 Feb 2021 15:41:49 GMT Content-Type: text/html; charset=utf-8 Content-Length: 18698 Connection: keep-alive X-Content-Type-Options: nosniff Referrer-Policy: same-origin X-Frame-Options: DENY Vary: Cookie, Origin","title":"SELinux Troubleshooting"},{"location":"installation/selinux-troubleshooting.html#selinux-troubleshooting","text":"When installing Nautobot for the first time on a Redhat-based Linux Distribution, SELinux may prevent the Nautobot stack from working properly. An example is SELinux preventing the HTTP daemon (NGINX, et al.) from communicating to the Django application stack on the backend.","title":"SELinux Troubleshooting"},{"location":"installation/selinux-troubleshooting.html#determine-if-selinux-is-the-culprit","text":"An example of a broken application can be seen in the Nginx error logs below: $ sudo tail -f /var/log/nginx/error.log 2021/02/26 15:16:55 [crit] 67245#0: *494 connect() to 127.0.0.1:8080 failed (13: Permission denied) while connecting to upstream, client: 47.221.167.40, server: nautobot.example.com, request: \"GET / HTTP/1.1\", upstream: \"http://127.0.0.1:8080/\", host: \"nautobot.example.com\" 2021/02/26 15:16:56 [crit] 67245#0: *494 connect() to 127.0.0.1:8080 failed (13: Permission denied) while connecting to upstream, client: 47.221.167.40, server: nautobot.example.com, request: \"GET /favicon.ico HTTP/1.1\", upstream: \"http://127.0.0.1:8080/favicon.ico\", host: \"nautobot.example.com\", referrer: \"https://nautobot.example.com/\" 2021/02/26 15:16:58 [crit] 67245#0: *544 connect() to 127.0.0.1:8080 failed (13: Permission denied) while connecting to upstream, client: 47.221.167.40, server: nautobot.example.com, request: \"GET / HTTP/1.1\", upstream: \"http://127.0.0.1:8080/\", host: \"nautobot.example.com\" 2021/02/26 15:16:58 [crit] 67245#0: *544 connect() to 127.0.0.1:8080 failed (13: Permission denied) while connecting to upstream, client: 47.221.167.40, server: nautobot.example.com, request: \"GET /favicon.ico HTTP/1.1\", upstream: \"http://127.0.0.1:8080/favicon.ico\", host: \"nautobot.example.com\", referrer: \"https://nautobot.example.com/\" A quick way to verify that SELinux is preventing the application from working is to first, verify that SELinux is indeed enabled and operating in enforcing mode, and second, temporarily put SELinux in permissive mode. With SELinux in permissive mode, the application stack can be tested again. If the application starts working as expected, then SELinux is most likely the culprit. # sestatus | egrep 'SELinux status|Current mode' SELinux status: enabled Current mode: enforcing To put SELinux in permissive mode, execute the setenforce command with the 0 flag. # setenforce 0 # sestatus | egrep 'SELinux status|Current mode' SELinux status: enabled Current mode: permissive With SELinux in permissive mode, test the application stack and ensure everything is working properly. If the application is working, put SELinux back into enforcing mode. This is done by executing the setenforce command with the 1 flag. # setenforce 1 # sestatus | egrep 'SELinux status|Current mode' SELinux status: enabled Current mode: enforcing","title":"Determine if SELinux is the Culprit"},{"location":"installation/selinux-troubleshooting.html#troubleshoot-selinux","text":"Troubleshooting SELinux in most instances is straightforward. Using the sealert command to parse /var/log/audit/audit.log is the fastest way to pin-point SELinux specific issues. In many cases, sealert will also provide guidance as to how to resolve the issue. # sealert -a /var/log/audit/audit.log 100% done found 1 alerts in /var/log/audit/audit.log -------------------------------------------------------------------------------- SELinux is preventing /usr/sbin/nginx from name_connect access on the tcp_socket port 8080. ***** Plugin connect_ports (85.9 confidence) suggests ********************* If you want to allow /usr/sbin/nginx to connect to network port 8080 Then you need to modify the port type. Do # semanage port -a -t PORT_TYPE -p tcp 8080 where PORT_TYPE is one of the following: dns_port_t, dnssec_port_t, kerberos_port_t, ocsp_port_t. ***** Plugin catchall_boolean (7.33 confidence) suggests ****************** If you want to allow httpd to can network connect Then you must tell SELinux about this by enabling the 'httpd_can_network_connect' boolean. Do setsebool -P httpd_can_network_connect 1 ***** Plugin catchall_boolean (7.33 confidence) suggests ****************** If you want to allow nis to enabled Then you must tell SELinux about this by enabling the 'nis_enabled' boolean. Do setsebool -P nis_enabled 1 ***** Plugin catchall (1.35 confidence) suggests ************************** If you believe that nginx should be allowed name_connect access on the port 8080 tcp_socket by default. Then you should report this as a bug. You can generate a local policy module to allow this access. Do allow this access for now by executing: # ausearch -c 'nginx' --raw | audit2allow -M my-nginx # semodule -X 300 -i my-nginx.pp Additional Information: Source Context system_u:system_r:httpd_t:s0 Target Context system_u:object_r:unreserved_port_t:s0 Target Objects port 8080 [ tcp_socket ] Source nginx Source Path /usr/sbin/nginx Port 8080 Host <Unknown> Source RPM Packages nginx-1.14.1-9.module_el8.0.0+184+e34fea82.x86_64 Target RPM Packages SELinux Policy RPM selinux-policy-targeted-3.14.3-54.el8_3.2.noarch Local Policy RPM selinux-policy-targeted-3.14.3-54.el8_3.2.noarch Selinux Enabled True Policy Type targeted Enforcing Mode Enforcing Host Name nautobot.example.com Platform Linux nautobot.example.com 4.18.0-240.1.1.el8_3.x86_64 #1 SMP Thu Nov 19 17:20:08 UTC 2020 x86_64 x86_64 Alert Count 5 First Seen 2021-02-26 15:16:55 UTC Last Seen 2021-02-26 15:23:12 UTC Local ID b83bb817-85f6-4f5c-b6e0-eee3acc85504 Raw Audit Messages type=AVC msg=audit(1614352992.209:585): avc: denied { name_connect } for pid=67245 comm=\"nginx\" dest=8080 scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:object_r:unreserved_port_t:s0 tclass=tcp_socket permissive=0 type=SYSCALL msg=audit(1614352992.209:585): arch=x86_64 syscall=connect success=no exit=EACCES a0=12 a1=55d061477358 a2=10 a3=7ffc0c62296c items=0 ppid=67243 pid=67245 auid=4294967295 uid=988 gid=985 euid=988 suid=988 fsuid=988 egid=985 sgid=985 fsgid=985 tty=(none) ses=4294967295 comm=nginx exe=/usr/sbin/nginx subj=system_u:system_r:httpd_t:s0 key=(null)ARCH=x86_64 SYSCALL=connect AUID=unset UID=nginx GID=nginx EUID=nginx SUID=nginx FSUID=nginx EGID=nginx SGID=nginx FSGID=nginx Hash: nginx,httpd_t,unreserved_port_t,tcp_socket,name_connect In the first few lines of the audit, sealert details what SELinux is blocking and provides some options to remedy the issue. Since Nginx is communicating with the Nautobot application via HTTP, the second option is the best option. SELinux is preventing /usr/sbin/nginx from name_connect access on the tcp_socket port 8080. ***** Plugin connect_ports (85.9 confidence) suggests ********************* If you want to allow /usr/sbin/nginx to connect to network port 8080 Then you need to modify the port type. Do # semanage port -a -t PORT_TYPE -p tcp 8080 where PORT_TYPE is one of the following: dns_port_t, dnssec_port_t, kerberos_port_t, ocsp_port_t. ***** Plugin catchall_boolean (7.33 confidence) suggests ****************** If you want to allow httpd to can network connect Then you must tell SELinux about this by enabling the 'httpd_can_network_connect' boolean. Do setsebool -P httpd_can_network_connect 1 Executing setsebool -P httpd_can_network_connect 1 should remedy the issue. Verify this by executing the setsebool command, verify that SELinux is enabled and in enforcing mode via the sestatus command, and test the application stack for functionality. The first curl command demonstrates the failure. Nginx responds with a HTTP response code of 502, indicating that it is unable to communicate with the Nautobot application. After executing the setsebool command, curl is used again to verify that Nginx is able to communicate with the Nautobot application. This is verified with the HTTP response code of 200. # curl -ik https://nautobot.example.com HTTP/1.1 502 Bad Gateway Server: nginx/1.14.1 Date: Fri, 26 Feb 2021 15:41:22 GMT Content-Type: text/html Content-Length: 173 Connection: keep-alive # sestatus SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: targeted Current mode: enforcing Mode from config file: enforcing Policy MLS status: enabled Policy deny_unknown status: allowed Memory protection checking: actual (secure) Max kernel policy version: 32 # setsebool -P httpd_can_network_connect 1 # curl -ik https://nautobot.example.com HTTP/1.1 200 OK Server: nginx/1.14.1 Date: Fri, 26 Feb 2021 15:41:49 GMT Content-Type: text/html; charset=utf-8 Content-Length: 18698 Connection: keep-alive X-Content-Type-Options: nosniff Referrer-Policy: same-origin X-Frame-Options: DENY Vary: Cookie, Origin","title":"Troubleshoot SELinux"},{"location":"installation/services.html","text":"Deploying Nautobot: Web Service and Workers \u00b6 Services Overview \u00b6 Like most Django applications, Nautobot runs as a WSGI application behind an HTTP server. Nautobot comes preinstalled with uWSGI to use as the WSGI server, however other WSGI servers are available and should work similarly well. Gunicorn is a popular alternative. Additionally, certain Nautobot features (including Git repository synchronization, Webhooks, Jobs, etc.) depend on the presence of Nautobot's Celery background worker process, which is not automatically started with Nautobot and is run as a separate service. This document will guide you through setting up uWSGI and establishing Nautobot web and Celery worker services to run on system startup. Web Service \u00b6 Nautobot includes a nautobot-server start management command that directly invokes uWSGI. This command behaves exactly as uWSGI does, but allows us to maintain a single entrypoint into the Nautobot application. $ nautobot-server start --help Worker Service \u00b6 Nautobot requires at least one worker to consume background tasks required for advanced background features. A nautobot-server celery command is included that directly invokes Celery. This command behaves exactly as the Celery command-line utility does, but launches it through Nautobot's environment to share Redis and database connection settings transparently. $ nautobot-server celery --help Changed in version 1.1.0 Prior to version 1.1.0, Nautobot utilized RQ as the primary background task worker. As of Nautobot 1.1.0, RQ is now deprecated . RQ and the @job decorator for custom tasks are still supported for now, but users should migrate the primary worker to Celery and then, only if still required , run RQ concurrently with the Celery worker . RQ and the @job decorator will no longer be documented, and support for RQ will be removed in a future release. Configuration \u00b6 As the nautobot user, copy and paste the following into $NAUTOBOT_ROOT/uwsgi.ini : [uwsgi] ; The IP address (typically localhost) and port that the WSGI process should listen on socket = 127.0.0.1:8001 ; Fail to start if any parameter in the configuration file isn\u2019t explicitly understood by uWSGI strict = true ; Enable master process to gracefully re-spawn and pre-fork workers master = true ; Allow Python app-generated threads to run enable-threads = true ;Try to remove all of the generated file/sockets during shutdown vacuum = true ; Do not use multiple interpreters, allowing only Nautobot to run single-interpreter = true ; Shutdown when receiving SIGTERM (default is respawn) die-on-term = true ; Prevents uWSGI from starting if it is unable load Nautobot (usually due to errors) need-app = true ; By default, uWSGI has rather verbose logging that can be noisy disable-logging = true ; Assert that critical 4xx and 5xx errors are still logged log-4xx = true log-5xx = true ; Enable HTTP 1.1 keepalive support http-keepalive = 1 ; ; Advanced settings (disabled by default) ; Customize these for your environment if and only if you need them. ; Ref: https://uwsgi-docs.readthedocs.io/en/latest/Options.html ; ; Number of uWSGI workers to spawn. This should typically be 2n+1, where n is the number of CPU cores present. ; processes = 5 ; If using subdirectory hosting e.g. example.com/nautobot, you must uncomment this line. Otherwise you'll get double paths e.g. example.com/nautobot/nautobot/. ; Ref: https://uwsgi-docs.readthedocs.io/en/latest/Changelog-2.0.11.html#fixpathinfo-routing-action ; route-run = fixpathinfo: ; If hosted behind a load balancer uncomment these lines, the harakiri timeout should be greater than your load balancer timeout. ; Ref: https://uwsgi-docs.readthedocs.io/en/latest/HTTP.html?highlight=keepalive#http-keep-alive ; harakiri = 65 ; add-header = Connection: Keep-Alive ; http-keepalive = 1 This configuration should suffice for most initial installations, you may wish to edit this file to change the bound IP address and/or port number, or to make performance-related adjustments. See uWSGI documentation for the available configuration parameters. Note If you are deploying uWSGI behind a load balancer be sure to configure the harakiri timeout and keep alive appropriately. Setup systemd \u00b6 We'll use systemd to control both uWSGI and Nautobot's background worker processes. Warning The following steps must be performed with root permissions. Nautobot Service \u00b6 First, we'll establish the systemd unit file for the Nautobot web service. Copy and paste the following into /etc/systemd/system/nautobot.service : [Unit] Description = Nautobot WSGI Service Documentation = https://docs.nautobot.com/en/stable/ After = network-online.target Wants = network-online.target [Service] Type = simple Environment = \"NAUTOBOT_ROOT=/opt/nautobot\" User = nautobot Group = nautobot PIDFile = /var/tmp/nautobot.pid WorkingDirectory = /opt/nautobot ExecStart = /opt/nautobot/bin/nautobot-server start --pidfile /var/tmp/nautobot.pid --ini /opt/nautobot/uwsgi.ini ExecStop = /opt/nautobot/bin/nautobot-server start --stop /var/tmp/nautobot.pid ExecReload = /opt/nautobot/bin/nautobot-server start --reload /var/tmp/nautobot.pid Restart = on-failure RestartSec = 30 PrivateTmp = true [Install] WantedBy = multi-user.target Nautobot Background Services \u00b6 Changed in version 1.1.0 Prior to version 1.1.0, Nautobot utilized RQ as the primary background task worker. As of Nautobot 1.1.0, RQ is now deprecated and has been replaced with Celery. RQ can still be used by plugins for now, but will be removed in a future release. Please migrate your deployment to utilize Celery as documented below . Next, we will setup the systemd units for the Celery worker and Celery Beat scheduler. Celery Worker \u00b6 Added in version 1.1.0 The Celery worker service consumes tasks from background task queues and is required for taking advantage of advanced Nautobot features including Jobs , Custom Fields , and Git Repositories , among others. To establish the systemd unit file for the Celery worker, copy and paste the following into /etc/systemd/system/nautobot-worker.service : [Unit] Description = Nautobot Celery Worker Documentation = https://docs.nautobot.com/en/stable/ After = network-online.target Wants = network-online.target [Service] Type = exec Environment = \"NAUTOBOT_ROOT=/opt/nautobot\" User = nautobot Group = nautobot PIDFile = /var/tmp/nautobot-worker.pid WorkingDirectory = /opt/nautobot ExecStart = /opt/nautobot/bin/nautobot-server celery worker --loglevel INFO --pidfile /var/tmp/nautobot-worker.pid Restart = always RestartSec = 30 PrivateTmp = true [Install] WantedBy = multi-user.target Celery Beat Scheduler \u00b6 Added in version 1.2.0 The Celery Beat scheduler enables the periodic execution of and scheduling of background tasks. It is required to take advantage of the job scheduling and approval features. To establish the systemd unit file for the Celery Beat scheduler, copy and paste the following into /etc/systemd/system/nautobot-scheduler.service : [Unit] Description = Nautobot Celery Beat Scheduler Documentation = https://docs.nautobot.com/en/stable/ After = network-online.target Wants = network-online.target [Service] Type = exec Environment = \"NAUTOBOT_ROOT=/opt/nautobot\" User = nautobot Group = nautobot PIDFile = /var/tmp/nautobot-scheduler.pid WorkingDirectory = /opt/nautobot ExecStart = /opt/nautobot/bin/nautobot-server celery beat --loglevel INFO --pidfile /var/tmp/nautobot-scheduler.pid Restart = always RestartSec = 30 PrivateTmp = true [Install] WantedBy = multi-user.target Migrating to Celery from RQ \u00b6 Prior to migrating, you need to determine whether you have any plugins installed that run custom background tasks that still rely on the RQ worker. There are a few ways to do this. Two of them are: Ask your developer or administrator if there are any plugins running background tasks still using the RQ worker If you are savvy with code, search your code for the @job decorator or for from django_rq import job If you're upgrading from Nautobot version 1.0.x and are NOT running plugins that use the RQ worker, all you really need to do are two things. First, you must replace the contents of /etc/systemd/system/nautobot-worker.service with the systemd unit file provided just above. Next, you must update any custom background tasks that you may have written. If you do not have any custom background tasks, then you may continue on to the next section to reload your worker service to use Celery. To update your custom tasks, you'll need to do the following. Replace each import from django_rq import job with from nautobot.core.celery import nautobot_task Replace each decorator of @job with @nautobot_task For example: diff --git a/task_example.py b/task_example.py index f84073fb5..52baf6096 100644 --- a/task_example.py +++ b/task_example.py @@ -1,6 +1,6 @@ -from django_rq import job +from nautobot.core.celery import nautobot_task -@job(\"default\") +@nautobot_task def example_task(*args, **kwargs): return \"examples are cool!\" (END) If you are using plugins that use custom background tasks but have not yet made the change described above, you must run the RQ worker concurrently with the Celery worker until the plugin can be updated. Warning Failure to account for the Celery-to-RQ migration may break your custom background tasks Concurrent Celery and RQ Nautobot Workers \u00b6 If you must run the Celery and RQ workers concurrently, you must also configure the (deprecated) RQ worker. Copy and paste the following into /etc/systemd/system/nautobot-rq-worker.service : [Unit] Description = Nautobot Request Queue Worker Documentation = https://docs.nautobot.com/en/stable/ After = network-online.target Wants = network-online.target [Service] Type = simple Environment = \"NAUTOBOT_ROOT=/opt/nautobot\" User = nautobot Group = nautobot WorkingDirectory = /opt/nautobot ExecStart = /opt/nautobot/bin/nautobot-server rqworker Restart = on-failure RestartSec = 30 PrivateTmp = true [Install] WantedBy = multi-user.target Configure systemd \u00b6 Because we just added new service files, you'll need to reload the systemd daemon: $ sudo systemctl daemon-reload Then, start the nautobot , nautobot-worker , and nautobot-scheduler services and enable them to initiate at boot time: $ sudo systemctl enable --now nautobot nautobot-worker nautobot-scheduler If you are also running the RQ worker, repeat the above command for the RQ service: sudo systemctl enable --now nautobot-rq-worker Tip If you are running the concurrent RQ worker, you must remember to enable/check/restart the nautobot-rq-worker process as needed, oftentimes in addition to the nautobot-worker process. Verify the service \u00b6 You can use the command systemctl status nautobot.service to verify that the WSGI service is running: \u25cf nautobot.service - Nautobot WSGI Service Loaded: loaded (/etc/systemd/system/nautobot.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2021-03-05 22:23:33 UTC; 35min ago Docs: https://docs.nautobot.com/en/stable/ Main PID: 6992 (nautobot-server) Tasks: 16 (limit: 9513) Memory: 221.1M CGroup: /system.slice/nautobot.service \u251c\u25006992 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> \u251c\u25007007 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> \u251c\u25007010 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> \u251c\u25007013 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> \u251c\u25007016 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> \u2514\u25007019 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> Note If the Nautobot service fails to start, issue the command journalctl -eu nautobot.service to check for log messages that may indicate the problem. Once you've verified that the WSGI service and worker are up and running, move on to HTTP server setup . Troubleshooting \u00b6 Operational Error: Incorrect string value \u00b6 When using MySQL as a database backend, if you encounter a server error along the lines of Incorrect string value: '\\\\xF0\\\\x9F\\\\x92\\\\x80' for column , it is because you are running afoul of the legacy implementation of Unicode (aka utf8 ) encoding in MySQL. This often occurs when using modern Unicode glyphs like the famous poop emoji. Please see the configuration guide on MySQL Unicode settings for instructions on how to address this. Please see Computed fields with fallback value that is unicode results in OperationalError (#645) for more details. SVG images not rendered \u00b6 When serving Nautobot directly from uWSGI on RedHat or CentOS there may be a problem rendering .svg images to include the Nautobot logo. On the RedHat based operating systems there is no file /etc/mime.types by default, unfortunately, uWSGI looks for this file to serve static files (see Serving static files with uWSGI ). To work around this copy the file /etc/mime.types from a known good system for example an Ubuntu/Debian system or even the Nautobot container to /opt/nautobot/mime.types. Then add the following line to your uwsgi.ini file and restart the Nautobot services: mime-file = /opt/nautobot/mime.types Alternatively, host Nautobot behind Nginx as instructed in HTTP server setup .","title":"Deploying Nautobot Services"},{"location":"installation/services.html#deploying-nautobot-web-service-and-workers","text":"","title":"Deploying Nautobot: Web Service and Workers"},{"location":"installation/services.html#services-overview","text":"Like most Django applications, Nautobot runs as a WSGI application behind an HTTP server. Nautobot comes preinstalled with uWSGI to use as the WSGI server, however other WSGI servers are available and should work similarly well. Gunicorn is a popular alternative. Additionally, certain Nautobot features (including Git repository synchronization, Webhooks, Jobs, etc.) depend on the presence of Nautobot's Celery background worker process, which is not automatically started with Nautobot and is run as a separate service. This document will guide you through setting up uWSGI and establishing Nautobot web and Celery worker services to run on system startup.","title":"Services Overview"},{"location":"installation/services.html#web-service","text":"Nautobot includes a nautobot-server start management command that directly invokes uWSGI. This command behaves exactly as uWSGI does, but allows us to maintain a single entrypoint into the Nautobot application. $ nautobot-server start --help","title":"Web Service"},{"location":"installation/services.html#worker-service","text":"Nautobot requires at least one worker to consume background tasks required for advanced background features. A nautobot-server celery command is included that directly invokes Celery. This command behaves exactly as the Celery command-line utility does, but launches it through Nautobot's environment to share Redis and database connection settings transparently. $ nautobot-server celery --help Changed in version 1.1.0 Prior to version 1.1.0, Nautobot utilized RQ as the primary background task worker. As of Nautobot 1.1.0, RQ is now deprecated . RQ and the @job decorator for custom tasks are still supported for now, but users should migrate the primary worker to Celery and then, only if still required , run RQ concurrently with the Celery worker . RQ and the @job decorator will no longer be documented, and support for RQ will be removed in a future release.","title":"Worker Service"},{"location":"installation/services.html#configuration","text":"As the nautobot user, copy and paste the following into $NAUTOBOT_ROOT/uwsgi.ini : [uwsgi] ; The IP address (typically localhost) and port that the WSGI process should listen on socket = 127.0.0.1:8001 ; Fail to start if any parameter in the configuration file isn\u2019t explicitly understood by uWSGI strict = true ; Enable master process to gracefully re-spawn and pre-fork workers master = true ; Allow Python app-generated threads to run enable-threads = true ;Try to remove all of the generated file/sockets during shutdown vacuum = true ; Do not use multiple interpreters, allowing only Nautobot to run single-interpreter = true ; Shutdown when receiving SIGTERM (default is respawn) die-on-term = true ; Prevents uWSGI from starting if it is unable load Nautobot (usually due to errors) need-app = true ; By default, uWSGI has rather verbose logging that can be noisy disable-logging = true ; Assert that critical 4xx and 5xx errors are still logged log-4xx = true log-5xx = true ; Enable HTTP 1.1 keepalive support http-keepalive = 1 ; ; Advanced settings (disabled by default) ; Customize these for your environment if and only if you need them. ; Ref: https://uwsgi-docs.readthedocs.io/en/latest/Options.html ; ; Number of uWSGI workers to spawn. This should typically be 2n+1, where n is the number of CPU cores present. ; processes = 5 ; If using subdirectory hosting e.g. example.com/nautobot, you must uncomment this line. Otherwise you'll get double paths e.g. example.com/nautobot/nautobot/. ; Ref: https://uwsgi-docs.readthedocs.io/en/latest/Changelog-2.0.11.html#fixpathinfo-routing-action ; route-run = fixpathinfo: ; If hosted behind a load balancer uncomment these lines, the harakiri timeout should be greater than your load balancer timeout. ; Ref: https://uwsgi-docs.readthedocs.io/en/latest/HTTP.html?highlight=keepalive#http-keep-alive ; harakiri = 65 ; add-header = Connection: Keep-Alive ; http-keepalive = 1 This configuration should suffice for most initial installations, you may wish to edit this file to change the bound IP address and/or port number, or to make performance-related adjustments. See uWSGI documentation for the available configuration parameters. Note If you are deploying uWSGI behind a load balancer be sure to configure the harakiri timeout and keep alive appropriately.","title":"Configuration"},{"location":"installation/services.html#setup-systemd","text":"We'll use systemd to control both uWSGI and Nautobot's background worker processes. Warning The following steps must be performed with root permissions.","title":"Setup systemd"},{"location":"installation/services.html#nautobot-service","text":"First, we'll establish the systemd unit file for the Nautobot web service. Copy and paste the following into /etc/systemd/system/nautobot.service : [Unit] Description = Nautobot WSGI Service Documentation = https://docs.nautobot.com/en/stable/ After = network-online.target Wants = network-online.target [Service] Type = simple Environment = \"NAUTOBOT_ROOT=/opt/nautobot\" User = nautobot Group = nautobot PIDFile = /var/tmp/nautobot.pid WorkingDirectory = /opt/nautobot ExecStart = /opt/nautobot/bin/nautobot-server start --pidfile /var/tmp/nautobot.pid --ini /opt/nautobot/uwsgi.ini ExecStop = /opt/nautobot/bin/nautobot-server start --stop /var/tmp/nautobot.pid ExecReload = /opt/nautobot/bin/nautobot-server start --reload /var/tmp/nautobot.pid Restart = on-failure RestartSec = 30 PrivateTmp = true [Install] WantedBy = multi-user.target","title":"Nautobot Service"},{"location":"installation/services.html#nautobot-background-services","text":"Changed in version 1.1.0 Prior to version 1.1.0, Nautobot utilized RQ as the primary background task worker. As of Nautobot 1.1.0, RQ is now deprecated and has been replaced with Celery. RQ can still be used by plugins for now, but will be removed in a future release. Please migrate your deployment to utilize Celery as documented below . Next, we will setup the systemd units for the Celery worker and Celery Beat scheduler.","title":"Nautobot Background Services"},{"location":"installation/services.html#celery-worker","text":"Added in version 1.1.0 The Celery worker service consumes tasks from background task queues and is required for taking advantage of advanced Nautobot features including Jobs , Custom Fields , and Git Repositories , among others. To establish the systemd unit file for the Celery worker, copy and paste the following into /etc/systemd/system/nautobot-worker.service : [Unit] Description = Nautobot Celery Worker Documentation = https://docs.nautobot.com/en/stable/ After = network-online.target Wants = network-online.target [Service] Type = exec Environment = \"NAUTOBOT_ROOT=/opt/nautobot\" User = nautobot Group = nautobot PIDFile = /var/tmp/nautobot-worker.pid WorkingDirectory = /opt/nautobot ExecStart = /opt/nautobot/bin/nautobot-server celery worker --loglevel INFO --pidfile /var/tmp/nautobot-worker.pid Restart = always RestartSec = 30 PrivateTmp = true [Install] WantedBy = multi-user.target","title":"Celery Worker"},{"location":"installation/services.html#celery-beat-scheduler","text":"Added in version 1.2.0 The Celery Beat scheduler enables the periodic execution of and scheduling of background tasks. It is required to take advantage of the job scheduling and approval features. To establish the systemd unit file for the Celery Beat scheduler, copy and paste the following into /etc/systemd/system/nautobot-scheduler.service : [Unit] Description = Nautobot Celery Beat Scheduler Documentation = https://docs.nautobot.com/en/stable/ After = network-online.target Wants = network-online.target [Service] Type = exec Environment = \"NAUTOBOT_ROOT=/opt/nautobot\" User = nautobot Group = nautobot PIDFile = /var/tmp/nautobot-scheduler.pid WorkingDirectory = /opt/nautobot ExecStart = /opt/nautobot/bin/nautobot-server celery beat --loglevel INFO --pidfile /var/tmp/nautobot-scheduler.pid Restart = always RestartSec = 30 PrivateTmp = true [Install] WantedBy = multi-user.target","title":"Celery Beat Scheduler"},{"location":"installation/services.html#migrating-to-celery-from-rq","text":"Prior to migrating, you need to determine whether you have any plugins installed that run custom background tasks that still rely on the RQ worker. There are a few ways to do this. Two of them are: Ask your developer or administrator if there are any plugins running background tasks still using the RQ worker If you are savvy with code, search your code for the @job decorator or for from django_rq import job If you're upgrading from Nautobot version 1.0.x and are NOT running plugins that use the RQ worker, all you really need to do are two things. First, you must replace the contents of /etc/systemd/system/nautobot-worker.service with the systemd unit file provided just above. Next, you must update any custom background tasks that you may have written. If you do not have any custom background tasks, then you may continue on to the next section to reload your worker service to use Celery. To update your custom tasks, you'll need to do the following. Replace each import from django_rq import job with from nautobot.core.celery import nautobot_task Replace each decorator of @job with @nautobot_task For example: diff --git a/task_example.py b/task_example.py index f84073fb5..52baf6096 100644 --- a/task_example.py +++ b/task_example.py @@ -1,6 +1,6 @@ -from django_rq import job +from nautobot.core.celery import nautobot_task -@job(\"default\") +@nautobot_task def example_task(*args, **kwargs): return \"examples are cool!\" (END) If you are using plugins that use custom background tasks but have not yet made the change described above, you must run the RQ worker concurrently with the Celery worker until the plugin can be updated. Warning Failure to account for the Celery-to-RQ migration may break your custom background tasks","title":"Migrating to Celery from RQ"},{"location":"installation/services.html#concurrent-celery-and-rq-nautobot-workers","text":"If you must run the Celery and RQ workers concurrently, you must also configure the (deprecated) RQ worker. Copy and paste the following into /etc/systemd/system/nautobot-rq-worker.service : [Unit] Description = Nautobot Request Queue Worker Documentation = https://docs.nautobot.com/en/stable/ After = network-online.target Wants = network-online.target [Service] Type = simple Environment = \"NAUTOBOT_ROOT=/opt/nautobot\" User = nautobot Group = nautobot WorkingDirectory = /opt/nautobot ExecStart = /opt/nautobot/bin/nautobot-server rqworker Restart = on-failure RestartSec = 30 PrivateTmp = true [Install] WantedBy = multi-user.target","title":"Concurrent Celery and RQ Nautobot Workers"},{"location":"installation/services.html#configure-systemd","text":"Because we just added new service files, you'll need to reload the systemd daemon: $ sudo systemctl daemon-reload Then, start the nautobot , nautobot-worker , and nautobot-scheduler services and enable them to initiate at boot time: $ sudo systemctl enable --now nautobot nautobot-worker nautobot-scheduler If you are also running the RQ worker, repeat the above command for the RQ service: sudo systemctl enable --now nautobot-rq-worker Tip If you are running the concurrent RQ worker, you must remember to enable/check/restart the nautobot-rq-worker process as needed, oftentimes in addition to the nautobot-worker process.","title":"Configure systemd"},{"location":"installation/services.html#verify-the-service","text":"You can use the command systemctl status nautobot.service to verify that the WSGI service is running: \u25cf nautobot.service - Nautobot WSGI Service Loaded: loaded (/etc/systemd/system/nautobot.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2021-03-05 22:23:33 UTC; 35min ago Docs: https://docs.nautobot.com/en/stable/ Main PID: 6992 (nautobot-server) Tasks: 16 (limit: 9513) Memory: 221.1M CGroup: /system.slice/nautobot.service \u251c\u25006992 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> \u251c\u25007007 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> \u251c\u25007010 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> \u251c\u25007013 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> \u251c\u25007016 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> \u2514\u25007019 /opt/nautobot/bin/python3 /opt/nautobot/bin/nautobot-server start /> Note If the Nautobot service fails to start, issue the command journalctl -eu nautobot.service to check for log messages that may indicate the problem. Once you've verified that the WSGI service and worker are up and running, move on to HTTP server setup .","title":"Verify the service"},{"location":"installation/services.html#troubleshooting","text":"","title":"Troubleshooting"},{"location":"installation/services.html#operational-error-incorrect-string-value","text":"When using MySQL as a database backend, if you encounter a server error along the lines of Incorrect string value: '\\\\xF0\\\\x9F\\\\x92\\\\x80' for column , it is because you are running afoul of the legacy implementation of Unicode (aka utf8 ) encoding in MySQL. This often occurs when using modern Unicode glyphs like the famous poop emoji. Please see the configuration guide on MySQL Unicode settings for instructions on how to address this. Please see Computed fields with fallback value that is unicode results in OperationalError (#645) for more details.","title":"Operational Error: Incorrect string value"},{"location":"installation/services.html#svg-images-not-rendered","text":"When serving Nautobot directly from uWSGI on RedHat or CentOS there may be a problem rendering .svg images to include the Nautobot logo. On the RedHat based operating systems there is no file /etc/mime.types by default, unfortunately, uWSGI looks for this file to serve static files (see Serving static files with uWSGI ). To work around this copy the file /etc/mime.types from a known good system for example an Ubuntu/Debian system or even the Nautobot container to /opt/nautobot/mime.types. Then add the following line to your uwsgi.ini file and restart the Nautobot services: mime-file = /opt/nautobot/mime.types Alternatively, host Nautobot behind Nginx as instructed in HTTP server setup .","title":"SVG images not rendered"},{"location":"installation/ubuntu.html","text":"Installing Nautobot Dependencies on Ubuntu \u00b6 This installation guide assumes that you are running Ubuntu version 20.04 on your system. Install System Packages \u00b6 Install the prerequisite system libraries and utilities. This will install: Git Python 3 Pip Redis server and client $ sudo apt update -y $ sudo apt install -y git python3 python3-pip python3-venv python3-dev redis-server Database Setup \u00b6 In this step you'll set up your database server, create a database and database user for use by Nautobot, and verify your connection to the database. You must select either MySQL or PostgreSQL. PostgreSQL is used by default with Nautobot, so if you just want to get started or don't have a preference, please stick with PostgreSQL. Please follow the steps for your selected database backend below. PostgreSQL Setup \u00b6 Install PostgreSQL \u00b6 This will install the PostgreSQL database server and client. $ sudo apt install -y postgresql Create a PostgreSQL Database \u00b6 At a minimum, we need to create a database for Nautobot and assign it a username and password for authentication. This is done with the following commands. Danger Do not use the password from the example. Choose a strong, random password to ensure secure database authentication for your Nautobot installation. $ sudo -u postgres psql psql (12.5 (Ubuntu 12.5-0ubuntu0.20.04.1)) Type \"help\" for help. postgres=# CREATE DATABASE nautobot; CREATE DATABASE postgres=# CREATE USER nautobot WITH PASSWORD 'insecure_password'; CREATE ROLE postgres=# GRANT ALL PRIVILEGES ON DATABASE nautobot TO nautobot; GRANT postgres=# \\q Verify PostgreSQL Service Status \u00b6 You can verify that authentication works issuing the following command and providing the configured password. (Replace localhost with your database server if using a remote database.) If successful, you will enter a nautobot prompt. Type \\conninfo to confirm your connection, or type \\q to exit. $ psql --username nautobot --password --host localhost nautobot Password for user nautobot: psql (12.5 (Ubuntu 12.5-0ubuntu0.20.04.1)) SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off) Type \"help\" for help. nautobot=> \\conninfo You are connected to database \"nautobot\" as user \"nautobot\" on host \"localhost\" (address \"127.0.0.1\") at port \"5432\". SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off) nautobot=> \\q MySQL Setup \u00b6 Install MySQL \u00b6 This will install the MySQL database server and client. Additionally, MySQL requires that the MySQL development libraries are installed so that we may compile the Python mysqlclient library during the Nautobot installation steps. sudo apt install -y libmysqlclient-dev mysql-server Create a MySQL Database \u00b6 At a minimum, we need to create a database for Nautobot and assign it a username and password for authentication. This is done with the following commands. Note Replace localhost below with your database server if using a remote database. Danger Do not use the password from the example. Choose a strong, random password to ensure secure database authentication for your Nautobot installation. $ sudo -u root mysql Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 11 Server version: 8.0.25-0ubuntu0.20.04.1 (Ubuntu) Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> CREATE DATABASE nautobot; Query OK, 1 row affected (0.00 sec) mysql> CREATE USER 'nautobot'@'localhost' IDENTIFIED BY 'insecure_password'; Query OK, 0 rows affected (0.01 sec) mysql> GRANT ALL ON nautobot.* TO 'nautobot'@'localhost'; Query OK, 0 rows affected (0.00 sec) mysql> \\q Bye Verify MySQL Service Status \u00b6 You can verify that authentication works issuing the following command and providing the configured password. If successful, you will enter a mysql> prompt. Type status to confirm your connection, or type \\q to exit. Note Replace localhost below with your database server if using a remote database. $ mysql --user nautobot --password --host localhost nautobot Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 13 Server version: 8.0.25-0ubuntu0.20.04.1 (Ubuntu) Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> status -------------- mysql Ver 8.0.25-0ubuntu0.20.04.1 for Linux on x86_64 ((Ubuntu)) Connection id: 13 Current database: nautobot Current user: nautobot@localhost SSL: Not in use Current pager: stdout Using outfile: '' Using delimiter: ; Server version: 8.0.25-0ubuntu0.20.04.1 (Ubuntu) Protocol version: 10 Connection: Localhost via UNIX socket Server characterset: utf8mb4 Db characterset: utf8mb4 Client characterset: utf8mb4 Conn. characterset: utf8mb4 UNIX socket: /var/run/mysqld/mysqld.sock Binary data as: Hexadecimal Uptime: 26 min 31 sec Threads: 2 Questions: 29 Slow queries: 0 Opens: 193 Flush tables: 3 Open tables: 112 Queries per second avg: 0.018 -------------- mysql> \\q Bye Redis Setup \u00b6 Since Redis was already installed, let's just verify that it's working using redis-cli : $ redis-cli ping PONG Deploy Nautobot \u00b6 Now that Nautobot dependencies are installed and configured, you're ready to Install Nautobot !","title":"Installing Nautobot Dependencies on Ubuntu"},{"location":"installation/ubuntu.html#installing-nautobot-dependencies-on-ubuntu","text":"This installation guide assumes that you are running Ubuntu version 20.04 on your system.","title":"Installing Nautobot Dependencies on Ubuntu"},{"location":"installation/ubuntu.html#install-system-packages","text":"Install the prerequisite system libraries and utilities. This will install: Git Python 3 Pip Redis server and client $ sudo apt update -y $ sudo apt install -y git python3 python3-pip python3-venv python3-dev redis-server","title":"Install System Packages"},{"location":"installation/ubuntu.html#database-setup","text":"In this step you'll set up your database server, create a database and database user for use by Nautobot, and verify your connection to the database. You must select either MySQL or PostgreSQL. PostgreSQL is used by default with Nautobot, so if you just want to get started or don't have a preference, please stick with PostgreSQL. Please follow the steps for your selected database backend below.","title":"Database Setup"},{"location":"installation/ubuntu.html#postgresql-setup","text":"","title":"PostgreSQL Setup"},{"location":"installation/ubuntu.html#install-postgresql","text":"This will install the PostgreSQL database server and client. $ sudo apt install -y postgresql","title":"Install PostgreSQL"},{"location":"installation/ubuntu.html#create-a-postgresql-database","text":"At a minimum, we need to create a database for Nautobot and assign it a username and password for authentication. This is done with the following commands. Danger Do not use the password from the example. Choose a strong, random password to ensure secure database authentication for your Nautobot installation. $ sudo -u postgres psql psql (12.5 (Ubuntu 12.5-0ubuntu0.20.04.1)) Type \"help\" for help. postgres=# CREATE DATABASE nautobot; CREATE DATABASE postgres=# CREATE USER nautobot WITH PASSWORD 'insecure_password'; CREATE ROLE postgres=# GRANT ALL PRIVILEGES ON DATABASE nautobot TO nautobot; GRANT postgres=# \\q","title":"Create a PostgreSQL Database"},{"location":"installation/ubuntu.html#verify-postgresql-service-status","text":"You can verify that authentication works issuing the following command and providing the configured password. (Replace localhost with your database server if using a remote database.) If successful, you will enter a nautobot prompt. Type \\conninfo to confirm your connection, or type \\q to exit. $ psql --username nautobot --password --host localhost nautobot Password for user nautobot: psql (12.5 (Ubuntu 12.5-0ubuntu0.20.04.1)) SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off) Type \"help\" for help. nautobot=> \\conninfo You are connected to database \"nautobot\" as user \"nautobot\" on host \"localhost\" (address \"127.0.0.1\") at port \"5432\". SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off) nautobot=> \\q","title":"Verify PostgreSQL Service Status"},{"location":"installation/ubuntu.html#mysql-setup","text":"","title":"MySQL Setup"},{"location":"installation/ubuntu.html#install-mysql","text":"This will install the MySQL database server and client. Additionally, MySQL requires that the MySQL development libraries are installed so that we may compile the Python mysqlclient library during the Nautobot installation steps. sudo apt install -y libmysqlclient-dev mysql-server","title":"Install MySQL"},{"location":"installation/ubuntu.html#create-a-mysql-database","text":"At a minimum, we need to create a database for Nautobot and assign it a username and password for authentication. This is done with the following commands. Note Replace localhost below with your database server if using a remote database. Danger Do not use the password from the example. Choose a strong, random password to ensure secure database authentication for your Nautobot installation. $ sudo -u root mysql Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 11 Server version: 8.0.25-0ubuntu0.20.04.1 (Ubuntu) Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> CREATE DATABASE nautobot; Query OK, 1 row affected (0.00 sec) mysql> CREATE USER 'nautobot'@'localhost' IDENTIFIED BY 'insecure_password'; Query OK, 0 rows affected (0.01 sec) mysql> GRANT ALL ON nautobot.* TO 'nautobot'@'localhost'; Query OK, 0 rows affected (0.00 sec) mysql> \\q Bye","title":"Create a MySQL Database"},{"location":"installation/ubuntu.html#verify-mysql-service-status","text":"You can verify that authentication works issuing the following command and providing the configured password. If successful, you will enter a mysql> prompt. Type status to confirm your connection, or type \\q to exit. Note Replace localhost below with your database server if using a remote database. $ mysql --user nautobot --password --host localhost nautobot Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 13 Server version: 8.0.25-0ubuntu0.20.04.1 (Ubuntu) Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> status -------------- mysql Ver 8.0.25-0ubuntu0.20.04.1 for Linux on x86_64 ((Ubuntu)) Connection id: 13 Current database: nautobot Current user: nautobot@localhost SSL: Not in use Current pager: stdout Using outfile: '' Using delimiter: ; Server version: 8.0.25-0ubuntu0.20.04.1 (Ubuntu) Protocol version: 10 Connection: Localhost via UNIX socket Server characterset: utf8mb4 Db characterset: utf8mb4 Client characterset: utf8mb4 Conn. characterset: utf8mb4 UNIX socket: /var/run/mysqld/mysqld.sock Binary data as: Hexadecimal Uptime: 26 min 31 sec Threads: 2 Questions: 29 Slow queries: 0 Opens: 193 Flush tables: 3 Open tables: 112 Queries per second avg: 0.018 -------------- mysql> \\q Bye","title":"Verify MySQL Service Status"},{"location":"installation/ubuntu.html#redis-setup","text":"Since Redis was already installed, let's just verify that it's working using redis-cli : $ redis-cli ping PONG","title":"Redis Setup"},{"location":"installation/ubuntu.html#deploy-nautobot","text":"Now that Nautobot dependencies are installed and configured, you're ready to Install Nautobot !","title":"Deploy Nautobot"},{"location":"installation/upgrading.html","text":"Upgrading to a New Nautobot Release \u00b6 Review the Release Notes \u00b6 Prior to upgrading your Nautobot instance, be sure to carefully review all release notes that have been published since your current version was released. Although the upgrade process typically does not involve additional work, certain releases may introduce breaking or backward-incompatible changes. These are called out in the release notes under the release in which the change went into effect. The below sub-sections describe some key changes that deployers should be aware of, but are not intended to be a replacement for reading the release notes carefully and in depth. Updating from Nautobot 1.0.x to 1.1.x \u00b6 Migration from RQ to Celery \u00b6 Prior to version 1.1.0, Nautobot utilized RQ as the primary background task worker. As of Nautobot 1.1.0, RQ is now deprecated , as Celery has been introduced to eventually replace RQ for executing background tasks within Nautobot. All Nautobot core usage of RQ has been migrated to use Celery. RQ support for custom tasks was not removed in order to give plugin authors time to migrate, however, to continue to utilize advanced Nautobot features such as Git repository synchronization, webhooks, jobs, etc. you must migrate your nautobot-worker deployment from RQ to Celery. Please see the section on migrating to Celery from RQ for more information on how to easily migrate your deployment. Updating from Nautobot 1.1.x to 1.2.x \u00b6 Introduction of Celery Beat Scheduler \u00b6 As of Nautobot v1.2.0, Nautobot supports deferring (\"scheduling\") Jobs. To facilitate this, a new service called celery-scheduler is now required. Please review the service installation documentation to find out how to set it up. Updating from Nautobot 1.2.x to 1.3.x \u00b6 Revision of Recommended MySQL UTF-8 Encoding \u00b6 The recommended database encoding settings have been revised to rely upon the default UTF-8 encoding provided by MySQL for collation of data in the database. Previously we were recommending in our documentation that the collation encoding be set explicitly to utf8mb4_bin . We are now recommending utf8mb4_0900_ai_ci which is configured by default on unmodified MySQL database server deployments. The collation encoding is used to inform MySQL how characters are sorted in the database. This is important when it comes to retrieving data that has special characters or special byte-encoding such as accents or ligatures, and also including emojis. In some cases, with the utf8mb4_bin encoding we were previously recommending, case-insensitive searching may return inconsistent or incorrect results. Danger It is strongly recommended that you backup your database before executing this query and that you perform this in a non-production environment to identify any potential issues prior to updating your production environment. If you have an existing MySQL database, you may update your database to use the recommended encoding by using nautobot-server dbshell to launch a database shell and executing the following command: $ nautobot-server dbshell mysql> ALTER DATABASE nautobot COLLATE utf8mb4_0900_ai_ci; Query OK, 1 row affected (0.07 sec) Please see the official MySQL documentation on migrating collation encoding settings for more information on troubleshooting any issues you may encounter. Update Prerequisites to Required Versions \u00b6 Nautobot v1.3.0 and later requires the following: Dependency Minimum Version Python 3.7 PostgreSQL 9.6 Redis 4.0 Nautobot v1.1.0 and later can optionally support the following: Nautobot v1.1.0 added support for MySQL 8.0 as a database backend as an alternative to PostgreSQL. Dependency Minimum Version MySQL 8.0 Tip If you wish to migrate from PostgreSQL to MySQL, we recommend creating a new Nautobot installation based on MySQL and then migrating the database contents to the new installation , rather than attempting an in-place upgrade or migration. Install the Latest Release \u00b6 As with the initial installation, you can upgrade Nautobot by installing the Python package directly from the Python Package Index (PyPI). Warning Unless explicitly stated, all steps requiring the use of pip3 or nautobot-server in this document should be performed as the nautobot user! Upgrade Nautobot using pip3 : $ pip3 install --upgrade nautobot Upgrade your Optional Dependencies \u00b6 If you do not have any optional dependencies, you may skip this step. Once the new code is in place, verify that any optional Python packages required by your deployment (e.g. napalm or django-auth-ldap ) are listed in local_requirements.txt . Then, upgrade your dependencies using pip3 : $ pip3 install --upgrade -r $NAUTOBOT_ROOT/local_requirements.txt Run the Post Upgrade Operations \u00b6 Finally, run Nautobot's post_upgrade management command: $ nautobot-server post_upgrade This command performs the following actions: Applies any database migrations that were included in the release Generates any missing cable paths among all cable termination objects in the database Collects all static files to be served by the HTTP service Deletes stale content types from the database Deletes all expired user sessions from the database Clears all cached data to prevent conflicts with the new release Restart the Nautobot Services \u00b6 Finally, with root permissions, restart the web and background services: $ sudo systemctl restart nautobot nautobot-worker nautobot-scheduler","title":"Upgrading Nautobot"},{"location":"installation/upgrading.html#upgrading-to-a-new-nautobot-release","text":"","title":"Upgrading to a New Nautobot Release"},{"location":"installation/upgrading.html#review-the-release-notes","text":"Prior to upgrading your Nautobot instance, be sure to carefully review all release notes that have been published since your current version was released. Although the upgrade process typically does not involve additional work, certain releases may introduce breaking or backward-incompatible changes. These are called out in the release notes under the release in which the change went into effect. The below sub-sections describe some key changes that deployers should be aware of, but are not intended to be a replacement for reading the release notes carefully and in depth.","title":"Review the Release Notes"},{"location":"installation/upgrading.html#updating-from-nautobot-10x-to-11x","text":"","title":"Updating from Nautobot 1.0.x to 1.1.x"},{"location":"installation/upgrading.html#migration-from-rq-to-celery","text":"Prior to version 1.1.0, Nautobot utilized RQ as the primary background task worker. As of Nautobot 1.1.0, RQ is now deprecated , as Celery has been introduced to eventually replace RQ for executing background tasks within Nautobot. All Nautobot core usage of RQ has been migrated to use Celery. RQ support for custom tasks was not removed in order to give plugin authors time to migrate, however, to continue to utilize advanced Nautobot features such as Git repository synchronization, webhooks, jobs, etc. you must migrate your nautobot-worker deployment from RQ to Celery. Please see the section on migrating to Celery from RQ for more information on how to easily migrate your deployment.","title":"Migration from RQ to Celery"},{"location":"installation/upgrading.html#updating-from-nautobot-11x-to-12x","text":"","title":"Updating from Nautobot 1.1.x to 1.2.x"},{"location":"installation/upgrading.html#introduction-of-celery-beat-scheduler","text":"As of Nautobot v1.2.0, Nautobot supports deferring (\"scheduling\") Jobs. To facilitate this, a new service called celery-scheduler is now required. Please review the service installation documentation to find out how to set it up.","title":"Introduction of Celery Beat Scheduler"},{"location":"installation/upgrading.html#updating-from-nautobot-12x-to-13x","text":"","title":"Updating from Nautobot 1.2.x to 1.3.x"},{"location":"installation/upgrading.html#revision-of-recommended-mysql-utf-8-encoding","text":"The recommended database encoding settings have been revised to rely upon the default UTF-8 encoding provided by MySQL for collation of data in the database. Previously we were recommending in our documentation that the collation encoding be set explicitly to utf8mb4_bin . We are now recommending utf8mb4_0900_ai_ci which is configured by default on unmodified MySQL database server deployments. The collation encoding is used to inform MySQL how characters are sorted in the database. This is important when it comes to retrieving data that has special characters or special byte-encoding such as accents or ligatures, and also including emojis. In some cases, with the utf8mb4_bin encoding we were previously recommending, case-insensitive searching may return inconsistent or incorrect results. Danger It is strongly recommended that you backup your database before executing this query and that you perform this in a non-production environment to identify any potential issues prior to updating your production environment. If you have an existing MySQL database, you may update your database to use the recommended encoding by using nautobot-server dbshell to launch a database shell and executing the following command: $ nautobot-server dbshell mysql> ALTER DATABASE nautobot COLLATE utf8mb4_0900_ai_ci; Query OK, 1 row affected (0.07 sec) Please see the official MySQL documentation on migrating collation encoding settings for more information on troubleshooting any issues you may encounter.","title":"Revision of Recommended MySQL UTF-8 Encoding"},{"location":"installation/upgrading.html#update-prerequisites-to-required-versions","text":"Nautobot v1.3.0 and later requires the following: Dependency Minimum Version Python 3.7 PostgreSQL 9.6 Redis 4.0 Nautobot v1.1.0 and later can optionally support the following: Nautobot v1.1.0 added support for MySQL 8.0 as a database backend as an alternative to PostgreSQL. Dependency Minimum Version MySQL 8.0 Tip If you wish to migrate from PostgreSQL to MySQL, we recommend creating a new Nautobot installation based on MySQL and then migrating the database contents to the new installation , rather than attempting an in-place upgrade or migration.","title":"Update Prerequisites to Required Versions"},{"location":"installation/upgrading.html#install-the-latest-release","text":"As with the initial installation, you can upgrade Nautobot by installing the Python package directly from the Python Package Index (PyPI). Warning Unless explicitly stated, all steps requiring the use of pip3 or nautobot-server in this document should be performed as the nautobot user! Upgrade Nautobot using pip3 : $ pip3 install --upgrade nautobot","title":"Install the Latest Release"},{"location":"installation/upgrading.html#upgrade-your-optional-dependencies","text":"If you do not have any optional dependencies, you may skip this step. Once the new code is in place, verify that any optional Python packages required by your deployment (e.g. napalm or django-auth-ldap ) are listed in local_requirements.txt . Then, upgrade your dependencies using pip3 : $ pip3 install --upgrade -r $NAUTOBOT_ROOT/local_requirements.txt","title":"Upgrade your Optional Dependencies"},{"location":"installation/upgrading.html#run-the-post-upgrade-operations","text":"Finally, run Nautobot's post_upgrade management command: $ nautobot-server post_upgrade This command performs the following actions: Applies any database migrations that were included in the release Generates any missing cable paths among all cable termination objects in the database Collects all static files to be served by the HTTP service Deletes stale content types from the database Deletes all expired user sessions from the database Clears all cached data to prevent conflicts with the new release","title":"Run the Post Upgrade Operations"},{"location":"installation/upgrading.html#restart-the-nautobot-services","text":"Finally, with root permissions, restart the web and background services: $ sudo systemctl restart nautobot nautobot-worker nautobot-scheduler","title":"Restart the Nautobot Services"},{"location":"models/circuits/circuit.html","text":"Circuits \u00b6 A communications circuit represents a single physical link connecting exactly two endpoints, commonly referred to as its A and Z terminations. A circuit in Nautobot may have zero, one, or two terminations defined. It is common to have only one termination defined when you don't necessarily care about the details of the provider side of the circuit, e.g. for Internet access circuits. Both terminations would likely be modeled for circuits which connect one customer site to another. Each circuit is associated with a provider and a user-defined type. For example, you might have Internet access circuits delivered to each site by one provider, and private MPLS circuits delivered by another. Each circuit must be assigned a circuit ID, each of which must be unique per provider. Each circuit must be assigned to a status . The following statuses are available by default: Planned Provisioning Active Offline Deprovisioning Decommissioned Circuits also have optional fields for annotating their installation date and commit rate, and may be assigned to Nautobot tenants. Note Nautobot currently models only physical circuits: those which have exactly two endpoints. It is common to layer virtualized constructs ( virtual circuits ) such as MPLS or EVPN tunnels on top of these, however Nautobot does not yet support virtual circuit modeling.","title":"Circuits"},{"location":"models/circuits/circuit.html#circuits","text":"A communications circuit represents a single physical link connecting exactly two endpoints, commonly referred to as its A and Z terminations. A circuit in Nautobot may have zero, one, or two terminations defined. It is common to have only one termination defined when you don't necessarily care about the details of the provider side of the circuit, e.g. for Internet access circuits. Both terminations would likely be modeled for circuits which connect one customer site to another. Each circuit is associated with a provider and a user-defined type. For example, you might have Internet access circuits delivered to each site by one provider, and private MPLS circuits delivered by another. Each circuit must be assigned a circuit ID, each of which must be unique per provider. Each circuit must be assigned to a status . The following statuses are available by default: Planned Provisioning Active Offline Deprovisioning Decommissioned Circuits also have optional fields for annotating their installation date and commit rate, and may be assigned to Nautobot tenants. Note Nautobot currently models only physical circuits: those which have exactly two endpoints. It is common to layer virtualized constructs ( virtual circuits ) such as MPLS or EVPN tunnels on top of these, however Nautobot does not yet support virtual circuit modeling.","title":"Circuits"},{"location":"models/circuits/circuittermination.html","text":"Circuit Terminations \u00b6 The association of a circuit with a particular site, location, and/or device is modeled separately as a circuit termination. A circuit may have up to two terminations, labeled A and Z. A single-termination circuit can be used when you don't know (or care) about the far end of a circuit (for example, an Internet access circuit which connects to a transit provider). A dual-termination circuit is useful for tracking circuits which connect two sites. Each circuit termination is attached to either a site (and optionally also a location within the site) or a provider network. Site terminations may optionally be connected via a cable to a specific device interface or port within that site. Each termination must be assigned a port speed, and can optionally be assigned an upstream speed if it differs from the downstream speed (a common scenario with e.g. DOCSIS cable modems). Fields are also available to track cross-connect and patch panel details. In adherence with Nautobot's philosophy of closely modeling the real world, a circuit may be connected only to a physical interface. For example, circuits may not terminate to LAG interfaces, which are virtual in nature. In such cases, a separate physical circuit is associated with each LAG member interface and each needs to be modeled discretely. Note A circuit in Nautobot represents a physical link, and cannot have more than two endpoints. When modeling a multi-point topology, each leg of the topology must be defined as a discrete circuit, with one end terminating within the provider's infrastructure. The provider network model is ideal for representing these networks.","title":"Circuit Terminations"},{"location":"models/circuits/circuittermination.html#circuit-terminations","text":"The association of a circuit with a particular site, location, and/or device is modeled separately as a circuit termination. A circuit may have up to two terminations, labeled A and Z. A single-termination circuit can be used when you don't know (or care) about the far end of a circuit (for example, an Internet access circuit which connects to a transit provider). A dual-termination circuit is useful for tracking circuits which connect two sites. Each circuit termination is attached to either a site (and optionally also a location within the site) or a provider network. Site terminations may optionally be connected via a cable to a specific device interface or port within that site. Each termination must be assigned a port speed, and can optionally be assigned an upstream speed if it differs from the downstream speed (a common scenario with e.g. DOCSIS cable modems). Fields are also available to track cross-connect and patch panel details. In adherence with Nautobot's philosophy of closely modeling the real world, a circuit may be connected only to a physical interface. For example, circuits may not terminate to LAG interfaces, which are virtual in nature. In such cases, a separate physical circuit is associated with each LAG member interface and each needs to be modeled discretely. Note A circuit in Nautobot represents a physical link, and cannot have more than two endpoints. When modeling a multi-point topology, each leg of the topology must be defined as a discrete circuit, with one end terminating within the provider's infrastructure. The provider network model is ideal for representing these networks.","title":"Circuit Terminations"},{"location":"models/circuits/circuittype.html","text":"Circuit Types \u00b6 Circuits are classified by functional type. These types are completely customizable, and are typically used to convey the type of service being delivered over a circuit. For example, you might define circuit types for: Internet transit Out-of-band connectivity Peering Private backhaul","title":"Circuit Types"},{"location":"models/circuits/circuittype.html#circuit-types","text":"Circuits are classified by functional type. These types are completely customizable, and are typically used to convey the type of service being delivered over a circuit. For example, you might define circuit types for: Internet transit Out-of-band connectivity Peering Private backhaul","title":"Circuit Types"},{"location":"models/circuits/provider.html","text":"Providers \u00b6 A circuit provider is any entity which provides some form of connectivity of among sites or organizations within a site. While this obviously includes carriers which offer Internet and private transit service, it might also include Internet exchange (IX) points and even organizations with whom you peer directly. Each circuit within Nautobot must be assigned a provider and a circuit ID which is unique to that provider. Each provider may be assigned an autonomous system number (ASN), an account number, and contact information.","title":"Providers"},{"location":"models/circuits/provider.html#providers","text":"A circuit provider is any entity which provides some form of connectivity of among sites or organizations within a site. While this obviously includes carriers which offer Internet and private transit service, it might also include Internet exchange (IX) points and even organizations with whom you peer directly. Each circuit within Nautobot must be assigned a provider and a circuit ID which is unique to that provider. Each provider may be assigned an autonomous system number (ASN), an account number, and contact information.","title":"Providers"},{"location":"models/circuits/providernetwork.html","text":"Provider Network \u00b6 Added in version 1.3.0 A provider network represents an abstract portion of network topology, just like in a topology diagram. For example, a provider network may be used to represent a provider's MPLS network. Each provider network must be assigned to a provider. A circuit may terminate to either a provider network or to a site.","title":"Provider Network"},{"location":"models/circuits/providernetwork.html#provider-network","text":"Added in version 1.3.0 A provider network represents an abstract portion of network topology, just like in a topology diagram. For example, a provider network may be used to represent a provider's MPLS network. Each provider network must be assigned to a provider. A circuit may terminate to either a provider network or to a site.","title":"Provider Network"},{"location":"models/dcim/cable.html","text":"Cables \u00b6 All connections between device components in Nautobot are represented using cables. A cable represents a direct physical connection between two termination points, such as between a console port and a patch panel port, or between two network interfaces. Each cable must have two endpoints defined. These endpoints are sometimes referenced as A and B for clarity, however cables are direction-agnostic and the order in which terminations are made has no meaning. Cables may be connected to the following objects: Circuit terminations Console ports Console server ports Interfaces Pass-through ports (front and rear) Power feeds Power outlets Power ports Each cable may be assigned a type, label, length, and color. Each cable must also assigned to an operational status . The following statuses are available by default: Active Planned Decommissioning Tracing Cables \u00b6 A cable may be traced from either of its endpoints by clicking the \"trace\" button. (A REST API endpoint also provides this functionality.) Nautobot will follow the path of connected cables from this termination across the directly connected cable to the far-end termination. If the cable connects to a pass-through port, and the peer port has another cable connected, Nautobot will continue following the cable path until it encounters a non-pass-through or unconnected termination point. The entire path will be displayed to the user. In the example below, three individual cables comprise a path between devices A and D: Traced from Interface 1 on Device A, Nautobot will show the following path: Cable 1: Interface 1 to Front Port 1 Cable 2: Rear Port 1 to Rear Port 2 Cable 3: Front Port 2 to Interface 2 A cable can also be traced through a circuit. Traced from Interface 1 on Device A, Nautobot will show the following path: Cable 1: Interface 1 to Side A Cable 2: Side Z to Interface 2","title":"Cables"},{"location":"models/dcim/cable.html#cables","text":"All connections between device components in Nautobot are represented using cables. A cable represents a direct physical connection between two termination points, such as between a console port and a patch panel port, or between two network interfaces. Each cable must have two endpoints defined. These endpoints are sometimes referenced as A and B for clarity, however cables are direction-agnostic and the order in which terminations are made has no meaning. Cables may be connected to the following objects: Circuit terminations Console ports Console server ports Interfaces Pass-through ports (front and rear) Power feeds Power outlets Power ports Each cable may be assigned a type, label, length, and color. Each cable must also assigned to an operational status . The following statuses are available by default: Active Planned Decommissioning","title":"Cables"},{"location":"models/dcim/cable.html#tracing-cables","text":"A cable may be traced from either of its endpoints by clicking the \"trace\" button. (A REST API endpoint also provides this functionality.) Nautobot will follow the path of connected cables from this termination across the directly connected cable to the far-end termination. If the cable connects to a pass-through port, and the peer port has another cable connected, Nautobot will continue following the cable path until it encounters a non-pass-through or unconnected termination point. The entire path will be displayed to the user. In the example below, three individual cables comprise a path between devices A and D: Traced from Interface 1 on Device A, Nautobot will show the following path: Cable 1: Interface 1 to Front Port 1 Cable 2: Rear Port 1 to Rear Port 2 Cable 3: Front Port 2 to Interface 2 A cable can also be traced through a circuit. Traced from Interface 1 on Device A, Nautobot will show the following path: Cable 1: Interface 1 to Side A Cable 2: Side Z to Interface 2","title":"Tracing Cables"},{"location":"models/dcim/consoleport.html","text":"Console Ports \u00b6 A console port provides connectivity to the physical console of a device. These are typically used for temporary access by someone who is physically near the device, or for remote out-of-band access provided via a networked console server. Each console port may be assigned a physical type. Cables can connect console ports to console server ports or pass-through ports.","title":"Console Ports"},{"location":"models/dcim/consoleport.html#console-ports","text":"A console port provides connectivity to the physical console of a device. These are typically used for temporary access by someone who is physically near the device, or for remote out-of-band access provided via a networked console server. Each console port may be assigned a physical type. Cables can connect console ports to console server ports or pass-through ports.","title":"Console Ports"},{"location":"models/dcim/consoleporttemplate.html","text":"Console Port Templates \u00b6 A template for a console port that will be created on all instantiations of the parent device type. Each console port can be assigned a physical type.","title":"Console Port Templates"},{"location":"models/dcim/consoleporttemplate.html#console-port-templates","text":"A template for a console port that will be created on all instantiations of the parent device type. Each console port can be assigned a physical type.","title":"Console Port Templates"},{"location":"models/dcim/consoleserverport.html","text":"Console Server Ports \u00b6 A console server is a device which provides remote access to the local consoles of connected devices. They are typically used to provide remote out-of-band access to network devices. Each console server port may be assigned a physical type. Cables can connect console server ports to console ports or pass-through ports.","title":"Console Server Ports"},{"location":"models/dcim/consoleserverport.html#console-server-ports","text":"A console server is a device which provides remote access to the local consoles of connected devices. They are typically used to provide remote out-of-band access to network devices. Each console server port may be assigned a physical type. Cables can connect console server ports to console ports or pass-through ports.","title":"Console Server Ports"},{"location":"models/dcim/consoleserverporttemplate.html","text":"Console Server Port Templates \u00b6 A template for a console server port that will be created on all instantiations of the parent device type. Each console server port can be assigned a physical type.","title":"Console Server Port Templates"},{"location":"models/dcim/consoleserverporttemplate.html#console-server-port-templates","text":"A template for a console server port that will be created on all instantiations of the parent device type. Each console server port can be assigned a physical type.","title":"Console Server Port Templates"},{"location":"models/dcim/device.html","text":"Devices \u00b6 Every piece of hardware which is installed within a site or rack exists in Nautobot as a device. Devices are measured in rack units (U) and can be half depth or full depth. A device may have a height of 0U: These devices do not consume vertical rack space and cannot be assigned to a particular rack unit. A common example of a 0U device is a vertically-mounted PDU. When assigning a multi-U device to a rack, it is considered to be mounted in the lowest-numbered rack unit which it occupies. For example, a 3U device which occupies U8 through U10 is said to be mounted in U8. This logic applies to racks with both ascending and descending unit numbering. A device is said to be full-depth if its installation on one rack face prevents the installation of any other device on the opposite face within the same rack unit(s). This could be either because the device is physically too deep to allow a device behind it, or because the installation of an opposing device would impede airflow. Each device must be instantiated from a pre-created device type, and its default components (console ports, power ports, interfaces, etc.) will be created automatically. (The device type associated with a device may be changed after its creation, however its components will not be updated retroactively.) Each device must be assigned a site, device role, and operational status , and may optionally be assigned to a specific location and/or rack within a site. A platform, serial number, and asset tag may optionally be assigned to each device. Device names must be unique within a site, unless the device has been assigned to a tenant. Devices may also be unnamed. When a device has one or more interfaces with IP addresses assigned, a primary IP for the device can be designated, for both IPv4 and IPv6.","title":"Devices"},{"location":"models/dcim/device.html#devices","text":"Every piece of hardware which is installed within a site or rack exists in Nautobot as a device. Devices are measured in rack units (U) and can be half depth or full depth. A device may have a height of 0U: These devices do not consume vertical rack space and cannot be assigned to a particular rack unit. A common example of a 0U device is a vertically-mounted PDU. When assigning a multi-U device to a rack, it is considered to be mounted in the lowest-numbered rack unit which it occupies. For example, a 3U device which occupies U8 through U10 is said to be mounted in U8. This logic applies to racks with both ascending and descending unit numbering. A device is said to be full-depth if its installation on one rack face prevents the installation of any other device on the opposite face within the same rack unit(s). This could be either because the device is physically too deep to allow a device behind it, or because the installation of an opposing device would impede airflow. Each device must be instantiated from a pre-created device type, and its default components (console ports, power ports, interfaces, etc.) will be created automatically. (The device type associated with a device may be changed after its creation, however its components will not be updated retroactively.) Each device must be assigned a site, device role, and operational status , and may optionally be assigned to a specific location and/or rack within a site. A platform, serial number, and asset tag may optionally be assigned to each device. Device names must be unique within a site, unless the device has been assigned to a tenant. Devices may also be unnamed. When a device has one or more interfaces with IP addresses assigned, a primary IP for the device can be designated, for both IPv4 and IPv6.","title":"Devices"},{"location":"models/dcim/devicebay.html","text":"Device Bays \u00b6 Device bays represent a space or slot within a parent device in which a child device may be installed. For example, a 2U parent chassis might house four individual blade servers. The chassis would appear in the rack elevation as a 2U device with four device bays, and each server within it would be defined as a 0U device installed in one of the device bays. Child devices do not appear within rack elevations or count as consuming rack units. Child devices are first-class Devices in their own right: That is, they are fully independent managed entities which don't share any control plane with the parent. Just like normal devices, child devices have their own platform (OS), role, tags, and components. LAG interfaces may not group interfaces belonging to different child devices. Note Device bays are not suitable for modeling line cards (such as those commonly found in chassis-based routers and switches), as these components depend on the control plane of the parent device to operate. Instead, line cards and similarly non-autonomous hardware should be modeled as inventory items within a device, with any associated interfaces or other components assigned directly to the device.","title":"Device Bays"},{"location":"models/dcim/devicebay.html#device-bays","text":"Device bays represent a space or slot within a parent device in which a child device may be installed. For example, a 2U parent chassis might house four individual blade servers. The chassis would appear in the rack elevation as a 2U device with four device bays, and each server within it would be defined as a 0U device installed in one of the device bays. Child devices do not appear within rack elevations or count as consuming rack units. Child devices are first-class Devices in their own right: That is, they are fully independent managed entities which don't share any control plane with the parent. Just like normal devices, child devices have their own platform (OS), role, tags, and components. LAG interfaces may not group interfaces belonging to different child devices. Note Device bays are not suitable for modeling line cards (such as those commonly found in chassis-based routers and switches), as these components depend on the control plane of the parent device to operate. Instead, line cards and similarly non-autonomous hardware should be modeled as inventory items within a device, with any associated interfaces or other components assigned directly to the device.","title":"Device Bays"},{"location":"models/dcim/devicebaytemplate.html","text":"Device Bay Templates \u00b6 A template for a device bay that will be created on all instantiations of the parent device type.","title":"Device Bay Templates"},{"location":"models/dcim/devicebaytemplate.html#device-bay-templates","text":"A template for a device bay that will be created on all instantiations of the parent device type.","title":"Device Bay Templates"},{"location":"models/dcim/devicerole.html","text":"Device Roles \u00b6 Devices can be organized by functional roles, which are fully customizable by the user. For example, you might create roles for core switches, distribution switches, and access switches within your network.","title":"Device Roles"},{"location":"models/dcim/devicerole.html#device-roles","text":"Devices can be organized by functional roles, which are fully customizable by the user. For example, you might create roles for core switches, distribution switches, and access switches within your network.","title":"Device Roles"},{"location":"models/dcim/devicetype.html","text":"Device Types \u00b6 A device type represents a particular make and model of hardware that exists in the real world. Device types define the physical attributes of a device (rack height and depth) and its individual components (console, power, network interfaces, and so on). Device types are instantiated as devices installed within sites and/or equipment racks. For example, you might define a device type to represent a Juniper EX4300-48T network switch with 48 Ethernet interfaces. You can then create multiple instances of this type named \"switch1,\" \"switch2,\" and so on. Each device will automatically inherit the components (such as interfaces) of its device type at the time of creation. However, changes made to a device type will not apply to instances of that device type retroactively. Some devices house child devices which share physical resources, like space and power, but which functional independently from one another. A common example of this is blade server chassis. Each device type is designated as one of the following: A parent device (which has device bays) A child device (which must be installed within a device bay) Neither Note This parent/child relationship is not suitable for modeling chassis-based devices, wherein child members share a common control plane. Instead, line cards and similarly non-autonomous hardware should be modeled as inventory items within a device, with any associated interfaces or other components assigned directly to the device.","title":"Device Types"},{"location":"models/dcim/devicetype.html#device-types","text":"A device type represents a particular make and model of hardware that exists in the real world. Device types define the physical attributes of a device (rack height and depth) and its individual components (console, power, network interfaces, and so on). Device types are instantiated as devices installed within sites and/or equipment racks. For example, you might define a device type to represent a Juniper EX4300-48T network switch with 48 Ethernet interfaces. You can then create multiple instances of this type named \"switch1,\" \"switch2,\" and so on. Each device will automatically inherit the components (such as interfaces) of its device type at the time of creation. However, changes made to a device type will not apply to instances of that device type retroactively. Some devices house child devices which share physical resources, like space and power, but which functional independently from one another. A common example of this is blade server chassis. Each device type is designated as one of the following: A parent device (which has device bays) A child device (which must be installed within a device bay) Neither Note This parent/child relationship is not suitable for modeling chassis-based devices, wherein child members share a common control plane. Instead, line cards and similarly non-autonomous hardware should be modeled as inventory items within a device, with any associated interfaces or other components assigned directly to the device.","title":"Device Types"},{"location":"models/dcim/frontport.html","text":"Front Ports \u00b6 Front ports are pass-through ports used to represent physical cable connections that comprise part of a longer path. For example, the ports on the front face of a UTP patch panel would be modeled in Nautobot as front ports. Each port is assigned a physical type, and must be mapped to a specific rear port on the same device. A single rear port may be mapped to multiple rear ports, using numeric positions to annotate the specific alignment of each.","title":"Front Ports"},{"location":"models/dcim/frontport.html#front-ports","text":"Front ports are pass-through ports used to represent physical cable connections that comprise part of a longer path. For example, the ports on the front face of a UTP patch panel would be modeled in Nautobot as front ports. Each port is assigned a physical type, and must be mapped to a specific rear port on the same device. A single rear port may be mapped to multiple rear ports, using numeric positions to annotate the specific alignment of each.","title":"Front Ports"},{"location":"models/dcim/frontporttemplate.html","text":"Front Port Templates \u00b6 A template for a front-facing pass-through port that will be created on all instantiations of the parent device type. Front ports may have a physical type assigned, and must be associated with a corresponding rear port and position. This association will be automatically replicated when the device type is instantiated.","title":"Front Port Templates"},{"location":"models/dcim/frontporttemplate.html#front-port-templates","text":"A template for a front-facing pass-through port that will be created on all instantiations of the parent device type. Front ports may have a physical type assigned, and must be associated with a corresponding rear port and position. This association will be automatically replicated when the device type is instantiated.","title":"Front Port Templates"},{"location":"models/dcim/interface.html","text":"Interfaces \u00b6 Interfaces in Nautobot represent network interfaces used to exchange data with connected devices. On modern networks, these are most commonly Ethernet, but other types are supported as well. Each interface must be assigned a type, an operational status and may optionally be assigned a MAC address, MTU, and IEEE 802.1Q mode (tagged or access). Each interface can also be enabled or disabled, and optionally designated as management-only (for out-of-band management). The following operational statuses are available by default: Planned Maintenance Active Decommissioning Failed Added in version 1.4.0 Added bridge field. Added parent_interface field. Added status field. Interfaces may be physical or virtual in nature, but only physical interfaces may be connected via cables. Cables can connect interfaces to pass-through ports, circuit terminations, or other interfaces. Physical interfaces may be arranged into a link aggregation group (LAG) and associated with a parent LAG (virtual) interface. LAG interfaces can be recursively nested to model bonding of trunk groups. Like all virtual interfaces, LAG interfaces cannot be connected physically. IP addresses can be assigned to interfaces. VLANs can also be assigned to each interface as either tagged or untagged. (An interface may have only one untagged VLAN.) Note Although devices and virtual machines both can have interfaces, a separate model is used for each. Thus, device interfaces have some properties that are not present on virtual machine interfaces and vice versa.","title":"Interfaces"},{"location":"models/dcim/interface.html#interfaces","text":"Interfaces in Nautobot represent network interfaces used to exchange data with connected devices. On modern networks, these are most commonly Ethernet, but other types are supported as well. Each interface must be assigned a type, an operational status and may optionally be assigned a MAC address, MTU, and IEEE 802.1Q mode (tagged or access). Each interface can also be enabled or disabled, and optionally designated as management-only (for out-of-band management). The following operational statuses are available by default: Planned Maintenance Active Decommissioning Failed Added in version 1.4.0 Added bridge field. Added parent_interface field. Added status field. Interfaces may be physical or virtual in nature, but only physical interfaces may be connected via cables. Cables can connect interfaces to pass-through ports, circuit terminations, or other interfaces. Physical interfaces may be arranged into a link aggregation group (LAG) and associated with a parent LAG (virtual) interface. LAG interfaces can be recursively nested to model bonding of trunk groups. Like all virtual interfaces, LAG interfaces cannot be connected physically. IP addresses can be assigned to interfaces. VLANs can also be assigned to each interface as either tagged or untagged. (An interface may have only one untagged VLAN.) Note Although devices and virtual machines both can have interfaces, a separate model is used for each. Thus, device interfaces have some properties that are not present on virtual machine interfaces and vice versa.","title":"Interfaces"},{"location":"models/dcim/interfacetemplate.html","text":"Interface Templates \u00b6 A template for a network interface that will be created on all instantiations of the parent device type. Each interface may be assigned a physical or virtual type, and may be designated as \"management-only.\"","title":"Interface Templates"},{"location":"models/dcim/interfacetemplate.html#interface-templates","text":"A template for a network interface that will be created on all instantiations of the parent device type. Each interface may be assigned a physical or virtual type, and may be designated as \"management-only.\"","title":"Interface Templates"},{"location":"models/dcim/inventoryitem.html","text":"Inventory Items \u00b6 Inventory items represent hardware components installed within a device, such as a power supply or CPU or line card. Inventory items are distinct from other device components in that they cannot be templatized on a device type, and cannot be connected by cables. They are intended to be used primarily for inventory purposes. Each inventory item can be assigned a manufacturer, part ID, serial number, and asset tag (all optional). A boolean toggle is also provided to indicate whether each item was entered manually or discovered automatically (by some process outside of Nautobot). Inventory items are hierarchical in nature, such that any individual item may be designated as the parent for other items. For example, an inventory item might be created to represent a line card which houses several SFP optics, each of which exists as a child item within the device.","title":"Inventory Items"},{"location":"models/dcim/inventoryitem.html#inventory-items","text":"Inventory items represent hardware components installed within a device, such as a power supply or CPU or line card. Inventory items are distinct from other device components in that they cannot be templatized on a device type, and cannot be connected by cables. They are intended to be used primarily for inventory purposes. Each inventory item can be assigned a manufacturer, part ID, serial number, and asset tag (all optional). A boolean toggle is also provided to indicate whether each item was entered manually or discovered automatically (by some process outside of Nautobot). Inventory items are hierarchical in nature, such that any individual item may be designated as the parent for other items. For example, an inventory item might be created to represent a line card which houses several SFP optics, each of which exists as a child item within the device.","title":"Inventory Items"},{"location":"models/dcim/location.html","text":"Locations \u00b6 Added in version 1.4.0 To locate network information more precisely than a Site defines, you can define a hierarchy of Locations within each Site. Data objects such as devices, prefixes, VLAN groups, etc. can thus be mapped or assigned to a specific building, wing, floor, room, etc. as appropriate to your needs. Once you have defined the hierarchy of Location Types that you wish to use, you can then define Locations. Any \"top-level\" Locations (those whose Location Type has no parent) belong directly to a Site, while \"child\" Locations belong to their immediate parent Location, rather than to the Site as a whole. Info At present, Locations fill the conceptual space between the more abstract Region and Site models and the more concrete Rack Group model. In a future Nautobot release, some or all of these other models may be collapsed into Locations. That is to say, in the future you might not deal with Regions and Sites as distinct models, but instead your Location Type hierarchy might include these higher-level categories, becoming something like Country \u2190 City \u2190 Site \u2190 Building \u2190 Floor \u2190 Room. Much like Sites, each Location must be assigned a name and operational status . The same default operational statuses are defined for Locations as for Sites, but as always, you can customize these to suit your needs. Locations can also be assigned to a tenant.","title":"Locations"},{"location":"models/dcim/location.html#locations","text":"Added in version 1.4.0 To locate network information more precisely than a Site defines, you can define a hierarchy of Locations within each Site. Data objects such as devices, prefixes, VLAN groups, etc. can thus be mapped or assigned to a specific building, wing, floor, room, etc. as appropriate to your needs. Once you have defined the hierarchy of Location Types that you wish to use, you can then define Locations. Any \"top-level\" Locations (those whose Location Type has no parent) belong directly to a Site, while \"child\" Locations belong to their immediate parent Location, rather than to the Site as a whole. Info At present, Locations fill the conceptual space between the more abstract Region and Site models and the more concrete Rack Group model. In a future Nautobot release, some or all of these other models may be collapsed into Locations. That is to say, in the future you might not deal with Regions and Sites as distinct models, but instead your Location Type hierarchy might include these higher-level categories, becoming something like Country \u2190 City \u2190 Site \u2190 Building \u2190 Floor \u2190 Room. Much like Sites, each Location must be assigned a name and operational status . The same default operational statuses are defined for Locations as for Sites, but as always, you can customize these to suit your needs. Locations can also be assigned to a tenant.","title":"Locations"},{"location":"models/dcim/locationtype.html","text":"Location Types \u00b6 Added in version 1.4.0 Before defining individual Locations, you must first define the hierarchy of Location Types that you wish to use for the organization of your network. An example hierarchy might be Building \u2190 Floor \u2190 Room , but you might have more or fewer distinct types depending on your specific organizational requirements. Each Location Type can define a set of \"content types\" that are permitted to associate to Locations of this type. For example, you might permit assigning Prefixes and VLAN Groups to an entire Building or Floor, but only allow Devices and Racks to be assigned to Rooms, never to a more abstract location. Doing so can help ensure consistency of your data. Tip Although it is possible to define a \"tree\" of Location Types with multiple \"branches\", in the majority of cases doing so adds more unnecessary complexity than it's worth. Consider the following hypothetical Location Type tree: Branch Office \u21b3 Branch Floor \u21b3 Branch Floor Room \u21b3 Branch Basement \u21b3 Branch Basement Room Headquarters \u21b3 Headquarters Floor \u21b3 Headquarters Floor Room \u21b3 Headquarters Basement \u21b3 Headquarters Basement Room This would complicate your life significantly when constructing queries, filters, and so forth to actually work with your data - for example, if you wanted a list of all Prefixes that are mapped to floors rather than individual rooms, you would now need to construct a query for Prefixes that are mapped to (a Branch Floor OR a Headquarters Floor OR a Branch Basement OR a Headquarters Basement ). In most cases you would be better served with a far simpler \"linear\" sequence of Location Types, such as Building \u2190 Floor \u2190 Room ; you could then use tags or custom fields to distinguish whether a given Building is a Branch Office or a Headquarters, if that distinction is even important to your network model.","title":"Location Types"},{"location":"models/dcim/locationtype.html#location-types","text":"Added in version 1.4.0 Before defining individual Locations, you must first define the hierarchy of Location Types that you wish to use for the organization of your network. An example hierarchy might be Building \u2190 Floor \u2190 Room , but you might have more or fewer distinct types depending on your specific organizational requirements. Each Location Type can define a set of \"content types\" that are permitted to associate to Locations of this type. For example, you might permit assigning Prefixes and VLAN Groups to an entire Building or Floor, but only allow Devices and Racks to be assigned to Rooms, never to a more abstract location. Doing so can help ensure consistency of your data. Tip Although it is possible to define a \"tree\" of Location Types with multiple \"branches\", in the majority of cases doing so adds more unnecessary complexity than it's worth. Consider the following hypothetical Location Type tree: Branch Office \u21b3 Branch Floor \u21b3 Branch Floor Room \u21b3 Branch Basement \u21b3 Branch Basement Room Headquarters \u21b3 Headquarters Floor \u21b3 Headquarters Floor Room \u21b3 Headquarters Basement \u21b3 Headquarters Basement Room This would complicate your life significantly when constructing queries, filters, and so forth to actually work with your data - for example, if you wanted a list of all Prefixes that are mapped to floors rather than individual rooms, you would now need to construct a query for Prefixes that are mapped to (a Branch Floor OR a Headquarters Floor OR a Branch Basement OR a Headquarters Basement ). In most cases you would be better served with a far simpler \"linear\" sequence of Location Types, such as Building \u2190 Floor \u2190 Room ; you could then use tags or custom fields to distinguish whether a given Building is a Branch Office or a Headquarters, if that distinction is even important to your network model.","title":"Location Types"},{"location":"models/dcim/manufacturer.html","text":"Manufacturers \u00b6 A manufacturer represents the \"make\" of a device; e.g. Cisco or Dell. Each device type must be assigned to a manufacturer. (Inventory items and platforms may also be associated with manufacturers.) Each manufacturer must have a unique name and may have a description assigned to it.","title":"Manufacturers"},{"location":"models/dcim/manufacturer.html#manufacturers","text":"A manufacturer represents the \"make\" of a device; e.g. Cisco or Dell. Each device type must be assigned to a manufacturer. (Inventory items and platforms may also be associated with manufacturers.) Each manufacturer must have a unique name and may have a description assigned to it.","title":"Manufacturers"},{"location":"models/dcim/platform.html","text":"Platforms \u00b6 A platform defines the type of software running on a device or virtual machine. This can be helpful to model when it is necessary to distinguish between different versions or feature sets. Note that two devices of the same type may be assigned different platforms: For example, one Juniper MX240 might run Junos 14 while another runs Junos 15. Platforms may optionally be limited by manufacturer: If a platform is assigned to a particular manufacturer, it can only be assigned to devices with a type belonging to that manufacturer. The platform model is also used to indicate which NAPALM driver and any associated arguments Nautobot should use when connecting to a remote device. The name of the driver along with optional parameters are stored with the platform. The assignment of platforms to devices is an optional feature, and may be disregarded if not desired.","title":"Platforms"},{"location":"models/dcim/platform.html#platforms","text":"A platform defines the type of software running on a device or virtual machine. This can be helpful to model when it is necessary to distinguish between different versions or feature sets. Note that two devices of the same type may be assigned different platforms: For example, one Juniper MX240 might run Junos 14 while another runs Junos 15. Platforms may optionally be limited by manufacturer: If a platform is assigned to a particular manufacturer, it can only be assigned to devices with a type belonging to that manufacturer. The platform model is also used to indicate which NAPALM driver and any associated arguments Nautobot should use when connecting to a remote device. The name of the driver along with optional parameters are stored with the platform. The assignment of platforms to devices is an optional feature, and may be disregarded if not desired.","title":"Platforms"},{"location":"models/dcim/powerfeed.html","text":"Power Feed \u00b6 A power feed represents the distribution of power from a power panel to a particular device, typically a power distribution unit (PDU). The power pot (inlet) on a device can be connected via a cable to a power feed. A power feed may optionally be assigned to a rack to allow more easily tracking the distribution of power among racks. Each power feed is assigned an operational type (primary or redundant) and a status . The following statuses are available by default: Offline Active Planned Failed Each power feed also defines the electrical characteristics of the circuit which it represents. These include the following: Supply type (AC or DC) Phase (single or three-phase) Voltage Amperage Maximum utilization (percentage) Info The power utilization of a rack is calculated when one or more power feeds are assigned to the rack and connected to devices that draw power.","title":"Power Feed"},{"location":"models/dcim/powerfeed.html#power-feed","text":"A power feed represents the distribution of power from a power panel to a particular device, typically a power distribution unit (PDU). The power pot (inlet) on a device can be connected via a cable to a power feed. A power feed may optionally be assigned to a rack to allow more easily tracking the distribution of power among racks. Each power feed is assigned an operational type (primary or redundant) and a status . The following statuses are available by default: Offline Active Planned Failed Each power feed also defines the electrical characteristics of the circuit which it represents. These include the following: Supply type (AC or DC) Phase (single or three-phase) Voltage Amperage Maximum utilization (percentage) Info The power utilization of a rack is calculated when one or more power feeds are assigned to the rack and connected to devices that draw power.","title":"Power Feed"},{"location":"models/dcim/poweroutlet.html","text":"Power Outlets \u00b6 Power outlets represent the outlets on a power distribution unit (PDU) or other device that supply power to dependent devices. Each power port may be assigned a physical type, and may be associated with a specific feed leg (where three-phase power is used) and/or a specific upstream power port. This association can be used to model the distribution of power within a device. For example, imagine a PDU with one power port which draws from a three-phase feed and 48 power outlets arranged into three banks of 16 outlets each. Outlets 1-16 would be associated with leg A on the port, and outlets 17-32 and 33-48 would be associated with legs B and C, respectively. Cables can connect power outlets only to downstream power ports. (Pass-through ports cannot be used to model power distribution.)","title":"Power Outlets"},{"location":"models/dcim/poweroutlet.html#power-outlets","text":"Power outlets represent the outlets on a power distribution unit (PDU) or other device that supply power to dependent devices. Each power port may be assigned a physical type, and may be associated with a specific feed leg (where three-phase power is used) and/or a specific upstream power port. This association can be used to model the distribution of power within a device. For example, imagine a PDU with one power port which draws from a three-phase feed and 48 power outlets arranged into three banks of 16 outlets each. Outlets 1-16 would be associated with leg A on the port, and outlets 17-32 and 33-48 would be associated with legs B and C, respectively. Cables can connect power outlets only to downstream power ports. (Pass-through ports cannot be used to model power distribution.)","title":"Power Outlets"},{"location":"models/dcim/poweroutlettemplate.html","text":"Power Outlet Templates \u00b6 A template for a power outlet that will be created on all instantiations of the parent device type. Each power outlet can be assigned a physical type, and its power source may be mapped to a specific feed leg and power port template. This association will be automatically replicated when the device type is instantiated.","title":"Power Outlet Templates"},{"location":"models/dcim/poweroutlettemplate.html#power-outlet-templates","text":"A template for a power outlet that will be created on all instantiations of the parent device type. Each power outlet can be assigned a physical type, and its power source may be mapped to a specific feed leg and power port template. This association will be automatically replicated when the device type is instantiated.","title":"Power Outlet Templates"},{"location":"models/dcim/powerpanel.html","text":"Power Panel \u00b6 A power panel represents the origin point in Nautobot for electrical power being disseminated by one or more power feeds. In a data center environment, one power panel often serves a group of racks, with an individual power feed extending to each rack, though this is not always the case. It is common to have two sets of panels and feeds arranged in parallel to provide redundant power to each rack. Each power panel must be assigned to a site, and may optionally be assigned to a particular location and/or rack group. Note Nautobot does not model the mechanism by which power is delivered to a power panel. Power panels define the root level of the power distribution hierarchy in Nautobot.","title":"Power Panel"},{"location":"models/dcim/powerpanel.html#power-panel","text":"A power panel represents the origin point in Nautobot for electrical power being disseminated by one or more power feeds. In a data center environment, one power panel often serves a group of racks, with an individual power feed extending to each rack, though this is not always the case. It is common to have two sets of panels and feeds arranged in parallel to provide redundant power to each rack. Each power panel must be assigned to a site, and may optionally be assigned to a particular location and/or rack group. Note Nautobot does not model the mechanism by which power is delivered to a power panel. Power panels define the root level of the power distribution hierarchy in Nautobot.","title":"Power Panel"},{"location":"models/dcim/powerport.html","text":"Power Ports \u00b6 A power port represents the inlet of a device where it draws its power, i.e. the connection port(s) on a device's power supply. Each power port may be assigned a physical type, as well as allocated and maximum draw values (in watts). These values can be used to calculate the overall utilization of an upstream power feed. Info When creating a power port on a device which supplies power to downstream devices, the allocated and maximum draw numbers should be left blank. Utilization will be calculated by taking the sum of all power ports of devices connected downstream. Cables can connect power ports only to power outlets or power feeds. (Pass-through ports cannot be used to model power distribution.)","title":"Power Ports"},{"location":"models/dcim/powerport.html#power-ports","text":"A power port represents the inlet of a device where it draws its power, i.e. the connection port(s) on a device's power supply. Each power port may be assigned a physical type, as well as allocated and maximum draw values (in watts). These values can be used to calculate the overall utilization of an upstream power feed. Info When creating a power port on a device which supplies power to downstream devices, the allocated and maximum draw numbers should be left blank. Utilization will be calculated by taking the sum of all power ports of devices connected downstream. Cables can connect power ports only to power outlets or power feeds. (Pass-through ports cannot be used to model power distribution.)","title":"Power Ports"},{"location":"models/dcim/powerporttemplate.html","text":"Power Port Templates \u00b6 A template for a power port that will be created on all instantiations of the parent device type. Each power port can be assigned a physical type, as well as a maximum and allocated draw in watts.","title":"Power Port Templates"},{"location":"models/dcim/powerporttemplate.html#power-port-templates","text":"A template for a power port that will be created on all instantiations of the parent device type. Each power port can be assigned a physical type, as well as a maximum and allocated draw in watts.","title":"Power Port Templates"},{"location":"models/dcim/rack.html","text":"Racks \u00b6 The rack model represents a physical two- or four-post equipment rack in which devices can be installed. Each rack must be assigned to a site, and may optionally be assigned to a location, rack group, and/or tenant. Racks can also be organized by user-defined functional roles. Rack height is measured in rack units (U); racks are commonly between 42U and 48U tall, but Nautobot allows you to define racks of arbitrary height. A toggle is provided to indicate whether rack units are in ascending (from the ground up) or descending order. Each rack is assigned a name and (optionally) a separate facility ID. This is helpful when leasing space in a data center your organization does not own: The facility will often assign a seemingly arbitrary ID to a rack (for example, \"M204.313\") whereas internally you refer to is simply as \"R113.\" A unique serial number and asset tag may also be associated with each rack. A rack must be designated as one of the following types: 2-post frame 4-post frame 4-post cabinet Wall-mounted frame Wall-mounted cabinet Similarly, each rack must be assigned an operational status . The following statuses are available by default: Reserved Available Planned Active Deprecated Each rack has two faces (front and rear) on which devices can be mounted. Rail-to-rail width may be 10, 19, 21, or 23 inches. The outer width and depth of a rack or cabinet can also be annotated in millimeters or inches.","title":"Racks"},{"location":"models/dcim/rack.html#racks","text":"The rack model represents a physical two- or four-post equipment rack in which devices can be installed. Each rack must be assigned to a site, and may optionally be assigned to a location, rack group, and/or tenant. Racks can also be organized by user-defined functional roles. Rack height is measured in rack units (U); racks are commonly between 42U and 48U tall, but Nautobot allows you to define racks of arbitrary height. A toggle is provided to indicate whether rack units are in ascending (from the ground up) or descending order. Each rack is assigned a name and (optionally) a separate facility ID. This is helpful when leasing space in a data center your organization does not own: The facility will often assign a seemingly arbitrary ID to a rack (for example, \"M204.313\") whereas internally you refer to is simply as \"R113.\" A unique serial number and asset tag may also be associated with each rack. A rack must be designated as one of the following types: 2-post frame 4-post frame 4-post cabinet Wall-mounted frame Wall-mounted cabinet Similarly, each rack must be assigned an operational status . The following statuses are available by default: Reserved Available Planned Active Deprecated Each rack has two faces (front and rear) on which devices can be mounted. Rail-to-rail width may be 10, 19, 21, or 23 inches. The outer width and depth of a rack or cabinet can also be annotated in millimeters or inches.","title":"Racks"},{"location":"models/dcim/rackgroup.html","text":"Rack Groups \u00b6 Racks can be organized into groups, which can be nested into themselves similar to regions. As with sites, how you choose to designate rack groups will depend on the nature of your organization. Each rack group must be assigned to a parent site (and optionally also a more specific location within that site). Rack groups may optionally be nested within one another to model a multi-level hierarchy. The name and facility ID of each rack within a group must be unique. (Racks not assigned to the same rack group may have identical names and/or facility IDs.)","title":"Rack Groups"},{"location":"models/dcim/rackgroup.html#rack-groups","text":"Racks can be organized into groups, which can be nested into themselves similar to regions. As with sites, how you choose to designate rack groups will depend on the nature of your organization. Each rack group must be assigned to a parent site (and optionally also a more specific location within that site). Rack groups may optionally be nested within one another to model a multi-level hierarchy. The name and facility ID of each rack within a group must be unique. (Racks not assigned to the same rack group may have identical names and/or facility IDs.)","title":"Rack Groups"},{"location":"models/dcim/rackreservation.html","text":"Rack Reservations \u00b6 Users can reserve specific units within a rack for future use. An arbitrary set of units within a rack can be associated with a single reservation, but reservations cannot span multiple racks. A description is required for each reservation, reservations may optionally be associated with a specific tenant.","title":"Rack Reservations"},{"location":"models/dcim/rackreservation.html#rack-reservations","text":"Users can reserve specific units within a rack for future use. An arbitrary set of units within a rack can be associated with a single reservation, but reservations cannot span multiple racks. A description is required for each reservation, reservations may optionally be associated with a specific tenant.","title":"Rack Reservations"},{"location":"models/dcim/rackrole.html","text":"Rack Roles \u00b6 Each rack can optionally be assigned a user-defined functional role. For example, you might designate a rack for compute or storage resources, or to house co-located customer devices. Rack roles are fully customizable and may be color-coded.","title":"Rack Roles"},{"location":"models/dcim/rackrole.html#rack-roles","text":"Each rack can optionally be assigned a user-defined functional role. For example, you might designate a rack for compute or storage resources, or to house co-located customer devices. Rack roles are fully customizable and may be color-coded.","title":"Rack Roles"},{"location":"models/dcim/rearport.html","text":"Rear Ports \u00b6 Like front ports, rear ports are pass-through ports which represent the continuation of a path from one cable to the next. Each rear port is defined with its physical type and a number of positions: Rear ports with more than one position can be mapped to multiple front ports. This can be useful for modeling instances where multiple paths share a common cable (for example, six discrete two-strand fiber connections sharing a 12-strand MPO cable). Note Front and rear ports need not necessarily reside on the actual front or rear device face. This terminology is used primarily to distinguish between the two components in a pass-through port pairing.","title":"Rear Ports"},{"location":"models/dcim/rearport.html#rear-ports","text":"Like front ports, rear ports are pass-through ports which represent the continuation of a path from one cable to the next. Each rear port is defined with its physical type and a number of positions: Rear ports with more than one position can be mapped to multiple front ports. This can be useful for modeling instances where multiple paths share a common cable (for example, six discrete two-strand fiber connections sharing a 12-strand MPO cable). Note Front and rear ports need not necessarily reside on the actual front or rear device face. This terminology is used primarily to distinguish between the two components in a pass-through port pairing.","title":"Rear Ports"},{"location":"models/dcim/rearporttemplate.html","text":"Rear Port Templates \u00b6 A template for a rear-facing pass-through port that will be created on all instantiations of the parent device type. Each rear port may have a physical type and one or more front port templates assigned to it. The number of positions associated with a rear port determines how many front ports can be assigned to it (the maximum is 1024).","title":"Rear Port Templates"},{"location":"models/dcim/rearporttemplate.html#rear-port-templates","text":"A template for a rear-facing pass-through port that will be created on all instantiations of the parent device type. Each rear port may have a physical type and one or more front port templates assigned to it. The number of positions associated with a rear port determines how many front ports can be assigned to it (the maximum is 1024).","title":"Rear Port Templates"},{"location":"models/dcim/region.html","text":"Regions \u00b6 Sites can be arranged geographically using regions. A region might represent a continent, country, city, campus, or other area depending on your use case. Regions can be nested recursively to construct a hierarchy. For example, you might define several country regions, and within each of those several state or city regions to which sites are assigned. Info In a future Nautobot release, regions may become another Location Type, and the Region model may be collapsed into the Location model.","title":"Regions"},{"location":"models/dcim/region.html#regions","text":"Sites can be arranged geographically using regions. A region might represent a continent, country, city, campus, or other area depending on your use case. Regions can be nested recursively to construct a hierarchy. For example, you might define several country regions, and within each of those several state or city regions to which sites are assigned. Info In a future Nautobot release, regions may become another Location Type, and the Region model may be collapsed into the Location model.","title":"Regions"},{"location":"models/dcim/site.html","text":"Sites \u00b6 How you choose to employ sites when modeling your network may vary depending on the nature of your organization, but generally a site will equate to a building or campus. For example, a chain of banks might create a site to represent each of its branches, a site for its corporate headquarters, and two additional sites for its presence in two co-location facilities. Each site must be assigned a unique name and operational status and may optionally be assigned to a region and/or tenant. The following operational statuses are available by default: Planned Staging Active Decommissioning Retired The site model also provides a facility ID field which can be used to annotate a facility ID (such as a data center name) associated with the site. Each site may also have an autonomous system (AS) number and time zone associated with it. (Time zones are provided by the pytz package.) The site model also includes several fields for storing contact and address information as well as geo-location data (GPS coordinates). Info In a future Nautobot release, sites may become just another Location Type, and the Site model may be collapsed into the Location model.","title":"Sites"},{"location":"models/dcim/site.html#sites","text":"How you choose to employ sites when modeling your network may vary depending on the nature of your organization, but generally a site will equate to a building or campus. For example, a chain of banks might create a site to represent each of its branches, a site for its corporate headquarters, and two additional sites for its presence in two co-location facilities. Each site must be assigned a unique name and operational status and may optionally be assigned to a region and/or tenant. The following operational statuses are available by default: Planned Staging Active Decommissioning Retired The site model also provides a facility ID field which can be used to annotate a facility ID (such as a data center name) associated with the site. Each site may also have an autonomous system (AS) number and time zone associated with it. (Time zones are provided by the pytz package.) The site model also includes several fields for storing contact and address information as well as geo-location data (GPS coordinates). Info In a future Nautobot release, sites may become just another Location Type, and the Site model may be collapsed into the Location model.","title":"Sites"},{"location":"models/dcim/virtualchassis.html","text":"Virtual Chassis \u00b6 A virtual chassis represents a set of devices which share a common control plane. A common example of this is a stack of switches which are connected and configured to operate as a single device. A virtual chassis must be assigned a name and may be assigned a domain. Each device in the virtual chassis is referred to as a VC member, and assigned a position and (optionally) a priority. VC member devices commonly reside within the same rack, though this is not a requirement. One of the devices may be designated as the VC master: This device will typically be assigned a name, services, and other attributes related to managing the VC. Note It's important to recognize the distinction between a virtual chassis and a chassis-based device. A virtual chassis is not suitable for modeling a chassis-based switch with removable line cards (such as the Juniper EX9208), as its line cards are not physically autonomous devices.","title":"Virtual Chassis"},{"location":"models/dcim/virtualchassis.html#virtual-chassis","text":"A virtual chassis represents a set of devices which share a common control plane. A common example of this is a stack of switches which are connected and configured to operate as a single device. A virtual chassis must be assigned a name and may be assigned a domain. Each device in the virtual chassis is referred to as a VC member, and assigned a position and (optionally) a priority. VC member devices commonly reside within the same rack, though this is not a requirement. One of the devices may be designated as the VC master: This device will typically be assigned a name, services, and other attributes related to managing the VC. Note It's important to recognize the distinction between a virtual chassis and a chassis-based device. A virtual chassis is not suitable for modeling a chassis-based switch with removable line cards (such as the Juniper EX9208), as its line cards are not physically autonomous devices.","title":"Virtual Chassis"},{"location":"models/extras/computedfield.html","text":"Computed Fields \u00b6 Added in version 1.1.0 Computed fields are very similar in design and implementation to custom fields. See the overview of Custom Fields . As the name suggests, computed fields serve the need for a custom field where the value is generated using data that Nautobot stores in its database and merging it into a Jinja2 template and associated filters. As an example, within your automation system, you may want to be able to have an automatically generated field on the Device model that combines the name of the device and the site name in uppercase. To do that, you would define a Jinja2 template for this field that looks like such: {{ obj.name }}_{{ obj.site.name | upper }} Important Every time an object with this computed field is loaded, the template gets re-rendered with the currently available data. These rendered values are not stored in the database; only the Jinja2 template is stored. Creating Computed Fields \u00b6 Computed fields can be created through the Nautobot UI under Extensibility > Computed Fields . Each computed field must have a slug and a label. Slug must be a simple, database-friendly string, e.g. device_with_site Label is used as the human-friendly display name for this field in the UI, for example, Device With Site . Tip Because computed field data can be included in the REST API and in GraphQL, we strongly recommend that when defining a computed field, you provide a slug that contains underscores rather than dashes ( my_field_slug , not my-field-slug ), as some features may not work optimally if dashes are included in the slug. Similar to custom fields, the weight value is used to order computed fields within a form. A description can also be provided, and will appear beneath the field in a form. Computed fields must define a template from which to render their values. The template field must contain a valid Jinja2 template string. A computed field must be assigned to an object type, or model, in Nautobot. Once created, a computed field will automatically appear as part of this model's display. See notes about viewing computed fields via the REST API below. When creating a computed field, if \"Move to Advanced tab\" is checked, this computed field won't appear on the object's main detail tab in the UI, but will appear in the \"Advanced\" tab. This is useful when the requirement is to hide this field from the main detail tab when, for instance, it is only required for machine-to-machine communication and not user consumption. Computed Field Template Context \u00b6 Computed field templates can utilize the context of the object the field is being rendered on. This context is available for use in templates via the obj keyword. As an example, for a computed field being rendered on a Device object, the name of the site that this Device belongs to can be accessed like this: {{ obj.site.name }} Computed Field Template Filters \u00b6 Computed field templates can also utilize built-in Jinja2 filters or custom ones that have been registered via plugins. These filters can be used by providing the name of the filter function. As an example: {{ obj.site.name | leet_speak }} See the documentation on built-in filters or registering custom Jinja2 filters in plugins. Computed Fields and the REST API \u00b6 When retrieving an object via the REST API, computed field data is not included by default in order to prevent potentially computationally expensive rendering operations that degrade the user experience. In order to retrieve computed field data, you must use the include query parameter. Take a look at an example URL that includes computed field data: http://localhost:8080/api/dcim/sites?include=computed_fields When explicitly requested as such, computed field data will be included in the computed_fields attribute. For example, below is the partial output of a site with one computed field defined: { \"id\" : 123 , \"url\" : \"http://localhost:8080/api/dcim/sites/123/\" , \"name\" : \"Raleigh 42\" , ... \"computed_fields\" : { \"site_name_uppercase\" : \"RALEIGH\" }, ... Note The slug value of each computed field is used as the key name for items in the computed_fields attribute.","title":"Computed Fields"},{"location":"models/extras/computedfield.html#computed-fields","text":"Added in version 1.1.0 Computed fields are very similar in design and implementation to custom fields. See the overview of Custom Fields . As the name suggests, computed fields serve the need for a custom field where the value is generated using data that Nautobot stores in its database and merging it into a Jinja2 template and associated filters. As an example, within your automation system, you may want to be able to have an automatically generated field on the Device model that combines the name of the device and the site name in uppercase. To do that, you would define a Jinja2 template for this field that looks like such: {{ obj.name }}_{{ obj.site.name | upper }} Important Every time an object with this computed field is loaded, the template gets re-rendered with the currently available data. These rendered values are not stored in the database; only the Jinja2 template is stored.","title":"Computed Fields"},{"location":"models/extras/computedfield.html#creating-computed-fields","text":"Computed fields can be created through the Nautobot UI under Extensibility > Computed Fields . Each computed field must have a slug and a label. Slug must be a simple, database-friendly string, e.g. device_with_site Label is used as the human-friendly display name for this field in the UI, for example, Device With Site . Tip Because computed field data can be included in the REST API and in GraphQL, we strongly recommend that when defining a computed field, you provide a slug that contains underscores rather than dashes ( my_field_slug , not my-field-slug ), as some features may not work optimally if dashes are included in the slug. Similar to custom fields, the weight value is used to order computed fields within a form. A description can also be provided, and will appear beneath the field in a form. Computed fields must define a template from which to render their values. The template field must contain a valid Jinja2 template string. A computed field must be assigned to an object type, or model, in Nautobot. Once created, a computed field will automatically appear as part of this model's display. See notes about viewing computed fields via the REST API below. When creating a computed field, if \"Move to Advanced tab\" is checked, this computed field won't appear on the object's main detail tab in the UI, but will appear in the \"Advanced\" tab. This is useful when the requirement is to hide this field from the main detail tab when, for instance, it is only required for machine-to-machine communication and not user consumption.","title":"Creating Computed Fields"},{"location":"models/extras/computedfield.html#computed-field-template-context","text":"Computed field templates can utilize the context of the object the field is being rendered on. This context is available for use in templates via the obj keyword. As an example, for a computed field being rendered on a Device object, the name of the site that this Device belongs to can be accessed like this: {{ obj.site.name }}","title":"Computed Field Template Context"},{"location":"models/extras/computedfield.html#computed-field-template-filters","text":"Computed field templates can also utilize built-in Jinja2 filters or custom ones that have been registered via plugins. These filters can be used by providing the name of the filter function. As an example: {{ obj.site.name | leet_speak }} See the documentation on built-in filters or registering custom Jinja2 filters in plugins.","title":"Computed Field Template Filters"},{"location":"models/extras/computedfield.html#computed-fields-and-the-rest-api","text":"When retrieving an object via the REST API, computed field data is not included by default in order to prevent potentially computationally expensive rendering operations that degrade the user experience. In order to retrieve computed field data, you must use the include query parameter. Take a look at an example URL that includes computed field data: http://localhost:8080/api/dcim/sites?include=computed_fields When explicitly requested as such, computed field data will be included in the computed_fields attribute. For example, below is the partial output of a site with one computed field defined: { \"id\" : 123 , \"url\" : \"http://localhost:8080/api/dcim/sites/123/\" , \"name\" : \"Raleigh 42\" , ... \"computed_fields\" : { \"site_name_uppercase\" : \"RALEIGH\" }, ... Note The slug value of each computed field is used as the key name for items in the computed_fields attribute.","title":"Computed Fields and the REST API"},{"location":"models/extras/configcontext.html","text":"Configuration Contexts \u00b6 Sometimes it is desirable to associate additional data with a group of devices or virtual machines to aid in automated configuration. For example, you might want to associate a set of syslog servers for all devices within a particular region. Context data enables the association of extra user-defined data with devices and virtual machines grouped by one or more of the following assignments: Region Site Role Device type Platform Cluster group Cluster Tenant group Tenant Tag Context data not specifically assigned to one or more of the above groups is by default associated with all devices and virtual machines. Configuration contexts may be managed within Nautobot via the UI and/or API; they may also be managed externally to Nautobot in a Git repository if desired. Hierarchical Rendering \u00b6 Context data is arranged hierarchically, so that data with a higher weight can be entered to override lower-weight data. Multiple instances of data are automatically merged by Nautobot to present a single dictionary for each object. For example, suppose we want to specify a set of syslog and NTP servers for all devices within a region. We could create a config context instance with a weight of 1000 assigned to the region, with the following JSON data: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ], \"syslog-servers\" : [ \"172.16.9.100\" , \"172.16.9.101\" ] } But suppose there's a problem at one particular site within this region preventing traffic from reaching the regional syslog server. Devices there need to use a local syslog server instead of the two defined above. We'll create a second config context assigned only to that site with a weight of 2000 and the following data: { \"syslog-servers\" : [ \"192.168.43.107\" ] } When the context data for a device at this site is rendered, the second, higher-weight data overwrite the first, resulting in the following: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ], \"syslog-servers\" : [ \"192.168.43.107\" ] } Data from the higher-weight context overwrites conflicting data from the lower-weight context, while the non-conflicting portion of the lower-weight context (the list of NTP servers) is preserved. Local Context Data \u00b6 Devices and virtual machines may also have a local config context defined. This local context will always take precedence over any separate config context objects which apply to the device/VM. This is useful in situations where we need to call out a specific deviation in the data for a particular object. Warning If you find that you're routinely defining local context data for many individual devices or virtual machines, custom fields may offer a more effective solution.","title":"Configuration Contexts"},{"location":"models/extras/configcontext.html#configuration-contexts","text":"Sometimes it is desirable to associate additional data with a group of devices or virtual machines to aid in automated configuration. For example, you might want to associate a set of syslog servers for all devices within a particular region. Context data enables the association of extra user-defined data with devices and virtual machines grouped by one or more of the following assignments: Region Site Role Device type Platform Cluster group Cluster Tenant group Tenant Tag Context data not specifically assigned to one or more of the above groups is by default associated with all devices and virtual machines. Configuration contexts may be managed within Nautobot via the UI and/or API; they may also be managed externally to Nautobot in a Git repository if desired.","title":"Configuration Contexts"},{"location":"models/extras/configcontext.html#hierarchical-rendering","text":"Context data is arranged hierarchically, so that data with a higher weight can be entered to override lower-weight data. Multiple instances of data are automatically merged by Nautobot to present a single dictionary for each object. For example, suppose we want to specify a set of syslog and NTP servers for all devices within a region. We could create a config context instance with a weight of 1000 assigned to the region, with the following JSON data: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ], \"syslog-servers\" : [ \"172.16.9.100\" , \"172.16.9.101\" ] } But suppose there's a problem at one particular site within this region preventing traffic from reaching the regional syslog server. Devices there need to use a local syslog server instead of the two defined above. We'll create a second config context assigned only to that site with a weight of 2000 and the following data: { \"syslog-servers\" : [ \"192.168.43.107\" ] } When the context data for a device at this site is rendered, the second, higher-weight data overwrite the first, resulting in the following: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ], \"syslog-servers\" : [ \"192.168.43.107\" ] } Data from the higher-weight context overwrites conflicting data from the lower-weight context, while the non-conflicting portion of the lower-weight context (the list of NTP servers) is preserved.","title":"Hierarchical Rendering"},{"location":"models/extras/configcontext.html#local-context-data","text":"Devices and virtual machines may also have a local config context defined. This local context will always take precedence over any separate config context objects which apply to the device/VM. This is useful in situations where we need to call out a specific deviation in the data for a particular object. Warning If you find that you're routinely defining local context data for many individual devices or virtual machines, custom fields may offer a more effective solution.","title":"Local Context Data"},{"location":"models/extras/configcontextschema.html","text":"Config Context Schemas \u00b6 Added in version 1.1.0 While config contexts allow for arbitrary data structures to be stored within Nautobot, at scale it is desirable to apply validation constraints to that data to ensure its consistency and to avoid data entry errors. To service this need, Nautobot supports optionally backing config contexts with JSON Schemas for validation. These schema are managed via the config context schema model and are optionally linked to config context instances, in addition to devices and virtual machines for the purpose of validating their local context data. A JSON Schema is capable of validating the structure, format, and type of your data, and acts as a form of documentation useful in a number of automation use cases. A config context is linked to a single schema object and thus they are meant to model individual units of the overall context. In this way, they validate each config context object, not the fully rendered context as viewed on a particular device or virtual machine. When a config context schema is employed on a config or local context, the data therein is validated when the object in question is saved. Should validation against the schema fail, a relevant error message is returned to the user and they are prevented from saving the data until the validation issue has been resolved. Here is an example JSON Schema which can be used to validate an NTP server config context: { \"type\" : \"object\" , \"properties\" : { \"ntp-servers\" : { \"type\" : \"array\" , \"minItems\" : 2 , \"maxItems\" : 2 , \"items\" : { \"type\" : \"string\" , \"format\" : \"ipv4\" } } }, \"additionalProperties\" : false } This schema would allow a config context with this data to pass: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ] } However it would not allow any of these examples to be saved: { \"ntp-servers\" : [ \"172.16.10.22\" ] } { \"ntp\" : \"172.16.10.22,172.16.10.22\" } { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" , \"5.5.4\" ] } For more information on JSON Schemas and specifically type formats for specialized objects like IP addresses, hostnames, and more see the JSON Schema docs . Note Config Context Schemas currently support the JSON Schema draft 7 specification.","title":"Config Context Schemas"},{"location":"models/extras/configcontextschema.html#config-context-schemas","text":"Added in version 1.1.0 While config contexts allow for arbitrary data structures to be stored within Nautobot, at scale it is desirable to apply validation constraints to that data to ensure its consistency and to avoid data entry errors. To service this need, Nautobot supports optionally backing config contexts with JSON Schemas for validation. These schema are managed via the config context schema model and are optionally linked to config context instances, in addition to devices and virtual machines for the purpose of validating their local context data. A JSON Schema is capable of validating the structure, format, and type of your data, and acts as a form of documentation useful in a number of automation use cases. A config context is linked to a single schema object and thus they are meant to model individual units of the overall context. In this way, they validate each config context object, not the fully rendered context as viewed on a particular device or virtual machine. When a config context schema is employed on a config or local context, the data therein is validated when the object in question is saved. Should validation against the schema fail, a relevant error message is returned to the user and they are prevented from saving the data until the validation issue has been resolved. Here is an example JSON Schema which can be used to validate an NTP server config context: { \"type\" : \"object\" , \"properties\" : { \"ntp-servers\" : { \"type\" : \"array\" , \"minItems\" : 2 , \"maxItems\" : 2 , \"items\" : { \"type\" : \"string\" , \"format\" : \"ipv4\" } } }, \"additionalProperties\" : false } This schema would allow a config context with this data to pass: { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ] } However it would not allow any of these examples to be saved: { \"ntp-servers\" : [ \"172.16.10.22\" ] } { \"ntp\" : \"172.16.10.22,172.16.10.22\" } { \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" , \"5.5.4\" ] } For more information on JSON Schemas and specifically type formats for specialized objects like IP addresses, hostnames, and more see the JSON Schema docs . Note Config Context Schemas currently support the JSON Schema draft 7 specification.","title":"Config Context Schemas"},{"location":"models/extras/customfield.html","text":"Custom Fields \u00b6 Each model in Nautobot is represented in the database as a discrete table, and each attribute of a model exists as a column within its table. For example, sites are stored in the dcim_site table, which has columns named name , facility , physical_address , and so on. As new attributes are added to objects throughout the development of Nautobot, tables are expanded to include new rows. However, some users might want to store additional object attributes that are somewhat esoteric in nature, and that would not make sense to include in the core Nautobot database schema. For instance, suppose your organization needs to associate each device with a ticket number correlating it with an internal support system record. This is certainly a legitimate use for Nautobot, but it's not a common enough need to warrant including a field for every Nautobot installation. Instead, you can create a custom field to hold this data. Within the database, custom fields are stored as JSON data directly alongside each object. This alleviates the need for complex queries when retrieving objects. Creating Custom Fields \u00b6 Custom fields can be created through the UI under Extensibility > Miscellaneous > Custom Fields or through the REST API. Nautobot supports these custom field types: Text: Free-form text (up to 255 characters) Integer: A whole number (positive or negative) Boolean: True or false Date: A date in ISO 8601 format (YYYY-MM-DD) URL: This will be presented as a link in the web UI JSON: Arbitrary JSON data Selection: A selection of one of several pre-defined custom choices Multiple selection: A selection field which supports the assignment of multiple values Added in version 1.3.0 Support for JSON-type custom fields was added. Each custom field must have a name and slug; this should be a simple database-friendly string, e.g. tps_report . You may also assign a corresponding human-friendly label (e.g. \"TPS report\"); the label will be displayed on web forms. A weight is also required: Higher-weight fields will be ordered lower within a form. (The default weight is 100.) If a description is provided, it will appear beneath the field in a form. Changed in version 1.4.0 Custom fields now have both a name and a slug ; in older versions there was no slug field. When migrating existing data to Nautobot 1.4.0 or later, the label and slug will be automatically populated for existing custom fields if necessary. Warning In all Nautobot 1.x versions, the custom field name is used as the key to store and retrieve custom field data via the database and GraphQL. In a future major release, the name field will be removed and custom field data will be accessible via the slug instead. See below for REST API versioning behavior in this area. Tip Because custom field data is included in the database, in the REST API and in GraphQL, we strongly recommend that when defining a custom field, you provide a slug that contains underscores rather than dashes ( my_field_slug , not my-field-slug ), as some features may not work optimally if dashes are included in the slug. Similarly, the provided name should also contain only alphanumeric characters and underscores, as it is currently treated in some cases like a slug. Note The name, slug, and type of a custom field cannot be modified once created, so take care in defining these fields. This helps to reduce the possibility of inconsistent data and enforces the importance of thinking about the data model when defining a new custom field. Marking a field as required will force the user to provide a value for the field when creating a new object or when saving an existing object. A default value for the field may also be provided. Use \"true\" or \"false\" for boolean fields, or the exact value of a choice for selection fields. The filter logic controls how values are matched when filtering objects by the custom field. Loose filtering (the default) matches on a partial value, whereas exact matching requires a complete match of the given string to a field's value. For example, exact filtering with the string \"red\" will only match the exact value \"red\", whereas loose filtering will match on the values \"red\", \"red-orange\", or \"bored\". Setting the filter logic to \"disabled\" disables filtering by the field entirely. A custom field must be assigned to one or object types, or models, in Nautobot. Once created, custom fields will automatically appear as part of these models in the web UI and REST API. When creating a custom field, if \"Move to Advanced tab\" is checked, this custom field won't appear on the object's main detail tab in the UI, but will appear in the \"Advanced\" tab. This is useful when the requirement is to hide this field from the main detail tab when, for instance, it is only required for machine-to-machine communication and not user consumption. Custom Field Validation \u00b6 Nautobot supports limited custom validation for custom field values. Following are the types of validation enforced for each field type: Text: Regular expression (optional) Integer: Minimum and/or maximum value (optional) JSON: If not empty, this field must contain valid JSON data Selection: Must exactly match one of the prescribed choices Selection Fields: Regular expression (optional) Custom Selection Fields \u00b6 Choices are stored as independent values and are assigned a numeric weight which affects their ordering in selection lists and dropdowns. Note that choice values are saved exactly as they appear, so it's best to avoid superfluous punctuation or symbols where possible. A regular expression can optionally be defined on custom selection choices to validate the defined field choices in the user interface and the API. If a default value is specified for a selection field, it must exactly match one of the provided choices. Note that the default value can only be set on the custom field after its corresponding choice has been added. The value of a multiple selection field will always return a list, even if only one value is selected. Filtering on Custom Fields \u00b6 There are a number of available built-in filters for custom fields. Filtering on an object's list view follows the same pattern as custom field filtering on the API . When using the ORM, you can filter on custom fields using _custom_field_data__<field name> (note the underscore before custom_field_data and the double-underscore before the field name). For example, if a custom field of string type with a name of \"site_code\" was created for Site objects, you could filter as follows: from nautobot.dcim.models import Site all_sites = Site . objects . all () # -> ['Raleigh', 'Charlotte', 'Greensboro'] filtered_sites_1 = Site . objects . filter ( _custom_field_data__site_code = \"US-NC-RAL42\" ) # -> ['Raleigh'] filtered_sites_2 = Site . objects . filter ( _custom_field_data__site_code__in = [ \"US-NC-RAL42\" , \"US-NC-CLT22\" ]) # -> ['Raleigh', 'Charlotte'] Custom Fields and the REST API \u00b6 When retrieving an object via the REST API, all of its custom field data will be included within the custom_fields attribute. For example, below is the partial output of a site with two custom fields defined: { \"id\" : 123 , \"url\" : \"http://localhost:8080/api/dcim/sites/123/\" , \"name\" : \"Raleigh 42\" , ... \"custom_fields\" : { \"deployed\" : \"2018-06-19\" , \"site_code\" : \"US-NC-RAL42\" }, ... Changed in API version 1.4 In REST API versions 1.3 and earlier, each custom field's name is used as the key under custom_fields in the REST API. As part of the planned future transition to removing the name attribute entirely from custom fields, when REST API version 1.4 or later is requested, the custom_fields data in the REST API is instead indexed by custom field slug . Refer to the documentation on REST API versioning for more information about REST API versioning and how to request a specific version of the REST API. To set or change custom field values, simply include nested JSON data in your REST API POST, PATCH, or PUT request. Unchanged fields may be omitted from the data. For example, the below would set a value for the deployed custom field but would leave the site_code value unchanged: { \"name\" : \"New Site\" , \"slug\" : \"new-site\" , \"custom_fields\" : { \"deployed\" : \"2019-03-24\" } }","title":"Custom Fields"},{"location":"models/extras/customfield.html#custom-fields","text":"Each model in Nautobot is represented in the database as a discrete table, and each attribute of a model exists as a column within its table. For example, sites are stored in the dcim_site table, which has columns named name , facility , physical_address , and so on. As new attributes are added to objects throughout the development of Nautobot, tables are expanded to include new rows. However, some users might want to store additional object attributes that are somewhat esoteric in nature, and that would not make sense to include in the core Nautobot database schema. For instance, suppose your organization needs to associate each device with a ticket number correlating it with an internal support system record. This is certainly a legitimate use for Nautobot, but it's not a common enough need to warrant including a field for every Nautobot installation. Instead, you can create a custom field to hold this data. Within the database, custom fields are stored as JSON data directly alongside each object. This alleviates the need for complex queries when retrieving objects.","title":"Custom Fields"},{"location":"models/extras/customfield.html#creating-custom-fields","text":"Custom fields can be created through the UI under Extensibility > Miscellaneous > Custom Fields or through the REST API. Nautobot supports these custom field types: Text: Free-form text (up to 255 characters) Integer: A whole number (positive or negative) Boolean: True or false Date: A date in ISO 8601 format (YYYY-MM-DD) URL: This will be presented as a link in the web UI JSON: Arbitrary JSON data Selection: A selection of one of several pre-defined custom choices Multiple selection: A selection field which supports the assignment of multiple values Added in version 1.3.0 Support for JSON-type custom fields was added. Each custom field must have a name and slug; this should be a simple database-friendly string, e.g. tps_report . You may also assign a corresponding human-friendly label (e.g. \"TPS report\"); the label will be displayed on web forms. A weight is also required: Higher-weight fields will be ordered lower within a form. (The default weight is 100.) If a description is provided, it will appear beneath the field in a form. Changed in version 1.4.0 Custom fields now have both a name and a slug ; in older versions there was no slug field. When migrating existing data to Nautobot 1.4.0 or later, the label and slug will be automatically populated for existing custom fields if necessary. Warning In all Nautobot 1.x versions, the custom field name is used as the key to store and retrieve custom field data via the database and GraphQL. In a future major release, the name field will be removed and custom field data will be accessible via the slug instead. See below for REST API versioning behavior in this area. Tip Because custom field data is included in the database, in the REST API and in GraphQL, we strongly recommend that when defining a custom field, you provide a slug that contains underscores rather than dashes ( my_field_slug , not my-field-slug ), as some features may not work optimally if dashes are included in the slug. Similarly, the provided name should also contain only alphanumeric characters and underscores, as it is currently treated in some cases like a slug. Note The name, slug, and type of a custom field cannot be modified once created, so take care in defining these fields. This helps to reduce the possibility of inconsistent data and enforces the importance of thinking about the data model when defining a new custom field. Marking a field as required will force the user to provide a value for the field when creating a new object or when saving an existing object. A default value for the field may also be provided. Use \"true\" or \"false\" for boolean fields, or the exact value of a choice for selection fields. The filter logic controls how values are matched when filtering objects by the custom field. Loose filtering (the default) matches on a partial value, whereas exact matching requires a complete match of the given string to a field's value. For example, exact filtering with the string \"red\" will only match the exact value \"red\", whereas loose filtering will match on the values \"red\", \"red-orange\", or \"bored\". Setting the filter logic to \"disabled\" disables filtering by the field entirely. A custom field must be assigned to one or object types, or models, in Nautobot. Once created, custom fields will automatically appear as part of these models in the web UI and REST API. When creating a custom field, if \"Move to Advanced tab\" is checked, this custom field won't appear on the object's main detail tab in the UI, but will appear in the \"Advanced\" tab. This is useful when the requirement is to hide this field from the main detail tab when, for instance, it is only required for machine-to-machine communication and not user consumption.","title":"Creating Custom Fields"},{"location":"models/extras/customfield.html#custom-field-validation","text":"Nautobot supports limited custom validation for custom field values. Following are the types of validation enforced for each field type: Text: Regular expression (optional) Integer: Minimum and/or maximum value (optional) JSON: If not empty, this field must contain valid JSON data Selection: Must exactly match one of the prescribed choices Selection Fields: Regular expression (optional)","title":"Custom Field Validation"},{"location":"models/extras/customfield.html#custom-selection-fields","text":"Choices are stored as independent values and are assigned a numeric weight which affects their ordering in selection lists and dropdowns. Note that choice values are saved exactly as they appear, so it's best to avoid superfluous punctuation or symbols where possible. A regular expression can optionally be defined on custom selection choices to validate the defined field choices in the user interface and the API. If a default value is specified for a selection field, it must exactly match one of the provided choices. Note that the default value can only be set on the custom field after its corresponding choice has been added. The value of a multiple selection field will always return a list, even if only one value is selected.","title":"Custom Selection Fields"},{"location":"models/extras/customfield.html#filtering-on-custom-fields","text":"There are a number of available built-in filters for custom fields. Filtering on an object's list view follows the same pattern as custom field filtering on the API . When using the ORM, you can filter on custom fields using _custom_field_data__<field name> (note the underscore before custom_field_data and the double-underscore before the field name). For example, if a custom field of string type with a name of \"site_code\" was created for Site objects, you could filter as follows: from nautobot.dcim.models import Site all_sites = Site . objects . all () # -> ['Raleigh', 'Charlotte', 'Greensboro'] filtered_sites_1 = Site . objects . filter ( _custom_field_data__site_code = \"US-NC-RAL42\" ) # -> ['Raleigh'] filtered_sites_2 = Site . objects . filter ( _custom_field_data__site_code__in = [ \"US-NC-RAL42\" , \"US-NC-CLT22\" ]) # -> ['Raleigh', 'Charlotte']","title":"Filtering on Custom Fields"},{"location":"models/extras/customfield.html#custom-fields-and-the-rest-api","text":"When retrieving an object via the REST API, all of its custom field data will be included within the custom_fields attribute. For example, below is the partial output of a site with two custom fields defined: { \"id\" : 123 , \"url\" : \"http://localhost:8080/api/dcim/sites/123/\" , \"name\" : \"Raleigh 42\" , ... \"custom_fields\" : { \"deployed\" : \"2018-06-19\" , \"site_code\" : \"US-NC-RAL42\" }, ... Changed in API version 1.4 In REST API versions 1.3 and earlier, each custom field's name is used as the key under custom_fields in the REST API. As part of the planned future transition to removing the name attribute entirely from custom fields, when REST API version 1.4 or later is requested, the custom_fields data in the REST API is instead indexed by custom field slug . Refer to the documentation on REST API versioning for more information about REST API versioning and how to request a specific version of the REST API. To set or change custom field values, simply include nested JSON data in your REST API POST, PATCH, or PUT request. Unchanged fields may be omitted from the data. For example, the below would set a value for the deployed custom field but would leave the site_code value unchanged: { \"name\" : \"New Site\" , \"slug\" : \"new-site\" , \"custom_fields\" : { \"deployed\" : \"2019-03-24\" } }","title":"Custom Fields and the REST API"},{"location":"models/extras/customlink.html","text":"Custom Links \u00b6 Custom links allow users to display arbitrary hyperlinks to external content within Nautobot object views. These are helpful for cross-referencing related records in systems outside of Nautobot. For example, you might create a custom link on the device view which links to the current device in a network monitoring system. Custom links can be created under the admin UI or web UI located in the navbar under Extensibility > Miscellaneous > Custom Links. Each link is associated with a particular Nautobot object type (site, device, prefix, etc.) and will be displayed on relevant views. Each link is assigned text and a URL, both of which support Jinja2 templating. The text and URL are rendered with the context variable obj representing the current object. For example, you might define a link like this: Text: View NMS URL: https://nms.example.com/nodes/?name={{ obj.name }} When viewing a device named Router4, this link would render as: <a href=\"https://nms.example.com/nodes/?name=Router4\">View NMS</a> Custom links appear as buttons at the top right corner of the page. Numeric weighting can be used to influence the ordering of links. Context Data \u00b6 The following context data is available within the template when rendering a custom link's text or URL. Variable Description obj The Nautobot object being displayed debug A boolean indicating whether debugging is enabled request The current WSGI request user The current user (if authenticated) perms The permissions assigned to the user All built-in Jinja2 filters are available and it's also possible to develop and register a custom Jinja2 filters . Conditional Rendering \u00b6 Only links which render with non-empty text are included on the page. You can employ conditional Jinja2 logic to control the conditions under which a link gets rendered. For example, if you only want to display a link for active devices, you could set the link text to {% if obj.status.slug == 'active' %}View NMS{% endif %} The link will not appear when viewing a device with any status other than \"active.\" As another example, if you wanted to show only devices belonging to a certain manufacturer, you could do something like this: {% if obj.device_type.manufacturer.name == 'Cisco' %}View NMS{% endif %} The link will only appear when viewing a device with a manufacturer name of \"Cisco.\" Link Groups \u00b6 Group names can be specified to organize links into groups. Links with the same group name will render as a dropdown menu beneath a single button bearing the name of the group.","title":"Custom Links"},{"location":"models/extras/customlink.html#custom-links","text":"Custom links allow users to display arbitrary hyperlinks to external content within Nautobot object views. These are helpful for cross-referencing related records in systems outside of Nautobot. For example, you might create a custom link on the device view which links to the current device in a network monitoring system. Custom links can be created under the admin UI or web UI located in the navbar under Extensibility > Miscellaneous > Custom Links. Each link is associated with a particular Nautobot object type (site, device, prefix, etc.) and will be displayed on relevant views. Each link is assigned text and a URL, both of which support Jinja2 templating. The text and URL are rendered with the context variable obj representing the current object. For example, you might define a link like this: Text: View NMS URL: https://nms.example.com/nodes/?name={{ obj.name }} When viewing a device named Router4, this link would render as: <a href=\"https://nms.example.com/nodes/?name=Router4\">View NMS</a> Custom links appear as buttons at the top right corner of the page. Numeric weighting can be used to influence the ordering of links.","title":"Custom Links"},{"location":"models/extras/customlink.html#context-data","text":"The following context data is available within the template when rendering a custom link's text or URL. Variable Description obj The Nautobot object being displayed debug A boolean indicating whether debugging is enabled request The current WSGI request user The current user (if authenticated) perms The permissions assigned to the user All built-in Jinja2 filters are available and it's also possible to develop and register a custom Jinja2 filters .","title":"Context Data"},{"location":"models/extras/customlink.html#conditional-rendering","text":"Only links which render with non-empty text are included on the page. You can employ conditional Jinja2 logic to control the conditions under which a link gets rendered. For example, if you only want to display a link for active devices, you could set the link text to {% if obj.status.slug == 'active' %}View NMS{% endif %} The link will not appear when viewing a device with any status other than \"active.\" As another example, if you wanted to show only devices belonging to a certain manufacturer, you could do something like this: {% if obj.device_type.manufacturer.name == 'Cisco' %}View NMS{% endif %} The link will only appear when viewing a device with a manufacturer name of \"Cisco.\"","title":"Conditional Rendering"},{"location":"models/extras/customlink.html#link-groups","text":"Group names can be specified to organize links into groups. Links with the same group name will render as a dropdown menu beneath a single button bearing the name of the group.","title":"Link Groups"},{"location":"models/extras/dynamicgroup.html","text":"Dynamic Groups \u00b6 Added in version 1.3.0 Dynamic Groups provide a way to organize objects of the same Content Type by matching filters. A Dynamic Group can be used to create unique groups of objects matching a given filter, such as Devices for a specific site location or set of locations. As indicated by the name, Dynamic Groups update in real time as potential member objects are created, updated, or deleted. When creating a Dynamic Group, one must select a Content Type to which it is associated, for example dcim.device . The filtering parameters saved to the group behave as a bi-directional search query that is used to identify members of that group, and can also be used to determine from an individual object the list of Dynamic Groups to which an individual object belongs. Once created the Content Type for a Dynamic Group may not be modified as this relationship is tightly-coupled to the available filtering parameters. All other fields may be updated at any time. Introduction \u00b6 Creating Dynamic Groups \u00b6 Dynamic Groups can be created through the UI under Organization > Dynamic Groups and clicking the \"Add\" button, or through the REST API. Each Dynamic Group must have a human-readable Name string, e.g. devices-site-ams01 and a Slug , which should be a simple database-friendly string. By default, the slug will be automatically generated from the name, however you may customize it if you like. You must select a Content Type for the group that determines the kind of objects that can be members of the group and the corresponding filtering parameters available. Finally, you may also assign an optional human-friendly Description (e.g. \"Devices in site AMS01\"). Once a new Dynamic Group is created, the group can be configured by clicking the \"Edit\" button to specify Filter Fields or Child Groups to use to narrow down the group's member objects. More on this below. Warning The content type of a Dynamic Group cannot be modified once created, so take care in selecting this initially. This is intended to prevent the possibility of inconsistent data and enforces the importance of thinking about the data model when defining a new Dynamic Group. Working with Dynamic Groups \u00b6 Once created and configured, Dynamic Groups can be accessed from the primary Dynamic Groups landing page in the web interface under the Organization > Dynamic Groups menu. From there you may view the list of available groups, search or filter the list, view or edit an individual group, or bulk delete groups. Additionally if a group's filter has matching members, the number of members may be clicked to take you to the list of members for that dynamic group containing those objects. Dynamic Groups cannot be imported nor can they be updated in bulk, as these operations would be complex and do not make sense in most cases. From an individual object's detail page, if it is a member of any groups, a \"Dynamic Groups\" tab will display in the navigation tabs. Clicking that tab will display all Dynamic Groups of which this object is a member. Filtering \u00b6 Dynamic Group filtering is powered by FilterSet objects underneath the hood. Basic filtering is performed using the filter that is defined on a given Dynamic Group. Advanced filtering is performed by aggregating filters from multiple nested Dynamic Groups to form a combined parent Dynamic Group, which will be explained later in this document. An object is considered to be a member of a Dynamic Group if it is of the same Content Type and it is not excluded by way of any of the filter criteria specified for that group. By default, a freshly created group has an empty filter ( {} ), which will include all objects of the matching Content Type, just as a default list view of those objects would display prior to any filter fields being filled in the web UI. For example, for a Dynamic Group with Content Type of dcim.device and an empty filter, the list of members would be equivalent to the default Device list view, which in turn is equivalent to the queryset for Device.objects.all() from the database ORM. Changed in version 1.4.0 In Nautobot v1.3.0 the default for a Dynamic Group with an empty filter was to \"fail closed\" and have zero members. As of v1.4.0, this behavior has been inverted to default to include all objects matching the Content Type, instead of matching no objects as was previously the case. This was necessary to implement the progressive layering of child filters similarly to how we use filters to reduce desired objects from basic list view filters. This will be described in more detail below. Basic Filtering \u00b6 When editing a Dynamic Group, under the Filter Options section, you will find a Filter Fields tab that allows one to specify filter criteria. The filter fields available for a given Content Type are backed and validated by underlying filterset classes (for example nautobot.dcim.filters.DeviceFilterSet ) and are represented in the web interface as a dynamically-generated filter form that corresponds to each eligible filter field. Advanced Filtering \u00b6 Added in version 1.4.0 Advanced filtering is performed using nested Dynamic Group memberships. An object is considered a member of an advanced Dynamic Group if it matches the aggregated filter criteria across all descendant groups. When editing a Dynamic Group, under the Filter Options section, you will find a Child Groups tab that allows one to make other Dynamic Groups of the same Content Type children of the parent group. Example Workflow \u00b6 Dynamic Groups are a complex topic and are perhaps best understood through a series of worked examples. Basic Filtering with a single Dynamic Group \u00b6 Let's say you want to create a Dynamic Group that contains all production Devices at your first two Sites. You can create a Dynamic Group called \"Devices at Sites A and B\" for Content Type dcim | device , then edit it and set the Filter Fields to match: a Site of either \"AMS01\" or \"BKK01\" a Status of \"Active\" or \"Offline\" After clicking \"Update\", you will be returned to the detail view for this Dynamic Group, where you can verify the filter logic that results, and click the \"Members\" tab to see the set of Devices that it contains. A key to understand here is that generally, within a single Dynamic Group, additional values specified for the same filter field (here, \"Site\") will broaden the group to include additional objects that match those additional values, while specifying values for additional filter fields (here, \"Status\") will narrow the group to match only the objects that match this additional filter. This is expressed in the \"Filter Query Logic\" panel by the use of OR and AND operators - the logic for this Dynamic Group is: ( ( site__slug='ams01' OR site__slug='bkk01' ) AND ( status__slug='active' OR status__slug='offline' ) ) Advanced Filtering - Combining Two Dynamic Groups into a Third \u00b6 Added in version 1.4.0 Now, let's say that you add a third site to your network. This site is currently being built out, and you don't care about Devices from this site that are Offline status at present. What you want for your \"Devices of Interest\" Dynamic Group is logic similar to: ( ( ( site__slug='ams01' OR site__slug='bkk01' ) AND ( status__slug='active' OR status__slug='offline' ) ) OR ( site__slug='can01' AND status__slug='active' ) ) This logic is too complex to express directly via a single Dynamic Group, but fear not! This is what combining Dynamic Groups allows you to do. First, you can create a new \"Devices of Interest\" group. Edit this group, and instead of specifying Filter Fields , switch to the Child Groups tab of the editor, select the operator \"Include (OR)\" and the group \"Devices at Sites A and B\", and update the group. In the new group's detail view, you can see that it now contains one child group, \"Devices at Sites A and B\", and its members are exactly the same as those of that group. But we're not done yet! Next, you'll create another group to represent the other part of your desired logic. Call this group \"Site C So Far\", and set its Filter Fields to match Site \"CAN01\" and Status \"Active\". Verify that it contains the expected set of Devices from Site C. Now, we'll add this group into the \"Devices of Interest\" parent group. Navigate back to the Dynamic Groups list view, and edit this group. Under the Child Groups tab, add another \"Include (OR)\" operator and select group \"Site C So Far\": Now things are getting interesting! The \"Devices of Interest\" Dynamic Group now contains the filtered Devices from both of its child groups, and the \"Filter Query Logic\" matches our intent as we stated it earlier: ( ( ( site__slug='ams01' OR site__slug='bkk01' ) AND ( status__slug='active' OR status__slug='offline' ) ) OR ( site__slug='can01' AND status__slug='active' ) ) Advanced Filtering: Nested Groups and Negation \u00b6 Added in version 1.4.0 Next, let's say you add a fourth site to your network. This site is in bad shape, and has Devices in a wide range of statuses. You want your \"Devices of Interest\" group to include all Devices from this site, except for those in Decommissioning status . To express this logic and add these devices to our parent group, we will need to use a combination of groups and the \"Exclude (NOT)\" operator. First, you will create an \"Site D All Devices\" group. This will simply match Devices at Site \"DEL01\", regardless of their status. Then create a \"Site D Decommissioning Devices\" group, which matches Site \"DEL01\" and Status \"Decommissioning\". Next create a \"Site D Devices of Interest\" group, and set its Child Groups to: Operator \"Include (OR)\", group \"Site D All Devices\" Operator \"Exclude (NOT)\", group \"Site D Decommissioning Devices\" Warning In general, but especially when using the AND and NOT operators, you must pay close attention to the order of the child groups. In this example, if you were to reverse the order of these two child groups, you would not get the desired final result! You can check this group and confirm that it contains the expected restricted subset of Devices. Finally, you can edit the parent \"Devices of Interest\" group and add a third Child Groups entry, \"Include (OR)\" on \"Site D Devices of Interest\". The final result is a Dynamic Group that contains the desired set of Devices across all four of your Sites. You can see the filter logic that this combination of groups results in: ( ( ( site__slug='ams01' OR site__slug='bkk01' ) AND ( status__slug='active' OR status__slug='offline' ) ) OR ( site__slug='can01' AND status__slug='active' ) OR ( site__slug='del01' AND ( NOT (site__slug='del01' AND status__slug='decommissioning') ) ) ) You can also see the hierarchy of nested groups that are being used to derive the \"Devices of Interest\" group: Most importantly, you now have a Dynamic Group that contains exactly the set of Devices you need! Technical Details \u00b6 Filter Generation \u00b6 Filters are always processed hiearchically from the top down starting from the parent group and descending recursively to the last nested child group in order by the weight value assigned to that group when it was associated to its parent. Note For the purpose illustration, we will use \"left to right\" terminology since when verbally describing precedence in English, we read from left to right, so that following it will be more intuitive. The nesting of Dynamic Groups is performed using two advanced patterns: Sets and graphs. Rules for each child group are processed using a set operator , and groups are sorted hierarchically as a directed acyclic graph (DAG), where the weight is used for sorting child groups topologically. In both cases, the ordering of the tree of descendants from a parent group to its nested children is significant and critically important to how each subsequent filter or group of filters are processed to result in a final set of member objects. Consider an example where there is a graph from the parent group to three direct child groups, the third of which has its own nested child group: parent - first-child - second-child - third-child - nested-child The filter generation would walk the graph: starting from the base (match-all) filter of parent , the filter of first-child would be applied, then second-child , as ordered by their weight . In the case of third-child , all of its children (only nested-child in this case) would be processed in order in the same way and the resultant filter from all of the child groups for third-child would be applied to the filter resulting from first-child and second-child , resulting in the final filter for parent . Weights \u00b6 Weights are used to define the order in which a parent group's child group filters are processed. Because this ordering is significant, care must be taken when constructing nested Dynamic Groups to result in filter parameters that have the desired outcome. Note Unique weights are only considered for directly-related child groups. In other words, the weights for each child group of a parent group are unique to that parent and only affect the sorting of the children for that parent. In practice, weights are automatically assigned in increments of 10 when associating child groups in the web UI, and child groups may be dragged and dropped to re-order them and re-assign their weights accordingly. When using the REST API, the weights must be explicitly provided as a part of your request payload. Using the example group hierarchy above, the weights would be as follows: parent - first-child {weight: 10} - second-child {weight: 20} - third-child {weight: 30} - nested-child {weight: 10} Operators \u00b6 Set theory is applied when a new group is added as a child group. Three key concepts are at play: Intersections, Unions, and Differences. We have attempted to simplify working with these operators by giving them both human-readable and Boolean name mappings. They are as follows: Restrict (Boolean AND ) - The Restrict operator performs a set intersection on the queryset, and is equivalent to a Boolean AND . The preceding filter is restricted (aka intersected ) by the objects matching the child filter. All filter criteria must match between the filters for a member object to be included in the resultant filter. Include (Boolean OR ) - The Include operator performs a set union on the queryset, and is equivalent to a Boolean OR . The preceding filter is extended to include (aka unioned with ) any objects matching the child filter. Any filter criteria may match between the filters for member objects to be included in the resultant filter. Exclude (Boolean NOT ) - The Exclude operator performs a set difference on the queryset, and is equivalent to a Boolean NOT . The preceding filter excludes (aka differences ) any objects matching the child filter. Any matching objects from the child filter will be negated from the members of the resultant filter. The following table maps the Nautobot operator to the corresponding set operations: Operator Set Operation Boolean Description Restrict Intersection AND Objects must match this child filter to be included in the parent group Include Union OR Objects may match the child filter to be included in the parent group Exclude Difference NOT Objects must not match this child filter to be included in the parent group Any filters provided by the child groups are used to filter the members from the parent group using one of the three operators: Restrict (AND) , Include (OR) , or Exclude (NOT) . Using the example group hierarchy from above, let's apply operators and explain how it would work: parent {filter: None} - first-child {weight: 10, operator: intersection (AND), filter: site=ams01} - second-child {weight: 20, operator: union (OR), filter: site=ang01} - third-child {weight: 30, operator: difference (NOT), filter: None} - nested-child {weight: 10, operator: intersectio (AND), filter: status=active} Logically, the filter will be expressed like so using the hierarchy above: ((first-child OR second-child) AND (NOT nested-child)) Which in turn would map to the object filter: ((site=ams01 OR site=ang01) AND (NOT status=active)) How does this work? \u00b6 First, the filters for direct children for a group will always be included in a parenthetical grouping, separated by the operator. Parent groups always start from their base filter containing \"all objects\" (equivalent to an empty filter or {} ). Therefore the filter of parent and the filter of third-child are not directly used for generating the filter and are instead passed through from parent left to right as the \"base\" filter. Similarly, by the time we get to third-child , the filter at that node in the graph will have already had the filter from first-child and second-child applied, and will merely be passed through to the filter generated from the children of third-child . Because first-child and second-child are \"included\" by way of the union set operator (Boolean OR ), but because the filter for third-child is empty as explained above, its filter condition is not directly included, resulting in (first-child OR second-child) as the first group, representing all direct filters for the group parent . Note But what about the AND coming from the association of first-child , you might be asking? Well, that AND is also passed through because the default behavior when performing queryset filtering is to join all filter statements together with AND . For example, consider when you perform this from the Django ORM Device.objects.filter(site__slug=\"ams01\") the outcome is in fact equivalent to AND site__slug=\"ams01\" . Therefore, for the first child group (literally first-child in this case), initial AND will be omitted. Continuing on to the children of third-child , the same iteration rules apply. The filter from nested-child gets applied to the filter being passed through from its parent, third-child , except that the \"exclude\" (boolean NOT ) operator is still applied from the association of third-child to parent resulting in (NOT nested-child ). Note You'll see NOT emitted as an AND NOT because NOT is actually just shorthand for this. They are in fact interchangeable as Boolean operators. While it's technically possible to perform an OR NOT query from a database perspective, it is not supported by Dynamic Groups because the NOT operator maps directly to the \"difference\" set operation which is distinctly an AND NOT Boolean operation when performing underlying SQL queries at the database. Piecing the rules together, we ended up with two filters and by wrapping them in their own set of parentheses we get our final generated filter: ( ( [ALL OBJECTS] AND first-child ) OR second-child ) AND NOT ( [ALL OBJECTS] AND nested-child ) Dynamic Groups and the REST API \u00b6 Dynamic Groups are fully supported by the API. Two distinct endpoints are required, one each for managing Dynamic Groups and for assigning child groups using Dynamic Group Memberships. Specifying Filter Conditions \u00b6 Dynamic Groups are fairly straightforward however it is important to understand how the filter field works before digging in. The filter is a JSON field and it must be able to be used as valid query parameters for filtering objects of the corresponding Content Type. It is an error to provide any value other than a JSON object ( {} or a Python dictionary) for the filter field. Multiple Values \u00b6 Most fields within the filter accept multiple values and must be represented as a JSON array (Python list), for example: { \"site\" : [ \"ams01\" , \"ang01\" ] } Single Values \u00b6 Certain fields take Boolean values (JSON true / false ) or single numeric integers or character strings. For example, consider this boolean filter that requires a single true / false and would result in a Dynamic Group of devices that have interfaces: { \"has_interfaces\" : true } Or this character filter that requires a single string and would result in a Dynamic Group with only one member matching this name : { \"name\" : \"ams01-edge-01\" } Field Validation \u00b6 Any invalid field values for valid field names will also result in a ValidationError , for example providing an integer to the name filter is invalid: { \"name\" : -42 } Note Please refer to either the source code definition of the {model_name}FilterSet (e.g. for Device it would be nautobot.dcim.filters.DeviceFilterSet ) or the API documentation for the list endpoint (e.g. /api/dcim/devices/ ) for a given model object, to view the available filter fields and their expectations. Changed in version 1.4.0 Prior to v1.4.0, any invalid field names that are not eligible for filtering objects will be discarded upon validation. As of v1.4.0, strict filtering is enabled by default , which causes any invalid field names to result in a ValidationError . Managing Dynamic Groups \u00b6 Creating a Dynamic Group \u00b6 A Dynamic Group may be created by performing a POST to the Dynamic Groups list endpoint at /api/extras/dynamic-groups/ . Note The filter field will default to an empty filter ( {} ) if not provided. Important It is not possible to perform a nested assignment of children when creating a new Dynamic Group. You must first create the new group and then use the endpoint for creating Dynamic Group Memberships as explained below under Assigning Child Groups . Request: POST /api/ex tras /dy na mic - groups/ { \"name\" : \"parent\" , \"slug\" : \"parent\" , \"description\" : \"I am a parent group with nested children.\" , \"content-type\" : \"dcim.device\" , \"filter\" : {}, } Response: { \"id\" : \"1f825078-b6dc-4b12-9463-be5a9189b03f\" , \"display\" : \"parent\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/1f825078-b6dc-4b12-9463-be5a9189b03f/\" , \"name\" : \"parent\" , \"slug\" : \"parent\" , \"description\" : \"I am the parent group with nested children.\" , \"content_type\" : \"dcim.device\" , \"filter\" : {}, \"children\" : [], \"created\" : \"2022-07-06\" , \"last_updated\" : \"2022-07-06T20:17:04.305663Z\" , \"custom_fields\" : {}, \"computed_fields\" : {}, \"relationships\" : {} } Updating or Deleting a Dynamic Group \u00b6 Important It is not possible to perform a nested update of children when updating a new Dynamic Group. You must use the endpoint for creating Dynamic Group Memberships as explained below under Updating or Deleting Child Groups . Updating or deleting a Dynamic Group is done by sending a request to the detail endpoint for that object. A Dynamic Group may be updated using PUT or PATCH (for a partial update) requests. A PUT request requires the entire object to be updated in place. For example if you wanted to update the name and the slug together, leaving every other field with their current values as provided: PUT /api/ex tras /dy na mic - groups/ { uuid } / { \"name\" : \"I am the best parent group\" , \"slug\" : \"best-parent\" , \"description\" : \"I am the parent group with nested children.\" , \"filter\" : {} } Performing a partial update using a PATCH request can allow any single field to be updated without affecting the other fields. For example, if we wanted to update only the slug for a group: PATCH /api/ex tras /dy na mic - group - memberships/ { uuid } / { \"slug\" : \"best-parent\" } To delete a Dynamic Group you would send a DELETE request to the detail endpoint: DELETE /api/ex tras /dy na mic - group - memberships/ { uuid } / Managing Child Groups \u00b6 Dynamic Groups may be nested to a parent group by creating a new Dynamic Group Membership. The act of assigning a Dynamic Group as a child to a parent group creates a Dynamic Group Membership. This can be done at the list endpoint found at /api/extras/dynamic-group-memberships/ . Assigning Child Groups \u00b6 Dynamic Group Membership objects may be created, updated, or deleted just like any other object and are represented as children on the parent group. Note When interacting with the REST API, the operator must be provided using the string representation that is stored in the database. The human-readable operator names (such as \"Exclude (AND)\" for \"intersection\") are not accepted. Request: POST /api/ex tras /dy na mic - group - memberships/ { \"group\" : { \"slug\" : \"first-child\" }, \"parent_group\" : { \"slug\" : \"parent\" }, \"operator\" : \"intersection\" , \"weight\" : 10 } Response: { \"id\" : \"4c8296de-42bc-49a6-8fed-fc1b1f6b93ca\" , \"display\" : \"parent > intersection (10) > first-child\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-group-memberships/4c8296de-42bc-49a6-8fed-fc1b1f6b93ca/\" , \"group\" : { \"display\" : \"first-child\" , \"id\" : \"97188a74-eddd-46d8-be41-909c1ece1d43\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/97188a74-eddd-46d8-be41-909c1ece1d43/\" , \"name\" : \"first-child\" , \"slug\" : \"first-child\" , \"content_type\" : \"dcim.device\" }, \"parent_group\" : { \"display\" : \"parent\" , \"id\" : \"1f825078-b6dc-4b12-9463-be5a9189b03f\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/1f825078-b6dc-4b12-9463-be5a9189b03f/\" , \"name\" : \"parent\" , \"slug\" : \"parent\" , \"content_type\" : \"dcim.device\" }, \"operator\" : \"intersection\" , \"weight\" : 10 } Observe that after adding this new membership object, the parent group now reflects this in its children : GET /api/ex tras /dy na mic - groups/ 1 f 825078- b 6 dc -4 b 12-9463- be 5 a 9189 b 03 f / { \"id\" : \"1f825078-b6dc-4b12-9463-be5a9189b03f\" , \"display\" : \"parent\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/1f825078-b6dc-4b12-9463-be5a9189b03f/\" , \"name\" : \"parent\" , \"slug\" : \"parent\" , \"description\" : \"\" , \"content_type\" : \"dcim.device\" , \"filter\" : {}, \"children\" : [ { \"id\" : \"4c8296de-42bc-49a6-8fed-fc1b1f6b93ca\" , \"display\" : \"parent > intersection (10) > first-child\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-group-memberships/4c8296de-42bc-49a6-8fed-fc1b1f6b93ca/\" , \"group\" : { \"display\" : \"first-child\" , \"id\" : \"97188a74-eddd-46d8-be41-909c1ece1d43\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/97188a74-eddd-46d8-be41-909c1ece1d43/\" , \"name\" : \"first-child\" , \"slug\" : \"first-child\" , \"content_type\" : \"dcim.device\" }, \"parent_group\" : { \"display\" : \"parent\" , \"id\" : \"1f825078-b6dc-4b12-9463-be5a9189b03f\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/1f825078-b6dc-4b12-9463-be5a9189b03f/\" , \"name\" : \"parent\" , \"slug\" : \"parent\" , \"content_type\" : \"dcim.device\" }, \"operator\" : \"intersection\" , \"weight\" : 10 } ], \"created\" : \"2022-07-15\" , \"last_updated\" : \"2022-07-15T16:50:45.453965Z\" , \"notes_url\" : \"http://localhost:6789/api/extras/dynamic-groups/1f825078-b6dc-4b12-9463-be5a9189b03f/notes/\" , \"custom_fields\" : {} } Updating or Deleting Child Groups \u00b6 Updating or deleting Dynamic Group Membership is done by sending a request to the detail endpoint for that membership object. A Dynamic Group Membership may be updated using PUT or PATCH (for a partial update) requests. A PUT request requires the entire object to be updated in place. For example if you wanted to update the operator and the weight together,leaving every other field with their current values as provided: PUT /api/ex tras /dy na mic - group - memberships/ { uuid } / { \"group\" : { \"slug\" : \"first-child\" }, \"parent_group\" : { \"slug\" : \"parent\" }, \"operator\" : \"difference\" , \"weight\" : 10 } Performing a partial update using a PATCH request can allow any single field to be updated without affecting the other fields. For example, if we only wanted to update the weight for a membership: PATCH /api/ex tras /dy na mic - group - memberships/ { uuid } / { \"weight\" : 15 } To delete a Dynamic Group Membership you would send a DELETE request to the detail endpoint: DELETE /api/ex tras /dy na mic - group - memberships/ { uuid } /","title":"Dynamic Groups"},{"location":"models/extras/dynamicgroup.html#dynamic-groups","text":"Added in version 1.3.0 Dynamic Groups provide a way to organize objects of the same Content Type by matching filters. A Dynamic Group can be used to create unique groups of objects matching a given filter, such as Devices for a specific site location or set of locations. As indicated by the name, Dynamic Groups update in real time as potential member objects are created, updated, or deleted. When creating a Dynamic Group, one must select a Content Type to which it is associated, for example dcim.device . The filtering parameters saved to the group behave as a bi-directional search query that is used to identify members of that group, and can also be used to determine from an individual object the list of Dynamic Groups to which an individual object belongs. Once created the Content Type for a Dynamic Group may not be modified as this relationship is tightly-coupled to the available filtering parameters. All other fields may be updated at any time.","title":"Dynamic Groups"},{"location":"models/extras/dynamicgroup.html#introduction","text":"","title":"Introduction"},{"location":"models/extras/dynamicgroup.html#creating-dynamic-groups","text":"Dynamic Groups can be created through the UI under Organization > Dynamic Groups and clicking the \"Add\" button, or through the REST API. Each Dynamic Group must have a human-readable Name string, e.g. devices-site-ams01 and a Slug , which should be a simple database-friendly string. By default, the slug will be automatically generated from the name, however you may customize it if you like. You must select a Content Type for the group that determines the kind of objects that can be members of the group and the corresponding filtering parameters available. Finally, you may also assign an optional human-friendly Description (e.g. \"Devices in site AMS01\"). Once a new Dynamic Group is created, the group can be configured by clicking the \"Edit\" button to specify Filter Fields or Child Groups to use to narrow down the group's member objects. More on this below. Warning The content type of a Dynamic Group cannot be modified once created, so take care in selecting this initially. This is intended to prevent the possibility of inconsistent data and enforces the importance of thinking about the data model when defining a new Dynamic Group.","title":"Creating Dynamic Groups"},{"location":"models/extras/dynamicgroup.html#working-with-dynamic-groups","text":"Once created and configured, Dynamic Groups can be accessed from the primary Dynamic Groups landing page in the web interface under the Organization > Dynamic Groups menu. From there you may view the list of available groups, search or filter the list, view or edit an individual group, or bulk delete groups. Additionally if a group's filter has matching members, the number of members may be clicked to take you to the list of members for that dynamic group containing those objects. Dynamic Groups cannot be imported nor can they be updated in bulk, as these operations would be complex and do not make sense in most cases. From an individual object's detail page, if it is a member of any groups, a \"Dynamic Groups\" tab will display in the navigation tabs. Clicking that tab will display all Dynamic Groups of which this object is a member.","title":"Working with Dynamic Groups"},{"location":"models/extras/dynamicgroup.html#filtering","text":"Dynamic Group filtering is powered by FilterSet objects underneath the hood. Basic filtering is performed using the filter that is defined on a given Dynamic Group. Advanced filtering is performed by aggregating filters from multiple nested Dynamic Groups to form a combined parent Dynamic Group, which will be explained later in this document. An object is considered to be a member of a Dynamic Group if it is of the same Content Type and it is not excluded by way of any of the filter criteria specified for that group. By default, a freshly created group has an empty filter ( {} ), which will include all objects of the matching Content Type, just as a default list view of those objects would display prior to any filter fields being filled in the web UI. For example, for a Dynamic Group with Content Type of dcim.device and an empty filter, the list of members would be equivalent to the default Device list view, which in turn is equivalent to the queryset for Device.objects.all() from the database ORM. Changed in version 1.4.0 In Nautobot v1.3.0 the default for a Dynamic Group with an empty filter was to \"fail closed\" and have zero members. As of v1.4.0, this behavior has been inverted to default to include all objects matching the Content Type, instead of matching no objects as was previously the case. This was necessary to implement the progressive layering of child filters similarly to how we use filters to reduce desired objects from basic list view filters. This will be described in more detail below.","title":"Filtering"},{"location":"models/extras/dynamicgroup.html#basic-filtering","text":"When editing a Dynamic Group, under the Filter Options section, you will find a Filter Fields tab that allows one to specify filter criteria. The filter fields available for a given Content Type are backed and validated by underlying filterset classes (for example nautobot.dcim.filters.DeviceFilterSet ) and are represented in the web interface as a dynamically-generated filter form that corresponds to each eligible filter field.","title":"Basic Filtering"},{"location":"models/extras/dynamicgroup.html#advanced-filtering","text":"Added in version 1.4.0 Advanced filtering is performed using nested Dynamic Group memberships. An object is considered a member of an advanced Dynamic Group if it matches the aggregated filter criteria across all descendant groups. When editing a Dynamic Group, under the Filter Options section, you will find a Child Groups tab that allows one to make other Dynamic Groups of the same Content Type children of the parent group.","title":"Advanced Filtering"},{"location":"models/extras/dynamicgroup.html#example-workflow","text":"Dynamic Groups are a complex topic and are perhaps best understood through a series of worked examples.","title":"Example Workflow"},{"location":"models/extras/dynamicgroup.html#basic-filtering-with-a-single-dynamic-group","text":"Let's say you want to create a Dynamic Group that contains all production Devices at your first two Sites. You can create a Dynamic Group called \"Devices at Sites A and B\" for Content Type dcim | device , then edit it and set the Filter Fields to match: a Site of either \"AMS01\" or \"BKK01\" a Status of \"Active\" or \"Offline\" After clicking \"Update\", you will be returned to the detail view for this Dynamic Group, where you can verify the filter logic that results, and click the \"Members\" tab to see the set of Devices that it contains. A key to understand here is that generally, within a single Dynamic Group, additional values specified for the same filter field (here, \"Site\") will broaden the group to include additional objects that match those additional values, while specifying values for additional filter fields (here, \"Status\") will narrow the group to match only the objects that match this additional filter. This is expressed in the \"Filter Query Logic\" panel by the use of OR and AND operators - the logic for this Dynamic Group is: ( ( site__slug='ams01' OR site__slug='bkk01' ) AND ( status__slug='active' OR status__slug='offline' ) )","title":"Basic Filtering with a single Dynamic Group"},{"location":"models/extras/dynamicgroup.html#advanced-filtering-combining-two-dynamic-groups-into-a-third","text":"Added in version 1.4.0 Now, let's say that you add a third site to your network. This site is currently being built out, and you don't care about Devices from this site that are Offline status at present. What you want for your \"Devices of Interest\" Dynamic Group is logic similar to: ( ( ( site__slug='ams01' OR site__slug='bkk01' ) AND ( status__slug='active' OR status__slug='offline' ) ) OR ( site__slug='can01' AND status__slug='active' ) ) This logic is too complex to express directly via a single Dynamic Group, but fear not! This is what combining Dynamic Groups allows you to do. First, you can create a new \"Devices of Interest\" group. Edit this group, and instead of specifying Filter Fields , switch to the Child Groups tab of the editor, select the operator \"Include (OR)\" and the group \"Devices at Sites A and B\", and update the group. In the new group's detail view, you can see that it now contains one child group, \"Devices at Sites A and B\", and its members are exactly the same as those of that group. But we're not done yet! Next, you'll create another group to represent the other part of your desired logic. Call this group \"Site C So Far\", and set its Filter Fields to match Site \"CAN01\" and Status \"Active\". Verify that it contains the expected set of Devices from Site C. Now, we'll add this group into the \"Devices of Interest\" parent group. Navigate back to the Dynamic Groups list view, and edit this group. Under the Child Groups tab, add another \"Include (OR)\" operator and select group \"Site C So Far\": Now things are getting interesting! The \"Devices of Interest\" Dynamic Group now contains the filtered Devices from both of its child groups, and the \"Filter Query Logic\" matches our intent as we stated it earlier: ( ( ( site__slug='ams01' OR site__slug='bkk01' ) AND ( status__slug='active' OR status__slug='offline' ) ) OR ( site__slug='can01' AND status__slug='active' ) )","title":"Advanced Filtering - Combining Two Dynamic Groups into a Third"},{"location":"models/extras/dynamicgroup.html#advanced-filtering-nested-groups-and-negation","text":"Added in version 1.4.0 Next, let's say you add a fourth site to your network. This site is in bad shape, and has Devices in a wide range of statuses. You want your \"Devices of Interest\" group to include all Devices from this site, except for those in Decommissioning status . To express this logic and add these devices to our parent group, we will need to use a combination of groups and the \"Exclude (NOT)\" operator. First, you will create an \"Site D All Devices\" group. This will simply match Devices at Site \"DEL01\", regardless of their status. Then create a \"Site D Decommissioning Devices\" group, which matches Site \"DEL01\" and Status \"Decommissioning\". Next create a \"Site D Devices of Interest\" group, and set its Child Groups to: Operator \"Include (OR)\", group \"Site D All Devices\" Operator \"Exclude (NOT)\", group \"Site D Decommissioning Devices\" Warning In general, but especially when using the AND and NOT operators, you must pay close attention to the order of the child groups. In this example, if you were to reverse the order of these two child groups, you would not get the desired final result! You can check this group and confirm that it contains the expected restricted subset of Devices. Finally, you can edit the parent \"Devices of Interest\" group and add a third Child Groups entry, \"Include (OR)\" on \"Site D Devices of Interest\". The final result is a Dynamic Group that contains the desired set of Devices across all four of your Sites. You can see the filter logic that this combination of groups results in: ( ( ( site__slug='ams01' OR site__slug='bkk01' ) AND ( status__slug='active' OR status__slug='offline' ) ) OR ( site__slug='can01' AND status__slug='active' ) OR ( site__slug='del01' AND ( NOT (site__slug='del01' AND status__slug='decommissioning') ) ) ) You can also see the hierarchy of nested groups that are being used to derive the \"Devices of Interest\" group: Most importantly, you now have a Dynamic Group that contains exactly the set of Devices you need!","title":"Advanced Filtering: Nested Groups and Negation"},{"location":"models/extras/dynamicgroup.html#technical-details","text":"","title":"Technical Details"},{"location":"models/extras/dynamicgroup.html#filter-generation","text":"Filters are always processed hiearchically from the top down starting from the parent group and descending recursively to the last nested child group in order by the weight value assigned to that group when it was associated to its parent. Note For the purpose illustration, we will use \"left to right\" terminology since when verbally describing precedence in English, we read from left to right, so that following it will be more intuitive. The nesting of Dynamic Groups is performed using two advanced patterns: Sets and graphs. Rules for each child group are processed using a set operator , and groups are sorted hierarchically as a directed acyclic graph (DAG), where the weight is used for sorting child groups topologically. In both cases, the ordering of the tree of descendants from a parent group to its nested children is significant and critically important to how each subsequent filter or group of filters are processed to result in a final set of member objects. Consider an example where there is a graph from the parent group to three direct child groups, the third of which has its own nested child group: parent - first-child - second-child - third-child - nested-child The filter generation would walk the graph: starting from the base (match-all) filter of parent , the filter of first-child would be applied, then second-child , as ordered by their weight . In the case of third-child , all of its children (only nested-child in this case) would be processed in order in the same way and the resultant filter from all of the child groups for third-child would be applied to the filter resulting from first-child and second-child , resulting in the final filter for parent .","title":"Filter Generation"},{"location":"models/extras/dynamicgroup.html#weights","text":"Weights are used to define the order in which a parent group's child group filters are processed. Because this ordering is significant, care must be taken when constructing nested Dynamic Groups to result in filter parameters that have the desired outcome. Note Unique weights are only considered for directly-related child groups. In other words, the weights for each child group of a parent group are unique to that parent and only affect the sorting of the children for that parent. In practice, weights are automatically assigned in increments of 10 when associating child groups in the web UI, and child groups may be dragged and dropped to re-order them and re-assign their weights accordingly. When using the REST API, the weights must be explicitly provided as a part of your request payload. Using the example group hierarchy above, the weights would be as follows: parent - first-child {weight: 10} - second-child {weight: 20} - third-child {weight: 30} - nested-child {weight: 10}","title":"Weights"},{"location":"models/extras/dynamicgroup.html#operators","text":"Set theory is applied when a new group is added as a child group. Three key concepts are at play: Intersections, Unions, and Differences. We have attempted to simplify working with these operators by giving them both human-readable and Boolean name mappings. They are as follows: Restrict (Boolean AND ) - The Restrict operator performs a set intersection on the queryset, and is equivalent to a Boolean AND . The preceding filter is restricted (aka intersected ) by the objects matching the child filter. All filter criteria must match between the filters for a member object to be included in the resultant filter. Include (Boolean OR ) - The Include operator performs a set union on the queryset, and is equivalent to a Boolean OR . The preceding filter is extended to include (aka unioned with ) any objects matching the child filter. Any filter criteria may match between the filters for member objects to be included in the resultant filter. Exclude (Boolean NOT ) - The Exclude operator performs a set difference on the queryset, and is equivalent to a Boolean NOT . The preceding filter excludes (aka differences ) any objects matching the child filter. Any matching objects from the child filter will be negated from the members of the resultant filter. The following table maps the Nautobot operator to the corresponding set operations: Operator Set Operation Boolean Description Restrict Intersection AND Objects must match this child filter to be included in the parent group Include Union OR Objects may match the child filter to be included in the parent group Exclude Difference NOT Objects must not match this child filter to be included in the parent group Any filters provided by the child groups are used to filter the members from the parent group using one of the three operators: Restrict (AND) , Include (OR) , or Exclude (NOT) . Using the example group hierarchy from above, let's apply operators and explain how it would work: parent {filter: None} - first-child {weight: 10, operator: intersection (AND), filter: site=ams01} - second-child {weight: 20, operator: union (OR), filter: site=ang01} - third-child {weight: 30, operator: difference (NOT), filter: None} - nested-child {weight: 10, operator: intersectio (AND), filter: status=active} Logically, the filter will be expressed like so using the hierarchy above: ((first-child OR second-child) AND (NOT nested-child)) Which in turn would map to the object filter: ((site=ams01 OR site=ang01) AND (NOT status=active))","title":"Operators"},{"location":"models/extras/dynamicgroup.html#how-does-this-work","text":"First, the filters for direct children for a group will always be included in a parenthetical grouping, separated by the operator. Parent groups always start from their base filter containing \"all objects\" (equivalent to an empty filter or {} ). Therefore the filter of parent and the filter of third-child are not directly used for generating the filter and are instead passed through from parent left to right as the \"base\" filter. Similarly, by the time we get to third-child , the filter at that node in the graph will have already had the filter from first-child and second-child applied, and will merely be passed through to the filter generated from the children of third-child . Because first-child and second-child are \"included\" by way of the union set operator (Boolean OR ), but because the filter for third-child is empty as explained above, its filter condition is not directly included, resulting in (first-child OR second-child) as the first group, representing all direct filters for the group parent . Note But what about the AND coming from the association of first-child , you might be asking? Well, that AND is also passed through because the default behavior when performing queryset filtering is to join all filter statements together with AND . For example, consider when you perform this from the Django ORM Device.objects.filter(site__slug=\"ams01\") the outcome is in fact equivalent to AND site__slug=\"ams01\" . Therefore, for the first child group (literally first-child in this case), initial AND will be omitted. Continuing on to the children of third-child , the same iteration rules apply. The filter from nested-child gets applied to the filter being passed through from its parent, third-child , except that the \"exclude\" (boolean NOT ) operator is still applied from the association of third-child to parent resulting in (NOT nested-child ). Note You'll see NOT emitted as an AND NOT because NOT is actually just shorthand for this. They are in fact interchangeable as Boolean operators. While it's technically possible to perform an OR NOT query from a database perspective, it is not supported by Dynamic Groups because the NOT operator maps directly to the \"difference\" set operation which is distinctly an AND NOT Boolean operation when performing underlying SQL queries at the database. Piecing the rules together, we ended up with two filters and by wrapping them in their own set of parentheses we get our final generated filter: ( ( [ALL OBJECTS] AND first-child ) OR second-child ) AND NOT ( [ALL OBJECTS] AND nested-child )","title":"How does this work?"},{"location":"models/extras/dynamicgroup.html#dynamic-groups-and-the-rest-api","text":"Dynamic Groups are fully supported by the API. Two distinct endpoints are required, one each for managing Dynamic Groups and for assigning child groups using Dynamic Group Memberships.","title":"Dynamic Groups and the REST API"},{"location":"models/extras/dynamicgroup.html#specifying-filter-conditions","text":"Dynamic Groups are fairly straightforward however it is important to understand how the filter field works before digging in. The filter is a JSON field and it must be able to be used as valid query parameters for filtering objects of the corresponding Content Type. It is an error to provide any value other than a JSON object ( {} or a Python dictionary) for the filter field.","title":"Specifying Filter Conditions"},{"location":"models/extras/dynamicgroup.html#multiple-values","text":"Most fields within the filter accept multiple values and must be represented as a JSON array (Python list), for example: { \"site\" : [ \"ams01\" , \"ang01\" ] }","title":"Multiple Values"},{"location":"models/extras/dynamicgroup.html#single-values","text":"Certain fields take Boolean values (JSON true / false ) or single numeric integers or character strings. For example, consider this boolean filter that requires a single true / false and would result in a Dynamic Group of devices that have interfaces: { \"has_interfaces\" : true } Or this character filter that requires a single string and would result in a Dynamic Group with only one member matching this name : { \"name\" : \"ams01-edge-01\" }","title":"Single Values"},{"location":"models/extras/dynamicgroup.html#field-validation","text":"Any invalid field values for valid field names will also result in a ValidationError , for example providing an integer to the name filter is invalid: { \"name\" : -42 } Note Please refer to either the source code definition of the {model_name}FilterSet (e.g. for Device it would be nautobot.dcim.filters.DeviceFilterSet ) or the API documentation for the list endpoint (e.g. /api/dcim/devices/ ) for a given model object, to view the available filter fields and their expectations. Changed in version 1.4.0 Prior to v1.4.0, any invalid field names that are not eligible for filtering objects will be discarded upon validation. As of v1.4.0, strict filtering is enabled by default , which causes any invalid field names to result in a ValidationError .","title":"Field Validation"},{"location":"models/extras/dynamicgroup.html#managing-dynamic-groups","text":"","title":"Managing Dynamic Groups"},{"location":"models/extras/dynamicgroup.html#creating-a-dynamic-group","text":"A Dynamic Group may be created by performing a POST to the Dynamic Groups list endpoint at /api/extras/dynamic-groups/ . Note The filter field will default to an empty filter ( {} ) if not provided. Important It is not possible to perform a nested assignment of children when creating a new Dynamic Group. You must first create the new group and then use the endpoint for creating Dynamic Group Memberships as explained below under Assigning Child Groups . Request: POST /api/ex tras /dy na mic - groups/ { \"name\" : \"parent\" , \"slug\" : \"parent\" , \"description\" : \"I am a parent group with nested children.\" , \"content-type\" : \"dcim.device\" , \"filter\" : {}, } Response: { \"id\" : \"1f825078-b6dc-4b12-9463-be5a9189b03f\" , \"display\" : \"parent\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/1f825078-b6dc-4b12-9463-be5a9189b03f/\" , \"name\" : \"parent\" , \"slug\" : \"parent\" , \"description\" : \"I am the parent group with nested children.\" , \"content_type\" : \"dcim.device\" , \"filter\" : {}, \"children\" : [], \"created\" : \"2022-07-06\" , \"last_updated\" : \"2022-07-06T20:17:04.305663Z\" , \"custom_fields\" : {}, \"computed_fields\" : {}, \"relationships\" : {} }","title":"Creating a Dynamic Group"},{"location":"models/extras/dynamicgroup.html#updating-or-deleting-a-dynamic-group","text":"Important It is not possible to perform a nested update of children when updating a new Dynamic Group. You must use the endpoint for creating Dynamic Group Memberships as explained below under Updating or Deleting Child Groups . Updating or deleting a Dynamic Group is done by sending a request to the detail endpoint for that object. A Dynamic Group may be updated using PUT or PATCH (for a partial update) requests. A PUT request requires the entire object to be updated in place. For example if you wanted to update the name and the slug together, leaving every other field with their current values as provided: PUT /api/ex tras /dy na mic - groups/ { uuid } / { \"name\" : \"I am the best parent group\" , \"slug\" : \"best-parent\" , \"description\" : \"I am the parent group with nested children.\" , \"filter\" : {} } Performing a partial update using a PATCH request can allow any single field to be updated without affecting the other fields. For example, if we wanted to update only the slug for a group: PATCH /api/ex tras /dy na mic - group - memberships/ { uuid } / { \"slug\" : \"best-parent\" } To delete a Dynamic Group you would send a DELETE request to the detail endpoint: DELETE /api/ex tras /dy na mic - group - memberships/ { uuid } /","title":"Updating or Deleting a Dynamic Group"},{"location":"models/extras/dynamicgroup.html#managing-child-groups","text":"Dynamic Groups may be nested to a parent group by creating a new Dynamic Group Membership. The act of assigning a Dynamic Group as a child to a parent group creates a Dynamic Group Membership. This can be done at the list endpoint found at /api/extras/dynamic-group-memberships/ .","title":"Managing Child Groups"},{"location":"models/extras/dynamicgroup.html#assigning-child-groups","text":"Dynamic Group Membership objects may be created, updated, or deleted just like any other object and are represented as children on the parent group. Note When interacting with the REST API, the operator must be provided using the string representation that is stored in the database. The human-readable operator names (such as \"Exclude (AND)\" for \"intersection\") are not accepted. Request: POST /api/ex tras /dy na mic - group - memberships/ { \"group\" : { \"slug\" : \"first-child\" }, \"parent_group\" : { \"slug\" : \"parent\" }, \"operator\" : \"intersection\" , \"weight\" : 10 } Response: { \"id\" : \"4c8296de-42bc-49a6-8fed-fc1b1f6b93ca\" , \"display\" : \"parent > intersection (10) > first-child\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-group-memberships/4c8296de-42bc-49a6-8fed-fc1b1f6b93ca/\" , \"group\" : { \"display\" : \"first-child\" , \"id\" : \"97188a74-eddd-46d8-be41-909c1ece1d43\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/97188a74-eddd-46d8-be41-909c1ece1d43/\" , \"name\" : \"first-child\" , \"slug\" : \"first-child\" , \"content_type\" : \"dcim.device\" }, \"parent_group\" : { \"display\" : \"parent\" , \"id\" : \"1f825078-b6dc-4b12-9463-be5a9189b03f\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/1f825078-b6dc-4b12-9463-be5a9189b03f/\" , \"name\" : \"parent\" , \"slug\" : \"parent\" , \"content_type\" : \"dcim.device\" }, \"operator\" : \"intersection\" , \"weight\" : 10 } Observe that after adding this new membership object, the parent group now reflects this in its children : GET /api/ex tras /dy na mic - groups/ 1 f 825078- b 6 dc -4 b 12-9463- be 5 a 9189 b 03 f / { \"id\" : \"1f825078-b6dc-4b12-9463-be5a9189b03f\" , \"display\" : \"parent\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/1f825078-b6dc-4b12-9463-be5a9189b03f/\" , \"name\" : \"parent\" , \"slug\" : \"parent\" , \"description\" : \"\" , \"content_type\" : \"dcim.device\" , \"filter\" : {}, \"children\" : [ { \"id\" : \"4c8296de-42bc-49a6-8fed-fc1b1f6b93ca\" , \"display\" : \"parent > intersection (10) > first-child\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-group-memberships/4c8296de-42bc-49a6-8fed-fc1b1f6b93ca/\" , \"group\" : { \"display\" : \"first-child\" , \"id\" : \"97188a74-eddd-46d8-be41-909c1ece1d43\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/97188a74-eddd-46d8-be41-909c1ece1d43/\" , \"name\" : \"first-child\" , \"slug\" : \"first-child\" , \"content_type\" : \"dcim.device\" }, \"parent_group\" : { \"display\" : \"parent\" , \"id\" : \"1f825078-b6dc-4b12-9463-be5a9189b03f\" , \"url\" : \"http://localhost:6789/api/extras/dynamic-groups/1f825078-b6dc-4b12-9463-be5a9189b03f/\" , \"name\" : \"parent\" , \"slug\" : \"parent\" , \"content_type\" : \"dcim.device\" }, \"operator\" : \"intersection\" , \"weight\" : 10 } ], \"created\" : \"2022-07-15\" , \"last_updated\" : \"2022-07-15T16:50:45.453965Z\" , \"notes_url\" : \"http://localhost:6789/api/extras/dynamic-groups/1f825078-b6dc-4b12-9463-be5a9189b03f/notes/\" , \"custom_fields\" : {} }","title":"Assigning Child Groups"},{"location":"models/extras/dynamicgroup.html#updating-or-deleting-child-groups","text":"Updating or deleting Dynamic Group Membership is done by sending a request to the detail endpoint for that membership object. A Dynamic Group Membership may be updated using PUT or PATCH (for a partial update) requests. A PUT request requires the entire object to be updated in place. For example if you wanted to update the operator and the weight together,leaving every other field with their current values as provided: PUT /api/ex tras /dy na mic - group - memberships/ { uuid } / { \"group\" : { \"slug\" : \"first-child\" }, \"parent_group\" : { \"slug\" : \"parent\" }, \"operator\" : \"difference\" , \"weight\" : 10 } Performing a partial update using a PATCH request can allow any single field to be updated without affecting the other fields. For example, if we only wanted to update the weight for a membership: PATCH /api/ex tras /dy na mic - group - memberships/ { uuid } / { \"weight\" : 15 } To delete a Dynamic Group Membership you would send a DELETE request to the detail endpoint: DELETE /api/ex tras /dy na mic - group - memberships/ { uuid } /","title":"Updating or Deleting Child Groups"},{"location":"models/extras/exporttemplate.html","text":"Export Templates \u00b6 Nautobot allows users to define custom templates that can be used when exporting objects. To create an export template, navigate to Extensibility > Automation > Export Templates under the navigation bar. Export templates can also be managed within an external Git repository if desired. Each export template is associated with a certain type of object. For instance, if you create an export template for VLANs, your custom template will appear under the \"Export\" button on the VLANs list. Export templates must be written in Jinja2 . The list of objects returned from the database when rendering an export template is stored in the queryset variable, which you'll typically want to iterate through using a for loop. Object properties can be access by name. For example: {% for rack in queryset %} Rack: {{ rack.name }} Site: {{ rack.site.name }} Height: {{ rack.u_height }}U {% endfor %} To access custom fields of an object within a template, use the cf attribute. For example, {{ obj.cf.color }} will return the value (if any) for a custom field named color on obj . A MIME type and file extension can optionally be defined for each export template. The default MIME type is text/plain . Example \u00b6 Here's an example device export template that will generate a simple Nagios configuration from a list of devices. {% for device in queryset %}{% if device.status and device.primary_ip %}define host{ use generic-switch host_name {{ device.name }} address {{ device.primary_ip.address.ip }} } {% endif %}{% endfor %} The generated output will look something like this: define host{ use generic-switch host_name switch1 address 192.0.2.1 } define host{ use generic-switch host_name switch2 address 192.0.2.2 } define host{ use generic-switch host_name switch3 address 192.0.2.3 }","title":"Export Templates"},{"location":"models/extras/exporttemplate.html#export-templates","text":"Nautobot allows users to define custom templates that can be used when exporting objects. To create an export template, navigate to Extensibility > Automation > Export Templates under the navigation bar. Export templates can also be managed within an external Git repository if desired. Each export template is associated with a certain type of object. For instance, if you create an export template for VLANs, your custom template will appear under the \"Export\" button on the VLANs list. Export templates must be written in Jinja2 . The list of objects returned from the database when rendering an export template is stored in the queryset variable, which you'll typically want to iterate through using a for loop. Object properties can be access by name. For example: {% for rack in queryset %} Rack: {{ rack.name }} Site: {{ rack.site.name }} Height: {{ rack.u_height }}U {% endfor %} To access custom fields of an object within a template, use the cf attribute. For example, {{ obj.cf.color }} will return the value (if any) for a custom field named color on obj . A MIME type and file extension can optionally be defined for each export template. The default MIME type is text/plain .","title":"Export Templates"},{"location":"models/extras/exporttemplate.html#example","text":"Here's an example device export template that will generate a simple Nagios configuration from a list of devices. {% for device in queryset %}{% if device.status and device.primary_ip %}define host{ use generic-switch host_name {{ device.name }} address {{ device.primary_ip.address.ip }} } {% endif %}{% endfor %} The generated output will look something like this: define host{ use generic-switch host_name switch1 address 192.0.2.1 } define host{ use generic-switch host_name switch2 address 192.0.2.2 } define host{ use generic-switch host_name switch3 address 192.0.2.3 }","title":"Example"},{"location":"models/extras/gitrepository.html","text":"Git Repositories \u00b6 Some text-based content is more conveniently stored in a separate Git repository rather than internally in the Nautobot database. Such a repository may currently include any or all of the following for Nautobot to consume: Job source files and associated data files, Configuration context data Export templates Additional data types as registered by any installed plugins Important Nautobot's Git integration depends on the availability of the git program. If git is not installed, Nautobot will be unable to pull data from Git repositories. Repository Configuration \u00b6 When defining a Git repository for Nautobot to consume, the name , remote URL , and branch parameters are mandatory - the name acts as a unique identifier, and the remote URL and branch are needed for Nautobot to be able to locate and access the specified repository. Additionally, if the repository is private you may specify a token and any associated username that can be used to grant access to the repository. Warning Beginning in Nautobot 1.2, there are two ways to define a token and/or username for a Git repository -- either by directly configuring them into the repository definition, or by associating the repository with a secrets group record (this latter approach is new in Nautobot 1.2). The direct-configuration approach should be considered as deprecated, as it is less secure and poses a number of maintainability issues. If at all possible, you should use a secrets group instead. The direct-configuration approach may be removed altogether as an option in a future release of Nautobot. The token implementation can vary from Git provider to Git provider, the following providers have been confirmed to work. In theory, additional providers using the same pattern will work, but there is currently no specific support for all providers. GitHub's token does not require a username . GitLab's token requires a username , conventions are to use the username \"oauth2\". In addition, GitLab's deploy tokens are also supported. For Bitbucket, there are two options: personal access tokens or OAuth2 depending on the product. Note When defining a secrets group for a Git repository, the group must contain assigned secret(s) with an access type of HTTP(S) and secret type(s) of Token (and Username , if required by the provider). Whenever a Git repository record is created, updated, or deleted, Nautobot automatically enqueues a background task that will asynchronously execute to clone, fetch, or delete a local copy of the Git repository on the filesystem (located under GIT_ROOT ) and then create, update, and/or delete any database records managed by this repository. The progress and eventual outcome of this background task are recorded as a JobResult record that may be viewed from the Git repository user interface. Important The repository branch must exist and have a commit against it. At this time, Nautobot will not initialize an empty repository. Note If you are using a self-signed Git repository, you will need to set the environment variable GIT_SSL_NO_VERIFY=\"1\" in order for the repository to sync. Repository Structure \u00b6 Jobs \u00b6 Jobs defined in Python files located in a /jobs/ directory at the root of a Git repository will automatically be discovered by Nautobot and made available to be run as a job, just as they would be if manually installed to the JOBS_ROOT directory. Note There must be an __init__.py file in the /jobs/ directory. Note Just as with jobs manually installed in JOBS_ROOT , jobs provided by a Git repository do not support inter-module relative Python imports (i.e., you cannot package Python \"libraries\" into a Git repository and then import them from Jobs in that repository). If you need to import libraries from Jobs, the libraries either must be installed as a standard Python packaging dependency or as a Nautobot plugin. When syncing or re-syncing a Git repository, the Nautobot database records corresponding to any provided jobs will automatically be refreshed. If a job is removed as a result of the sync, the corresponding database record will not be automatically deleted, but will be marked as installed = False and will no longer be runnable. A user with appropriate access permissions can delete leftover Job database records if desired, but note that this will result in any existing JobResult records no longer having a direct reference back to the Job that they originated from. Configuration Contexts \u00b6 Config contexts may be provided as JSON or YAML files located in /config_contexts/ . There are three different types of config context scopes; explicit , implicit , and local . Explicit : Defined as JSON or YAML files at the root of the /config_contexts/ folder. Multiple config contexts can be specified within the each file. The metadata regarding the naming and scoping of the config context is determined by the _metadata key for each list element. Implicit : They're defined using a specific folder and file structure to apply the config context to a specific scope. Local : Defined at the device/virtual machine level and only being applied to the specific device/virtual machine. Metadata \u00b6 The metadata used to create the config context has the following options and is specified by the _metadata key. Key Required Default Description name True N/A The name that will be assigned to the Config Context weight False 1000 The weight that will be assigned to the Config Context that determines precedence description False N/A The description applied to the Config Context is_active False True Whether or not the Config Context is active schema False N/A Config Context Schema that it should be validated against There are several other keys that can be defined that match the scope of what the Config Context will be assigned to. Here is an example _metadata key defined: { \"_metadata\" : { \"name\" : \"Region NYC servers\" , \"weight\" : 1000 , \"description\" : \"NTP and Syslog servers for region NYC\" , \"is_active\" : true , \"regions\" : [{ \"slug\" : \"nyc\" }], \"schema\" : \"Config Context Schema 1\" }, \"acl\" : { \"definitions\" : { \"named \" : { \"PERMIT_ROUTES\" : [ \"10 permit ip any any\" ] } } }, \"route-maps\" : { \"PERMIT_CONN_ROUTES\" : { \"seq\" : 10 , \"statements\" : [ \"match ip address PERMIT_ROUTES\" ], \"type\" : \"permit\" } } } Important The only config context scope that does not require any metadata defined is the local configuration context Explicit Config Contexts \u00b6 As stated above, these explicit files live at the root of /config_contexts . These files will be imported as described below, with no special meaning attributed to their filenames (the name of the constructed config context will be taken from the _metadata key within the file, not the filename). To provide a visual, the context_1.json and context_2.yml are explicit config context scopes. config_contexts/ context_1.json # JSON data will be imported as-is, with scoping explicit from its contents context_2.yaml # YAML data will be imported as-is, with scoping explicit from its contents For files in the root of the /config_contexts/ directory, a single file may define a single config context as above, or alternatively it may contain a list of config context data definitions, as in the following example: --- - _metadata : name : \"Router hostname pattern\" roles : - slug : \"router\" hostname_pattern_string : \"rtr-.+\" - _metadata : name : \"Console Server hostname pattern\" roles : - slug : \"console-server\" hostname_pattern_string : \"cs-.+\" - _metadata : name : \"Switches hostname pattern\" roles : - slug : \"aggr-switch\" - slug : \"services-switch\" hostname_pattern_string : \"switch-.+\" - _metadata : name : \"Appliance hostname pattern\" roles : - slug : \"security-appliance\" hostname_pattern_string : \"fw-.+\" ... The _metadata key will map to the attributes required when creating a config context via the UI or API such as name and the scope of the config context. If we take a look at the first element, the name assigned to the config context will be \"Router hostname pattern\" and be scoped to roles with a slug of router . Any key/value pair defined at the same level as _metadata will be converted to the config context data. Keeping with the first element, it will have a key set as hostname_pattern_string with a value of rtr-.+ . Implicit Config Contexts \u00b6 Implicit config context files will have the following folder/file structure /config_contexts/<filter>/<slug>.[json|yaml] , in which case their path and filename will be taken as an implicit scope for the context. For example: config_contexts/ regions/ nyc.yaml # YAML data, with implicit scoping to the Region with slug \"nyc\" sites/ nyc-01.json # JSON data, with implicit scoping to the Site with slug \"nyc-01\" The implicit config contexts will be defined using dictionaries for both _metadata and any context data for the config context. JSON \u00b6 { \"_metadata\" : { \"name\" : \"Region NYC servers\" , \"weight\" : 1000 , \"description\" : \"NTP and Syslog servers for region NYC\" , \"is_active\" : true , \"schema\" : \"Config Context Schema 1\" }, \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ], \"syslog-servers\" : [ \"172.16.9.100\" , \"172.16.9.101\" ] } YAML \u00b6 _metadata\" : name : \"Region NYC servers\" weight : 1000 description : \"NTP and Syslog servers for region NYC\" is_active : true schema : \"Config Context Schema 1\" ntp-servers : - 172.16.10.22 - 172.16.10.33 syslog-servers : - 172.16.9.100 - 172.16.9.101 This will create a config context with two keys: ntp-servers and syslog-servers . Local Configuration Contexts \u00b6 Files in a config_contexts/devices/ and/or config_contexts/virtual_machines/ directory will be used to populate \"local\" config context data for individual devices or virtual machines. For these files, the device/VM name will always be taken from the filename, and the data in the file will be used precisely as-is (there is no need, or support, for a _metadata key in these files). config_contexts/ devices/ rtr-01.yaml # YAML data, local to the Device named \"rtr-01\" virtual_machines/ vm001.json # JSON data, local to the VirtualMachine named \"vm001\" Note While virtual machines are always uniquely identified by their name, it is possible for devices associated with different sites and/or tenants to share an identical name. Currently, Nautobot is unable to automatically apply local config context via Git to devices that have a non-globally-unique name (or no name at all). Configuration Context Schemas \u00b6 Config context schemas may be provided as JSON or YAML files located in /config_context_schemas/ . Files in the root of the /config_context_schemas/ directory will be imported as described below, with no special meaning attributed to their filenames (the name of the constructed config context schema will be taken from the _metadata within the file, not the filename). Similar to config context definitions, a single file may define a single config context schema or a list of such schemas - see examples below. config_context_schemas/ context_schema_1.json context_schema_2.yaml When loading the schema, the key _metadata will be extracted from the loaded data and used to define the config context schema's metadata, while the actual config context data schema will be based on the key data_schema . JSON single example: { \"_metadata\" : { \"name\" : \"Config Context Schema 1\" , \"description\" : \"Schema for defining first names.\" }, \"data_schema\" : { \"title\" : \"Person\" , \"properties\" : { \"firstName\" : { \"type\" : \"string\" , \"description\" : \"The person's first name.\" } } } } JSON list example: [ { \"_metadata\" : { \"name\" : \"Config Context Schema 1\" , \"description\" : \"Schema for defining first names.\" }, \"data_schema\" : { \"title\" : \"Person\" , \"properties\" : { \"firstName\" : { \"type\" : \"string\" , \"description\" : \"The person's first name.\" }, } } }, { \"_metadata\" : { \"name\" : \"Config Context Schema 2\" , \"description\" : \"Schema for defining last names.\" }, \"data_schema\" : { \"title\" : \"Person\" , \"properties\" : { \"lastName\" : { \"type\" : \"string\" , \"description\" : \"The person's last name.\" }, } } }, ] YAML single example: --- _metadata : name : \"Config Context Schema 1\" description : \"Schema for defining first names.\" data_schema : title : \"Person\" properties : firstName : type : \"string\" description : \"The person's first name\" YAML list example: --- - _metadata : name : \"Config Context Schema 1\" description : \"Schema for defining first names.\" data_schema : title : \"Person\" properties : firstName : type : \"string\" description : \"The person's first name\" - _metadata : name : \"Config Context Schema 2\" description : \"Schema for defining last names.\" data_schema : title : \"Person\" properties : lastName : type : \"string\" description : \"The person's last name\" Export Templates \u00b6 Export templates may be provided as files located in /export_templates/<grouping>/<model>/<template_file> ; for example, a JSON export template for Device records might be /export_templates/dcim/device/mytemplate.json . The name of a discovered export template will be presented in Nautobot as <repository name>: <filename> . The MIME type of a file rendered from a discovered export template will try to match the extension to IANA's list . If not detected, it will default to text/plain . The file extension of a file rendered from a discovered export template will match that of the template itself (so, in the above example, the extension would be .json )","title":"Git Repositories"},{"location":"models/extras/gitrepository.html#git-repositories","text":"Some text-based content is more conveniently stored in a separate Git repository rather than internally in the Nautobot database. Such a repository may currently include any or all of the following for Nautobot to consume: Job source files and associated data files, Configuration context data Export templates Additional data types as registered by any installed plugins Important Nautobot's Git integration depends on the availability of the git program. If git is not installed, Nautobot will be unable to pull data from Git repositories.","title":"Git Repositories"},{"location":"models/extras/gitrepository.html#repository-configuration","text":"When defining a Git repository for Nautobot to consume, the name , remote URL , and branch parameters are mandatory - the name acts as a unique identifier, and the remote URL and branch are needed for Nautobot to be able to locate and access the specified repository. Additionally, if the repository is private you may specify a token and any associated username that can be used to grant access to the repository. Warning Beginning in Nautobot 1.2, there are two ways to define a token and/or username for a Git repository -- either by directly configuring them into the repository definition, or by associating the repository with a secrets group record (this latter approach is new in Nautobot 1.2). The direct-configuration approach should be considered as deprecated, as it is less secure and poses a number of maintainability issues. If at all possible, you should use a secrets group instead. The direct-configuration approach may be removed altogether as an option in a future release of Nautobot. The token implementation can vary from Git provider to Git provider, the following providers have been confirmed to work. In theory, additional providers using the same pattern will work, but there is currently no specific support for all providers. GitHub's token does not require a username . GitLab's token requires a username , conventions are to use the username \"oauth2\". In addition, GitLab's deploy tokens are also supported. For Bitbucket, there are two options: personal access tokens or OAuth2 depending on the product. Note When defining a secrets group for a Git repository, the group must contain assigned secret(s) with an access type of HTTP(S) and secret type(s) of Token (and Username , if required by the provider). Whenever a Git repository record is created, updated, or deleted, Nautobot automatically enqueues a background task that will asynchronously execute to clone, fetch, or delete a local copy of the Git repository on the filesystem (located under GIT_ROOT ) and then create, update, and/or delete any database records managed by this repository. The progress and eventual outcome of this background task are recorded as a JobResult record that may be viewed from the Git repository user interface. Important The repository branch must exist and have a commit against it. At this time, Nautobot will not initialize an empty repository. Note If you are using a self-signed Git repository, you will need to set the environment variable GIT_SSL_NO_VERIFY=\"1\" in order for the repository to sync.","title":"Repository Configuration"},{"location":"models/extras/gitrepository.html#repository-structure","text":"","title":"Repository Structure"},{"location":"models/extras/gitrepository.html#jobs","text":"Jobs defined in Python files located in a /jobs/ directory at the root of a Git repository will automatically be discovered by Nautobot and made available to be run as a job, just as they would be if manually installed to the JOBS_ROOT directory. Note There must be an __init__.py file in the /jobs/ directory. Note Just as with jobs manually installed in JOBS_ROOT , jobs provided by a Git repository do not support inter-module relative Python imports (i.e., you cannot package Python \"libraries\" into a Git repository and then import them from Jobs in that repository). If you need to import libraries from Jobs, the libraries either must be installed as a standard Python packaging dependency or as a Nautobot plugin. When syncing or re-syncing a Git repository, the Nautobot database records corresponding to any provided jobs will automatically be refreshed. If a job is removed as a result of the sync, the corresponding database record will not be automatically deleted, but will be marked as installed = False and will no longer be runnable. A user with appropriate access permissions can delete leftover Job database records if desired, but note that this will result in any existing JobResult records no longer having a direct reference back to the Job that they originated from.","title":"Jobs"},{"location":"models/extras/gitrepository.html#configuration-contexts","text":"Config contexts may be provided as JSON or YAML files located in /config_contexts/ . There are three different types of config context scopes; explicit , implicit , and local . Explicit : Defined as JSON or YAML files at the root of the /config_contexts/ folder. Multiple config contexts can be specified within the each file. The metadata regarding the naming and scoping of the config context is determined by the _metadata key for each list element. Implicit : They're defined using a specific folder and file structure to apply the config context to a specific scope. Local : Defined at the device/virtual machine level and only being applied to the specific device/virtual machine.","title":"Configuration Contexts"},{"location":"models/extras/gitrepository.html#metadata","text":"The metadata used to create the config context has the following options and is specified by the _metadata key. Key Required Default Description name True N/A The name that will be assigned to the Config Context weight False 1000 The weight that will be assigned to the Config Context that determines precedence description False N/A The description applied to the Config Context is_active False True Whether or not the Config Context is active schema False N/A Config Context Schema that it should be validated against There are several other keys that can be defined that match the scope of what the Config Context will be assigned to. Here is an example _metadata key defined: { \"_metadata\" : { \"name\" : \"Region NYC servers\" , \"weight\" : 1000 , \"description\" : \"NTP and Syslog servers for region NYC\" , \"is_active\" : true , \"regions\" : [{ \"slug\" : \"nyc\" }], \"schema\" : \"Config Context Schema 1\" }, \"acl\" : { \"definitions\" : { \"named \" : { \"PERMIT_ROUTES\" : [ \"10 permit ip any any\" ] } } }, \"route-maps\" : { \"PERMIT_CONN_ROUTES\" : { \"seq\" : 10 , \"statements\" : [ \"match ip address PERMIT_ROUTES\" ], \"type\" : \"permit\" } } } Important The only config context scope that does not require any metadata defined is the local configuration context","title":"Metadata"},{"location":"models/extras/gitrepository.html#explicit-config-contexts","text":"As stated above, these explicit files live at the root of /config_contexts . These files will be imported as described below, with no special meaning attributed to their filenames (the name of the constructed config context will be taken from the _metadata key within the file, not the filename). To provide a visual, the context_1.json and context_2.yml are explicit config context scopes. config_contexts/ context_1.json # JSON data will be imported as-is, with scoping explicit from its contents context_2.yaml # YAML data will be imported as-is, with scoping explicit from its contents For files in the root of the /config_contexts/ directory, a single file may define a single config context as above, or alternatively it may contain a list of config context data definitions, as in the following example: --- - _metadata : name : \"Router hostname pattern\" roles : - slug : \"router\" hostname_pattern_string : \"rtr-.+\" - _metadata : name : \"Console Server hostname pattern\" roles : - slug : \"console-server\" hostname_pattern_string : \"cs-.+\" - _metadata : name : \"Switches hostname pattern\" roles : - slug : \"aggr-switch\" - slug : \"services-switch\" hostname_pattern_string : \"switch-.+\" - _metadata : name : \"Appliance hostname pattern\" roles : - slug : \"security-appliance\" hostname_pattern_string : \"fw-.+\" ... The _metadata key will map to the attributes required when creating a config context via the UI or API such as name and the scope of the config context. If we take a look at the first element, the name assigned to the config context will be \"Router hostname pattern\" and be scoped to roles with a slug of router . Any key/value pair defined at the same level as _metadata will be converted to the config context data. Keeping with the first element, it will have a key set as hostname_pattern_string with a value of rtr-.+ .","title":"Explicit Config Contexts"},{"location":"models/extras/gitrepository.html#implicit-config-contexts","text":"Implicit config context files will have the following folder/file structure /config_contexts/<filter>/<slug>.[json|yaml] , in which case their path and filename will be taken as an implicit scope for the context. For example: config_contexts/ regions/ nyc.yaml # YAML data, with implicit scoping to the Region with slug \"nyc\" sites/ nyc-01.json # JSON data, with implicit scoping to the Site with slug \"nyc-01\" The implicit config contexts will be defined using dictionaries for both _metadata and any context data for the config context.","title":"Implicit Config Contexts"},{"location":"models/extras/gitrepository.html#json","text":"{ \"_metadata\" : { \"name\" : \"Region NYC servers\" , \"weight\" : 1000 , \"description\" : \"NTP and Syslog servers for region NYC\" , \"is_active\" : true , \"schema\" : \"Config Context Schema 1\" }, \"ntp-servers\" : [ \"172.16.10.22\" , \"172.16.10.33\" ], \"syslog-servers\" : [ \"172.16.9.100\" , \"172.16.9.101\" ] }","title":"JSON"},{"location":"models/extras/gitrepository.html#yaml","text":"_metadata\" : name : \"Region NYC servers\" weight : 1000 description : \"NTP and Syslog servers for region NYC\" is_active : true schema : \"Config Context Schema 1\" ntp-servers : - 172.16.10.22 - 172.16.10.33 syslog-servers : - 172.16.9.100 - 172.16.9.101 This will create a config context with two keys: ntp-servers and syslog-servers .","title":"YAML"},{"location":"models/extras/gitrepository.html#local-configuration-contexts","text":"Files in a config_contexts/devices/ and/or config_contexts/virtual_machines/ directory will be used to populate \"local\" config context data for individual devices or virtual machines. For these files, the device/VM name will always be taken from the filename, and the data in the file will be used precisely as-is (there is no need, or support, for a _metadata key in these files). config_contexts/ devices/ rtr-01.yaml # YAML data, local to the Device named \"rtr-01\" virtual_machines/ vm001.json # JSON data, local to the VirtualMachine named \"vm001\" Note While virtual machines are always uniquely identified by their name, it is possible for devices associated with different sites and/or tenants to share an identical name. Currently, Nautobot is unable to automatically apply local config context via Git to devices that have a non-globally-unique name (or no name at all).","title":"Local Configuration Contexts"},{"location":"models/extras/gitrepository.html#configuration-context-schemas","text":"Config context schemas may be provided as JSON or YAML files located in /config_context_schemas/ . Files in the root of the /config_context_schemas/ directory will be imported as described below, with no special meaning attributed to their filenames (the name of the constructed config context schema will be taken from the _metadata within the file, not the filename). Similar to config context definitions, a single file may define a single config context schema or a list of such schemas - see examples below. config_context_schemas/ context_schema_1.json context_schema_2.yaml When loading the schema, the key _metadata will be extracted from the loaded data and used to define the config context schema's metadata, while the actual config context data schema will be based on the key data_schema . JSON single example: { \"_metadata\" : { \"name\" : \"Config Context Schema 1\" , \"description\" : \"Schema for defining first names.\" }, \"data_schema\" : { \"title\" : \"Person\" , \"properties\" : { \"firstName\" : { \"type\" : \"string\" , \"description\" : \"The person's first name.\" } } } } JSON list example: [ { \"_metadata\" : { \"name\" : \"Config Context Schema 1\" , \"description\" : \"Schema for defining first names.\" }, \"data_schema\" : { \"title\" : \"Person\" , \"properties\" : { \"firstName\" : { \"type\" : \"string\" , \"description\" : \"The person's first name.\" }, } } }, { \"_metadata\" : { \"name\" : \"Config Context Schema 2\" , \"description\" : \"Schema for defining last names.\" }, \"data_schema\" : { \"title\" : \"Person\" , \"properties\" : { \"lastName\" : { \"type\" : \"string\" , \"description\" : \"The person's last name.\" }, } } }, ] YAML single example: --- _metadata : name : \"Config Context Schema 1\" description : \"Schema for defining first names.\" data_schema : title : \"Person\" properties : firstName : type : \"string\" description : \"The person's first name\" YAML list example: --- - _metadata : name : \"Config Context Schema 1\" description : \"Schema for defining first names.\" data_schema : title : \"Person\" properties : firstName : type : \"string\" description : \"The person's first name\" - _metadata : name : \"Config Context Schema 2\" description : \"Schema for defining last names.\" data_schema : title : \"Person\" properties : lastName : type : \"string\" description : \"The person's last name\"","title":"Configuration Context Schemas"},{"location":"models/extras/gitrepository.html#export-templates","text":"Export templates may be provided as files located in /export_templates/<grouping>/<model>/<template_file> ; for example, a JSON export template for Device records might be /export_templates/dcim/device/mytemplate.json . The name of a discovered export template will be presented in Nautobot as <repository name>: <filename> . The MIME type of a file rendered from a discovered export template will try to match the extension to IANA's list . If not detected, it will default to text/plain . The file extension of a file rendered from a discovered export template will match that of the template itself (so, in the above example, the extension would be .json )","title":"Export Templates"},{"location":"models/extras/graphqlquery.html","text":"GraphQL Queries \u00b6 Nautobot provides the ability to store GraphQL queries in the database for simple maintaining and re-running. Saved Query Views \u00b6 Navigate to Extensibility > Data Management > GraphQL Queries under the navigation bar. Located here are the views to manage saved query objects in the database. When queries get saved to the database from the form, the query is first loaded into GraphQL to ensure that syntax is correct. If there is an issue with the query, an error message is displayed below the textarea. GraphiQL Interface \u00b6 Modifications have been made to the GraphiQL page to allow the running, editing and saving of this model. A dropdown button called \"Queries\" has been added to the GraphiQL toolbar. This lists all saved queries in the database allowing the user to open them into GraphiQL. If a saved query has been opened, a button will appear next to the name inside the \"Queries\" dropdown called \"Save Changes\". This allows the user to save any changes to the model object. If the user wants to create a new query, at the bottom of the \"Queries\" tab there is an option called \"Save Current Query As...\". This will open a modal form to input data, such as the name of the query, and then save the query to the database. API Endpoint \u00b6 An API endpoint has be created to allow running of saved queries through a simple POST request. Request: POST URL: {server_address}/api/extras/graphql-queries/{slug}/run/ Content-type: application/json Body: JSON of query variables {\"variable_1\": \"value_1\", \"variable_2\": \"value_2\"}","title":"GraphQL Queries"},{"location":"models/extras/graphqlquery.html#graphql-queries","text":"Nautobot provides the ability to store GraphQL queries in the database for simple maintaining and re-running.","title":"GraphQL Queries"},{"location":"models/extras/graphqlquery.html#saved-query-views","text":"Navigate to Extensibility > Data Management > GraphQL Queries under the navigation bar. Located here are the views to manage saved query objects in the database. When queries get saved to the database from the form, the query is first loaded into GraphQL to ensure that syntax is correct. If there is an issue with the query, an error message is displayed below the textarea.","title":"Saved Query Views"},{"location":"models/extras/graphqlquery.html#graphiql-interface","text":"Modifications have been made to the GraphiQL page to allow the running, editing and saving of this model. A dropdown button called \"Queries\" has been added to the GraphiQL toolbar. This lists all saved queries in the database allowing the user to open them into GraphiQL. If a saved query has been opened, a button will appear next to the name inside the \"Queries\" dropdown called \"Save Changes\". This allows the user to save any changes to the model object. If the user wants to create a new query, at the bottom of the \"Queries\" tab there is an option called \"Save Current Query As...\". This will open a modal form to input data, such as the name of the query, and then save the query to the database.","title":"GraphiQL Interface"},{"location":"models/extras/graphqlquery.html#api-endpoint","text":"An API endpoint has be created to allow running of saved queries through a simple POST request. Request: POST URL: {server_address}/api/extras/graphql-queries/{slug}/run/ Content-type: application/json Body: JSON of query variables {\"variable_1\": \"value_1\", \"variable_2\": \"value_2\"}","title":"API Endpoint"},{"location":"models/extras/imageattachment.html","text":"Image Attachments \u00b6 Certain objects in Nautobot support the attachment of uploaded images. These will be saved to the Nautobot server and made available whenever the object is viewed. The location of where image attachments are stored can be customized using the MEDIA_ROOT setting in your nautobot_config.py . Currently, the following types of image attachments can be stored in Nautobot: Device type images are stored at $MEDIA_ROOT/devicetype-images Generic image attachments are stored at $MEDIA_ROOT/image-attachments","title":"Image Attachments"},{"location":"models/extras/imageattachment.html#image-attachments","text":"Certain objects in Nautobot support the attachment of uploaded images. These will be saved to the Nautobot server and made available whenever the object is viewed. The location of where image attachments are stored can be customized using the MEDIA_ROOT setting in your nautobot_config.py . Currently, the following types of image attachments can be stored in Nautobot: Device type images are stored at $MEDIA_ROOT/devicetype-images Generic image attachments are stored at $MEDIA_ROOT/image-attachments","title":"Image Attachments"},{"location":"models/extras/job.html","text":"Jobs \u00b6 Added in version 1.3.0 The Job data model provides a database representation of metadata describing a specific installed Job. It also serves as an anchor point for other data models (JobResult and ScheduledJob in particular) to link against. For any given Job record, most of its fields are populated initially from data defined in the source code of the corresponding job class. These fields may be explicitly overridden by editing the Job record via the Nautobot UI or REST API if desired. This is generally accomplished by setting a value for the desired field (e.g. grouping ) and also setting the corresponding override flag (e.g. grouping_override ) to True . If the override flag for a field is cleared (set back to False ) then the corresponding flag will automatically revert to the original value defined by the Job class source code when the record is saved. Note For metadata fields that are not explicitly overridden, changes in the job source code will be detected and reflected in the corresponding database records when nautobot-server migrate or nautobot-server post_upgrade is next run; changes are not detected \"live\" while the server is running. For jobs stored in Git repositories, re-syncing the Git repository will also refresh the Job records corresponding to this repository. Records of this type store the following data as read-only (not modifiable via the UI or REST API): The source of the job (local installation, Git repository, plugin) The name of the module containing the Job The name of the Job class Whether the job is installed presently Whether the job is a Job Hook Receiver Note As presently implemented, after a job is uninstalled, when the database is next refreshed, the corresponding Job database record will not be deleted - only its installed flag will be set to False. This allows existing JobResult and ScheduledJob records to continue to reference the Job that they originated from. An administrator or sufficiently privileged user can manually delete uninstalled Job records if desired, though this will result in the foreign-key from the corresponding JobResult and ScheduledJob records (if any exist) becoming null. In any case, for tracking and auditing purposes, deleting a Job does not automatically delete its related JobResult and ScheduledJob records.","title":"Jobs"},{"location":"models/extras/job.html#jobs","text":"Added in version 1.3.0 The Job data model provides a database representation of metadata describing a specific installed Job. It also serves as an anchor point for other data models (JobResult and ScheduledJob in particular) to link against. For any given Job record, most of its fields are populated initially from data defined in the source code of the corresponding job class. These fields may be explicitly overridden by editing the Job record via the Nautobot UI or REST API if desired. This is generally accomplished by setting a value for the desired field (e.g. grouping ) and also setting the corresponding override flag (e.g. grouping_override ) to True . If the override flag for a field is cleared (set back to False ) then the corresponding flag will automatically revert to the original value defined by the Job class source code when the record is saved. Note For metadata fields that are not explicitly overridden, changes in the job source code will be detected and reflected in the corresponding database records when nautobot-server migrate or nautobot-server post_upgrade is next run; changes are not detected \"live\" while the server is running. For jobs stored in Git repositories, re-syncing the Git repository will also refresh the Job records corresponding to this repository. Records of this type store the following data as read-only (not modifiable via the UI or REST API): The source of the job (local installation, Git repository, plugin) The name of the module containing the Job The name of the Job class Whether the job is installed presently Whether the job is a Job Hook Receiver Note As presently implemented, after a job is uninstalled, when the database is next refreshed, the corresponding Job database record will not be deleted - only its installed flag will be set to False. This allows existing JobResult and ScheduledJob records to continue to reference the Job that they originated from. An administrator or sufficiently privileged user can manually delete uninstalled Job records if desired, though this will result in the foreign-key from the corresponding JobResult and ScheduledJob records (if any exist) becoming null. In any case, for tracking and auditing purposes, deleting a Job does not automatically delete its related JobResult and ScheduledJob records.","title":"Jobs"},{"location":"models/extras/jobhook.html","text":"Job Hooks \u00b6 Added in version 1.4.0 A Job Hook is a mechanism for automatically starting a job when an object is changed. Job Hooks are similar to webhooks except that an object change event initiates a JobHookReceiver job instead of a web request. Job hooks are configured in the web UI under Jobs > Job Hooks . Configuration \u00b6 Name - A unique name for the job hook. Content type(s) - The type or types of Nautobot object that will trigger the job hook. Job - The job hook receiver that this job hook will run. Enabled - If unchecked, the job hook will be inactive. Events - A job hook may trigger on any combination of create, update, and delete events. At least one event type must be selected. Job Hook Receivers \u00b6 Job Hooks are only able to initiate a specific type of job called a Job Hook Receiver . These are jobs that subclass the nautobot.extras.jobs.JobHookReceiver class. Job hook receivers are similar to normal jobs except they are hard coded to accept only an object_change variable . Job Hook Receivers are hidden from the jobs listing UI by default but otherwise function similarly to other jobs. The JobHookReceiver class only implements one method called receive_job_hook . Important To prevent negatively impacting system performance through an infinite loop, a change that was made by a JobHookReceiver job will not trigger another JobHookReceiver job to run. Example Job Hook Receiver \u00b6 from nautobot.extras.choices import ObjectChangeActionChoices from nautobot.extras.jobs import JobHookReceiver class ExampleJobHookReceiver ( JobHookReceiver ): def receive_job_hook ( self , change , action , changed_object ): # return on delete action if action == ObjectChangeActionChoices . ACTION_DELETE : return # log diff output snapshots = change . get_snapshots () self . log_info ( f \"DIFF: { snapshots [ 'differences' ] } \" ) # validate changes to serial field if \"serial\" in snapshots [ \"differences\" ][ \"added\" ]: old_serial = snapshots [ \"differences\" ][ \"removed\" ][ \"serial\" ] new_serial = snapshots [ \"differences\" ][ \"added\" ][ \"serial\" ] self . log_info ( f \" { changed_object } serial has been changed from { old_serial } to { new_serial } \" ) # Check the new serial is valid and revert if necessary if not self . validate_serial ( new_serial ): changed_object . serial = old_serial changed_object . save () self . log_info ( f \" { changed_object } serial { new_serial } was not valid. Reverted to { old_serial } \" ) self . log_success ( message = f \"Serial validation completed for { changed_object } \" ) def validate_serial ( self , serial ): # add business logic to validate serial return False The receive_job_hook() Method \u00b6 All JobHookReceiver subclasses must implement a receive_job_hook() method. This method accepts three arguments: change - An instance of nautobot.extras.models.ObjectChange action - A string with the action performed on the changed object (\"create\", \"update\" or \"delete\") changed_object - An instance of the object that was changed, or None if the object has been deleted","title":"Job Hooks"},{"location":"models/extras/jobhook.html#job-hooks","text":"Added in version 1.4.0 A Job Hook is a mechanism for automatically starting a job when an object is changed. Job Hooks are similar to webhooks except that an object change event initiates a JobHookReceiver job instead of a web request. Job hooks are configured in the web UI under Jobs > Job Hooks .","title":"Job Hooks"},{"location":"models/extras/jobhook.html#configuration","text":"Name - A unique name for the job hook. Content type(s) - The type or types of Nautobot object that will trigger the job hook. Job - The job hook receiver that this job hook will run. Enabled - If unchecked, the job hook will be inactive. Events - A job hook may trigger on any combination of create, update, and delete events. At least one event type must be selected.","title":"Configuration"},{"location":"models/extras/jobhook.html#job-hook-receivers","text":"Job Hooks are only able to initiate a specific type of job called a Job Hook Receiver . These are jobs that subclass the nautobot.extras.jobs.JobHookReceiver class. Job hook receivers are similar to normal jobs except they are hard coded to accept only an object_change variable . Job Hook Receivers are hidden from the jobs listing UI by default but otherwise function similarly to other jobs. The JobHookReceiver class only implements one method called receive_job_hook . Important To prevent negatively impacting system performance through an infinite loop, a change that was made by a JobHookReceiver job will not trigger another JobHookReceiver job to run.","title":"Job Hook Receivers"},{"location":"models/extras/jobhook.html#example-job-hook-receiver","text":"from nautobot.extras.choices import ObjectChangeActionChoices from nautobot.extras.jobs import JobHookReceiver class ExampleJobHookReceiver ( JobHookReceiver ): def receive_job_hook ( self , change , action , changed_object ): # return on delete action if action == ObjectChangeActionChoices . ACTION_DELETE : return # log diff output snapshots = change . get_snapshots () self . log_info ( f \"DIFF: { snapshots [ 'differences' ] } \" ) # validate changes to serial field if \"serial\" in snapshots [ \"differences\" ][ \"added\" ]: old_serial = snapshots [ \"differences\" ][ \"removed\" ][ \"serial\" ] new_serial = snapshots [ \"differences\" ][ \"added\" ][ \"serial\" ] self . log_info ( f \" { changed_object } serial has been changed from { old_serial } to { new_serial } \" ) # Check the new serial is valid and revert if necessary if not self . validate_serial ( new_serial ): changed_object . serial = old_serial changed_object . save () self . log_info ( f \" { changed_object } serial { new_serial } was not valid. Reverted to { old_serial } \" ) self . log_success ( message = f \"Serial validation completed for { changed_object } \" ) def validate_serial ( self , serial ): # add business logic to validate serial return False","title":"Example Job Hook Receiver"},{"location":"models/extras/jobhook.html#the-receive_job_hook-method","text":"All JobHookReceiver subclasses must implement a receive_job_hook() method. This method accepts three arguments: change - An instance of nautobot.extras.models.ObjectChange action - A string with the action performed on the changed object (\"create\", \"update\" or \"delete\") changed_object - An instance of the object that was changed, or None if the object has been deleted","title":"The receive_job_hook() Method"},{"location":"models/extras/joblogentry.html","text":"Job Log Entry \u00b6 Added in version 1.2.0 Log messages from Jobs are stored in as JobLogEntry objects. This allows more performant querying of log messages and even allows viewing of logs while the job is still running. Records of this type store the following data: A reference to the JobResult object that created the log. Timestamps indicating when the log message was created. The logging level of the log message. The log message. If provided, the string format of the logged object and it's absolute url. Added in version 1.2.2 REST API and GraphQL support for querying JobLogEntry records were added.","title":"Job Log Entry"},{"location":"models/extras/joblogentry.html#job-log-entry","text":"Added in version 1.2.0 Log messages from Jobs are stored in as JobLogEntry objects. This allows more performant querying of log messages and even allows viewing of logs while the job is still running. Records of this type store the following data: A reference to the JobResult object that created the log. Timestamps indicating when the log message was created. The logging level of the log message. The log message. If provided, the string format of the logged object and it's absolute url. Added in version 1.2.2 REST API and GraphQL support for querying JobLogEntry records were added.","title":"Job Log Entry"},{"location":"models/extras/jobresult.html","text":"Job Results \u00b6 Nautobot provides a generic data model for storing and reporting the results of background tasks, such as the execution of custom jobs or the synchronization of data from a Git repository. Records of this type store the following data: A reference to the type and name of the object or feature that the task was associated with A reference to the user who initiated the task The arguments that were passed to the task (allowing for later queuing of the task for re-execution if desired) Timestamps indicating when the task was created and when it completed An overall status such as \"pending\", \"running\", \"errored\", or \"completed\". A block of structured data (often rendered as JSON); Any return values from the .run() and any test methods go to the key output . In addition any Job or plugin using the JobResult model can store arbitrary structured data here if needed. (Note that prior to Nautobot 1.2, job log records were stored in this field; they are now stored as distinct JobLogEntry records instead.)","title":"Job Results"},{"location":"models/extras/jobresult.html#job-results","text":"Nautobot provides a generic data model for storing and reporting the results of background tasks, such as the execution of custom jobs or the synchronization of data from a Git repository. Records of this type store the following data: A reference to the type and name of the object or feature that the task was associated with A reference to the user who initiated the task The arguments that were passed to the task (allowing for later queuing of the task for re-execution if desired) Timestamps indicating when the task was created and when it completed An overall status such as \"pending\", \"running\", \"errored\", or \"completed\". A block of structured data (often rendered as JSON); Any return values from the .run() and any test methods go to the key output . In addition any Job or plugin using the JobResult model can store arbitrary structured data here if needed. (Note that prior to Nautobot 1.2, job log records were stored in this field; they are now stored as distinct JobLogEntry records instead.)","title":"Job Results"},{"location":"models/extras/note.html","text":"Note \u00b6 Added in version 1.4.0 Notes provide a place for you to store notes or general information on an object, such as a Device, that may not require a specific field for. This could be a note on a recent upgrade, a warning about a problematic device, or the reason the Rack was marked with the Status Retired . The note field supports Markdown Basic Syntax .","title":"Notes"},{"location":"models/extras/note.html#note","text":"Added in version 1.4.0 Notes provide a place for you to store notes or general information on an object, such as a Device, that may not require a specific field for. This could be a note on a recent upgrade, a warning about a problematic device, or the reason the Rack was marked with the Status Retired . The note field supports Markdown Basic Syntax .","title":"Note"},{"location":"models/extras/relationship.html","text":"Relationships \u00b6 Sometimes it is desirable to create a new kind of relationship between one (or more) objects in your source of truth to reflect business logic or other relationships that may be useful to you but that haven't been defined. This is where the Relationships feature comes in: like defining custom fields to hold attributes specific to your use cases, relationships define specific links between objects that might be specific to your network or data. To create a relationship, from the top-level navigation menu select Extensibility > Data Management > Relationships Tip Because relationship information can be included in the REST API and in GraphQL, we strongly recommend that when defining a relationship, you provide a slug that contains underscores rather than dashes ( my_relationship_slug , not my-relationship-slug ), as some features may not work optimally if dashes are included in the slug. Relationship Types \u00b6 Many-to-many - where both sides of the relationship connection can be connected to multiple objects. For example, VLANs can be connected to multiple devices and devices will have multiple VLANs. One-to-many - where one side of the connection can only have one object. For example, where a controller has many supplicants like FEX and parent switch. A FEX can be uplinked to one parent switch (in most cases), but the parent switch can have many FEX. One-to-one - where there can be only one object on either side of the relationship. For example, an IP address serving as a router-id for a device. Each device has at most one router-id, and each IP address can be a router-id for at most one device. Added in version 1.2.0 Additionally, there are two symmetric relationship types that can be used when defining a relationship between objects of the same type. These relationship types treat the two sides of a relationship as interchangeable (much like the A/Z sides of a circuit, or the endpoints of a cable) rather than distinguishing between the source and destination of a relationship as the non-symmetric relationship types above do. Symmetric Many-to-many - as in Many-to-many, but acting more as an undirected graph of similar objects. For example, this could be used to define a set of devices participating in a routing topology, where each device has some number of peers and there's no distinction between source and destination peers. Symmetric One-to-one - as in One-to-one, but defining a relationship between exactly two objects of the same type. For example, a HSRP/VRRP pair of redundant devices, where each device has exactly one peer device. Note A symmetric many-to-many relationship can be, but is not necessarily, a complete graph or full mesh . For example, in the routing topology example above, if Device A and Device B are peers, and Device B and Device C are peers, this does not automatically imply a relationship between Devices A and C -- they might or might not also be peers, depending on how you define and populate the specific associations for this relationship. Relationship Filters \u00b6 Filters can be defined to restrict the type or selection of objects for either side of the connection. An important note is that the filters have to be defined in FilterSet rather than QuerySet format. In practice this means that you can use any of the filters that are valid in the REST API for a given object type, but cannot necessarily use complex nested attribute lookups (such as interfaces__ip_addresses__prefix_length on a Device, for example). As an example, let's create a relationship between Circuits and Devices. In our situation we only would terminate Circuits on Devices with the Device Role of edge . To prevent the Circuit Relationship from showing up on any other Device, use a JSON filter to limit the Relationship to only Devices with Device Role whose slug is edge : { \"role\" : [ \"edge\" ] } Note There are a few ways to tell what attributes are available to filter on for a given object. In the case of the Device object used in the example, the user could: look at the code nautobot/dcim/filters.py -> DeviceFilterSet class (available options there include manufacturer_id , manufacturer , etc) check the filter options available in the REST API: https://<server-name>/api/docs , and in this case checking the dcim_devices_list API endpoint for the parameter names For context, here is an image of the entire Relationship: Now, the Circuit Relationship field will show up on a Device with an edge role: The Circuit Relationship field will not show up on a Device with a role leaf : Relationship Labels \u00b6 Relationship connections can be labeled with a friendly name so that when they are displayed in the GUI, they will have a more descriptive or friendly name. From the Devices/Circuits example above, you might label the relationship so that on the Device side the connection appears as 'Terminated Circuits' and on the Circuit side the connection appears as 'Terminating Devices'. Options \u00b6 It's also possible to hide the relationship from either side of the connection. Creating New Relationships \u00b6 Relationships can be added through the UI under Extensibility > Relationships Each relationship must have a Name, Slug, Type, Source Object(s), and Destination Object(s). Optionally, Source Labels, Source Filters, Destination Labels, and Destination Filters may be configured. Once a new relationship is added, the Relationship configuration section will appear under that device in the UI edit screen. Once a specific instance relationship has been configured for the object, that new relationship will appear under the Relationship section heading when viewing the object. When creating a relationship, if \"Move to Advanced tab\" is checked, this relationship won't appear on the object's main detail tab in the UI, but will appear in the \"Advanced\" tab. This is useful when the requirement is to hide this relationship from the main detail tab when, for instance, it is only required for machine-to-machine communication and not user consumption. REST API \u00b6 Relationships are fully supported by the API. Adding a new type of Relationship \u00b6 The API endpoint for relationship creation is /extras/relationships/ From our many to many example above, we would use the following data to create the relationship. { \"name\" : \"Device VLANs\" , \"slug\" : \"device-vlans\" , \"type\" : \"many-to-many\" , \"source_type\" : \"ipam.vlan\" , \"destination_type\" : \"dcim.device\" } Configuring the Relationship between Objects \u00b6 Via Object Endpoints \u00b6 Added in version 1.4.0 To get object relationships and associations from the REST API, you can query any object endpoint with the ?include=relationships query parameter included, for example GET /api/dcim/devices/f472bb77-7f56-4e79-ac25-2dc73eb63924/?include=relationships . The API response will include a nested dictionary of relationships and associations applicable to the object(s) retrieved. Similarly, you can update the relationship associations for a given object via an HTTP POST or PATCH request, generally by including the nested key [\"relationships\"][<relationship-slug>][\"source\"|\"destination\"|\"peer\"][\"objects\"] with a list of objects to associate. For more details on this feature, refer to the REST API documentation . Via Relationship-Associations Endpoint \u00b6 Alternatively, relationship associations may be configured by sending a request to /extras/relationship-associations/ like the following: Here we specify the IDs of each object. We specify the UUID of each object in their respective fields. { \"relationship\" : \"bff38197-26ed-4bbd-b637-3e688acf361c\" , \"source_type\" : \"ipam.vlan\" , \"source_id\" : \"89588629-2d70-45ce-9e20-f6b159b41b0c\" , \"destination_type\" : \"dcim.device\" , \"destination_id\" : \"6e8e72da-ce6e-468d-90f9-b4473d449db7\" } In the relationship field, you may specify a dictionary of object attributes instead: { \"relationship\" : { \"slug\" : \"device-vlans\" }, \"source_type\" : \"ipam.vlan\" , \"source_id\" : \"89588629-2d70-45ce-9e20-f6b159b41b0c\" , \"destination_type\" : \"dcim.device\" , \"destination_id\" : \"6e8e72da-ce6e-468d-90f9-b4473d449db7\" }","title":"Relationships"},{"location":"models/extras/relationship.html#relationships","text":"Sometimes it is desirable to create a new kind of relationship between one (or more) objects in your source of truth to reflect business logic or other relationships that may be useful to you but that haven't been defined. This is where the Relationships feature comes in: like defining custom fields to hold attributes specific to your use cases, relationships define specific links between objects that might be specific to your network or data. To create a relationship, from the top-level navigation menu select Extensibility > Data Management > Relationships Tip Because relationship information can be included in the REST API and in GraphQL, we strongly recommend that when defining a relationship, you provide a slug that contains underscores rather than dashes ( my_relationship_slug , not my-relationship-slug ), as some features may not work optimally if dashes are included in the slug.","title":"Relationships"},{"location":"models/extras/relationship.html#relationship-types","text":"Many-to-many - where both sides of the relationship connection can be connected to multiple objects. For example, VLANs can be connected to multiple devices and devices will have multiple VLANs. One-to-many - where one side of the connection can only have one object. For example, where a controller has many supplicants like FEX and parent switch. A FEX can be uplinked to one parent switch (in most cases), but the parent switch can have many FEX. One-to-one - where there can be only one object on either side of the relationship. For example, an IP address serving as a router-id for a device. Each device has at most one router-id, and each IP address can be a router-id for at most one device. Added in version 1.2.0 Additionally, there are two symmetric relationship types that can be used when defining a relationship between objects of the same type. These relationship types treat the two sides of a relationship as interchangeable (much like the A/Z sides of a circuit, or the endpoints of a cable) rather than distinguishing between the source and destination of a relationship as the non-symmetric relationship types above do. Symmetric Many-to-many - as in Many-to-many, but acting more as an undirected graph of similar objects. For example, this could be used to define a set of devices participating in a routing topology, where each device has some number of peers and there's no distinction between source and destination peers. Symmetric One-to-one - as in One-to-one, but defining a relationship between exactly two objects of the same type. For example, a HSRP/VRRP pair of redundant devices, where each device has exactly one peer device. Note A symmetric many-to-many relationship can be, but is not necessarily, a complete graph or full mesh . For example, in the routing topology example above, if Device A and Device B are peers, and Device B and Device C are peers, this does not automatically imply a relationship between Devices A and C -- they might or might not also be peers, depending on how you define and populate the specific associations for this relationship.","title":"Relationship Types"},{"location":"models/extras/relationship.html#relationship-filters","text":"Filters can be defined to restrict the type or selection of objects for either side of the connection. An important note is that the filters have to be defined in FilterSet rather than QuerySet format. In practice this means that you can use any of the filters that are valid in the REST API for a given object type, but cannot necessarily use complex nested attribute lookups (such as interfaces__ip_addresses__prefix_length on a Device, for example). As an example, let's create a relationship between Circuits and Devices. In our situation we only would terminate Circuits on Devices with the Device Role of edge . To prevent the Circuit Relationship from showing up on any other Device, use a JSON filter to limit the Relationship to only Devices with Device Role whose slug is edge : { \"role\" : [ \"edge\" ] } Note There are a few ways to tell what attributes are available to filter on for a given object. In the case of the Device object used in the example, the user could: look at the code nautobot/dcim/filters.py -> DeviceFilterSet class (available options there include manufacturer_id , manufacturer , etc) check the filter options available in the REST API: https://<server-name>/api/docs , and in this case checking the dcim_devices_list API endpoint for the parameter names For context, here is an image of the entire Relationship: Now, the Circuit Relationship field will show up on a Device with an edge role: The Circuit Relationship field will not show up on a Device with a role leaf :","title":"Relationship Filters"},{"location":"models/extras/relationship.html#relationship-labels","text":"Relationship connections can be labeled with a friendly name so that when they are displayed in the GUI, they will have a more descriptive or friendly name. From the Devices/Circuits example above, you might label the relationship so that on the Device side the connection appears as 'Terminated Circuits' and on the Circuit side the connection appears as 'Terminating Devices'.","title":"Relationship Labels"},{"location":"models/extras/relationship.html#options","text":"It's also possible to hide the relationship from either side of the connection.","title":"Options"},{"location":"models/extras/relationship.html#creating-new-relationships","text":"Relationships can be added through the UI under Extensibility > Relationships Each relationship must have a Name, Slug, Type, Source Object(s), and Destination Object(s). Optionally, Source Labels, Source Filters, Destination Labels, and Destination Filters may be configured. Once a new relationship is added, the Relationship configuration section will appear under that device in the UI edit screen. Once a specific instance relationship has been configured for the object, that new relationship will appear under the Relationship section heading when viewing the object. When creating a relationship, if \"Move to Advanced tab\" is checked, this relationship won't appear on the object's main detail tab in the UI, but will appear in the \"Advanced\" tab. This is useful when the requirement is to hide this relationship from the main detail tab when, for instance, it is only required for machine-to-machine communication and not user consumption.","title":"Creating New Relationships"},{"location":"models/extras/relationship.html#rest-api","text":"Relationships are fully supported by the API.","title":"REST API"},{"location":"models/extras/relationship.html#adding-a-new-type-of-relationship","text":"The API endpoint for relationship creation is /extras/relationships/ From our many to many example above, we would use the following data to create the relationship. { \"name\" : \"Device VLANs\" , \"slug\" : \"device-vlans\" , \"type\" : \"many-to-many\" , \"source_type\" : \"ipam.vlan\" , \"destination_type\" : \"dcim.device\" }","title":"Adding a new type of Relationship"},{"location":"models/extras/relationship.html#configuring-the-relationship-between-objects","text":"","title":"Configuring the Relationship between Objects"},{"location":"models/extras/relationship.html#via-object-endpoints","text":"Added in version 1.4.0 To get object relationships and associations from the REST API, you can query any object endpoint with the ?include=relationships query parameter included, for example GET /api/dcim/devices/f472bb77-7f56-4e79-ac25-2dc73eb63924/?include=relationships . The API response will include a nested dictionary of relationships and associations applicable to the object(s) retrieved. Similarly, you can update the relationship associations for a given object via an HTTP POST or PATCH request, generally by including the nested key [\"relationships\"][<relationship-slug>][\"source\"|\"destination\"|\"peer\"][\"objects\"] with a list of objects to associate. For more details on this feature, refer to the REST API documentation .","title":"Via Object Endpoints"},{"location":"models/extras/relationship.html#via-relationship-associations-endpoint","text":"Alternatively, relationship associations may be configured by sending a request to /extras/relationship-associations/ like the following: Here we specify the IDs of each object. We specify the UUID of each object in their respective fields. { \"relationship\" : \"bff38197-26ed-4bbd-b637-3e688acf361c\" , \"source_type\" : \"ipam.vlan\" , \"source_id\" : \"89588629-2d70-45ce-9e20-f6b159b41b0c\" , \"destination_type\" : \"dcim.device\" , \"destination_id\" : \"6e8e72da-ce6e-468d-90f9-b4473d449db7\" } In the relationship field, you may specify a dictionary of object attributes instead: { \"relationship\" : { \"slug\" : \"device-vlans\" }, \"source_type\" : \"ipam.vlan\" , \"source_id\" : \"89588629-2d70-45ce-9e20-f6b159b41b0c\" , \"destination_type\" : \"dcim.device\" , \"destination_id\" : \"6e8e72da-ce6e-468d-90f9-b4473d449db7\" }","title":"Via Relationship-Associations Endpoint"},{"location":"models/extras/secret.html","text":"Secrets \u00b6 Added in version 1.2.0 For security reasons, Nautobot generally does not store sensitive secrets (device access credentials, systems-integration API tokens, etc.) in its own database. There are other approaches and systems better suited to this purpose, ranging from simple solutions such as process-specific environment variables or restricted-access files on disk, all the way through to dedicated systems such as Hashicorp Vault or AWS Secrets Manager. However, any number of Nautobot features (including, but not limited to, device access via NAPALM, Git repository access, custom Jobs, and various plugins seeking to integrate with third-party systems) do need the ability to retrieve and make use of such secrets. Towards that end, Nautobot provides a Secret database model. This model does not store the secret value itself, but instead defines how Nautobot can retrieve the secret value as and when it is needed. By using this model as an abstraction of the underlying secrets storage implementation, this makes it possible for any Nautobot feature to make use of secret values without needing to know or care where or how the secret is actually stored. Secrets can be grouped and assigned a specific purpose as members of a Secrets Group, which can then be attached to a Git repository, device, or other data model as needed for a given purpose. Secrets Providers \u00b6 Each Secret is associated with a secrets provider (not to be confused with a circuit provider), which provides the functionality needed to retrieve a specific value from a particular source of secrets. Each secrets provider also defines the set of parameters that a given Secret must specify in order to retrieve a secret value from this provider. Nautobot includes the following built-in secrets providers: Environment Variable - for retrieving a secret value defined in an environment variable; Secrets using this provider must specify the variable name to retrieve. Text File - for retrieving a secret value stored in a text file; Secrets using this provider must specify the absolute path of the file to retrieve. Changed in version 1.4.3 When using the Text File secrets provider, any leading and trailing whitespace or newlines will be stripped. When defining a new Secret, you will need to select the desired secrets provider and then fill in the specific parameters required by that provider in order to have a completely specified, usable Secret record. Tip Nautobot plugins can also implement and register additional secrets providers as desired to support other sources such as Hashicorp Vault or AWS Secrets Manager. Templated Secret Parameters \u00b6 In some cases you may have a collection of closely related secrets values that all follow a similar retrieval pattern. For example you might have a directory of text files each containing the unique password for a specific device, or have defined a set of environment variables providing authentication tokens for each different Git repository. In this case, to reduce the need for repeated data entry, Nautobot provides an option to use Jinja2 templates to dynamically alter the provider parameters of a given Secret based on the requesting object. The relevant object is passed to Jinja2 as obj . Thus, for example: A \"Device Password\" secret could use the Text File provider and specify the file path as \"/opt/nautobot/device_passwords/{{ obj.site.slug }}/{{ obj.name }}.txt\" , so that a device csr1 at site nyc would be able to retrieve its password value from /opt/nautobot/device_passwords/nyc/csr1.txt . A \"Git Token\" secret could use the Environment Variable provider and specify the variable name as \"GIT_TOKEN_{{ obj.slug | replace('-', '_') | upper }}\" , so that a Git repository golden-config would be able to retrieve its token value from $GIT_TOKEN_GOLDEN_CONFIG .","title":"Secrets"},{"location":"models/extras/secret.html#secrets","text":"Added in version 1.2.0 For security reasons, Nautobot generally does not store sensitive secrets (device access credentials, systems-integration API tokens, etc.) in its own database. There are other approaches and systems better suited to this purpose, ranging from simple solutions such as process-specific environment variables or restricted-access files on disk, all the way through to dedicated systems such as Hashicorp Vault or AWS Secrets Manager. However, any number of Nautobot features (including, but not limited to, device access via NAPALM, Git repository access, custom Jobs, and various plugins seeking to integrate with third-party systems) do need the ability to retrieve and make use of such secrets. Towards that end, Nautobot provides a Secret database model. This model does not store the secret value itself, but instead defines how Nautobot can retrieve the secret value as and when it is needed. By using this model as an abstraction of the underlying secrets storage implementation, this makes it possible for any Nautobot feature to make use of secret values without needing to know or care where or how the secret is actually stored. Secrets can be grouped and assigned a specific purpose as members of a Secrets Group, which can then be attached to a Git repository, device, or other data model as needed for a given purpose.","title":"Secrets"},{"location":"models/extras/secret.html#secrets-providers","text":"Each Secret is associated with a secrets provider (not to be confused with a circuit provider), which provides the functionality needed to retrieve a specific value from a particular source of secrets. Each secrets provider also defines the set of parameters that a given Secret must specify in order to retrieve a secret value from this provider. Nautobot includes the following built-in secrets providers: Environment Variable - for retrieving a secret value defined in an environment variable; Secrets using this provider must specify the variable name to retrieve. Text File - for retrieving a secret value stored in a text file; Secrets using this provider must specify the absolute path of the file to retrieve. Changed in version 1.4.3 When using the Text File secrets provider, any leading and trailing whitespace or newlines will be stripped. When defining a new Secret, you will need to select the desired secrets provider and then fill in the specific parameters required by that provider in order to have a completely specified, usable Secret record. Tip Nautobot plugins can also implement and register additional secrets providers as desired to support other sources such as Hashicorp Vault or AWS Secrets Manager.","title":"Secrets Providers"},{"location":"models/extras/secret.html#templated-secret-parameters","text":"In some cases you may have a collection of closely related secrets values that all follow a similar retrieval pattern. For example you might have a directory of text files each containing the unique password for a specific device, or have defined a set of environment variables providing authentication tokens for each different Git repository. In this case, to reduce the need for repeated data entry, Nautobot provides an option to use Jinja2 templates to dynamically alter the provider parameters of a given Secret based on the requesting object. The relevant object is passed to Jinja2 as obj . Thus, for example: A \"Device Password\" secret could use the Text File provider and specify the file path as \"/opt/nautobot/device_passwords/{{ obj.site.slug }}/{{ obj.name }}.txt\" , so that a device csr1 at site nyc would be able to retrieve its password value from /opt/nautobot/device_passwords/nyc/csr1.txt . A \"Git Token\" secret could use the Environment Variable provider and specify the variable name as \"GIT_TOKEN_{{ obj.slug | replace('-', '_') | upper }}\" , so that a Git repository golden-config would be able to retrieve its token value from $GIT_TOKEN_GOLDEN_CONFIG .","title":"Templated Secret Parameters"},{"location":"models/extras/secretsgroup.html","text":"Secrets Groups \u00b6 Added in version 1.2.0 A Secrets Group provides a way to collect and assign a purpose to one or more Secrets. The Secrets Group can then be attached to any object that needs to reference and make use of these Secrets, such as a Git repository needing a username/token to authenticate to a private GitHub repository, or a device using a group of Secrets to drive its NAPALM integration. When creating or editing a Secrets Group, you can assign any number of defined Secrets to this group, assigning each secret an access type and a secret type that are unique within the context of this group. Some examples of how a Secrets Group might be populated for use by a given feature: Feature Access Type Secrets Type(s) Git private repository HTTP(S) Token , possibly also Username Device NAPALM integration Generic Username , Password , possibly an enable Secret A Secrets Group is not limited to containing secrets of a single access type either - for example, a plugin that supports both NETCONF and gNMI protocols to interact with a device could be able to make use of a Secrets Group containing distinct secrets for each protocol.","title":"Secrets Groups"},{"location":"models/extras/secretsgroup.html#secrets-groups","text":"Added in version 1.2.0 A Secrets Group provides a way to collect and assign a purpose to one or more Secrets. The Secrets Group can then be attached to any object that needs to reference and make use of these Secrets, such as a Git repository needing a username/token to authenticate to a private GitHub repository, or a device using a group of Secrets to drive its NAPALM integration. When creating or editing a Secrets Group, you can assign any number of defined Secrets to this group, assigning each secret an access type and a secret type that are unique within the context of this group. Some examples of how a Secrets Group might be populated for use by a given feature: Feature Access Type Secrets Type(s) Git private repository HTTP(S) Token , possibly also Username Device NAPALM integration Generic Username , Password , possibly an enable Secret A Secrets Group is not limited to containing secrets of a single access type either - for example, a plugin that supports both NETCONF and gNMI protocols to interact with a device could be able to make use of a Secrets Group containing distinct secrets for each protocol.","title":"Secrets Groups"},{"location":"models/extras/status.html","text":"Statuses \u00b6 Nautobot provides the ability for custom statuses to be defined within an organization to be used on various objects to facilitate business workflows around object statuses. Status Basics \u00b6 The value of a status field on a model (such as Device.status ) will be represented as a nautobot.extras.models.Status object. When created, a Status can be associated to one or more model content-types using a many-to-many relationship. The relationship to each model is referenced across all user interfaces using the {app_label}.{model} naming convention (e.g. dcim.device ). Statuses may be managed by navigating to Organization > Statuses in the navigation menu. Importing Objects with a status Field \u00b6 When using CSV import to reference a status field on an object, the Status.slug field is used. For example, the default Active status has a slug of active , so the active value would be used for import. Customizing Statuses \u00b6 With Status as a model, statuses can be customized. This can be as simple as removing the option to configure an existing status with a particular model or to remove that status entirely. The real benefit of custom status is adding your own organization status and process names directly to Nautobot. An example of custom statuses would be including End of Life information for your devices. A simple End of Life status could be EOx for a device hitting any end of life milestone; more specific statuses like EOSS (End of Software Support), EOS (End of Sale), and Pre-EOS (for 1 year prior to EOS) to be more specific. Once the end of life information is tracked as a status, developing a report for Devices that have reached EOSS is trivial. Another example for sites is tracking the nature of a specific site's installation status. A site that is under construction could received a status like 'Pre Production'. For Virtual Machines, if utilizing OpenStack, statuses in Nautobot could be customized to reflect the specific Nova virtual machine states . Status Internals \u00b6 Warning The section below is largely intended for developers who may need to create data models of their own that implement a status field. Proceed at your own risk! Any model that is intended to have a status field must inherit from nautobot.extras.models.statuses.StatusModel . This abstract model will add an nautobot.extras.models.statuses.StatusField to the model. The abstract base will automatically assign a related_name for the reverse relationship back to the inheriting model's name (e.g. devices ). StatusField model field \u00b6 The StatusField field type subclass of a django.db.models.ForeignKey with extra extensions to have it behave like field with choices. Because this pattern is replacing hard-coded ChoiceSets (such as dcim.choices.DeviceStatusChoices ) with database objects, it is not possible to use the choices= argument on a foreign key. Because of this, StatusField implements a .contribute_to_class() method which will automatically bind .get_status_display() and .get_status_color() methods to any model that implements this field, so that these do not need to be manually defined on each model. This model field also emits its own form field to eliminate the requirement for a form field to be explicitly added to model forms. StatusFilter filter field \u00b6 Any filter that is intended to have a status field must inherit from nautobot.extras.filters.StatusModelFilterSetMixin . This will add a nautobot.extras.filters.StatusFilter to the filter, which allows filtering by the name of the status. Form fields \u00b6 Any model form that is intended to have a status field must inherit from one of three mixins, depending on the use-case: nautobot.extras.forms.StatusModelFilterFormMixin should be used to add a non-required, multiple-choice status filter field to UI filter forms. This multiple-choice field allows for multiple status values to be selected for filtering objects in list views in the web UI. nautobot.extras.forms.StatusModelBulkEditFormMixin should be used to add a non-required status form field to a an object's model form. This field constrains status choices eligible to the object type being edited. Changed in version 1.4.0 In prior Nautobot versions these mixins were named StatusFilterFormMixin and StatusBulkEditFormMixin ; the old names are still available as aliases but will be removed in a future major release. FIXME: CSV import forms StatusSerializerField serializer field \u00b6 Any serializer that is intended to have a status field must inherit from nautobot.extras.api.serializers.StatusModelSerializerMixin . This adds an nautobot.extras.api.fields.StatusSerializerField to the serializer. The StatusSerializerField is a writable slug-related choice field that allows writing to the field using the name value of the status (e.g. \"active\" ). Writing to this field is normalized to always be converted to lowercase. Table field \u00b6 If you wish for a table to include a status field, your table must inherit from nautobot.extras.tables.StatusTableMixin . This includes a ColorColumn on the table. Status object integrations \u00b6 To fully integrate a model to include a status field, assert the following: Model \u00b6 The model must inherit from nautobot.extras.models.statuses.StatusModel Decorate the model class with @extras_features('statuses') ( from nautobot.extras.utils import extras_features ) Forms \u00b6 Generic model forms will automatically include a StatusField CSV model import forms must inherit from nautobot.extras.forms.StatusModelCSVFormMixin Bulk edit model forms must inherit from nautobot.extras.forms.StatusModelBulkEditFormMixin Filter forms must inherit from nautobot.extras.forms.StatusModelFilterFormMixin Changed in version 1.4.0 In prior Nautobot releases the latter two mixins were named StatusBulkEditFormMixin and StatusFilterFormMixin respectively; the old names are still available as aliases but will be removed in a future major release. Filters \u00b6 Filtersets for your model must inherit from nautobot.extras.filters.StatusModelFilterSetMixin Serializers \u00b6 Serializers for your model must inherit from nautobot.extras.api.serializers.StatusModelSerializerMixin Tables \u00b6 The table class for your model must inherit from nautobot.extras.tables.StatusTableMixin","title":"Statuses"},{"location":"models/extras/status.html#statuses","text":"Nautobot provides the ability for custom statuses to be defined within an organization to be used on various objects to facilitate business workflows around object statuses.","title":"Statuses"},{"location":"models/extras/status.html#status-basics","text":"The value of a status field on a model (such as Device.status ) will be represented as a nautobot.extras.models.Status object. When created, a Status can be associated to one or more model content-types using a many-to-many relationship. The relationship to each model is referenced across all user interfaces using the {app_label}.{model} naming convention (e.g. dcim.device ). Statuses may be managed by navigating to Organization > Statuses in the navigation menu.","title":"Status Basics"},{"location":"models/extras/status.html#importing-objects-with-a-status-field","text":"When using CSV import to reference a status field on an object, the Status.slug field is used. For example, the default Active status has a slug of active , so the active value would be used for import.","title":"Importing Objects with a status Field"},{"location":"models/extras/status.html#customizing-statuses","text":"With Status as a model, statuses can be customized. This can be as simple as removing the option to configure an existing status with a particular model or to remove that status entirely. The real benefit of custom status is adding your own organization status and process names directly to Nautobot. An example of custom statuses would be including End of Life information for your devices. A simple End of Life status could be EOx for a device hitting any end of life milestone; more specific statuses like EOSS (End of Software Support), EOS (End of Sale), and Pre-EOS (for 1 year prior to EOS) to be more specific. Once the end of life information is tracked as a status, developing a report for Devices that have reached EOSS is trivial. Another example for sites is tracking the nature of a specific site's installation status. A site that is under construction could received a status like 'Pre Production'. For Virtual Machines, if utilizing OpenStack, statuses in Nautobot could be customized to reflect the specific Nova virtual machine states .","title":"Customizing Statuses"},{"location":"models/extras/status.html#status-internals","text":"Warning The section below is largely intended for developers who may need to create data models of their own that implement a status field. Proceed at your own risk! Any model that is intended to have a status field must inherit from nautobot.extras.models.statuses.StatusModel . This abstract model will add an nautobot.extras.models.statuses.StatusField to the model. The abstract base will automatically assign a related_name for the reverse relationship back to the inheriting model's name (e.g. devices ).","title":"Status Internals"},{"location":"models/extras/status.html#statusfield-model-field","text":"The StatusField field type subclass of a django.db.models.ForeignKey with extra extensions to have it behave like field with choices. Because this pattern is replacing hard-coded ChoiceSets (such as dcim.choices.DeviceStatusChoices ) with database objects, it is not possible to use the choices= argument on a foreign key. Because of this, StatusField implements a .contribute_to_class() method which will automatically bind .get_status_display() and .get_status_color() methods to any model that implements this field, so that these do not need to be manually defined on each model. This model field also emits its own form field to eliminate the requirement for a form field to be explicitly added to model forms.","title":"StatusField model field"},{"location":"models/extras/status.html#statusfilter-filter-field","text":"Any filter that is intended to have a status field must inherit from nautobot.extras.filters.StatusModelFilterSetMixin . This will add a nautobot.extras.filters.StatusFilter to the filter, which allows filtering by the name of the status.","title":"StatusFilter filter field"},{"location":"models/extras/status.html#form-fields","text":"Any model form that is intended to have a status field must inherit from one of three mixins, depending on the use-case: nautobot.extras.forms.StatusModelFilterFormMixin should be used to add a non-required, multiple-choice status filter field to UI filter forms. This multiple-choice field allows for multiple status values to be selected for filtering objects in list views in the web UI. nautobot.extras.forms.StatusModelBulkEditFormMixin should be used to add a non-required status form field to a an object's model form. This field constrains status choices eligible to the object type being edited. Changed in version 1.4.0 In prior Nautobot versions these mixins were named StatusFilterFormMixin and StatusBulkEditFormMixin ; the old names are still available as aliases but will be removed in a future major release. FIXME: CSV import forms","title":"Form fields"},{"location":"models/extras/status.html#statusserializerfield-serializer-field","text":"Any serializer that is intended to have a status field must inherit from nautobot.extras.api.serializers.StatusModelSerializerMixin . This adds an nautobot.extras.api.fields.StatusSerializerField to the serializer. The StatusSerializerField is a writable slug-related choice field that allows writing to the field using the name value of the status (e.g. \"active\" ). Writing to this field is normalized to always be converted to lowercase.","title":"StatusSerializerField serializer field"},{"location":"models/extras/status.html#table-field","text":"If you wish for a table to include a status field, your table must inherit from nautobot.extras.tables.StatusTableMixin . This includes a ColorColumn on the table.","title":"Table field"},{"location":"models/extras/status.html#status-object-integrations","text":"To fully integrate a model to include a status field, assert the following:","title":"Status object integrations"},{"location":"models/extras/status.html#model","text":"The model must inherit from nautobot.extras.models.statuses.StatusModel Decorate the model class with @extras_features('statuses') ( from nautobot.extras.utils import extras_features )","title":"Model"},{"location":"models/extras/status.html#forms","text":"Generic model forms will automatically include a StatusField CSV model import forms must inherit from nautobot.extras.forms.StatusModelCSVFormMixin Bulk edit model forms must inherit from nautobot.extras.forms.StatusModelBulkEditFormMixin Filter forms must inherit from nautobot.extras.forms.StatusModelFilterFormMixin Changed in version 1.4.0 In prior Nautobot releases the latter two mixins were named StatusBulkEditFormMixin and StatusFilterFormMixin respectively; the old names are still available as aliases but will be removed in a future major release.","title":"Forms"},{"location":"models/extras/status.html#filters","text":"Filtersets for your model must inherit from nautobot.extras.filters.StatusModelFilterSetMixin","title":"Filters"},{"location":"models/extras/status.html#serializers","text":"Serializers for your model must inherit from nautobot.extras.api.serializers.StatusModelSerializerMixin","title":"Serializers"},{"location":"models/extras/status.html#tables","text":"The table class for your model must inherit from nautobot.extras.tables.StatusTableMixin","title":"Tables"},{"location":"models/extras/tag.html","text":"Tags \u00b6 Tags are user-defined labels which can be applied to a variety of objects within Nautobot. They can be used to establish dimensions of organization beyond the relationships built into Nautobot. For example, you might create a tag to identify a particular ownership or condition across several types of objects. Added in version 1.3.0 When created, a Tag can be associated to one or more model content-types using a many-to-many relationship. The tag will then apply only to models belonging to those associated content-types. Each tag has a name, label, color, content-types and a URL-friendly slug. For example, the slug for a tag named \"Dunder Mifflin, Inc.\" would be dunder-mifflin-inc . The slug is generated automatically and makes tags easier to work with as URL parameters. Each tag can also be assigned a description indicating its purpose. Objects can be filtered by the tags they have applied. For example, the following API request will retrieve all devices tagged as \"monitored\": GET /api/dcim/devices/?tag=monitored The tag filter can be specified multiple times to match only objects which have all of the specified tags assigned: GET /api/dcim/devices/?tag=monitored&tag=deprecated Tags can also be created in the ORM or REST API of Nautobot. The following HEX color values in the table below correspond to the dropdown selection when building tags using the UI. Any HEX color value can be used with the ORM or REST API, but a non-standard color will cause some inconsistency when editing the tag via the UI. Color HEX value Dark Red aa1409 Red f44336 Pink e91e63 Rose ffe4e1 Fuchsia ff66ff Purple 9c27b0 Dark Purple 673ab7 Indigo 3f51b5 Blue 2196f3 Light blue 03a9f4 Cyan 00bcd4 Teal 009688 Aqua 00ffff Dark green 2f6a31 Green 4caf50 Light green 8bc34a Lime cddc39 Yellow ffeb3b Amber ffc107 Orange ff9800 Dark orange ff5722 Brown 795548 Light grey c0c0c0 Grey 9e9e9e Dark grey 607d8b Black 111111 White ffffff Example of ORM creation: Tag . objects . get_or_create ( name = \"Cisco-3650CX\" , slug = \"cisco-3650cx\" , description = \"Device tag for Cisco 3650CX series\" , color = \"2196f3\" )","title":"Tags"},{"location":"models/extras/tag.html#tags","text":"Tags are user-defined labels which can be applied to a variety of objects within Nautobot. They can be used to establish dimensions of organization beyond the relationships built into Nautobot. For example, you might create a tag to identify a particular ownership or condition across several types of objects. Added in version 1.3.0 When created, a Tag can be associated to one or more model content-types using a many-to-many relationship. The tag will then apply only to models belonging to those associated content-types. Each tag has a name, label, color, content-types and a URL-friendly slug. For example, the slug for a tag named \"Dunder Mifflin, Inc.\" would be dunder-mifflin-inc . The slug is generated automatically and makes tags easier to work with as URL parameters. Each tag can also be assigned a description indicating its purpose. Objects can be filtered by the tags they have applied. For example, the following API request will retrieve all devices tagged as \"monitored\": GET /api/dcim/devices/?tag=monitored The tag filter can be specified multiple times to match only objects which have all of the specified tags assigned: GET /api/dcim/devices/?tag=monitored&tag=deprecated Tags can also be created in the ORM or REST API of Nautobot. The following HEX color values in the table below correspond to the dropdown selection when building tags using the UI. Any HEX color value can be used with the ORM or REST API, but a non-standard color will cause some inconsistency when editing the tag via the UI. Color HEX value Dark Red aa1409 Red f44336 Pink e91e63 Rose ffe4e1 Fuchsia ff66ff Purple 9c27b0 Dark Purple 673ab7 Indigo 3f51b5 Blue 2196f3 Light blue 03a9f4 Cyan 00bcd4 Teal 009688 Aqua 00ffff Dark green 2f6a31 Green 4caf50 Light green 8bc34a Lime cddc39 Yellow ffeb3b Amber ffc107 Orange ff9800 Dark orange ff5722 Brown 795548 Light grey c0c0c0 Grey 9e9e9e Dark grey 607d8b Black 111111 White ffffff Example of ORM creation: Tag . objects . get_or_create ( name = \"Cisco-3650CX\" , slug = \"cisco-3650cx\" , description = \"Device tag for Cisco 3650CX series\" , color = \"2196f3\" )","title":"Tags"},{"location":"models/extras/webhook.html","text":"Webhooks \u00b6 A webhook is a mechanism for conveying to some external system a change that took place in Nautobot. For example, you may want to notify a monitoring system whenever the status of a device is updated in Nautobot. This can be done by creating a webhook for the device model in Nautobot and identifying the webhook receiver. When Nautobot detects a change to a device, an HTTP request containing the details of the change and who made it be sent to the specified receiver. Webhooks are configured in the web UI under Extensibility > Webhooks. Configuration \u00b6 Name - A unique name for the webhook. The name is not included with outbound messages. Object type(s) - The type or types of Nautobot object that will trigger the webhook. Enabled - If unchecked, the webhook will be inactive. Events - A webhook may trigger on any combination of create, update, and delete events. At least one event type must be selected. HTTP method - The type of HTTP request to send. Options include GET , POST , PUT , PATCH , and DELETE . URL - The fuly-qualified URL of the request to be sent. This may specify a destination port number if needed. HTTP content type - The value of the request's Content-Type header. (Defaults to application/json ) Additional headers - Any additional headers to include with the request (optional). Add one header per line in the format Name: Value . Jinja2 templating is supported for this field (see below). Body template - The content of the request being sent (optional). Jinja2 templating is supported for this field (see below). If blank, Nautobot will populate the request body with a raw dump of the webhook context. (If the HTTP content-type is set to application/json , this will be formatted as a JSON object.) Secret - A secret string used to prove authenticity of the request (optional). This will append a X-Hook-Signature header to the request, consisting of a HMAC (SHA-512) hex digest of the request body using the secret as the key. SSL verification - Uncheck this option to disable validation of the receiver's SSL certificate. (Disable with caution!) CA file path - The file path to a particular certificate authority (CA) file to use when validating the receiver's SSL certificate (optional). Jinja2 Template Support \u00b6 Jinja2 templating is supported for the additional_headers and body_template fields. This enables the user to convey object data in the request headers as well as to craft a customized request body. Request content can be crafted to enable the direct interaction with external systems by ensuring the outgoing message is in a format the receiver expects and understands. For example, you might create a Nautobot webhook to trigger a Slack message any time an IP address is created. You can accomplish this using the following configuration: Object type: IPAM > IP address HTTP method: POST URL: Slack incoming webhook URL HTTP content type: application/json Body template: {\"text\": \"IP address {{ data['address'] }} was created by {{ username }}!\"} Available Context \u00b6 The following data is available as context for Jinja2 templates: event - The type of event which triggered the webhook: created, updated, or deleted. model - The Nautobot model which triggered the change. timestamp - The time at which the event occurred (in ISO 8601 format). username - The name of the user account associated with the change. request_id - The unique request ID. This may be used to correlate multiple changes associated with a single request. data - A serialized representation of the object after the change was made. This is typically equivalent to the model's representation in Nautobot's REST API. Added in version 1.3.0 snapshots - snapshots of the serialized object state both before and after the change was made; provided as a dictionary with keys named prechange , postchange and differences . Default Request Body \u00b6 If no body template is specified, the request body will be populated with a JSON object containing the context data. For example, a newly created site might appear as follows: { \"event\": \"created\", \"timestamp\": \"2020-02-25 15:10:26.010582+00:00\", \"model\": \"site\", \"username\": \"jstretch\", \"request_id\": \"fdbca812-3142-4783-b364-2e2bd5c16c6a\", \"data\": { \"id\": 19, \"name\": \"Site 1\", \"slug\": \"site-1\", \"status\": \"value\": \"active\", \"label\": \"Active\", \"id\": 1 }, \"region\": null, ... }, \"snapshots\": { \"prechange\": null, \"postchange\": { \"created\": \"2020-02-25\", \"last_updated\": \"2020-02-25 15:10:26.010582+00:00\", \"name\": \"Site 1\", \"slug\": \"site-1\", ... }, \"differences\": { \"removed\": null, \"added\": { \"created\": \"2020-02-25\", \"last_updated\": \"2020-02-25 15:10:26.010582+00:00\", \"name\": \"Site 1\", \"slug\": \"site-1\", ... } } } } Webhook Processing \u00b6 When a change is detected, any resulting webhooks are placed into a Redis queue for processing. This allows the user's request to complete without needing to wait for the outgoing webhook(s) to be processed. The webhooks are then extracted from the queue by the rqworker process and HTTP requests are sent to their respective destinations. The current webhook queue and any failed webhooks can be inspected in the admin UI under Django RQ > Queues. A request is considered successful if the response has a 2XX status code; otherwise, the request is marked as having failed. Failed requests may be retried manually via the admin UI. Troubleshooting \u00b6 To assist with verifying that the content of outgoing webhooks is rendered correctly, Nautobot provides a simple HTTP listener that can be run locally to receive and display webhook requests. First, modify the target URL of the desired webhook to http://localhost:9000/ . This will instruct Nautobot to send the request to the local server on TCP port 9000. Then, start the webhook receiver service from the Nautobot root directory: $ nautobot-server webhook_receiver Listening on port http://localhost:9000. Stop with CONTROL-C. You can test the receiver itself by sending any HTTP request to it. For example: $ curl -X POST http://localhost:9000 --data '{\"foo\": \"bar\"}' The server will print output similar to the following: [1] Tue, 07 Apr 2020 17:44:02 GMT 127.0.0.1 \"POST / HTTP/1.1\" 200 - Host: localhost:9000 User-Agent: curl/7.58.0 Accept: */* Content-Length: 14 Content-Type: application/x-www-form-urlencoded {\"foo\": \"bar\"} ------------ Note that webhook_receiver does not actually do anything with the information received: It merely prints the request headers and body for inspection. Now, when the Nautobot webhook is triggered and processed, you should see its headers and content appear in the terminal where the webhook receiver is listening. If you don't, check that the rqworker process is running and that webhook events are being placed into the queue (visible under the Nautobot admin UI).","title":"Webhooks"},{"location":"models/extras/webhook.html#webhooks","text":"A webhook is a mechanism for conveying to some external system a change that took place in Nautobot. For example, you may want to notify a monitoring system whenever the status of a device is updated in Nautobot. This can be done by creating a webhook for the device model in Nautobot and identifying the webhook receiver. When Nautobot detects a change to a device, an HTTP request containing the details of the change and who made it be sent to the specified receiver. Webhooks are configured in the web UI under Extensibility > Webhooks.","title":"Webhooks"},{"location":"models/extras/webhook.html#configuration","text":"Name - A unique name for the webhook. The name is not included with outbound messages. Object type(s) - The type or types of Nautobot object that will trigger the webhook. Enabled - If unchecked, the webhook will be inactive. Events - A webhook may trigger on any combination of create, update, and delete events. At least one event type must be selected. HTTP method - The type of HTTP request to send. Options include GET , POST , PUT , PATCH , and DELETE . URL - The fuly-qualified URL of the request to be sent. This may specify a destination port number if needed. HTTP content type - The value of the request's Content-Type header. (Defaults to application/json ) Additional headers - Any additional headers to include with the request (optional). Add one header per line in the format Name: Value . Jinja2 templating is supported for this field (see below). Body template - The content of the request being sent (optional). Jinja2 templating is supported for this field (see below). If blank, Nautobot will populate the request body with a raw dump of the webhook context. (If the HTTP content-type is set to application/json , this will be formatted as a JSON object.) Secret - A secret string used to prove authenticity of the request (optional). This will append a X-Hook-Signature header to the request, consisting of a HMAC (SHA-512) hex digest of the request body using the secret as the key. SSL verification - Uncheck this option to disable validation of the receiver's SSL certificate. (Disable with caution!) CA file path - The file path to a particular certificate authority (CA) file to use when validating the receiver's SSL certificate (optional).","title":"Configuration"},{"location":"models/extras/webhook.html#jinja2-template-support","text":"Jinja2 templating is supported for the additional_headers and body_template fields. This enables the user to convey object data in the request headers as well as to craft a customized request body. Request content can be crafted to enable the direct interaction with external systems by ensuring the outgoing message is in a format the receiver expects and understands. For example, you might create a Nautobot webhook to trigger a Slack message any time an IP address is created. You can accomplish this using the following configuration: Object type: IPAM > IP address HTTP method: POST URL: Slack incoming webhook URL HTTP content type: application/json Body template: {\"text\": \"IP address {{ data['address'] }} was created by {{ username }}!\"}","title":"Jinja2 Template Support"},{"location":"models/extras/webhook.html#available-context","text":"The following data is available as context for Jinja2 templates: event - The type of event which triggered the webhook: created, updated, or deleted. model - The Nautobot model which triggered the change. timestamp - The time at which the event occurred (in ISO 8601 format). username - The name of the user account associated with the change. request_id - The unique request ID. This may be used to correlate multiple changes associated with a single request. data - A serialized representation of the object after the change was made. This is typically equivalent to the model's representation in Nautobot's REST API. Added in version 1.3.0 snapshots - snapshots of the serialized object state both before and after the change was made; provided as a dictionary with keys named prechange , postchange and differences .","title":"Available Context"},{"location":"models/extras/webhook.html#default-request-body","text":"If no body template is specified, the request body will be populated with a JSON object containing the context data. For example, a newly created site might appear as follows: { \"event\": \"created\", \"timestamp\": \"2020-02-25 15:10:26.010582+00:00\", \"model\": \"site\", \"username\": \"jstretch\", \"request_id\": \"fdbca812-3142-4783-b364-2e2bd5c16c6a\", \"data\": { \"id\": 19, \"name\": \"Site 1\", \"slug\": \"site-1\", \"status\": \"value\": \"active\", \"label\": \"Active\", \"id\": 1 }, \"region\": null, ... }, \"snapshots\": { \"prechange\": null, \"postchange\": { \"created\": \"2020-02-25\", \"last_updated\": \"2020-02-25 15:10:26.010582+00:00\", \"name\": \"Site 1\", \"slug\": \"site-1\", ... }, \"differences\": { \"removed\": null, \"added\": { \"created\": \"2020-02-25\", \"last_updated\": \"2020-02-25 15:10:26.010582+00:00\", \"name\": \"Site 1\", \"slug\": \"site-1\", ... } } } }","title":"Default Request Body"},{"location":"models/extras/webhook.html#webhook-processing","text":"When a change is detected, any resulting webhooks are placed into a Redis queue for processing. This allows the user's request to complete without needing to wait for the outgoing webhook(s) to be processed. The webhooks are then extracted from the queue by the rqworker process and HTTP requests are sent to their respective destinations. The current webhook queue and any failed webhooks can be inspected in the admin UI under Django RQ > Queues. A request is considered successful if the response has a 2XX status code; otherwise, the request is marked as having failed. Failed requests may be retried manually via the admin UI.","title":"Webhook Processing"},{"location":"models/extras/webhook.html#troubleshooting","text":"To assist with verifying that the content of outgoing webhooks is rendered correctly, Nautobot provides a simple HTTP listener that can be run locally to receive and display webhook requests. First, modify the target URL of the desired webhook to http://localhost:9000/ . This will instruct Nautobot to send the request to the local server on TCP port 9000. Then, start the webhook receiver service from the Nautobot root directory: $ nautobot-server webhook_receiver Listening on port http://localhost:9000. Stop with CONTROL-C. You can test the receiver itself by sending any HTTP request to it. For example: $ curl -X POST http://localhost:9000 --data '{\"foo\": \"bar\"}' The server will print output similar to the following: [1] Tue, 07 Apr 2020 17:44:02 GMT 127.0.0.1 \"POST / HTTP/1.1\" 200 - Host: localhost:9000 User-Agent: curl/7.58.0 Accept: */* Content-Length: 14 Content-Type: application/x-www-form-urlencoded {\"foo\": \"bar\"} ------------ Note that webhook_receiver does not actually do anything with the information received: It merely prints the request headers and body for inspection. Now, when the Nautobot webhook is triggered and processed, you should see its headers and content appear in the terminal where the webhook receiver is listening. If you don't, check that the rqworker process is running and that webhook events are being placed into the queue (visible under the Nautobot admin UI).","title":"Troubleshooting"},{"location":"models/ipam/aggregate.html","text":"Aggregates \u00b6 IP addressing is by nature hierarchical. The first few levels of the IPv4 hierarchy, for example, look like this: 0.0.0.0/0 0.0.0.0/1 0.0.0.0/2 64.0.0.0/2 128.0.0.0/1 128.0.0.0/2 192.0.0.0/2 This hierarchy comprises 33 tiers of addressing, from /0 all the way down to individual /32 address (and much, much further to /128 for IPv6). Of course, most organizations are concerned with only relatively small portions of the total IP space, so tracking the uppermost of these tiers isn't necessary. Nautobot allows us to specify the portions of IP space that are interesting to us by defining aggregates . Typically, an aggregate will correspond to either an allocation of public (globally routable) IP space granted by a regional authority, or a private (internally-routable) designation. Common private designations include: 10.0.0.0/8 (RFC 1918) 100.64.0.0/10 (RFC 6598) 172.16.0.0/12 (RFC 1918) 192.168.0.0/16 (RFC 1918) One or more /48s within fd00::/8 (IPv6 unique local addressing) Each aggregate is assigned to a RIR. For \"public\" aggregates, this will be the real-world authority which has granted your organization permission to use the specified IP space on the public Internet. For \"private\" aggregates, this will be a statutory authority, such as RFC 1918. Each aggregate can also annotate that date on which it was allocated, where applicable. Prefixes are automatically arranged beneath their parent aggregates in Nautobot. Typically you'll want to create aggregates only for the prefixes and IP addresses that your organization actually manages: There is no need to define aggregates for provider-assigned space which is only used on Internet circuits, for example. Note Because aggregates represent swaths of the global IP space, they cannot overlap with one another: They can only exist side-by-side. For instance, you cannot define both 10.0.0.0/8 and 10.16.0.0/16 as aggregates, because they overlap. 10.16.0.0/16 in this example would be created as a container prefix and automatically grouped under the 10.0.0.0/8 aggregate. Remember, the purpose of aggregates is to establish the root of your IP addressing hierarchy.","title":"Aggregates"},{"location":"models/ipam/aggregate.html#aggregates","text":"IP addressing is by nature hierarchical. The first few levels of the IPv4 hierarchy, for example, look like this: 0.0.0.0/0 0.0.0.0/1 0.0.0.0/2 64.0.0.0/2 128.0.0.0/1 128.0.0.0/2 192.0.0.0/2 This hierarchy comprises 33 tiers of addressing, from /0 all the way down to individual /32 address (and much, much further to /128 for IPv6). Of course, most organizations are concerned with only relatively small portions of the total IP space, so tracking the uppermost of these tiers isn't necessary. Nautobot allows us to specify the portions of IP space that are interesting to us by defining aggregates . Typically, an aggregate will correspond to either an allocation of public (globally routable) IP space granted by a regional authority, or a private (internally-routable) designation. Common private designations include: 10.0.0.0/8 (RFC 1918) 100.64.0.0/10 (RFC 6598) 172.16.0.0/12 (RFC 1918) 192.168.0.0/16 (RFC 1918) One or more /48s within fd00::/8 (IPv6 unique local addressing) Each aggregate is assigned to a RIR. For \"public\" aggregates, this will be the real-world authority which has granted your organization permission to use the specified IP space on the public Internet. For \"private\" aggregates, this will be a statutory authority, such as RFC 1918. Each aggregate can also annotate that date on which it was allocated, where applicable. Prefixes are automatically arranged beneath their parent aggregates in Nautobot. Typically you'll want to create aggregates only for the prefixes and IP addresses that your organization actually manages: There is no need to define aggregates for provider-assigned space which is only used on Internet circuits, for example. Note Because aggregates represent swaths of the global IP space, they cannot overlap with one another: They can only exist side-by-side. For instance, you cannot define both 10.0.0.0/8 and 10.16.0.0/16 as aggregates, because they overlap. 10.16.0.0/16 in this example would be created as a container prefix and automatically grouped under the 10.0.0.0/8 aggregate. Remember, the purpose of aggregates is to establish the root of your IP addressing hierarchy.","title":"Aggregates"},{"location":"models/ipam/ipaddress.html","text":"IP Addresses \u00b6 An IP address comprises a single host address (either IPv4 or IPv6) and its subnet mask. Its mask should match exactly how the IP address is configured on an interface in the real world. Like a prefix, an IP address can optionally be assigned to a VRF (otherwise, it will appear in the \"global\" table). IP addresses are automatically arranged under parent prefixes within their respective VRFs according to the IP hierarchy. Each IP address can also be assigned an operational status and a functional role. The following statuses are available by default: Active Reserved Deprecated DHCP SLAAC (IPv6 Stateless Address Autoconfiguration) Roles are used to indicate some special attribute of an IP address; for example, use as a loopback or as the the virtual IP for a VRRP group. (Note that functional roles are conceptual in nature, and thus cannot be customized by the user.) Available roles include: Loopback Secondary Anycast VIP VRRP HSRP GLBP An IP address can be assigned to any device or virtual machine interface, and an interface may have multiple IP addresses assigned to it. Further, each device and virtual machine may have one of its interface IPs designated as its primary IP per address family (one for IPv4 and one for IPv6). Note When primary IPs are set for both IPv4 and IPv6, Nautobot will prefer IPv6. This can be changed by setting the PREFER_IPV4 configuration parameter. Network Address Translation (NAT) \u00b6 An IP address can be designated as the network address translation (NAT) inside IP address for one or more other IP addresses. This is useful primarily to denote a translation between public and private IP addresses. This relationship is followed in both directions: For example, if 10.0.0.1 is assigned as the inside IP for 192.0.2.1, 192.0.2.1 will be displayed as the outside IP for 10.0.0.1. Added in version 1.3.0 Support for multiple outside NAT IP addresses was added.","title":"IP Addresses"},{"location":"models/ipam/ipaddress.html#ip-addresses","text":"An IP address comprises a single host address (either IPv4 or IPv6) and its subnet mask. Its mask should match exactly how the IP address is configured on an interface in the real world. Like a prefix, an IP address can optionally be assigned to a VRF (otherwise, it will appear in the \"global\" table). IP addresses are automatically arranged under parent prefixes within their respective VRFs according to the IP hierarchy. Each IP address can also be assigned an operational status and a functional role. The following statuses are available by default: Active Reserved Deprecated DHCP SLAAC (IPv6 Stateless Address Autoconfiguration) Roles are used to indicate some special attribute of an IP address; for example, use as a loopback or as the the virtual IP for a VRRP group. (Note that functional roles are conceptual in nature, and thus cannot be customized by the user.) Available roles include: Loopback Secondary Anycast VIP VRRP HSRP GLBP An IP address can be assigned to any device or virtual machine interface, and an interface may have multiple IP addresses assigned to it. Further, each device and virtual machine may have one of its interface IPs designated as its primary IP per address family (one for IPv4 and one for IPv6). Note When primary IPs are set for both IPv4 and IPv6, Nautobot will prefer IPv6. This can be changed by setting the PREFER_IPV4 configuration parameter.","title":"IP Addresses"},{"location":"models/ipam/ipaddress.html#network-address-translation-nat","text":"An IP address can be designated as the network address translation (NAT) inside IP address for one or more other IP addresses. This is useful primarily to denote a translation between public and private IP addresses. This relationship is followed in both directions: For example, if 10.0.0.1 is assigned as the inside IP for 192.0.2.1, 192.0.2.1 will be displayed as the outside IP for 10.0.0.1. Added in version 1.3.0 Support for multiple outside NAT IP addresses was added.","title":"Network Address Translation (NAT)"},{"location":"models/ipam/prefix.html","text":"Prefixes \u00b6 A prefix is an IPv4 or IPv6 network and mask expressed in CIDR notation (e.g. 192.0.2.0/24). A prefix entails only the \"network portion\" of an IP address: All bits in the address not covered by the mask must be zero. (In other words, a prefix cannot be a specific IP address.) Prefixes are automatically organized by their parent aggregates. Additionally, each prefix can be assigned to a particular site (optionally also to a location within the site) and a virtual routing and forwarding instance (VRF). Each VRF represents a separate IP space or routing table. All prefixes not assigned to a VRF are considered to be in the \"global\" table. Each prefix must be assigned a status and can optionally be assigned a role. These terms are often used interchangeably so it's important to recognize the difference between them. The status defines a prefix's operational state. The following statuses are provided by default: Container - A summary of child prefixes Active - Provisioned and in use Reserved - Designated for future use Deprecated - No longer in use On the other hand, a prefix's role defines its function. Role assignment is optional and roles are fully customizable. For example, you might create roles to differentiate between production and development infrastructure. A prefix may also be assigned to a VLAN. This association is helpful for associating address space with layer two domains. A VLAN may have multiple prefixes assigned to it. The prefix model include an \"is pool\" flag. If enabled, Nautobot will treat this prefix as a range (such as a NAT pool) wherein every IP address is valid and assignable. This logic is used when identifying available IP addresses within a prefix. If this flag is disabled, Nautobot will assume that the first and last (broadcast) address within an IPv4 prefix are unusable.","title":"Prefixes"},{"location":"models/ipam/prefix.html#prefixes","text":"A prefix is an IPv4 or IPv6 network and mask expressed in CIDR notation (e.g. 192.0.2.0/24). A prefix entails only the \"network portion\" of an IP address: All bits in the address not covered by the mask must be zero. (In other words, a prefix cannot be a specific IP address.) Prefixes are automatically organized by their parent aggregates. Additionally, each prefix can be assigned to a particular site (optionally also to a location within the site) and a virtual routing and forwarding instance (VRF). Each VRF represents a separate IP space or routing table. All prefixes not assigned to a VRF are considered to be in the \"global\" table. Each prefix must be assigned a status and can optionally be assigned a role. These terms are often used interchangeably so it's important to recognize the difference between them. The status defines a prefix's operational state. The following statuses are provided by default: Container - A summary of child prefixes Active - Provisioned and in use Reserved - Designated for future use Deprecated - No longer in use On the other hand, a prefix's role defines its function. Role assignment is optional and roles are fully customizable. For example, you might create roles to differentiate between production and development infrastructure. A prefix may also be assigned to a VLAN. This association is helpful for associating address space with layer two domains. A VLAN may have multiple prefixes assigned to it. The prefix model include an \"is pool\" flag. If enabled, Nautobot will treat this prefix as a range (such as a NAT pool) wherein every IP address is valid and assignable. This logic is used when identifying available IP addresses within a prefix. If this flag is disabled, Nautobot will assume that the first and last (broadcast) address within an IPv4 prefix are unusable.","title":"Prefixes"},{"location":"models/ipam/rir.html","text":"Regional Internet Registries (RIRs) \u00b6 Regional Internet registries are responsible for the allocation of globally-routable address space. The five RIRs are ARIN, RIPE, APNIC, LACNIC, and AFRINIC. However, some address space has been set aside for internal use, such as defined in RFCs 1918 and 6598. Nautobot considers these RFCs as a sort of RIR as well; that is, an authority which \"owns\" certain address space. There also exist lower-tier registries which serve particular geographic areas. Users can create whatever RIRs they like, but each aggregate must be assigned to one RIR. The RIR model includes a boolean flag which indicates whether the RIR allocates only private IP space. For example, suppose your organization has been allocated 104.131.0.0/16 by ARIN. It also makes use of RFC 1918 addressing internally. You would first create RIRs named \"ARIN\" and \"RFC 1918,\" then create an aggregate for each of these top-level prefixes, assigning it to its respective RIR.","title":"Regional Internet Registries (RIRs)"},{"location":"models/ipam/rir.html#regional-internet-registries-rirs","text":"Regional Internet registries are responsible for the allocation of globally-routable address space. The five RIRs are ARIN, RIPE, APNIC, LACNIC, and AFRINIC. However, some address space has been set aside for internal use, such as defined in RFCs 1918 and 6598. Nautobot considers these RFCs as a sort of RIR as well; that is, an authority which \"owns\" certain address space. There also exist lower-tier registries which serve particular geographic areas. Users can create whatever RIRs they like, but each aggregate must be assigned to one RIR. The RIR model includes a boolean flag which indicates whether the RIR allocates only private IP space. For example, suppose your organization has been allocated 104.131.0.0/16 by ARIN. It also makes use of RFC 1918 addressing internally. You would first create RIRs named \"ARIN\" and \"RFC 1918,\" then create an aggregate for each of these top-level prefixes, assigning it to its respective RIR.","title":"Regional Internet Registries (RIRs)"},{"location":"models/ipam/role.html","text":"Prefix/VLAN Roles \u00b6 A role indicates the function of a prefix or VLAN. For example, you might define Data, Voice, and Security roles. Generally, a prefix will be assigned the same functional role as the VLAN to which it is assigned (if any).","title":"Prefix/VLAN Roles"},{"location":"models/ipam/role.html#prefixvlan-roles","text":"A role indicates the function of a prefix or VLAN. For example, you might define Data, Voice, and Security roles. Generally, a prefix will be assigned the same functional role as the VLAN to which it is assigned (if any).","title":"Prefix/VLAN Roles"},{"location":"models/ipam/routetarget.html","text":"Route Targets \u00b6 A route target is a particular type of extended BGP community used to control the redistribution of routes among VRF tables in a network. Route targets can be assigned to individual VRFs in Nautobot as import or export targets (or both) to model this exchange in an L3VPN. Each route target must be given a unique name, which should be in a format prescribed by RFC 4364 , similar to a VR route distinguisher. Each route target can optionally be assigned to a tenant, and may have tags assigned to it.","title":"Route Targets"},{"location":"models/ipam/routetarget.html#route-targets","text":"A route target is a particular type of extended BGP community used to control the redistribution of routes among VRF tables in a network. Route targets can be assigned to individual VRFs in Nautobot as import or export targets (or both) to model this exchange in an L3VPN. Each route target must be given a unique name, which should be in a format prescribed by RFC 4364 , similar to a VR route distinguisher. Each route target can optionally be assigned to a tenant, and may have tags assigned to it.","title":"Route Targets"},{"location":"models/ipam/service.html","text":"Services \u00b6 A service represents a layer four TCP or UDP service available on a device or virtual machine. For example, you might want to document that an HTTP service is running on a device. Each service includes a name, protocol, and port number; for example, \"SSH (TCP/22)\" or \"DNS (UDP/53).\" A service may optionally be bound to one or more specific IP addresses belonging to its parent device or VM. (If no IP addresses are bound, the service is assumed to be reachable via any assigned IP address.)","title":"Services"},{"location":"models/ipam/service.html#services","text":"A service represents a layer four TCP or UDP service available on a device or virtual machine. For example, you might want to document that an HTTP service is running on a device. Each service includes a name, protocol, and port number; for example, \"SSH (TCP/22)\" or \"DNS (UDP/53).\" A service may optionally be bound to one or more specific IP addresses belonging to its parent device or VM. (If no IP addresses are bound, the service is assumed to be reachable via any assigned IP address.)","title":"Services"},{"location":"models/ipam/vlan.html","text":"VLANs \u00b6 A VLAN represents an isolated layer two domain, identified by a name and a numeric ID (1-4094) as defined in IEEE 802.1Q . Each VLAN may be assigned to a site, location, tenant, and/or VLAN group. Each VLAN must be assigned a status . The following statuses are available by default: Active Reserved Deprecated As with prefixes, each VLAN may also be assigned a functional role. Prefixes and VLANs share the same set of customizable roles.","title":"VLANs"},{"location":"models/ipam/vlan.html#vlans","text":"A VLAN represents an isolated layer two domain, identified by a name and a numeric ID (1-4094) as defined in IEEE 802.1Q . Each VLAN may be assigned to a site, location, tenant, and/or VLAN group. Each VLAN must be assigned a status . The following statuses are available by default: Active Reserved Deprecated As with prefixes, each VLAN may also be assigned a functional role. Prefixes and VLANs share the same set of customizable roles.","title":"VLANs"},{"location":"models/ipam/vlangroup.html","text":"VLAN Groups \u00b6 VLAN groups can be used to organize VLANs within Nautobot. Each group may optionally be assigned to a specific site or a location within a site, but a group cannot belong to multiple sites. Groups can also be used to enforce uniqueness: Each VLAN within a group must have a unique ID and name. VLANs which are not assigned to a group may have overlapping names and IDs (including VLANs which belong to a common site). For example, you can create two VLANs with ID 123, but they cannot both be assigned to the same group.","title":"VLAN Groups"},{"location":"models/ipam/vlangroup.html#vlan-groups","text":"VLAN groups can be used to organize VLANs within Nautobot. Each group may optionally be assigned to a specific site or a location within a site, but a group cannot belong to multiple sites. Groups can also be used to enforce uniqueness: Each VLAN within a group must have a unique ID and name. VLANs which are not assigned to a group may have overlapping names and IDs (including VLANs which belong to a common site). For example, you can create two VLANs with ID 123, but they cannot both be assigned to the same group.","title":"VLAN Groups"},{"location":"models/ipam/vrf.html","text":"Virtual Routing and Forwarding (VRF) \u00b6 A VRF object in Nautobot represents a virtual routing and forwarding (VRF) domain. Each VRF is essentially a separate routing table. VRFs are commonly used to isolate customers or organizations from one another within a network, or to route overlapping address space (e.g. multiple instances of the 10.0.0.0/8 space). Each VRF may be assigned to a specific tenant to aid in organizing the available IP space by customer or internal user. Each VRF is assigned a unique name and an optional route distinguisher (RD). The RD is expected to take one of the forms prescribed in RFC 4364 , however its formatting is not strictly enforced. Each prefix and IP address may be assigned to one (and only one) VRF. If you have a prefix or IP address which exists in multiple VRFs, you will need to create a separate instance of it in Nautobot for each VRF. Any prefix or IP address not assigned to a VRF is said to belong to the \"global\" table. By default, Nautobot will allow duplicate prefixes to be assigned to a VRF. This behavior can be toggled by setting the \"enforce unique\" flag on the VRF model. Note Enforcement of unique IP space can be toggled for global table (non-VRF prefixes) using the ENFORCE_GLOBAL_UNIQUE configuration setting. Each VRF may have one or more import and/or export route targets applied to it. Route targets are used to control the exchange of routes (prefixes) among VRFs in L3VPNs.","title":"Virtual Routing and Forwarding (VRF)"},{"location":"models/ipam/vrf.html#virtual-routing-and-forwarding-vrf","text":"A VRF object in Nautobot represents a virtual routing and forwarding (VRF) domain. Each VRF is essentially a separate routing table. VRFs are commonly used to isolate customers or organizations from one another within a network, or to route overlapping address space (e.g. multiple instances of the 10.0.0.0/8 space). Each VRF may be assigned to a specific tenant to aid in organizing the available IP space by customer or internal user. Each VRF is assigned a unique name and an optional route distinguisher (RD). The RD is expected to take one of the forms prescribed in RFC 4364 , however its formatting is not strictly enforced. Each prefix and IP address may be assigned to one (and only one) VRF. If you have a prefix or IP address which exists in multiple VRFs, you will need to create a separate instance of it in Nautobot for each VRF. Any prefix or IP address not assigned to a VRF is said to belong to the \"global\" table. By default, Nautobot will allow duplicate prefixes to be assigned to a VRF. This behavior can be toggled by setting the \"enforce unique\" flag on the VRF model. Note Enforcement of unique IP space can be toggled for global table (non-VRF prefixes) using the ENFORCE_GLOBAL_UNIQUE configuration setting. Each VRF may have one or more import and/or export route targets applied to it. Route targets are used to control the exchange of routes (prefixes) among VRFs in L3VPNs.","title":"Virtual Routing and Forwarding (VRF)"},{"location":"models/tenancy/tenant.html","text":"Tenants \u00b6 A tenant represents a discrete grouping of resources used for administrative purposes. Typically, tenants are used to represent individual customers or internal departments within an organization. The following objects can be assigned to tenants: Sites Racks Rack reservations Devices VRFs Prefixes IP addresses VLANs Circuits Clusters Virtual machines Tenant assignment is used to signify the ownership of an object in Nautobot. As such, each object may only be owned by a single tenant. For example, if you have a firewall dedicated to a particular customer, you would assign it to the tenant which represents that customer. However, if the firewall serves multiple customers, it doesn't belong to any particular customer, so tenant assignment would not be appropriate.","title":"Tenants"},{"location":"models/tenancy/tenant.html#tenants","text":"A tenant represents a discrete grouping of resources used for administrative purposes. Typically, tenants are used to represent individual customers or internal departments within an organization. The following objects can be assigned to tenants: Sites Racks Rack reservations Devices VRFs Prefixes IP addresses VLANs Circuits Clusters Virtual machines Tenant assignment is used to signify the ownership of an object in Nautobot. As such, each object may only be owned by a single tenant. For example, if you have a firewall dedicated to a particular customer, you would assign it to the tenant which represents that customer. However, if the firewall serves multiple customers, it doesn't belong to any particular customer, so tenant assignment would not be appropriate.","title":"Tenants"},{"location":"models/tenancy/tenantgroup.html","text":"Tenant Groups \u00b6 Tenants can be organized by custom groups. For instance, you might create one group called \"Customers\" and one called \"Departments.\" The assignment of a tenant to a group is optional. Tenant groups may be nested recursively to achieve a multi-level hierarchy. For example, you might have a group called \"Customers\" containing subgroups of individual tenants grouped by product or account team.","title":"Tenant Groups"},{"location":"models/tenancy/tenantgroup.html#tenant-groups","text":"Tenants can be organized by custom groups. For instance, you might create one group called \"Customers\" and one called \"Departments.\" The assignment of a tenant to a group is optional. Tenant groups may be nested recursively to achieve a multi-level hierarchy. For example, you might have a group called \"Customers\" containing subgroups of individual tenants grouped by product or account team.","title":"Tenant Groups"},{"location":"models/users/objectpermission.html","text":"Object Permissions \u00b6 A permission in Nautobot represents a relationship shared by several components: Object type(s) - One or more types of object in Nautobot User(s)/Group(s) - One or more users or groups of users Action(s) - The action(s) that can be performed on an object Constraints - An arbitrary filter used to limit the granted action(s) to a specific subset of objects At a minimum, a permission assignment must specify one object type, one user or group, and one action. The specification of constraints is optional: A permission without any constraints specified will apply to all instances of the selected model(s). Actions \u00b6 There are four core actions that can be permitted for each type of object within Nautobot, roughly analogous to the CRUD convention (create, read, update, and delete): View - Retrieve an object from the database Add - Create a new object Change - Modify an existing object Delete - Delete an existing object In addition to these, permissions can also grant custom actions that may be required by a specific model or plugin. For example, the napalm_read permission on the device model allows a user to execute NAPALM queries on a device via Nautobot's REST API. These can be specified when granting a permission in the \"additional actions\" field. Note Internally, all actions granted by a permission (both built-in and custom) are stored as strings in an array field named actions . Constraints \u00b6 Constraints are expressed as a JSON object or list representing a Django query filter . This is the same syntax that you would pass to the QuerySet filter() method when performing a query using the Django ORM. As with query filters, double underscores can be used to traverse related objects or invoke lookup expressions. Some example queries and their corresponding definitions are shown below. All attributes defined within a single JSON object are applied with a logical AND. For example, suppose you assign a permission for the site model with the following constraints. { \"status\" : \"active\" , \"region__name\" : \"Americas\" } The permission will grant access only to sites which have a status of \"active\" and which are assigned to the \"Americas\" region. To achieve a logical OR with a different set of constraints, define multiple objects within a list. For example, if you want to constrain the permission to VLANs with an ID between 100 and 199 or a status of \"reserved,\" do the following: [ { \"vid__gte\" : 100 , \"vid__lt\" : 200 }, { \"status\" : \"reserved\" } ] Additionally, where multiple permissions have been assigned for an object type, their collective constraints will be merged using a logical \"OR\" operation.","title":"Object Permissions"},{"location":"models/users/objectpermission.html#object-permissions","text":"A permission in Nautobot represents a relationship shared by several components: Object type(s) - One or more types of object in Nautobot User(s)/Group(s) - One or more users or groups of users Action(s) - The action(s) that can be performed on an object Constraints - An arbitrary filter used to limit the granted action(s) to a specific subset of objects At a minimum, a permission assignment must specify one object type, one user or group, and one action. The specification of constraints is optional: A permission without any constraints specified will apply to all instances of the selected model(s).","title":"Object Permissions"},{"location":"models/users/objectpermission.html#actions","text":"There are four core actions that can be permitted for each type of object within Nautobot, roughly analogous to the CRUD convention (create, read, update, and delete): View - Retrieve an object from the database Add - Create a new object Change - Modify an existing object Delete - Delete an existing object In addition to these, permissions can also grant custom actions that may be required by a specific model or plugin. For example, the napalm_read permission on the device model allows a user to execute NAPALM queries on a device via Nautobot's REST API. These can be specified when granting a permission in the \"additional actions\" field. Note Internally, all actions granted by a permission (both built-in and custom) are stored as strings in an array field named actions .","title":"Actions"},{"location":"models/users/objectpermission.html#constraints","text":"Constraints are expressed as a JSON object or list representing a Django query filter . This is the same syntax that you would pass to the QuerySet filter() method when performing a query using the Django ORM. As with query filters, double underscores can be used to traverse related objects or invoke lookup expressions. Some example queries and their corresponding definitions are shown below. All attributes defined within a single JSON object are applied with a logical AND. For example, suppose you assign a permission for the site model with the following constraints. { \"status\" : \"active\" , \"region__name\" : \"Americas\" } The permission will grant access only to sites which have a status of \"active\" and which are assigned to the \"Americas\" region. To achieve a logical OR with a different set of constraints, define multiple objects within a list. For example, if you want to constrain the permission to VLANs with an ID between 100 and 199 or a status of \"reserved,\" do the following: [ { \"vid__gte\" : 100 , \"vid__lt\" : 200 }, { \"status\" : \"reserved\" } ] Additionally, where multiple permissions have been assigned for an object type, their collective constraints will be merged using a logical \"OR\" operation.","title":"Constraints"},{"location":"models/users/token.html","text":"Tokens \u00b6 A token is a unique identifier mapped to a Nautobot user account. Each user may have one or more tokens which he or she can use for authentication when making REST API requests. To create a token, navigate to the API tokens page under your user profile. Sign into Nautobot On the upper right hand corner, select your username, then Profile On the left hand side, under User Profile, select API Tokens Select +Add a token Leave Key blank to automatically create a token, or fill one in for yourself Check or uncheck \"Write enabled\", as desired (Optional) Set an expiration date for this token (Optional) Add a description Note The creation and modification of API tokens can be restricted per user by an administrator. If you don't see an option to create an API token, ask an administrator to grant you access. Each token contains a 160-bit key represented as 40 hexadecimal characters. When creating a token, you'll typically leave the key field blank so that a random key will be automatically generated. However, Nautobot allows you to specify a key in case you need to restore a previously deleted token to operation. By default, a token can be used to perform all actions via the API that a user would be permitted to do via the web UI. Deselecting the \"write enabled\" option will restrict API requests made with the token to read operations (e.g. GET) only. Additionally, a token can be set to expire at a specific time. This can be useful if an external client needs to be granted temporary access to Nautobot.","title":"Tokens"},{"location":"models/users/token.html#tokens","text":"A token is a unique identifier mapped to a Nautobot user account. Each user may have one or more tokens which he or she can use for authentication when making REST API requests. To create a token, navigate to the API tokens page under your user profile. Sign into Nautobot On the upper right hand corner, select your username, then Profile On the left hand side, under User Profile, select API Tokens Select +Add a token Leave Key blank to automatically create a token, or fill one in for yourself Check or uncheck \"Write enabled\", as desired (Optional) Set an expiration date for this token (Optional) Add a description Note The creation and modification of API tokens can be restricted per user by an administrator. If you don't see an option to create an API token, ask an administrator to grant you access. Each token contains a 160-bit key represented as 40 hexadecimal characters. When creating a token, you'll typically leave the key field blank so that a random key will be automatically generated. However, Nautobot allows you to specify a key in case you need to restore a previously deleted token to operation. By default, a token can be used to perform all actions via the API that a user would be permitted to do via the web UI. Deselecting the \"write enabled\" option will restrict API requests made with the token to read operations (e.g. GET) only. Additionally, a token can be set to expire at a specific time. This can be useful if an external client needs to be granted temporary access to Nautobot.","title":"Tokens"},{"location":"models/virtualization/cluster.html","text":"Clusters \u00b6 A cluster is a logical grouping of physical resources within which virtual machines run. A cluster must be assigned a type (technological classification), and may optionally be assigned to a cluster group, site, location, and/or tenant. Physical devices may be associated with clusters as hosts. This allows users to track on which host(s) a particular virtual machine may reside. However, Nautobot does not support pinning a specific VM within a cluster to a particular host device.","title":"Clusters"},{"location":"models/virtualization/cluster.html#clusters","text":"A cluster is a logical grouping of physical resources within which virtual machines run. A cluster must be assigned a type (technological classification), and may optionally be assigned to a cluster group, site, location, and/or tenant. Physical devices may be associated with clusters as hosts. This allows users to track on which host(s) a particular virtual machine may reside. However, Nautobot does not support pinning a specific VM within a cluster to a particular host device.","title":"Clusters"},{"location":"models/virtualization/clustergroup.html","text":"Cluster Groups \u00b6 Cluster groups may be created for the purpose of organizing clusters. The arrangement of clusters into groups is optional.","title":"Cluster Groups"},{"location":"models/virtualization/clustergroup.html#cluster-groups","text":"Cluster groups may be created for the purpose of organizing clusters. The arrangement of clusters into groups is optional.","title":"Cluster Groups"},{"location":"models/virtualization/clustertype.html","text":"Cluster Types \u00b6 A cluster type represents a technology or mechanism by which a cluster is formed. For example, you might create a cluster type named \"VMware vSphere\" for a locally hosted cluster or \"DigitalOcean NYC3\" for one hosted by a cloud provider.","title":"Cluster Types"},{"location":"models/virtualization/clustertype.html#cluster-types","text":"A cluster type represents a technology or mechanism by which a cluster is formed. For example, you might create a cluster type named \"VMware vSphere\" for a locally hosted cluster or \"DigitalOcean NYC3\" for one hosted by a cloud provider.","title":"Cluster Types"},{"location":"models/virtualization/virtualmachine.html","text":"Virtual Machines \u00b6 A virtual machine represents a virtual compute instance hosted within a cluster. Each VM must be assigned to exactly one cluster. Like devices, each VM can be assigned a platform and/or functional role, and an operational status . The following statuses are available by default: Active Offline Planned Staged Failed Decommissioning Additional fields are available for annotating the vCPU count, memory (GB), and disk (GB) allocated to each VM. Each VM may optionally be assigned to a tenant. Virtual machines may have virtual interfaces assigned to them, but do not support any physical component.","title":"Virtual Machines"},{"location":"models/virtualization/virtualmachine.html#virtual-machines","text":"A virtual machine represents a virtual compute instance hosted within a cluster. Each VM must be assigned to exactly one cluster. Like devices, each VM can be assigned a platform and/or functional role, and an operational status . The following statuses are available by default: Active Offline Planned Staged Failed Decommissioning Additional fields are available for annotating the vCPU count, memory (GB), and disk (GB) allocated to each VM. Each VM may optionally be assigned to a tenant. Virtual machines may have virtual interfaces assigned to them, but do not support any physical component.","title":"Virtual Machines"},{"location":"models/virtualization/vminterface.html","text":"Interfaces \u00b6 Virtual machine interfaces behave similarly to device interfaces, and can be assigned IP addresses, VLANs, an operational status and services. However, given their virtual nature, they lack properties pertaining to physical attributes. For example, VM interfaces do not have a physical type and cannot have cables attached to them. The following operational statuses are available by default: Planned Maintenance Active Decommissioning Failed Added in version 1.4.0 Added bridge field. Added parent_interface field. Added status field.","title":"Interfaces"},{"location":"models/virtualization/vminterface.html#interfaces","text":"Virtual machine interfaces behave similarly to device interfaces, and can be assigned IP addresses, VLANs, an operational status and services. However, given their virtual nature, they lack properties pertaining to physical attributes. For example, VM interfaces do not have a physical type and cannot have cables attached to them. The following operational statuses are available by default: Planned Maintenance Active Decommissioning Failed Added in version 1.4.0 Added bridge field. Added parent_interface field. Added status field.","title":"Interfaces"},{"location":"plugins/index.html","text":"Installing and Using Plugins \u00b6 Plugins are packaged Django apps that can be installed alongside Nautobot to provide custom functionality not present in the core application. Plugins can introduce their own models and views, but cannot interfere with existing components. A Nautobot user may opt to install plugins provided by the community or build his or her own. Capabilities \u00b6 The Nautobot plugin architecture allows for plugins to do any or all of the following: Extend the existing Nautobot UI \u00b6 Add navigation menu items. A plugin can extend the navigation menus with new links and buttons or even entirely new menus. Add home page content. A plugin can add custom items or custom panels to the Nautobot home page. Add content to existing model detail views. A plugin can inject custom HTML content within the view of a core Nautobot model. This content can appear in the left column, right column, or full width of the page, and can also include custom buttons at the top of the page. Added in version 1.2.0 Add a banner. A plugin can add a custom banner to the top of any appropriate views. Added in version 1.4.0 Add extra tabs to existing model detail views. A plugin can inject additional tabs which will appear at the end of the object detail tabs list. Extend and customize existing Nautobot functionality \u00b6 Add custom validation logic to existing data models. A plugin can provide additional logic to customize the rules for validating created/updated data records. Provide Jobs. A plugin can serve as a convenient way to package and install Jobs . Add additional Git data types. A plugin can add support for processing additional types of data stored in a Git repository . Added in version 1.1.0 Register additional Jinja2 filters. A plugin can define custom Jinja2 filters to be used in computed fields, webhooks, custom links, and export templates. Added in version 1.2.0 Populate extensibility features in the database. A plugin can add content to the Nautobot database when installed, such as automatically creating new custom fields, relationships, and so forth. Add additional secrets providers. A plugin can add support for retrieving Secret values from additional sources or external systems. Added in version 1.4.0 Override already-defined views. A plugin can define a view which can be set to override a view from the core set of views or another plugin's view. Add entirely new features \u00b6 Add new data models. A plugin can introduce one or more models to hold data. (A model is essentially a table in the SQL database.) These models can be integrated with core implementations of GraphQL, webhooks, logging, custom relationships, custom fields, and tags. Add new URLs and views. A plugin can register URLs under the /plugins/ root path to provide browseable views (pages) for users. Add new REST API endpoints. A plugin can register URLs under the /api/plugins/ root path to provide new REST API views. Add custom middleware. A plugin can provide and register custom Django middleware. Declare dependencies and requirements \u00b6 Declare configuration parameters. A plugin can define required, optional, and default configuration parameters within its unique namespace. Plugin configuration parameters are configurable under PLUGINS_CONFIG in nautobot_config.py . Limit installation by Nautobot version. A plugin can specify a minimum and/or maximum Nautobot version with which it is compatible. Add additional Django dependencies. A plugin can define additional Django application dependencies to require when the plugin is enabled. Details on how to implement any of these features are described in the plugin development documentation. Limitations \u00b6 Either by policy or by technical limitation, the interaction of plugins with Nautobot core is restricted in certain ways. A plugin may not: Modify core models. Plugins may not alter, remove, or override core Nautobot models in any way. This rule is in place to ensure the integrity of the core data model. Register URLs outside the /plugins root. All plugin URLs are restricted to this path to prevent path collisions with core or other plugins. Override core templates. Plugins can inject additional content where supported, but may not manipulate or remove core content. Modify core settings. A configuration registry is provided for plugins, however they cannot alter or delete the core configuration. Disable core components. Plugins are not permitted to disable or hide core Nautobot components. Installing Plugins \u00b6 The instructions below detail the process for installing and enabling a Nautobot plugin. You must be absolutely sure to install the plugin within Nautobot's virtual environment. Note If you installed Nautobot in a production environment, you'll want to sudo to the nautobot user first using sudo -iu nautobot . Install the Package \u00b6 Download and install the plugin package per its installation instructions. Plugins published via PyPI are typically installed using pip3 . $ pip3 install <package> Alternatively, if you're or installing a plugin from from a local source copy, you may wish to install the plugin manually by running python setup.py install . If you are developing a plugin and want to install it only temporarily, run python setup.py develop instead. Enable the Plugin \u00b6 In your nautobot_config.py , add the plugin's name to the PLUGINS list: PLUGINS = [ 'plugin_name' , ] Configure the Plugin \u00b6 If the plugin requires any configuration, define it in nautobot_config.py under the PLUGINS_CONFIG parameter. The available configuration parameters should be detailed in the plugin's README file. PLUGINS_CONFIG = { 'plugin_name' : { 'foo' : 'bar' , 'buzz' : 'bazz' } } Run nautobot-server post_upgrade \u00b6 After installing or upgrading a plugin, you should always run nautobot-server post_upgrade . This command will ensure that any necessary post-installation tasks are run, for example: Migrating the database to include any new or updated data models from the plugin Collecting any static files provided by the plugin Etc. # nautobot-server post_upgrade Performing database migrations... Operations to perform: Apply all migrations: admin, auth, circuits, contenttypes, db, dcim, extras, ipam, nautobot_plugin_example, sessions, social_django, taggit, tenancy, users, virtualization Running migrations: No migrations to apply. Generating cable paths... Found no missing circuit termination paths; skipping Found no missing console port paths; skipping Found no missing console server port paths; skipping Found no missing interface paths; skipping Found no missing power feed paths; skipping Found no missing power outlet paths; skipping Found no missing power port paths; skipping Finished. Collecting static files... 0 static files copied to '/opt/nautobot/static', 972 unmodified. Removing stale content types... Removing expired sessions... Invalidating cache... Restart the WSGI Service \u00b6 Restart the WSGI service to load the new plugin: # sudo systemctl restart nautobot nautobot-worker Verify that the Plugin is Installed \u00b6 In the Nautobot UI, navigate to Plugins -> Installed Plugins . The newly installed plugin should appear in the displayed table if everything is configured correctly. You can also click on the plugin's name in this table to view more detailed information about this plugin.","title":"Installing and Using Plugins"},{"location":"plugins/index.html#installing-and-using-plugins","text":"Plugins are packaged Django apps that can be installed alongside Nautobot to provide custom functionality not present in the core application. Plugins can introduce their own models and views, but cannot interfere with existing components. A Nautobot user may opt to install plugins provided by the community or build his or her own.","title":"Installing and Using Plugins"},{"location":"plugins/index.html#capabilities","text":"The Nautobot plugin architecture allows for plugins to do any or all of the following:","title":"Capabilities"},{"location":"plugins/index.html#extend-the-existing-nautobot-ui","text":"Add navigation menu items. A plugin can extend the navigation menus with new links and buttons or even entirely new menus. Add home page content. A plugin can add custom items or custom panels to the Nautobot home page. Add content to existing model detail views. A plugin can inject custom HTML content within the view of a core Nautobot model. This content can appear in the left column, right column, or full width of the page, and can also include custom buttons at the top of the page. Added in version 1.2.0 Add a banner. A plugin can add a custom banner to the top of any appropriate views. Added in version 1.4.0 Add extra tabs to existing model detail views. A plugin can inject additional tabs which will appear at the end of the object detail tabs list.","title":"Extend the existing Nautobot UI"},{"location":"plugins/index.html#extend-and-customize-existing-nautobot-functionality","text":"Add custom validation logic to existing data models. A plugin can provide additional logic to customize the rules for validating created/updated data records. Provide Jobs. A plugin can serve as a convenient way to package and install Jobs . Add additional Git data types. A plugin can add support for processing additional types of data stored in a Git repository . Added in version 1.1.0 Register additional Jinja2 filters. A plugin can define custom Jinja2 filters to be used in computed fields, webhooks, custom links, and export templates. Added in version 1.2.0 Populate extensibility features in the database. A plugin can add content to the Nautobot database when installed, such as automatically creating new custom fields, relationships, and so forth. Add additional secrets providers. A plugin can add support for retrieving Secret values from additional sources or external systems. Added in version 1.4.0 Override already-defined views. A plugin can define a view which can be set to override a view from the core set of views or another plugin's view.","title":"Extend and customize existing Nautobot functionality"},{"location":"plugins/index.html#add-entirely-new-features","text":"Add new data models. A plugin can introduce one or more models to hold data. (A model is essentially a table in the SQL database.) These models can be integrated with core implementations of GraphQL, webhooks, logging, custom relationships, custom fields, and tags. Add new URLs and views. A plugin can register URLs under the /plugins/ root path to provide browseable views (pages) for users. Add new REST API endpoints. A plugin can register URLs under the /api/plugins/ root path to provide new REST API views. Add custom middleware. A plugin can provide and register custom Django middleware.","title":"Add entirely new features"},{"location":"plugins/index.html#declare-dependencies-and-requirements","text":"Declare configuration parameters. A plugin can define required, optional, and default configuration parameters within its unique namespace. Plugin configuration parameters are configurable under PLUGINS_CONFIG in nautobot_config.py . Limit installation by Nautobot version. A plugin can specify a minimum and/or maximum Nautobot version with which it is compatible. Add additional Django dependencies. A plugin can define additional Django application dependencies to require when the plugin is enabled. Details on how to implement any of these features are described in the plugin development documentation.","title":"Declare dependencies and requirements"},{"location":"plugins/index.html#limitations","text":"Either by policy or by technical limitation, the interaction of plugins with Nautobot core is restricted in certain ways. A plugin may not: Modify core models. Plugins may not alter, remove, or override core Nautobot models in any way. This rule is in place to ensure the integrity of the core data model. Register URLs outside the /plugins root. All plugin URLs are restricted to this path to prevent path collisions with core or other plugins. Override core templates. Plugins can inject additional content where supported, but may not manipulate or remove core content. Modify core settings. A configuration registry is provided for plugins, however they cannot alter or delete the core configuration. Disable core components. Plugins are not permitted to disable or hide core Nautobot components.","title":"Limitations"},{"location":"plugins/index.html#installing-plugins","text":"The instructions below detail the process for installing and enabling a Nautobot plugin. You must be absolutely sure to install the plugin within Nautobot's virtual environment. Note If you installed Nautobot in a production environment, you'll want to sudo to the nautobot user first using sudo -iu nautobot .","title":"Installing Plugins"},{"location":"plugins/index.html#install-the-package","text":"Download and install the plugin package per its installation instructions. Plugins published via PyPI are typically installed using pip3 . $ pip3 install <package> Alternatively, if you're or installing a plugin from from a local source copy, you may wish to install the plugin manually by running python setup.py install . If you are developing a plugin and want to install it only temporarily, run python setup.py develop instead.","title":"Install the Package"},{"location":"plugins/index.html#enable-the-plugin","text":"In your nautobot_config.py , add the plugin's name to the PLUGINS list: PLUGINS = [ 'plugin_name' , ]","title":"Enable the Plugin"},{"location":"plugins/index.html#configure-the-plugin","text":"If the plugin requires any configuration, define it in nautobot_config.py under the PLUGINS_CONFIG parameter. The available configuration parameters should be detailed in the plugin's README file. PLUGINS_CONFIG = { 'plugin_name' : { 'foo' : 'bar' , 'buzz' : 'bazz' } }","title":"Configure the Plugin"},{"location":"plugins/index.html#run-nautobot-server-post_upgrade","text":"After installing or upgrading a plugin, you should always run nautobot-server post_upgrade . This command will ensure that any necessary post-installation tasks are run, for example: Migrating the database to include any new or updated data models from the plugin Collecting any static files provided by the plugin Etc. # nautobot-server post_upgrade Performing database migrations... Operations to perform: Apply all migrations: admin, auth, circuits, contenttypes, db, dcim, extras, ipam, nautobot_plugin_example, sessions, social_django, taggit, tenancy, users, virtualization Running migrations: No migrations to apply. Generating cable paths... Found no missing circuit termination paths; skipping Found no missing console port paths; skipping Found no missing console server port paths; skipping Found no missing interface paths; skipping Found no missing power feed paths; skipping Found no missing power outlet paths; skipping Found no missing power port paths; skipping Finished. Collecting static files... 0 static files copied to '/opt/nautobot/static', 972 unmodified. Removing stale content types... Removing expired sessions... Invalidating cache...","title":"Run nautobot-server post_upgrade"},{"location":"plugins/index.html#restart-the-wsgi-service","text":"Restart the WSGI service to load the new plugin: # sudo systemctl restart nautobot nautobot-worker","title":"Restart the WSGI Service"},{"location":"plugins/index.html#verify-that-the-plugin-is-installed","text":"In the Nautobot UI, navigate to Plugins -> Installed Plugins . The newly installed plugin should appear in the displayed table if everything is configured correctly. You can also click on the plugin's name in this table to view more detailed information about this plugin.","title":"Verify that the Plugin is Installed"},{"location":"plugins/development.html","text":"Plugin Development \u00b6 This documentation covers the development of custom plugins for Nautobot. Plugins are essentially self-contained Django applications which integrate with Nautobot to provide custom functionality. Since the development of Django applications is already very well-documented, this will only be covering the aspects that are specific to Nautobot. Plugins can do a lot of different things , all of which will be covered in detail in this document. Keep in mind that each piece of functionality is entirely optional. For example, if your plugin merely adds a piece of middleware or an API endpoint for existing data, there's no need to define any new models. Tip The plugin detail view ( /plugins/installed-plugins/<plugin_name>/ , accessible via Plugins -> Installed Plugins in the navigation menu, then selecting a specific plugin) provides in-depth information about which features any installed plugin is implementing or making use of. Initial Setup \u00b6 Use a Development Environment, Not Production For Plugin Development You should not use your production environment for plugin development. For information on getting started with a development environment, check out Nautobot development guide . Plugin Structure \u00b6 Although the specific structure of a plugin is largely left to the discretion of its authors, a Nautobot plugin that makes use of all available plugin features described in this document could potentially look something like this: plugin_name/ - plugin_name/ - __init__.py # required - admin.py # Django Admin Interface - api/ - serializers.py # REST API Model serializers - urls.py # REST API URL patterns - views.py # REST API view sets - banner.py # Banners - custom_validators.py # Custom Validators - datasources.py # Loading Data from a Git Repository - filter_extensions.py # Extending Filters - filters.py # Filtersets for UI, REST API, and GraphQL Model Filtering - forms.py # UI Forms and Filter Forms - graphql/ - types.py # GraphQL Type Objects - homepage.py # Home Page Content - jinja_filters.py # Jinja Filters - jobs.py # Job classes - middleware.py # Request/response middleware - migrations/ - 0001_initial.py # Database Models - models.py # Database Models - navigation.py # Navigation Menu Items - secrets.py # Secret Providers - signals.py # Signal Handler Functions - template_content.py # Extending Core Templates - templates/ - plugin_name/ - *.html # UI content templates - urls.py # UI URL Patterns - views.py # UI Views and any view override definitions - pyproject.toml # *** REQUIRED *** - Project package definition - README.md The top level is the project root. Immediately within the root should exist several items: pyproject.toml - This is the new unified Python project settings file that replaces setup.py , requirements.txt , and various other setup files (like setup.cfg , MANIFEST.in , among others). README.md - A brief introduction to your plugin, how to install and configure it, where to find help, and any other pertinent information. It is recommended to write README files using a markup language such as Markdown. The plugin source directory, with the same name as your plugin. The plugin source directory contains all of the actual Python code and other resources used by your plugin. Its structure is left to the author's discretion, however it is recommended to follow best practices as outlined in the Django documentation . At a minimum, this directory must contain an __init__.py file containing an instance of Nautobot's PluginConfig class. Note Nautobot includes a command to help create the plugin directory: nautobot-server startplugin [app_name] Please see the Nautobot Server Guide for more information. Create pyproject.toml \u00b6 Poetry Init (Recommended) \u00b6 To get started with a project using Python Poetry you use the poetry init command. This will guide you through the prompts necessary to generate a pyproject.toml with details required for packaging. This command will guide you through creating your pyproject.toml config. Package name [tmp]: nautobot-animal-sounds Version [0.1.0]: Description []: An example Nautobot plugin Author [, n to skip]: Bob Jones License []: Apache 2.0 Compatible Python versions [^3.8]: ^3.7 Would you like to define your main dependencies interactively? (yes/no) [yes] no Would you like to define your development dependencies interactively? (yes/no) [yes] no Generated file [tool.poetry] name = \"nautobot-animal-sounds\" version = \"0.1.0\" description = \"An example Nautobot plugin\" authors = [\"Bob Jones\"] license = \"Apache 2.0\" [tool.poetry.dependencies] python = \"^3.7\" [tool.poetry.dev-dependencies] [build-system] requires = [\"poetry-core>=1.0.0\"] build-backend = \"poetry.core.masonry.api\" Do you confirm generation? (yes/no) [yes] Define a PluginConfig \u00b6 The PluginConfig class is a Nautobot-specific wrapper around Django's built-in AppConfig class. It is used to declare Nautobot plugin functionality within a Python package. Each plugin should provide its own subclass, defining its name, metadata, and default and required configuration parameters. An example is below: from nautobot.extras.plugins import PluginConfig class AnimalSoundsConfig ( PluginConfig ): name = 'nautobot_animal_sounds' verbose_name = 'Animal Sounds' description = 'An example plugin for development purposes' version = '0.1' author = 'Bob Jones' author_email = 'bob@example.com' base_url = 'animal-sounds' required_settings = [] default_settings = { 'loud' : False } config = AnimalSoundsConfig Nautobot looks for the config variable within a plugin's __init__.py to load its configuration. Typically, this will be set to the PluginConfig subclass, but you may wish to dynamically generate a PluginConfig class based on environment variables or other factors. Required PluginConfig Attributes \u00b6 Name Description author Name of plugin's author author_email Author's public email address description Brief description of the plugin's purpose name Raw plugin name; same as the plugin's source directory verbose_name Human-friendly name for the plugin version Current release ( semantic versioning is encouraged) Optional PluginConfig Attributes \u00b6 Name Default Description base_url Same as specified name Base path to use for plugin URLs caching_config {\"*\":{\"ops\":\"all\"}} Plugin-specific query caching configuration config_view_name None URL name for a \"configuration\" view defined by this plugin default_settings {} A dictionary of configuration parameters and their default values home_view_name None URL name for a \"home\" or \"dashboard\" view defined by this plugin docs_view_name None URL name for a \"documentation\" view defined by this plugin installed_apps [] A list of additional Django application dependencies to automatically enable when the plugin is activated (you must still make sure these underlying dependent libraries are installed) max_version None Maximum version of Nautobot with which the plugin is compatible middleware [] A list of middleware classes to append after Nautobot's built-in middleware min_version None Minimum version of Nautobot with which the plugin is compatible required_settings [] A list of any configuration parameters that must be defined by the user Note All required_settings must be configured in PLUGINS_CONFIG in nautobot_config.py before the plugin can be used. Warning If a configuration parameter is listed in both required_settings and default_settings , the default setting will be ignored. PluginConfig Code Location Attributes \u00b6 The following PluginConfig attributes can be configured to customize where Nautobot will look to locate various pieces of plugin code. In most cases you will not need to change these, but they are provided as options in case your plugin has a non-standard organizational structure. Info As used below, a \"dotted path\" is the combination of a Python module path within the plugin and the name of a variable within that module. For example, \"template_content.template_extensions\" refers to a variable named template_extensions inside a template_content module located at the root of the plugin. Name Default Description banner_function \"banner.banner\" Dotted path to a function that can render a custom banner custom_validators \"custom_validators.custom_validators\" Dotted path to a list of custom validator classes datasource_contents \"datasources.datasource_contents\" Dotted path to a list of datasource (Git, etc.) content types to register graphql_types graphql.types.graphql_types Dotted path to a list of GraphQL type classes homepage_layout \"homepage.layout\" Dotted path to a list of home page items provided by the plugin jinja_filters \"jinja_filters\" Path to a module that contains Jinja2 filters to be registered jobs \"jobs.jobs\" Dotted path to a list of Job classes menu_items \"navigation.menu_items\" Dotted path to a list of navigation menu items provided by the plugin secrets_providers \"secrets.secrets_providers\" Dotted path to a list of secrets providers in the plugin template_extensions \"template_content.template_extensions\" Dotted path to a list of template extension classes Install the Plugin for Development \u00b6 The plugin needs to be installed into the same python environment where Nautobot is, so that we can get access to nautobot-server command, and also so that the nautobot-server is aware of the new plugin. If you installed Nautobot using Poetry, then go to the root directory of your clone of the Nautobot repository and run poetry shell there. Afterward, return to the root directory of your plugin to continue development. Otherwise if using the pip install or Docker workflows, manually activate nautobot using source /opt/nautobot/bin/activate . To install the plugin for development the following steps should be taken: Activate the Nautobot virtual environment (as detailed above) Navigate to the project root, where the pyproject.toml file exists for the plugin Execute the command poetry install to install the local package into the Nautobot virtual environment Note Poetry installs the current project and its dependencies in editable mode (aka \"development mode\" ). This should be done in development environment You should not use your production environment for plugin development. For information on getting started with a development environment, check out Nautobot development guide . $ poetry install Once the plugin has been installed, add it to the plugin configuration for Nautobot: PLUGINS = [ \"animal_sounds\" ] Verify that the Plugin is Installed \u00b6 In the Nautobot UI, navigate to Plugins -> Installed Plugins . The newly installed plugin should appear in the displayed table if everything is configured correctly. You can also click on the plugin's name in this table to view more detailed information about this plugin based on its PluginConfig and other contents. Extending the Existing Nautobot UI \u00b6 Extending Object Detail Views \u00b6 Plugins can inject custom content into certain areas of the detail views of applicable models. This is accomplished by subclassing PluginTemplateExtension , designating a particular Nautobot model, and defining the desired methods to render custom content. Four methods are available: left_page() - Inject content on the left side of the page right_page() - Inject content on the right side of the page full_width_page() - Inject content across the entire bottom of the page buttons() - Add buttons to the top of the page detail_tabs() - Add extra tabs to the end of the list of tabs within the page tabs navigation Additionally, a render() method is available for convenience. This method accepts the name of a template to render, and any additional context data you want to pass. Its use is optional, however. When a PluginTemplateExtension is instantiated, context data is assigned to self.context . Available data include: object - The object being viewed request - The current request settings - Global Nautobot settings config - Plugin-specific configuration parameters For example, accessing {{ request.user }} within a template will return the current user. Declared subclasses should be gathered into a list or tuple for integration with Nautobot. By default, Nautobot looks for an iterable named template_extensions within a template_content.py file. (This can be overridden by setting template_extensions to a custom value on the plugin's PluginConfig .) An example is below. # template_content.py from django.urls import reverse from nautobot.extras.plugins import PluginTemplateExtension from .models import Animal class SiteAnimalCount ( PluginTemplateExtension ): \"\"\"Template extension to display animal count on the right side of the page.\"\"\" model = 'dcim.site' def right_page ( self ): return self . render ( 'nautobot_animal_sounds/inc/animal_count.html' , extra_context = { 'animal_count' : Animal . objects . count (), }) class DeviceExtraTabs ( PluginTemplateExtension ): \"\"\"Template extension to add extra tabs to the object detail tabs.\"\"\" model = 'dcim.device' def detail_tabs ( self ): \"\"\" You may define extra tabs to render on a model's detail page by utilizing this method. Each tab is defined as a dict in a list of dicts. For each of the tabs defined: - The <title> key's value will become the tab link's title. - The <url> key's value is used to render the HTML link for the tab These tabs will be visible (in this instance) on the Device model's detail page as set by the DeviceContent.model attribute \"dcim.device\" This example demonstrates defining two tabs. The tabs will be ordered by their position in list. \"\"\" return [ { \"title\" : \"Plugin Tab 1\" , \"url\" : reverse ( \"plugins:example_plugin:device_detail_tab_1\" , kwargs = { \"pk\" : self . context [ \"object\" ] . pk }), }, { \"title\" : \"Plugin Tab 2\" , \"url\" : reverse ( \"plugins:example_plugin:device_detail_tab_2\" , kwargs = { \"pk\" : self . context [ \"object\" ] . pk }), }, ] template_extensions = [ DeviceExtraTabs , SiteAnimalCount ] Adding Extra Tabs \u00b6 Added in version 1.4.0 In order for any extra tabs to work properly, the \"url\" key must reference a view which inherits from the nautobot.core.views.generic.ObjectView class and the template must extend the object's detail template such as: <!-- example_plugin/tab_device_detail_1.html --> {% extends 'dcim/device.html' %} {% block content %} < h2 > Device Plugin Tab 1 </ h2 > < p > I am some content for the example plugin's device ({{ object.pk }}) detail tab 1. </ p > {% endblock %} Here's a basic example of a tab's view # views.py from nautobot.core.views import generic from nautobot.dcim.models import Device class DeviceDetailPluginTabOne ( generic . ObjectView ): \"\"\" This view's template extends the device detail template, making it suitable to show as a tab on the device detail page. Views that are intended to be for an object detail tab's content rendering must always inherit from nautobot.core.views.generic.ObjectView. \"\"\" queryset = Device . objects . all () template_name = \"example_plugin/tab_device_detail_1.html\" You must also add the view to the url_patterns like so (make sure to read the note after this code snippet): # urls.py from django.urls import path from example_plugin import views urlpatterns = [ # ... previously defined urls path ( \"devices/<uuid:pk>/example-plugin-tab-1/\" , views . DeviceDetailPluginTabOne . as_view (), name = \"device_detail_tab_1\" ), ] Note For added tab views, we recommend for consistency that you follow the URL pattern established by the base model detail view and tabs (if any). For example, nautobot/dcim/urls.py references Device tab views with the URL pattern devices/<uuid:pk>/TAB-NAME/ , so above we have followed that same pattern. Adding a Banner \u00b6 Added in version 1.2.0 A plugin can provide a function that renders a custom banner on any number of Nautobot views. By default Nautobot looks for a function banner() inside of banner.py . (This can be overridden by setting banner_function to a custom value on the plugin's PluginConfig .) This function currently receives a single argument, context , which is the Django request context in which the current page is being rendered. The function can return None if no banner is needed for a given page view, or can return a PluginBanner object describing the banner contents. Here's a simple example banner.py : # banner.py from django.utils.html import format_html from nautobot.extras.choices import BannerClassChoices from nautobot.extras.plugins import PluginBanner def banner ( context , * args , ** kwargs ): \"\"\"Greet the user, if logged in.\"\"\" # Request parameters can be accessed via context.request if not context . request . user . is_authenticated : # No banner if the user isn't logged in return None else : return PluginBanner ( content = format_html ( \"Hello, <strong> {} </strong>! \ud83d\udc4b\" , context . request . user ), banner_class = BannerClassChoices . CLASS_SUCCESS , ) Adding Navigation Menu Items \u00b6 Plugins can extend the existing navigation bar layout. By default, Nautobot looks for a menu_items list inside of navigation.py . (This can be overridden by setting menu_items to a custom value on the plugin's PluginConfig .) Using a key and weight system, a developer can integrate the plugin's menu additions amongst existing menu tabs, groups, items and buttons, and/or create entirely new menus as desired. More documentation and examples can be found in the Navigation Menu guide. Tip To reduce the amount of clutter in the navigation menu, if your plugin provides a \"plugin configuration\" view, we recommend linking it from the main \"Installed Plugins\" page rather than adding it as a separate item in the navigation menu. Similarly, if your plugin provides a \"plugin home\" or \"dashboard\" view, consider linking it from the \"Installed Plugins\" page, and/or adding a link from the Nautobot home page (see below), rather than adding it to the navigation menu. Adding Home Page Content \u00b6 Added in version 1.2.0 Plugins can add content to the Nautobot home page. By default, Nautobot looks for a layout list inside of homepage.py . (This can be overridden by setting homepage_layout to a custom value on the plugin's PluginConfig .) Using a key and weight system, a developer can integrate the plugin content amongst existing panels, groups, and items and/or create entirely new panels as desired. More documentation and examples can be found in the guide on Home Page Panels . Adding Links to the Installed Plugins View \u00b6 Added in version 1.2.0 It's common for many plugins to provide a \"plugin configuration\" view used for interactive configuration of aspects of the plugin that don't necessarily need to be managed by a system administrator via PLUGINS_CONFIG . The PluginConfig setting of config_view_name lets you provide the URL pattern name defined for this view, which will then be accessible via a button on the Plugins -> Installed Plugins UI view. For example, if the animal_sounds plugin provides a configuration view, which is set up in urls.py as follows: # urls.py from django.urls import path from . import views urlpatterns = [ path ( \"configuration/\" , views . AnimalSoundsConfigView . as_view (), name = \"config\" ), ] then in your AnimalSoundsConfig you could refer to the view by name: # __init__.py from nautobot.extras.plugins import PluginConfig class AnimalSoundsConfig ( PluginConfig ): # ... config_view_name = \"plugins:animal_sounds:config\" config = AnimalSoundsConfig and now the \"Configuration\" button that appears in the Installed Plugins table next to \"Animal Sounds\" will be a link to your configuration view. Similarly, if your plugin provides a \"plugin home\" or \"dashboard\" view, you can provide a link for the \"Home\" button in the Installed Plugins table by defining home_view_name on your PluginConfig class. This can also be done for documentation by defining docs_view_name on your PluginConfig class. Extending Existing Functionality \u00b6 Adding Jinja2 Filters \u00b6 Added in version 1.1.0 Plugins can define custom Jinja2 filters to be used when rendering templates defined in computed fields. Check out the official Jinja2 documentation on how to create filter functions. In the file that defines your filters (by default jinja_filters.py , but configurable in the PluginConfig if desired), you must import the library module from the django_jinja library. Filters must then be decorated with @library.filter . See an example below that defines a filter called leet_speak . from django_jinja import library @library . filter def leet_speak ( input_str ): charset = { \"a\" : \"4\" , \"e\" : \"3\" , \"l\" : \"1\" , \"o\" : \"0\" , \"s\" : \"5\" , \"t\" : \"7\" } output_str = \"\" for char in input_str : output_str += charset . get ( char . lower (), char ) return output_str This filter will then be available for use in computed field templates like so: {{ \"HELLO WORLD\" | leet_speak }} The output of this template results in the string \"H3110 W0R1D\" . Including Jobs \u00b6 Plugins can provide Jobs to take advantage of all the built-in functionality provided by that feature (user input forms, background execution, results logging and reporting, etc.). By default, for each plugin, Nautobot looks for an iterable named jobs within a jobs.py file. (This can be overridden by setting jobs to a custom value on the plugin's PluginConfig .) A brief example is below; for more details on Job design and implementation, refer to the Jobs feature documentation. # jobs.py from nautobot.extras.jobs import Job class CreateDevices ( Job ): ... class DeviceConnectionsReport ( Job ): ... class DeviceIPsReport ( Job ): ... jobs = [ CreateDevices , DeviceConnectionsReport , DeviceIPsReport ] Implementing Custom Validators \u00b6 Plugins can register custom validator classes which implement model validation logic to be executed during a model's clean() method. Like template extensions, custom validators are registered to a single model and offer a method which plugin authors override to implement their validation logic. This is accomplished by subclassing PluginCustomValidator and implementing the clean() method. Plugin authors must raise django.core.exceptions.ValidationError within the clean() method to trigger validation error messages which are propagated to the user and prevent saving of the model instance. A convenience method validation_error() may be used to simplify this process. Raising a ValidationError is no different than vanilla Django, and the convenience method will simply pass the provided message through to the exception. When a PluginCustomValidator is instantiated, the model instance is assigned to context dictionary using the object key, much like PluginTemplateExtensions. E.g. self.context['object'] . Declared subclasses should be gathered into a list or tuple for integration with Nautobot. By default, Nautobot looks for an iterable named custom_validators within a custom_validators.py file. (This can be overridden by setting custom_validators to a custom value on the plugin's PluginConfig .) An example is below. # custom_validators.py from nautobot.extras.plugins import PluginCustomValidator class SiteValidator ( PluginCustomValidator ): \"\"\"Custom validator for Sites to enforce that they must have a Region.\"\"\" model = 'dcim.site' def clean ( self ): if self . context [ 'object' ] . region is None : # Enforce that all sites must be assigned to a region self . validation_error ({ \"region\" : \"All sites must be assigned to a region\" }) custom_validators = [ SiteValidator ] Loading Data from a Git Repository \u00b6 It's possible for a plugin to register additional types of data that can be provided by a Git repository and be automatically notified when such a repository is refreshed with new data. By default, Nautobot looks for an iterable named datasource_contents within a datasources.py file. (This can be overridden by setting datasource_contents to a custom value on the plugin's PluginConfig .) An example is below. # datasources.py import yaml import os from nautobot.extras.choices import LogLevelChoices from nautobot.extras.registry import DatasourceContent from .models import Animal def refresh_git_animals ( repository_record , job_result , delete = False ): \"\"\"Callback for GitRepository updates - refresh Animals managed by it.\"\"\" if 'nautobot_animal_sounds.Animal' not in repository_record . provided_contents or delete : # This repository is defined not to provide Animal records. # In a more complete worked example, we might want to iterate over any # Animals that might have been previously created by this GitRepository # and ensure their deletion, but for now this is a no-op. return # We have decided that a Git repository can provide YAML files in a # /animals/ directory at the repository root. animal_path = os . path . join ( repository_record . filesystem_path , 'animals' ) for filename in os . listdir ( animal_path ): with open ( os . path . join ( animal_path , filename )) as fd : animal_data = yaml . safe_load ( fd ) # Create or update an Animal record based on the provided data animal_record , created = Animal . objects . update_or_create ( name = animal_data [ 'name' ], defaults = { 'sound' : animal_data [ 'sound' ]} ) # Record the outcome in the JobResult record job_result . log ( \"Successfully created/updated animal\" , obj = animal_record , level_choice = LogLevelChoices . LOG_SUCCESS , grouping = \"animals\" , ) # Register that Animal records can be loaded from a Git repository, # and register the callback function used to do so datasource_contents = [ ( 'extras.gitrepository' , # datasource class we are registering for DatasourceContent ( name = 'animals' , # human-readable name to display in the UI content_identifier = 'nautobot_animal_sounds.animal' , # internal slug to identify the data type icon = 'mdi-paw' , # Material Design Icons icon to use in UI callback = refresh_git_animals , # callback function on GitRepository refresh ) ) ] With this code, once your plugin is installed, the Git repository creation/editing UI will now include \"Animals\" as an option for the type(s) of data that a given repository may provide. If this option is selected for a given Git repository, your refresh_git_animals function will be automatically called when the repository is synced. Populating Extensibility Features \u00b6 Added in version 1.2.0 In many cases, a plugin may wish to make use of Nautobot's various extensibility features, such as custom fields or relationships . It can be useful for a plugin to automatically create a custom field definition or relationship definition as a consequence of being installed and activated, so that everyday usage of the plugin can rely upon these definitions to be present. To make this possible, Nautobot provides a custom signal , nautobot_database_ready , that plugins can register to listen for. This signal is triggered when nautobot-server migrate or nautobot-server post_upgrade is run after installing a plugin, and provides an opportunity for the plugin to make any desired additions to the database at this time. For example, maybe we want our plugin to make use of a Relationship allowing each Site to be linked to our Animal model. We would define our callback function that makes sure this Relationship exists, by convention in a signals.py file: # signals.py from nautobot.extras.choices import RelationshipTypeChoices def create_site_to_animal_relationship ( sender , apps , ** kwargs ): \"\"\"Create a Site-to-Animal Relationship if it doesn't already exist.\"\"\" # Use apps.get_model to look up Nautobot core models ContentType = apps . get_model ( \"contenttypes\" , \"ContentType\" ) Relationship = apps . get_model ( \"extras\" , \"Relationship\" ) Site = apps . get_model ( \"dcim\" , \"Site\" ) # Use sender.get_model to look up models from this plugin Animal = sender . get_model ( \"Animal\" ) # Ensure that the Relationship exists Relationship . objects . update_or_create ( slug = \"site-favorite-animal\" , defaults = { \"name\" : \"Site's Favorite Animal\" , \"type\" : RelationshipTypeChoices . TYPE_ONE_TO_MANY , \"source_type\" : ContentType . objects . get_for_model ( Animal ), \"source_label\" : \"Sites that love this Animal\" , \"destination_type\" : ContentType . objects . get_for_model ( Site ), \"destination_label\" : \"Favorite Animal\" , }, ) Then, in the PluginConfig ready() function, we connect this callback function to the nautobot_database_ready signal: # __init__.py from nautobot.core.signals import nautobot_database_ready from nautobot.extras.plugins import PluginConfig from .signals import create_site_to_animal_relationship class AnimalSoundsConfig ( PluginConfig ): # ... def ready ( self ): super () . ready () nautobot_database_ready . connect ( create_site_to_animal_relationship , sender = self ) config = AnimalSoundsConfig After writing this code, run nautobot-server migrate or nautobot-server post_upgrade , then restart the Nautobot server, and you should see that this custom Relationship has now been automatically created. Implementing Secrets Providers \u00b6 A plugin can define and register additional providers (sources) for Secrets , allowing Nautobot to retrieve secret values from additional systems or data sources. By default, Nautobot looks for an iterable named secrets_providers within a secrets.py file. (This can be overridden by setting secrets_providers to a custom value on the plugin's PluginConfig .) To define a new SecretsProvider subclass, we must specify the following: A unique slug string identifying this provider A human-readable name string (optional; the slug will be used if this is not specified) A Django form for entering the parameters required by this provider, as an inner class named ParametersForm An implementation of the get_value_for_secret() API to actually retrieve the value of a given secret For a simple (insecure!) example, we could define a \"constant-value\" provider that simply stores a constant value in Nautobot itself and returns this value on demand. Warning This is an intentionally simplistic example and should not be used in practice! Sensitive secret data should never be stored directly in Nautobot's database itself. # secrets.py from nautobot.extras.secrets import SecretsProvider class ConstantValueSecretsProvider ( SecretsProvider ): \"\"\" Example SecretsProvider - this one just returns a user-specified constant value. Obviously this is insecure and not something you'd want to actually use! \"\"\" slug = \"constant-value\" name = \"Constant Value\" class ParametersForm ( BootstrapMixin , forms . Form ): \"\"\" User-friendly form for specifying the required parameters of this provider. \"\"\" constant = forms . CharField ( required = True , help_text = \"Constant secret value. <strong>DO NOT USE FOR REAL DATA</strong>\" ) @classmethod def get_value_for_secret ( cls , secret , obj = None , ** kwargs ): \"\"\" Return the value defined in the Secret.parameters \"constant\" key. A more realistic SecretsProvider would make calls to external APIs, etc., to retrieve a secret from another system as desired. Args: secret (nautobot.extras.models.Secret): The secret whose value should be retrieved. obj (object): The object (Django model or similar) providing context for the secret's parameters. \"\"\" return secret . rendered_parameters ( obj = obj ) . get ( \"constant\" ) secrets_providers = [ ConstantValueSecretsProvider ] After installing and enabling your plugin, you should now be able to navigate to Secrets > Secrets and create a new Secret, at which point \"constant-value\" should now be available as a new secrets provider to use. Extending Filters \u00b6 Added in version 1.3.0 Plugins can extend any model-based FilterSet and FilterForm classes that are provided by the Nautobot core. The requirements to extend a filter set or a filter form (or both) are: The file must be named filter_extensions.py The variable filter_extensions must be declared in that file, and contain a list of PluginFilterExtension subclasses The model attribute of each PluginFilterExtension subclass must be set to a valid model name in the dotted pair format ( {app_label}.{model} , e.g. tenant.tenant or dcim.device ) Nautobot dynamically creates many additional filters based upon the defined filter type. Specifically, there are additional lookup expressions (referred to in code as lookup_expr ) that are created for each filter, when there is neither a lookup_expr nor method parameter already set. These dynamically-added lookup expressions are added using a shorthand notation (e.g. icontains is ic ). Nautobot will also add the negation of each, for example, so icontains will be added along with not icontains using the ic and nic expressions respectively. The dynamically-added lookup expressions can be found in the source code at nautobot/utilities/constants.py and the mapping logic can be found in nautobot/utilities/filters.py . Please see the documentation on filtering for more information. Tip For developers of plugins that define their own model filters, note that the above are added dynamically, as long as the class inherits from nautobot.utilities.filters.BaseFilterSet . However, that does not cover every possible use case, to list a few examples: Usage of a custom method argument on a filter that points to a FilterSet method, which would allow arbitrary filtering using custom logic. This is how the q field search logic is currently performed. Creation of a filter on a field that does not currently have filtering support Convenience methods for highly nested fields There are several conditions that must be met in order to extend a filter: The original FilterSet must follow the pattern: f\"{model.__name__}FilterSet\" e.g. TenantFilterSet The PluginFilterExtension.filterset_fields attribute must be a valid dict, with each key being the filter name (which must start with the plugin's name + _ , e.g. \"example_plugin_description\" , not merely \"description\" ) and each value being a valid django-filter filter Nautobot will dynamically generate the additional relevant lookup expressions of a plugin's defined custom FilterSet field, so no need to additionally register example_plugin_description__ic , etc. Similar to FilterSet fields, Nautobot provides a default filter form for each model, however that does not cover every possible use case. To list a few examples of why one may want to extend a filter form: The base filter form does not include a custom filter defined by the plugin as described above The base filter form does not provide a specific lookup expression to a filterable field, such as allowing regex on name There are several conditions that must be met in order to extend a filter: The original FilterForm must follow the pattern: f\"{model.__name__}FilterForm\" , e.g. TenantFilterForm The filterform_fields attribute must be a valid dictionary of Django form fields Note A plugin is not required to define both filterset_fields and filterform_fields . You can view an example of filter_extensions.py by viewing the one provided with the Example Plugin. Tip The method parameter, if used, must be a callable (method/function). Note that because filters with a method do their filtering in Python code rather than at the database level, performance of method filters is generally much poorer than pure-database filters. The method parameter is not supported when using Dynamic Groups . Adding Database Models \u00b6 If your plugin introduces a new type of object in Nautobot, you'll probably want to create a Django model for it. A model is essentially a Python representation of a database table, with attributes that represent individual columns. Model instances can be created, manipulated, and deleted using queries . Models must be defined within a file named models.py . It is highly recommended to have plugin models inherit from at least nautobot.core.models.BaseModel which provides base functionality and convenience methods common to all models. For more advanced usage, you may want to instead inherit from one of Nautobot's \"generic\" models derived from BaseModel -- nautobot.core.models.generics.OrganizationalModel or nautobot.core.models.generics.PrimaryModel . The inherent capabilities provided by inheriting from these various parent models differ as follows: Feature django.db.models.Model BaseModel OrganizationalModel PrimaryModel UUID primary key \u274c \u2705 \u2705 \u2705 Object permissions \u274c \u2705 \u2705 \u2705 validated_save() \u274c \u2705 \u2705 \u2705 Change logging \u274c \u274c \u2705 \u2705 Custom fields \u274c \u274c \u2705 \u2705 Relationships \u274c \u274c \u2705 \u2705 Note \u274c \u274c \u2705 \u2705 Tags \u274c \u274c \u274c \u2705 Note When using OrganizationalModel or PrimaryModel , you also must use the @extras_features decorator to specify support for (at a minimum) the \"custom_fields\" and \"relationships\" features. Below is an example models.py file containing a basic model with two character fields: # models.py from django.db import models from nautobot.core.models import BaseModel class Animal ( BaseModel ): \"\"\"Base model for animals.\"\"\" name = models . CharField ( max_length = 50 ) sound = models . CharField ( max_length = 50 ) def __str__ ( self ): return self . name Once you have defined the model(s) for your plugin, you'll need to create the database schema migrations. A migration file is essentially a set of instructions for manipulating the database to support your new model, or to alter existing models. Creating migrations can be done automatically using the nautobot-server makemigrations <plugin_name> management command, where <plugin_name> is the name of the Python package for your plugin (e.g. animal_sounds ): $ nautobot-server makemigrations nautobot_animal_sounds Note A plugin must be installed before it can be used with Django management commands. If you skipped this step above, run poetry install from the plugin's root directory. $ nautobot-server makemigrations nautobot_animal_sounds Migrations for 'nautobot_animal_sounds': /home/bjones/animal_sounds/nautobot_animal_sounds/migrations/0001_initial.py - Create model Animal Next, apply the migration to the database with the nautobot-server migrate <plugin_name> command: $ nautobot-server migrate nautobot_animal_sounds Operations to perform: Apply all migrations: nautobot_animal_sounds Running migrations: Applying nautobot_animal_sounds.0001_initial... OK For more background on schema migrations, see the Django documentation . Using the Django Admin Interface \u00b6 Plugins can optionally expose their models via Django's built-in administrative interface . This can greatly improve troubleshooting ability, particularly during development. To expose a model, simply register it using Django's admin.register() function. An example admin.py file for the above model is shown below: # admin.py from nautobot.core.admin import NautobotModelAdmin from .models import Animal @admin . register ( Animal ) class AnimalAdmin ( NautobotModelAdmin ): list_display = ( 'name' , 'sound' ) This will display the plugin and its model in the admin UI. Staff users can create, change, and delete model instances via the admin UI without needing to create a custom view. Integrating with GraphQL \u00b6 Plugins can optionally expose their models via the GraphQL interface to allow the models to be part of the Graph and to be queried easily. There are two mutually exclusive ways to expose a model to the GraphQL interface. By using the @extras_features decorator By creating your own GraphQL type definition and registering it within graphql/types.py of your plugin (the decorator should not be used in this case) All GraphQL model types defined by your plugin, regardless of which method is chosen, will automatically support some built-in Nautobot features: Support for object permissions based on their associated Model class Include any custom fields defined for their Model Include any relationships defined for their Model Include tags , if the Model supports them Using the @extras_features Decorator for GraphQL \u00b6 To expose a model via GraphQL, simply register it using the @extras_features(\"graphql\") decorator. Nautobot will detect this and will automatically create a GraphQL type definition based on the model. Additionally, if a FilterSet is available at <app_name>.filters.<ModelName>FilterSet , Nautobot will automatically use the filterset to generate GraphQL filtering options for this type as well. # models.py from django.db import models from nautobot.core.models import BaseModel from nautobot.extras.utils import extras_features @extras_features ( \"graphql\" ) class Animal ( BaseModel ): \"\"\"Base model for animals.\"\"\" name = models . CharField ( max_length = 50 ) sound = models . CharField ( max_length = 50 ) def __str__ ( self ): return self . name Creating Your Own GraphQL Type Object \u00b6 In some cases, such as when a model is using Generic Foreign Keys, or when a model has constructed fields that should also be reflected in GraphQL, the default GraphQL type definition generated by the @extras_features decorator may not work as the developer intends, and it will be preferable to provide custom GraphQL types. By default, Nautobot looks for custom GraphQL types in an iterable named graphql_types within a graphql/types.py file. (This can be overridden by setting graphql_types to a custom value on the plugin's PluginConfig .) Each type defined in this way must be a class inheriting from graphene_django.DjangoObjectType or graphene_django_optimizer.OptimizedDjangoObjectType and must follow the standards defined by graphene-django . Nautobot uses a library called graphene-django-optimizer to decrease the time queries take to process. By inheriting from graphene_django_optimizer type classes are automatically optimized. Warning When defining types this way, do not use the @extras_features(\"graphql\") decorator on the corresponding Model class, as no auto-generated GraphQL type is desired for this model. # graphql/types.py import graphene_django_optimizer as gql_optimizer from nautobot_animal_sounds.models import Animal class AnimalType ( gql_optimizer . OptimizedDjangoObjectType ): \"\"\"GraphQL Type for Animal\"\"\" class Meta : model = Animal exclude = [ \"sound\" ] graphql_types = [ AnimalType ] Using GraphQL ORM Utilities \u00b6 GraphQL utility functions: execute_query() : Runs string as a query against GraphQL. execute_saved_query() : Execute a saved query from Nautobot database. Both functions have the same arguments other than execute_saved_query() which requires a slug to identify the saved query rather than a string holding a query. For authentication either a request object or user object needs to be passed in. If there is none, the function will error out. Arguments: execute_query() : query (str): String with GraphQL query. variables (dict, optional): If the query has variables they need to be passed in as a dictionary. request (django.test.client.RequestFactory, optional): Used to authenticate. user (django.contrib.auth.models.User, optional): Used to authenticate. execute_saved_query() : saved_query_slug (str): Slug of a saved GraphQL query. variables (dict, optional): If the query has variables they need to be passed in as a dictionary. request (django.test.client.RequestFactory, optional): Used to authenticate. user (django.contrib.auth.models.User, optional): Used to authenticate. Returned is a GraphQL object which holds the same data as returned from GraphiQL. Use execute_query().to_dict() to get the data back inside of a dictionary. Adding Web UI Views \u00b6 If your plugin needs its own page or pages in the Nautobot web UI, you'll need to define views. A view is a particular page tied to a URL within Nautobot, which renders content using a template. Views are typically defined in views.py , and URL patterns in urls.py . As an example, let's write a view which displays a random animal and the sound it makes. First, create the view in views.py : # views.py from django.shortcuts import render from django.views.generic import View from .models import Animal class RandomAnimalView ( View ): \"\"\"Display a randomly-selected Animal.\"\"\" def get ( self , request ): animal = Animal . objects . order_by ( '?' ) . first () return render ( request , 'nautobot_animal_sounds/animal.html' , { 'animal' : animal , }) This view retrieves a random animal from the database and and passes it as a context variable when rendering a template named animal.html , which doesn't exist yet. To create this template, first create a directory named templates/nautobot_animal_sounds/ within the plugin source directory. (We use the plugin's name as a subdirectory to guard against naming collisions with other plugins.) Then, create a template named animal.html as described below. Utilizing Nautobot Generic Views \u00b6 Starting in Nautobot 1.1.0 via PR , some generic views have been exposed to help aid in plugin development. These views have some requirements that must be in place in order to work. These can be used by importing them from from nautobot.core.views import generic . More documentation and examples can be found in Generic Views guide. Extending the Base Template \u00b6 Nautobot provides a base template to ensure a consistent user experience, which plugins can extend with their own content. This template includes four content blocks: title - The page title header - The upper portion of the page content - The main page body javascript - A section at the end of the page for including Javascript code For more information on how template blocks work, consult the Django documentation . {# templates/nautobot_animal_sounds/animal.html #} {% extends 'base.html' %} {% block content %} {% with config=settings.PLUGINS_CONFIG.nautobot_animal_sounds %} <h2 class=\"text-center\" style=\"margin-top: 200px\"> {% if animal %} The {{ animal.name|lower }} says {% if config.loud %} {{ animal.sound|upper }}! {% else %} {{ animal.sound }} {% endif %} {% else %} No animals have been created yet! {% endif %} </h2> {% endwith %} {% endblock %} The first line of the template instructs Django to extend the Nautobot base template and inject our custom content within its content block. Note Django renders templates with its own custom template language . This template language is very similar to Jinja2, however there are some important differences to keep in mind. Registering URL Patterns \u00b6 Finally, to make the view accessible to users, we need to register a URL for it. We do this in urls.py by defining a urlpatterns variable containing a list of paths. # urls.py from django.urls import path from . import views urlpatterns = [ path ( 'random/' , views . RandomAnimalView . as_view (), name = 'random_animal' ), ] A URL pattern has three components: route - The unique portion of the URL dedicated to this view view - The view itself name - A short name used to identify the URL path internally This makes our view accessible at the URL /plugins/animal-sounds/random/ . (Remember, our AnimalSoundsConfig class sets our plugin's base URL to animal-sounds .) Viewing this URL should show the base Nautobot template with our custom content inside it. Tip As a next step, you would typically want to add links from the Nautobot UI to this view, either from the navigation menu , the Nautobot home page , and/or the Installed Plugins view . NautobotUIViewSet \u00b6 Added in version 1.4.0 New in Nautobot 1.4 is the debut of NautobotUIViewSet : A powerful plugin development tool that can save plugin developer hundreds of lines of code compared to using legacy generic.views . Using it to gain access to default functionalities previous provided by generic.views such as create() , bulk_create() , update() , partial_update() , bulk_update() , destroy() , bulk_destroy() , retrieve() and list() actions. Note that this ViewSet is catered specifically to the UI, not the API. Concrete examples on how to use NautobotUIViewSet resides in nautobot.circuits.views . Below we provide an example on how to use NautobotUIViewSet on a theoretical plugin model. from nautobot.core.views.viewsets import NautobotUIViewset class YourPluginModelUIViewSet ( NautobotUIViewSet ): bulk_create_form_class = YourPluginModelCSVForm bulk_update_form_class = YourPluginModelBulkEditForm filterset_class = YourPluginModelFilterSet filterset_form_class = YourPluginModelFilterForm form_class = YourPluginModelForm queryset = YourPluginModel . objects . all () serializer_class = serializers . YourPluginModelSerializer table_class = YourPluginModelTable Setting ViewSet Attributes \u00b6 One caveat of using the NautobotUIViewSet is that the queryset , serializer_class and table_class attribute of the YourPluginModelUIViewSet has to be set before most of the NautobotUIViewSet functionalities will become available. By default the URL patterns generated by a NautobotUIViewSet are based on the model's slug ( /model-name/<slug>/ for the detail view, /model-name/<slug>/edit/ for the edit view, etc.). If your model lacks a slug field, or if you otherwise need to use a different field to look up an object, just override the default lookup_field in your ViewSet attributes: from nautobot.core.views.viewsets import NautobotUIViewset class YourPluginModelUIViewSet ( NautobotUIViewSet ): ... lookup_field = \"pk\" ... View Template Context \u00b6 Templates can benefit from a very rich context passed down from the views and renderer, including forms, tables, as well as any other information that may be helpful for rendering templates. The keys it provides are as follows: content_type : The ContentType object for the associated model filter_form : The FilterForm object for the associated model form : A Form object for the associated model if relevant ( None for list and detail/retrieve views) object : An instance of the associated mode if available ( None for list and bulk operation views) permissions : Summary of user permissions for the given model return_url : The relevant return URL table : A Table object for the associated model if relevant ( None for detail/retrieve and update views) table_config_form : A TableConfigForm object for the associated table , providing the ability to customize the table verbose_name : The singular form of the model's name verbose_name_plural : The plural form of the model's name An example from editing a Provider object: { 'content_type' : < ContentType : circuits | provider > , 'filter_form' : < ProviderFilterForm bound = True , valid = Unknown , fields = ( region ; site ; location ; q ; asn ; tag ) > , 'form' : < ProviderForm bound = False , valid = Unknown , fields = ( name ; slug ; asn ; account ; portal_url ; noc_contact ; admin_contact ; comments ; tags ; object_note ) > , 'object' : < Provider : NautobotProvider > , 'permissions' : { 'add' : True , 'change' : True , 'delete' : True , 'view' : True }, 'return_url' : '/circuits/providers/nautobotprovider' , 'table' : None , 'table_config_form' : None , 'verbose_name' : 'provider' , 'verbose_name_plural' : 'providers' } Other context keys may be available for certain views: editing : Provided for create and update views to help the template determine if this is a new or existing object action_buttons : Provided for the list view for the top of table buttons (such as \"Add\" and \"Export\") You may see other context keys as well, but any not documented above should not be relied upon as they may be removed in a future release. Some examples of those are: changelog_url : This can now be retrieved from the object itself, via object.get_changelog_url , if the object supports change-logging obj : Please use object instead obj_type : Please use verbose_name instead obj_type_plural : Please use verbose_name_plural instead Excluding ViewMixins from NautobotUIViewSet \u00b6 For plugin models that do not require certain views, simply inherit directly from the ViewMixins available in nautobot.core.views.mixins instead of NautobotUIViewSet . Concrete examples for excluding ViewMixins , checkout CircuitTerminationUIViewSet and CircuitTypeUIViewSet in nautobot.circuits.views . ## A plugin model viewset that does not support bulk views and operations from nautobot.core.views import mixins as view_mixins class YourPluginModelUIViewSet ( view_mixins . ObjectListViewMixin , view_mixins . ObjectDetailViewMixin , view_mixins . ObjectEditViewMixin , view_mixins . ObjectDestroyViewMixin , ): filterset_class = YourPluginModelFilterSet filterset_form_class = YourPluginModelFilterForm form_class = YourPluginModelForm queryset = YourPluginModel . objects . all () serializer_class = serializers . YourPluginModelSerializer table_class = YourPluginModelTable # You do not need to specify attributes that are not needed. Excluding unwanted urls from NautobotUIViewSetRouter is done for you at the ViewSet level. If you do not inherit the unwanted ViewMixins, the corresponding route from the router will not be published. # urls.py # All the urls correspond to BulkViewMixins will not be published when you register your ViewSet with the router. router . register ( \"yourpluginmodel\" , views . YourPluginModelUIViewSet ) Template Naming for NautobotUIViewSet \u00b6 Template naming is very intuitive in NautobotUIViewSet. In templates/yourpluginmodel folder, name your templates following this convention {app_label}/{model_name}_{self.action}.html . ViewMixins self.action ObjectListViewMixin list ObjectDetailViewMixin retrieve ObjectEditViewMixin create/update ObjectDestroyViewMixin destroy ObjectBulkDestroyViewMixin bulk_destroy ObjectBulkCreateViewMixin bulk_create ObjectBulkUpdateViewMixin bulk_update For example, for a DetailView template for YourPluginModel , the template name will be yourplugin/yourpluginmodel_retrieve.html , for a BulkCreateView template for yourpluginmodel , the template name will be yourplugin/yourpluginmodel_bulk_create.html and etc. If you do not provide your own templates in the yourplugin/templates/yourplugin folder, NautobotUIViewSet will fall back to generic/object_{self.action}.html . Since in many cases the create and update templates for a model will be identical, you are not required to create both. If you provide a {app_label}/{model_opts.model_name}_create.html file but not a {app_label}/{model_opts.model_name}_update.html file, then when you update an object, it will fall back to {app_label}/{model_opts.model_name}_create.html and vice versa. NautobotUIViewSetRouter \u00b6 With NautobotUIViewSet as the base UI ViewSet for YourPluginModel , it is required to register your urls with the help of NautobotUIViewSetRouter . For a concrete example on how to use NautobotUIViewSetRouter , see nautobot.circuits.urls . Below is a theoretical urls.py file for YourPluginModel : from django.urls import path from nautobot.core.views.routers import NautobotUIViewSetRouter from nautobot.plugins import views router = NautobotUIViewSetRouter () router . register ( \"yourpluginmodel\" , views . YourPluginModelUIViewSet ) urlpatterns = [ # Extra urls that do not follow the patterns of `NautobotUIViewSetRouter` go here. # changelog, notes and etc. ... path ( \"yourpluginmodels/<slug:slug>/changelog/\" , ObjectChangeLogView . as_view (), name = \"yourpluginmodel_changelog\" , kwargs = { \"model\" : yourpluginmodel }, ), path ( \"yourpluginmodels/<slug:slug>/notes/\" , ObjectNotesView . as_view (), name = \"yourpluginmodel_notes\" , kwargs = { \"model\" : yourpluginmodel }, ), ... ] urlpatterns += router . urls Adding REST API Endpoints \u00b6 Plugins can declare custom endpoints on Nautobot's REST API to retrieve or manipulate models or other data. These behave very similarly to views, except that instead of rendering arbitrary content using a template, data is returned in JSON format using a serializer. Nautobot uses the Django REST Framework , which makes writing API serializers and views very simple. First, create a serializer for the Animal model, in api/serializers.py : # api/serializers.py from rest_framework.serializers import ModelSerializer from nautobot_animal_sounds.models import Animal class AnimalSerializer ( ModelSerializer ): \"\"\"API serializer for interacting with Animal objects.\"\"\" class Meta : model = Animal fields = ( 'id' , 'name' , 'sound' ) Next, create a generic API view set that allows basic CRUD (create, read, update, and delete) operations for Animal instances. This is defined in api/views.py : # api/views.py from rest_framework.viewsets import ModelViewSet from nautobot_animal_sounds.models import Animal from .serializers import AnimalSerializer class AnimalViewSet ( ModelViewSet ): \"\"\"API viewset for interacting with Animal objects.\"\"\" queryset = Animal . objects . all () serializer_class = AnimalSerializer Finally, register a URL for our endpoint in api/urls.py . This file must define a variable named urlpatterns . # api/urls.py from rest_framework import routers from .views import AnimalViewSet router = routers . DefaultRouter () router . register ( 'animals' , AnimalViewSet ) urlpatterns = router . urls With these three components in place, we can request /api/plugins/animal-sounds/animals/ to retrieve a list of all Animal objects defined. Warning This example is provided as a minimal reference implementation only. It does not address authentication, performance, or the myriad of other concerns that plugin authors should have. Adding Help Documentation \u00b6 If you are using the generic.ObjectEditView from Nautobot for your object, the form can automatically include a help icon with a link to that object's documentation. For this to happen, Nautobot must be able to find the documentation for this object in a specific directory tree within your plugin: plugin_name/ # \"nautobot_animal_sounds\" - static/ - plugin_name/ # \"nautobot_animal_sounds\" - docs/ - index.html - models/ - object_model.html # \"animal.html\" Overriding Existing Functionality \u00b6 Replacing Views \u00b6 Added in version 1.4.0 You may override any of the core or plugin views by providing an override_views dict in a plugin's views.py file. To override a view, you must specify the view's fully qualified name as the dict key which consists of the app name followed by the view's name separated by a colon, for instance dcim:device . The dict value should be the overriding view function. A simple example to override the device detail view: # views.py from django.shortcuts import HttpResponse from nautobot.core.views import generic class DeviceViewOverride ( generic . View ): def get ( self , request , * args , ** kwargs ): return HttpResponse (( \"Hello world! I'm a view which \" \"overrides the device object detail view.\" )) override_views = { \"dcim:device\" : DeviceViewOverride . as_view (), } Note URL Endpoint \u00b6 Added in version 1.4.0 Models that inherit from PrimaryModel and OrganizationalModel can have notes associated. In order to utilize this new feature you will need to add the endpoint to urls.py . Here is an option to be able to support both 1.4+ and older versions of Nautobot: urlpatterns = [ path ( 'random/' , views . RandomAnimalView . as_view (), name = 'random_animal' ), ] try : from nautobot.extras.views import ObjectNotesView urlpatterns . append ( path ( 'random/<slug:slug>/notes/), ObjectNotesView . as_view (), name = \"random_notes\" , kwargs = { \"model\" : Random }, ) ) except ImportError : pass","title":"Developing Plugins"},{"location":"plugins/development.html#plugin-development","text":"This documentation covers the development of custom plugins for Nautobot. Plugins are essentially self-contained Django applications which integrate with Nautobot to provide custom functionality. Since the development of Django applications is already very well-documented, this will only be covering the aspects that are specific to Nautobot. Plugins can do a lot of different things , all of which will be covered in detail in this document. Keep in mind that each piece of functionality is entirely optional. For example, if your plugin merely adds a piece of middleware or an API endpoint for existing data, there's no need to define any new models. Tip The plugin detail view ( /plugins/installed-plugins/<plugin_name>/ , accessible via Plugins -> Installed Plugins in the navigation menu, then selecting a specific plugin) provides in-depth information about which features any installed plugin is implementing or making use of.","title":"Plugin Development"},{"location":"plugins/development.html#initial-setup","text":"Use a Development Environment, Not Production For Plugin Development You should not use your production environment for plugin development. For information on getting started with a development environment, check out Nautobot development guide .","title":"Initial Setup"},{"location":"plugins/development.html#plugin-structure","text":"Although the specific structure of a plugin is largely left to the discretion of its authors, a Nautobot plugin that makes use of all available plugin features described in this document could potentially look something like this: plugin_name/ - plugin_name/ - __init__.py # required - admin.py # Django Admin Interface - api/ - serializers.py # REST API Model serializers - urls.py # REST API URL patterns - views.py # REST API view sets - banner.py # Banners - custom_validators.py # Custom Validators - datasources.py # Loading Data from a Git Repository - filter_extensions.py # Extending Filters - filters.py # Filtersets for UI, REST API, and GraphQL Model Filtering - forms.py # UI Forms and Filter Forms - graphql/ - types.py # GraphQL Type Objects - homepage.py # Home Page Content - jinja_filters.py # Jinja Filters - jobs.py # Job classes - middleware.py # Request/response middleware - migrations/ - 0001_initial.py # Database Models - models.py # Database Models - navigation.py # Navigation Menu Items - secrets.py # Secret Providers - signals.py # Signal Handler Functions - template_content.py # Extending Core Templates - templates/ - plugin_name/ - *.html # UI content templates - urls.py # UI URL Patterns - views.py # UI Views and any view override definitions - pyproject.toml # *** REQUIRED *** - Project package definition - README.md The top level is the project root. Immediately within the root should exist several items: pyproject.toml - This is the new unified Python project settings file that replaces setup.py , requirements.txt , and various other setup files (like setup.cfg , MANIFEST.in , among others). README.md - A brief introduction to your plugin, how to install and configure it, where to find help, and any other pertinent information. It is recommended to write README files using a markup language such as Markdown. The plugin source directory, with the same name as your plugin. The plugin source directory contains all of the actual Python code and other resources used by your plugin. Its structure is left to the author's discretion, however it is recommended to follow best practices as outlined in the Django documentation . At a minimum, this directory must contain an __init__.py file containing an instance of Nautobot's PluginConfig class. Note Nautobot includes a command to help create the plugin directory: nautobot-server startplugin [app_name] Please see the Nautobot Server Guide for more information.","title":"Plugin Structure"},{"location":"plugins/development.html#create-pyprojecttoml","text":"","title":"Create pyproject.toml"},{"location":"plugins/development.html#poetry-init-recommended","text":"To get started with a project using Python Poetry you use the poetry init command. This will guide you through the prompts necessary to generate a pyproject.toml with details required for packaging. This command will guide you through creating your pyproject.toml config. Package name [tmp]: nautobot-animal-sounds Version [0.1.0]: Description []: An example Nautobot plugin Author [, n to skip]: Bob Jones License []: Apache 2.0 Compatible Python versions [^3.8]: ^3.7 Would you like to define your main dependencies interactively? (yes/no) [yes] no Would you like to define your development dependencies interactively? (yes/no) [yes] no Generated file [tool.poetry] name = \"nautobot-animal-sounds\" version = \"0.1.0\" description = \"An example Nautobot plugin\" authors = [\"Bob Jones\"] license = \"Apache 2.0\" [tool.poetry.dependencies] python = \"^3.7\" [tool.poetry.dev-dependencies] [build-system] requires = [\"poetry-core>=1.0.0\"] build-backend = \"poetry.core.masonry.api\" Do you confirm generation? (yes/no) [yes]","title":"Poetry Init (Recommended)"},{"location":"plugins/development.html#define-a-pluginconfig","text":"The PluginConfig class is a Nautobot-specific wrapper around Django's built-in AppConfig class. It is used to declare Nautobot plugin functionality within a Python package. Each plugin should provide its own subclass, defining its name, metadata, and default and required configuration parameters. An example is below: from nautobot.extras.plugins import PluginConfig class AnimalSoundsConfig ( PluginConfig ): name = 'nautobot_animal_sounds' verbose_name = 'Animal Sounds' description = 'An example plugin for development purposes' version = '0.1' author = 'Bob Jones' author_email = 'bob@example.com' base_url = 'animal-sounds' required_settings = [] default_settings = { 'loud' : False } config = AnimalSoundsConfig Nautobot looks for the config variable within a plugin's __init__.py to load its configuration. Typically, this will be set to the PluginConfig subclass, but you may wish to dynamically generate a PluginConfig class based on environment variables or other factors.","title":"Define a PluginConfig"},{"location":"plugins/development.html#required-pluginconfig-attributes","text":"Name Description author Name of plugin's author author_email Author's public email address description Brief description of the plugin's purpose name Raw plugin name; same as the plugin's source directory verbose_name Human-friendly name for the plugin version Current release ( semantic versioning is encouraged)","title":"Required PluginConfig Attributes"},{"location":"plugins/development.html#optional-pluginconfig-attributes","text":"Name Default Description base_url Same as specified name Base path to use for plugin URLs caching_config {\"*\":{\"ops\":\"all\"}} Plugin-specific query caching configuration config_view_name None URL name for a \"configuration\" view defined by this plugin default_settings {} A dictionary of configuration parameters and their default values home_view_name None URL name for a \"home\" or \"dashboard\" view defined by this plugin docs_view_name None URL name for a \"documentation\" view defined by this plugin installed_apps [] A list of additional Django application dependencies to automatically enable when the plugin is activated (you must still make sure these underlying dependent libraries are installed) max_version None Maximum version of Nautobot with which the plugin is compatible middleware [] A list of middleware classes to append after Nautobot's built-in middleware min_version None Minimum version of Nautobot with which the plugin is compatible required_settings [] A list of any configuration parameters that must be defined by the user Note All required_settings must be configured in PLUGINS_CONFIG in nautobot_config.py before the plugin can be used. Warning If a configuration parameter is listed in both required_settings and default_settings , the default setting will be ignored.","title":"Optional PluginConfig Attributes"},{"location":"plugins/development.html#pluginconfig-code-location-attributes","text":"The following PluginConfig attributes can be configured to customize where Nautobot will look to locate various pieces of plugin code. In most cases you will not need to change these, but they are provided as options in case your plugin has a non-standard organizational structure. Info As used below, a \"dotted path\" is the combination of a Python module path within the plugin and the name of a variable within that module. For example, \"template_content.template_extensions\" refers to a variable named template_extensions inside a template_content module located at the root of the plugin. Name Default Description banner_function \"banner.banner\" Dotted path to a function that can render a custom banner custom_validators \"custom_validators.custom_validators\" Dotted path to a list of custom validator classes datasource_contents \"datasources.datasource_contents\" Dotted path to a list of datasource (Git, etc.) content types to register graphql_types graphql.types.graphql_types Dotted path to a list of GraphQL type classes homepage_layout \"homepage.layout\" Dotted path to a list of home page items provided by the plugin jinja_filters \"jinja_filters\" Path to a module that contains Jinja2 filters to be registered jobs \"jobs.jobs\" Dotted path to a list of Job classes menu_items \"navigation.menu_items\" Dotted path to a list of navigation menu items provided by the plugin secrets_providers \"secrets.secrets_providers\" Dotted path to a list of secrets providers in the plugin template_extensions \"template_content.template_extensions\" Dotted path to a list of template extension classes","title":"PluginConfig Code Location Attributes"},{"location":"plugins/development.html#install-the-plugin-for-development","text":"The plugin needs to be installed into the same python environment where Nautobot is, so that we can get access to nautobot-server command, and also so that the nautobot-server is aware of the new plugin. If you installed Nautobot using Poetry, then go to the root directory of your clone of the Nautobot repository and run poetry shell there. Afterward, return to the root directory of your plugin to continue development. Otherwise if using the pip install or Docker workflows, manually activate nautobot using source /opt/nautobot/bin/activate . To install the plugin for development the following steps should be taken: Activate the Nautobot virtual environment (as detailed above) Navigate to the project root, where the pyproject.toml file exists for the plugin Execute the command poetry install to install the local package into the Nautobot virtual environment Note Poetry installs the current project and its dependencies in editable mode (aka \"development mode\" ). This should be done in development environment You should not use your production environment for plugin development. For information on getting started with a development environment, check out Nautobot development guide . $ poetry install Once the plugin has been installed, add it to the plugin configuration for Nautobot: PLUGINS = [ \"animal_sounds\" ]","title":"Install the Plugin for Development"},{"location":"plugins/development.html#verify-that-the-plugin-is-installed","text":"In the Nautobot UI, navigate to Plugins -> Installed Plugins . The newly installed plugin should appear in the displayed table if everything is configured correctly. You can also click on the plugin's name in this table to view more detailed information about this plugin based on its PluginConfig and other contents.","title":"Verify that the Plugin is Installed"},{"location":"plugins/development.html#extending-the-existing-nautobot-ui","text":"","title":"Extending the Existing Nautobot UI"},{"location":"plugins/development.html#extending-object-detail-views","text":"Plugins can inject custom content into certain areas of the detail views of applicable models. This is accomplished by subclassing PluginTemplateExtension , designating a particular Nautobot model, and defining the desired methods to render custom content. Four methods are available: left_page() - Inject content on the left side of the page right_page() - Inject content on the right side of the page full_width_page() - Inject content across the entire bottom of the page buttons() - Add buttons to the top of the page detail_tabs() - Add extra tabs to the end of the list of tabs within the page tabs navigation Additionally, a render() method is available for convenience. This method accepts the name of a template to render, and any additional context data you want to pass. Its use is optional, however. When a PluginTemplateExtension is instantiated, context data is assigned to self.context . Available data include: object - The object being viewed request - The current request settings - Global Nautobot settings config - Plugin-specific configuration parameters For example, accessing {{ request.user }} within a template will return the current user. Declared subclasses should be gathered into a list or tuple for integration with Nautobot. By default, Nautobot looks for an iterable named template_extensions within a template_content.py file. (This can be overridden by setting template_extensions to a custom value on the plugin's PluginConfig .) An example is below. # template_content.py from django.urls import reverse from nautobot.extras.plugins import PluginTemplateExtension from .models import Animal class SiteAnimalCount ( PluginTemplateExtension ): \"\"\"Template extension to display animal count on the right side of the page.\"\"\" model = 'dcim.site' def right_page ( self ): return self . render ( 'nautobot_animal_sounds/inc/animal_count.html' , extra_context = { 'animal_count' : Animal . objects . count (), }) class DeviceExtraTabs ( PluginTemplateExtension ): \"\"\"Template extension to add extra tabs to the object detail tabs.\"\"\" model = 'dcim.device' def detail_tabs ( self ): \"\"\" You may define extra tabs to render on a model's detail page by utilizing this method. Each tab is defined as a dict in a list of dicts. For each of the tabs defined: - The <title> key's value will become the tab link's title. - The <url> key's value is used to render the HTML link for the tab These tabs will be visible (in this instance) on the Device model's detail page as set by the DeviceContent.model attribute \"dcim.device\" This example demonstrates defining two tabs. The tabs will be ordered by their position in list. \"\"\" return [ { \"title\" : \"Plugin Tab 1\" , \"url\" : reverse ( \"plugins:example_plugin:device_detail_tab_1\" , kwargs = { \"pk\" : self . context [ \"object\" ] . pk }), }, { \"title\" : \"Plugin Tab 2\" , \"url\" : reverse ( \"plugins:example_plugin:device_detail_tab_2\" , kwargs = { \"pk\" : self . context [ \"object\" ] . pk }), }, ] template_extensions = [ DeviceExtraTabs , SiteAnimalCount ]","title":"Extending Object Detail Views"},{"location":"plugins/development.html#adding-extra-tabs","text":"Added in version 1.4.0 In order for any extra tabs to work properly, the \"url\" key must reference a view which inherits from the nautobot.core.views.generic.ObjectView class and the template must extend the object's detail template such as: <!-- example_plugin/tab_device_detail_1.html --> {% extends 'dcim/device.html' %} {% block content %} < h2 > Device Plugin Tab 1 </ h2 > < p > I am some content for the example plugin's device ({{ object.pk }}) detail tab 1. </ p > {% endblock %} Here's a basic example of a tab's view # views.py from nautobot.core.views import generic from nautobot.dcim.models import Device class DeviceDetailPluginTabOne ( generic . ObjectView ): \"\"\" This view's template extends the device detail template, making it suitable to show as a tab on the device detail page. Views that are intended to be for an object detail tab's content rendering must always inherit from nautobot.core.views.generic.ObjectView. \"\"\" queryset = Device . objects . all () template_name = \"example_plugin/tab_device_detail_1.html\" You must also add the view to the url_patterns like so (make sure to read the note after this code snippet): # urls.py from django.urls import path from example_plugin import views urlpatterns = [ # ... previously defined urls path ( \"devices/<uuid:pk>/example-plugin-tab-1/\" , views . DeviceDetailPluginTabOne . as_view (), name = \"device_detail_tab_1\" ), ] Note For added tab views, we recommend for consistency that you follow the URL pattern established by the base model detail view and tabs (if any). For example, nautobot/dcim/urls.py references Device tab views with the URL pattern devices/<uuid:pk>/TAB-NAME/ , so above we have followed that same pattern.","title":"Adding Extra Tabs"},{"location":"plugins/development.html#adding-a-banner","text":"Added in version 1.2.0 A plugin can provide a function that renders a custom banner on any number of Nautobot views. By default Nautobot looks for a function banner() inside of banner.py . (This can be overridden by setting banner_function to a custom value on the plugin's PluginConfig .) This function currently receives a single argument, context , which is the Django request context in which the current page is being rendered. The function can return None if no banner is needed for a given page view, or can return a PluginBanner object describing the banner contents. Here's a simple example banner.py : # banner.py from django.utils.html import format_html from nautobot.extras.choices import BannerClassChoices from nautobot.extras.plugins import PluginBanner def banner ( context , * args , ** kwargs ): \"\"\"Greet the user, if logged in.\"\"\" # Request parameters can be accessed via context.request if not context . request . user . is_authenticated : # No banner if the user isn't logged in return None else : return PluginBanner ( content = format_html ( \"Hello, <strong> {} </strong>! \ud83d\udc4b\" , context . request . user ), banner_class = BannerClassChoices . CLASS_SUCCESS , )","title":"Adding a Banner"},{"location":"plugins/development.html#adding-navigation-menu-items","text":"Plugins can extend the existing navigation bar layout. By default, Nautobot looks for a menu_items list inside of navigation.py . (This can be overridden by setting menu_items to a custom value on the plugin's PluginConfig .) Using a key and weight system, a developer can integrate the plugin's menu additions amongst existing menu tabs, groups, items and buttons, and/or create entirely new menus as desired. More documentation and examples can be found in the Navigation Menu guide. Tip To reduce the amount of clutter in the navigation menu, if your plugin provides a \"plugin configuration\" view, we recommend linking it from the main \"Installed Plugins\" page rather than adding it as a separate item in the navigation menu. Similarly, if your plugin provides a \"plugin home\" or \"dashboard\" view, consider linking it from the \"Installed Plugins\" page, and/or adding a link from the Nautobot home page (see below), rather than adding it to the navigation menu.","title":"Adding Navigation Menu Items"},{"location":"plugins/development.html#adding-home-page-content","text":"Added in version 1.2.0 Plugins can add content to the Nautobot home page. By default, Nautobot looks for a layout list inside of homepage.py . (This can be overridden by setting homepage_layout to a custom value on the plugin's PluginConfig .) Using a key and weight system, a developer can integrate the plugin content amongst existing panels, groups, and items and/or create entirely new panels as desired. More documentation and examples can be found in the guide on Home Page Panels .","title":"Adding Home Page Content"},{"location":"plugins/development.html#adding-links-to-the-installed-plugins-view","text":"Added in version 1.2.0 It's common for many plugins to provide a \"plugin configuration\" view used for interactive configuration of aspects of the plugin that don't necessarily need to be managed by a system administrator via PLUGINS_CONFIG . The PluginConfig setting of config_view_name lets you provide the URL pattern name defined for this view, which will then be accessible via a button on the Plugins -> Installed Plugins UI view. For example, if the animal_sounds plugin provides a configuration view, which is set up in urls.py as follows: # urls.py from django.urls import path from . import views urlpatterns = [ path ( \"configuration/\" , views . AnimalSoundsConfigView . as_view (), name = \"config\" ), ] then in your AnimalSoundsConfig you could refer to the view by name: # __init__.py from nautobot.extras.plugins import PluginConfig class AnimalSoundsConfig ( PluginConfig ): # ... config_view_name = \"plugins:animal_sounds:config\" config = AnimalSoundsConfig and now the \"Configuration\" button that appears in the Installed Plugins table next to \"Animal Sounds\" will be a link to your configuration view. Similarly, if your plugin provides a \"plugin home\" or \"dashboard\" view, you can provide a link for the \"Home\" button in the Installed Plugins table by defining home_view_name on your PluginConfig class. This can also be done for documentation by defining docs_view_name on your PluginConfig class.","title":"Adding Links to the Installed Plugins View"},{"location":"plugins/development.html#extending-existing-functionality","text":"","title":"Extending Existing Functionality"},{"location":"plugins/development.html#adding-jinja2-filters","text":"Added in version 1.1.0 Plugins can define custom Jinja2 filters to be used when rendering templates defined in computed fields. Check out the official Jinja2 documentation on how to create filter functions. In the file that defines your filters (by default jinja_filters.py , but configurable in the PluginConfig if desired), you must import the library module from the django_jinja library. Filters must then be decorated with @library.filter . See an example below that defines a filter called leet_speak . from django_jinja import library @library . filter def leet_speak ( input_str ): charset = { \"a\" : \"4\" , \"e\" : \"3\" , \"l\" : \"1\" , \"o\" : \"0\" , \"s\" : \"5\" , \"t\" : \"7\" } output_str = \"\" for char in input_str : output_str += charset . get ( char . lower (), char ) return output_str This filter will then be available for use in computed field templates like so: {{ \"HELLO WORLD\" | leet_speak }} The output of this template results in the string \"H3110 W0R1D\" .","title":"Adding Jinja2 Filters"},{"location":"plugins/development.html#including-jobs","text":"Plugins can provide Jobs to take advantage of all the built-in functionality provided by that feature (user input forms, background execution, results logging and reporting, etc.). By default, for each plugin, Nautobot looks for an iterable named jobs within a jobs.py file. (This can be overridden by setting jobs to a custom value on the plugin's PluginConfig .) A brief example is below; for more details on Job design and implementation, refer to the Jobs feature documentation. # jobs.py from nautobot.extras.jobs import Job class CreateDevices ( Job ): ... class DeviceConnectionsReport ( Job ): ... class DeviceIPsReport ( Job ): ... jobs = [ CreateDevices , DeviceConnectionsReport , DeviceIPsReport ]","title":"Including Jobs"},{"location":"plugins/development.html#implementing-custom-validators","text":"Plugins can register custom validator classes which implement model validation logic to be executed during a model's clean() method. Like template extensions, custom validators are registered to a single model and offer a method which plugin authors override to implement their validation logic. This is accomplished by subclassing PluginCustomValidator and implementing the clean() method. Plugin authors must raise django.core.exceptions.ValidationError within the clean() method to trigger validation error messages which are propagated to the user and prevent saving of the model instance. A convenience method validation_error() may be used to simplify this process. Raising a ValidationError is no different than vanilla Django, and the convenience method will simply pass the provided message through to the exception. When a PluginCustomValidator is instantiated, the model instance is assigned to context dictionary using the object key, much like PluginTemplateExtensions. E.g. self.context['object'] . Declared subclasses should be gathered into a list or tuple for integration with Nautobot. By default, Nautobot looks for an iterable named custom_validators within a custom_validators.py file. (This can be overridden by setting custom_validators to a custom value on the plugin's PluginConfig .) An example is below. # custom_validators.py from nautobot.extras.plugins import PluginCustomValidator class SiteValidator ( PluginCustomValidator ): \"\"\"Custom validator for Sites to enforce that they must have a Region.\"\"\" model = 'dcim.site' def clean ( self ): if self . context [ 'object' ] . region is None : # Enforce that all sites must be assigned to a region self . validation_error ({ \"region\" : \"All sites must be assigned to a region\" }) custom_validators = [ SiteValidator ]","title":"Implementing Custom Validators"},{"location":"plugins/development.html#loading-data-from-a-git-repository","text":"It's possible for a plugin to register additional types of data that can be provided by a Git repository and be automatically notified when such a repository is refreshed with new data. By default, Nautobot looks for an iterable named datasource_contents within a datasources.py file. (This can be overridden by setting datasource_contents to a custom value on the plugin's PluginConfig .) An example is below. # datasources.py import yaml import os from nautobot.extras.choices import LogLevelChoices from nautobot.extras.registry import DatasourceContent from .models import Animal def refresh_git_animals ( repository_record , job_result , delete = False ): \"\"\"Callback for GitRepository updates - refresh Animals managed by it.\"\"\" if 'nautobot_animal_sounds.Animal' not in repository_record . provided_contents or delete : # This repository is defined not to provide Animal records. # In a more complete worked example, we might want to iterate over any # Animals that might have been previously created by this GitRepository # and ensure their deletion, but for now this is a no-op. return # We have decided that a Git repository can provide YAML files in a # /animals/ directory at the repository root. animal_path = os . path . join ( repository_record . filesystem_path , 'animals' ) for filename in os . listdir ( animal_path ): with open ( os . path . join ( animal_path , filename )) as fd : animal_data = yaml . safe_load ( fd ) # Create or update an Animal record based on the provided data animal_record , created = Animal . objects . update_or_create ( name = animal_data [ 'name' ], defaults = { 'sound' : animal_data [ 'sound' ]} ) # Record the outcome in the JobResult record job_result . log ( \"Successfully created/updated animal\" , obj = animal_record , level_choice = LogLevelChoices . LOG_SUCCESS , grouping = \"animals\" , ) # Register that Animal records can be loaded from a Git repository, # and register the callback function used to do so datasource_contents = [ ( 'extras.gitrepository' , # datasource class we are registering for DatasourceContent ( name = 'animals' , # human-readable name to display in the UI content_identifier = 'nautobot_animal_sounds.animal' , # internal slug to identify the data type icon = 'mdi-paw' , # Material Design Icons icon to use in UI callback = refresh_git_animals , # callback function on GitRepository refresh ) ) ] With this code, once your plugin is installed, the Git repository creation/editing UI will now include \"Animals\" as an option for the type(s) of data that a given repository may provide. If this option is selected for a given Git repository, your refresh_git_animals function will be automatically called when the repository is synced.","title":"Loading Data from a Git Repository"},{"location":"plugins/development.html#populating-extensibility-features","text":"Added in version 1.2.0 In many cases, a plugin may wish to make use of Nautobot's various extensibility features, such as custom fields or relationships . It can be useful for a plugin to automatically create a custom field definition or relationship definition as a consequence of being installed and activated, so that everyday usage of the plugin can rely upon these definitions to be present. To make this possible, Nautobot provides a custom signal , nautobot_database_ready , that plugins can register to listen for. This signal is triggered when nautobot-server migrate or nautobot-server post_upgrade is run after installing a plugin, and provides an opportunity for the plugin to make any desired additions to the database at this time. For example, maybe we want our plugin to make use of a Relationship allowing each Site to be linked to our Animal model. We would define our callback function that makes sure this Relationship exists, by convention in a signals.py file: # signals.py from nautobot.extras.choices import RelationshipTypeChoices def create_site_to_animal_relationship ( sender , apps , ** kwargs ): \"\"\"Create a Site-to-Animal Relationship if it doesn't already exist.\"\"\" # Use apps.get_model to look up Nautobot core models ContentType = apps . get_model ( \"contenttypes\" , \"ContentType\" ) Relationship = apps . get_model ( \"extras\" , \"Relationship\" ) Site = apps . get_model ( \"dcim\" , \"Site\" ) # Use sender.get_model to look up models from this plugin Animal = sender . get_model ( \"Animal\" ) # Ensure that the Relationship exists Relationship . objects . update_or_create ( slug = \"site-favorite-animal\" , defaults = { \"name\" : \"Site's Favorite Animal\" , \"type\" : RelationshipTypeChoices . TYPE_ONE_TO_MANY , \"source_type\" : ContentType . objects . get_for_model ( Animal ), \"source_label\" : \"Sites that love this Animal\" , \"destination_type\" : ContentType . objects . get_for_model ( Site ), \"destination_label\" : \"Favorite Animal\" , }, ) Then, in the PluginConfig ready() function, we connect this callback function to the nautobot_database_ready signal: # __init__.py from nautobot.core.signals import nautobot_database_ready from nautobot.extras.plugins import PluginConfig from .signals import create_site_to_animal_relationship class AnimalSoundsConfig ( PluginConfig ): # ... def ready ( self ): super () . ready () nautobot_database_ready . connect ( create_site_to_animal_relationship , sender = self ) config = AnimalSoundsConfig After writing this code, run nautobot-server migrate or nautobot-server post_upgrade , then restart the Nautobot server, and you should see that this custom Relationship has now been automatically created.","title":"Populating Extensibility Features"},{"location":"plugins/development.html#implementing-secrets-providers","text":"A plugin can define and register additional providers (sources) for Secrets , allowing Nautobot to retrieve secret values from additional systems or data sources. By default, Nautobot looks for an iterable named secrets_providers within a secrets.py file. (This can be overridden by setting secrets_providers to a custom value on the plugin's PluginConfig .) To define a new SecretsProvider subclass, we must specify the following: A unique slug string identifying this provider A human-readable name string (optional; the slug will be used if this is not specified) A Django form for entering the parameters required by this provider, as an inner class named ParametersForm An implementation of the get_value_for_secret() API to actually retrieve the value of a given secret For a simple (insecure!) example, we could define a \"constant-value\" provider that simply stores a constant value in Nautobot itself and returns this value on demand. Warning This is an intentionally simplistic example and should not be used in practice! Sensitive secret data should never be stored directly in Nautobot's database itself. # secrets.py from nautobot.extras.secrets import SecretsProvider class ConstantValueSecretsProvider ( SecretsProvider ): \"\"\" Example SecretsProvider - this one just returns a user-specified constant value. Obviously this is insecure and not something you'd want to actually use! \"\"\" slug = \"constant-value\" name = \"Constant Value\" class ParametersForm ( BootstrapMixin , forms . Form ): \"\"\" User-friendly form for specifying the required parameters of this provider. \"\"\" constant = forms . CharField ( required = True , help_text = \"Constant secret value. <strong>DO NOT USE FOR REAL DATA</strong>\" ) @classmethod def get_value_for_secret ( cls , secret , obj = None , ** kwargs ): \"\"\" Return the value defined in the Secret.parameters \"constant\" key. A more realistic SecretsProvider would make calls to external APIs, etc., to retrieve a secret from another system as desired. Args: secret (nautobot.extras.models.Secret): The secret whose value should be retrieved. obj (object): The object (Django model or similar) providing context for the secret's parameters. \"\"\" return secret . rendered_parameters ( obj = obj ) . get ( \"constant\" ) secrets_providers = [ ConstantValueSecretsProvider ] After installing and enabling your plugin, you should now be able to navigate to Secrets > Secrets and create a new Secret, at which point \"constant-value\" should now be available as a new secrets provider to use.","title":"Implementing Secrets Providers"},{"location":"plugins/development.html#extending-filters","text":"Added in version 1.3.0 Plugins can extend any model-based FilterSet and FilterForm classes that are provided by the Nautobot core. The requirements to extend a filter set or a filter form (or both) are: The file must be named filter_extensions.py The variable filter_extensions must be declared in that file, and contain a list of PluginFilterExtension subclasses The model attribute of each PluginFilterExtension subclass must be set to a valid model name in the dotted pair format ( {app_label}.{model} , e.g. tenant.tenant or dcim.device ) Nautobot dynamically creates many additional filters based upon the defined filter type. Specifically, there are additional lookup expressions (referred to in code as lookup_expr ) that are created for each filter, when there is neither a lookup_expr nor method parameter already set. These dynamically-added lookup expressions are added using a shorthand notation (e.g. icontains is ic ). Nautobot will also add the negation of each, for example, so icontains will be added along with not icontains using the ic and nic expressions respectively. The dynamically-added lookup expressions can be found in the source code at nautobot/utilities/constants.py and the mapping logic can be found in nautobot/utilities/filters.py . Please see the documentation on filtering for more information. Tip For developers of plugins that define their own model filters, note that the above are added dynamically, as long as the class inherits from nautobot.utilities.filters.BaseFilterSet . However, that does not cover every possible use case, to list a few examples: Usage of a custom method argument on a filter that points to a FilterSet method, which would allow arbitrary filtering using custom logic. This is how the q field search logic is currently performed. Creation of a filter on a field that does not currently have filtering support Convenience methods for highly nested fields There are several conditions that must be met in order to extend a filter: The original FilterSet must follow the pattern: f\"{model.__name__}FilterSet\" e.g. TenantFilterSet The PluginFilterExtension.filterset_fields attribute must be a valid dict, with each key being the filter name (which must start with the plugin's name + _ , e.g. \"example_plugin_description\" , not merely \"description\" ) and each value being a valid django-filter filter Nautobot will dynamically generate the additional relevant lookup expressions of a plugin's defined custom FilterSet field, so no need to additionally register example_plugin_description__ic , etc. Similar to FilterSet fields, Nautobot provides a default filter form for each model, however that does not cover every possible use case. To list a few examples of why one may want to extend a filter form: The base filter form does not include a custom filter defined by the plugin as described above The base filter form does not provide a specific lookup expression to a filterable field, such as allowing regex on name There are several conditions that must be met in order to extend a filter: The original FilterForm must follow the pattern: f\"{model.__name__}FilterForm\" , e.g. TenantFilterForm The filterform_fields attribute must be a valid dictionary of Django form fields Note A plugin is not required to define both filterset_fields and filterform_fields . You can view an example of filter_extensions.py by viewing the one provided with the Example Plugin. Tip The method parameter, if used, must be a callable (method/function). Note that because filters with a method do their filtering in Python code rather than at the database level, performance of method filters is generally much poorer than pure-database filters. The method parameter is not supported when using Dynamic Groups .","title":"Extending Filters"},{"location":"plugins/development.html#adding-database-models","text":"If your plugin introduces a new type of object in Nautobot, you'll probably want to create a Django model for it. A model is essentially a Python representation of a database table, with attributes that represent individual columns. Model instances can be created, manipulated, and deleted using queries . Models must be defined within a file named models.py . It is highly recommended to have plugin models inherit from at least nautobot.core.models.BaseModel which provides base functionality and convenience methods common to all models. For more advanced usage, you may want to instead inherit from one of Nautobot's \"generic\" models derived from BaseModel -- nautobot.core.models.generics.OrganizationalModel or nautobot.core.models.generics.PrimaryModel . The inherent capabilities provided by inheriting from these various parent models differ as follows: Feature django.db.models.Model BaseModel OrganizationalModel PrimaryModel UUID primary key \u274c \u2705 \u2705 \u2705 Object permissions \u274c \u2705 \u2705 \u2705 validated_save() \u274c \u2705 \u2705 \u2705 Change logging \u274c \u274c \u2705 \u2705 Custom fields \u274c \u274c \u2705 \u2705 Relationships \u274c \u274c \u2705 \u2705 Note \u274c \u274c \u2705 \u2705 Tags \u274c \u274c \u274c \u2705 Note When using OrganizationalModel or PrimaryModel , you also must use the @extras_features decorator to specify support for (at a minimum) the \"custom_fields\" and \"relationships\" features. Below is an example models.py file containing a basic model with two character fields: # models.py from django.db import models from nautobot.core.models import BaseModel class Animal ( BaseModel ): \"\"\"Base model for animals.\"\"\" name = models . CharField ( max_length = 50 ) sound = models . CharField ( max_length = 50 ) def __str__ ( self ): return self . name Once you have defined the model(s) for your plugin, you'll need to create the database schema migrations. A migration file is essentially a set of instructions for manipulating the database to support your new model, or to alter existing models. Creating migrations can be done automatically using the nautobot-server makemigrations <plugin_name> management command, where <plugin_name> is the name of the Python package for your plugin (e.g. animal_sounds ): $ nautobot-server makemigrations nautobot_animal_sounds Note A plugin must be installed before it can be used with Django management commands. If you skipped this step above, run poetry install from the plugin's root directory. $ nautobot-server makemigrations nautobot_animal_sounds Migrations for 'nautobot_animal_sounds': /home/bjones/animal_sounds/nautobot_animal_sounds/migrations/0001_initial.py - Create model Animal Next, apply the migration to the database with the nautobot-server migrate <plugin_name> command: $ nautobot-server migrate nautobot_animal_sounds Operations to perform: Apply all migrations: nautobot_animal_sounds Running migrations: Applying nautobot_animal_sounds.0001_initial... OK For more background on schema migrations, see the Django documentation .","title":"Adding Database Models"},{"location":"plugins/development.html#using-the-django-admin-interface","text":"Plugins can optionally expose their models via Django's built-in administrative interface . This can greatly improve troubleshooting ability, particularly during development. To expose a model, simply register it using Django's admin.register() function. An example admin.py file for the above model is shown below: # admin.py from nautobot.core.admin import NautobotModelAdmin from .models import Animal @admin . register ( Animal ) class AnimalAdmin ( NautobotModelAdmin ): list_display = ( 'name' , 'sound' ) This will display the plugin and its model in the admin UI. Staff users can create, change, and delete model instances via the admin UI without needing to create a custom view.","title":"Using the Django Admin Interface"},{"location":"plugins/development.html#integrating-with-graphql","text":"Plugins can optionally expose their models via the GraphQL interface to allow the models to be part of the Graph and to be queried easily. There are two mutually exclusive ways to expose a model to the GraphQL interface. By using the @extras_features decorator By creating your own GraphQL type definition and registering it within graphql/types.py of your plugin (the decorator should not be used in this case) All GraphQL model types defined by your plugin, regardless of which method is chosen, will automatically support some built-in Nautobot features: Support for object permissions based on their associated Model class Include any custom fields defined for their Model Include any relationships defined for their Model Include tags , if the Model supports them","title":"Integrating with GraphQL"},{"location":"plugins/development.html#using-the-extras_features-decorator-for-graphql","text":"To expose a model via GraphQL, simply register it using the @extras_features(\"graphql\") decorator. Nautobot will detect this and will automatically create a GraphQL type definition based on the model. Additionally, if a FilterSet is available at <app_name>.filters.<ModelName>FilterSet , Nautobot will automatically use the filterset to generate GraphQL filtering options for this type as well. # models.py from django.db import models from nautobot.core.models import BaseModel from nautobot.extras.utils import extras_features @extras_features ( \"graphql\" ) class Animal ( BaseModel ): \"\"\"Base model for animals.\"\"\" name = models . CharField ( max_length = 50 ) sound = models . CharField ( max_length = 50 ) def __str__ ( self ): return self . name","title":"Using the @extras_features Decorator for GraphQL"},{"location":"plugins/development.html#creating-your-own-graphql-type-object","text":"In some cases, such as when a model is using Generic Foreign Keys, or when a model has constructed fields that should also be reflected in GraphQL, the default GraphQL type definition generated by the @extras_features decorator may not work as the developer intends, and it will be preferable to provide custom GraphQL types. By default, Nautobot looks for custom GraphQL types in an iterable named graphql_types within a graphql/types.py file. (This can be overridden by setting graphql_types to a custom value on the plugin's PluginConfig .) Each type defined in this way must be a class inheriting from graphene_django.DjangoObjectType or graphene_django_optimizer.OptimizedDjangoObjectType and must follow the standards defined by graphene-django . Nautobot uses a library called graphene-django-optimizer to decrease the time queries take to process. By inheriting from graphene_django_optimizer type classes are automatically optimized. Warning When defining types this way, do not use the @extras_features(\"graphql\") decorator on the corresponding Model class, as no auto-generated GraphQL type is desired for this model. # graphql/types.py import graphene_django_optimizer as gql_optimizer from nautobot_animal_sounds.models import Animal class AnimalType ( gql_optimizer . OptimizedDjangoObjectType ): \"\"\"GraphQL Type for Animal\"\"\" class Meta : model = Animal exclude = [ \"sound\" ] graphql_types = [ AnimalType ]","title":"Creating Your Own GraphQL Type Object"},{"location":"plugins/development.html#using-graphql-orm-utilities","text":"GraphQL utility functions: execute_query() : Runs string as a query against GraphQL. execute_saved_query() : Execute a saved query from Nautobot database. Both functions have the same arguments other than execute_saved_query() which requires a slug to identify the saved query rather than a string holding a query. For authentication either a request object or user object needs to be passed in. If there is none, the function will error out. Arguments: execute_query() : query (str): String with GraphQL query. variables (dict, optional): If the query has variables they need to be passed in as a dictionary. request (django.test.client.RequestFactory, optional): Used to authenticate. user (django.contrib.auth.models.User, optional): Used to authenticate. execute_saved_query() : saved_query_slug (str): Slug of a saved GraphQL query. variables (dict, optional): If the query has variables they need to be passed in as a dictionary. request (django.test.client.RequestFactory, optional): Used to authenticate. user (django.contrib.auth.models.User, optional): Used to authenticate. Returned is a GraphQL object which holds the same data as returned from GraphiQL. Use execute_query().to_dict() to get the data back inside of a dictionary.","title":"Using GraphQL ORM Utilities"},{"location":"plugins/development.html#adding-web-ui-views","text":"If your plugin needs its own page or pages in the Nautobot web UI, you'll need to define views. A view is a particular page tied to a URL within Nautobot, which renders content using a template. Views are typically defined in views.py , and URL patterns in urls.py . As an example, let's write a view which displays a random animal and the sound it makes. First, create the view in views.py : # views.py from django.shortcuts import render from django.views.generic import View from .models import Animal class RandomAnimalView ( View ): \"\"\"Display a randomly-selected Animal.\"\"\" def get ( self , request ): animal = Animal . objects . order_by ( '?' ) . first () return render ( request , 'nautobot_animal_sounds/animal.html' , { 'animal' : animal , }) This view retrieves a random animal from the database and and passes it as a context variable when rendering a template named animal.html , which doesn't exist yet. To create this template, first create a directory named templates/nautobot_animal_sounds/ within the plugin source directory. (We use the plugin's name as a subdirectory to guard against naming collisions with other plugins.) Then, create a template named animal.html as described below.","title":"Adding Web UI Views"},{"location":"plugins/development.html#utilizing-nautobot-generic-views","text":"Starting in Nautobot 1.1.0 via PR , some generic views have been exposed to help aid in plugin development. These views have some requirements that must be in place in order to work. These can be used by importing them from from nautobot.core.views import generic . More documentation and examples can be found in Generic Views guide.","title":"Utilizing Nautobot Generic Views"},{"location":"plugins/development.html#extending-the-base-template","text":"Nautobot provides a base template to ensure a consistent user experience, which plugins can extend with their own content. This template includes four content blocks: title - The page title header - The upper portion of the page content - The main page body javascript - A section at the end of the page for including Javascript code For more information on how template blocks work, consult the Django documentation . {# templates/nautobot_animal_sounds/animal.html #} {% extends 'base.html' %} {% block content %} {% with config=settings.PLUGINS_CONFIG.nautobot_animal_sounds %} <h2 class=\"text-center\" style=\"margin-top: 200px\"> {% if animal %} The {{ animal.name|lower }} says {% if config.loud %} {{ animal.sound|upper }}! {% else %} {{ animal.sound }} {% endif %} {% else %} No animals have been created yet! {% endif %} </h2> {% endwith %} {% endblock %} The first line of the template instructs Django to extend the Nautobot base template and inject our custom content within its content block. Note Django renders templates with its own custom template language . This template language is very similar to Jinja2, however there are some important differences to keep in mind.","title":"Extending the Base Template"},{"location":"plugins/development.html#registering-url-patterns","text":"Finally, to make the view accessible to users, we need to register a URL for it. We do this in urls.py by defining a urlpatterns variable containing a list of paths. # urls.py from django.urls import path from . import views urlpatterns = [ path ( 'random/' , views . RandomAnimalView . as_view (), name = 'random_animal' ), ] A URL pattern has three components: route - The unique portion of the URL dedicated to this view view - The view itself name - A short name used to identify the URL path internally This makes our view accessible at the URL /plugins/animal-sounds/random/ . (Remember, our AnimalSoundsConfig class sets our plugin's base URL to animal-sounds .) Viewing this URL should show the base Nautobot template with our custom content inside it. Tip As a next step, you would typically want to add links from the Nautobot UI to this view, either from the navigation menu , the Nautobot home page , and/or the Installed Plugins view .","title":"Registering URL Patterns"},{"location":"plugins/development.html#nautobotuiviewset","text":"Added in version 1.4.0 New in Nautobot 1.4 is the debut of NautobotUIViewSet : A powerful plugin development tool that can save plugin developer hundreds of lines of code compared to using legacy generic.views . Using it to gain access to default functionalities previous provided by generic.views such as create() , bulk_create() , update() , partial_update() , bulk_update() , destroy() , bulk_destroy() , retrieve() and list() actions. Note that this ViewSet is catered specifically to the UI, not the API. Concrete examples on how to use NautobotUIViewSet resides in nautobot.circuits.views . Below we provide an example on how to use NautobotUIViewSet on a theoretical plugin model. from nautobot.core.views.viewsets import NautobotUIViewset class YourPluginModelUIViewSet ( NautobotUIViewSet ): bulk_create_form_class = YourPluginModelCSVForm bulk_update_form_class = YourPluginModelBulkEditForm filterset_class = YourPluginModelFilterSet filterset_form_class = YourPluginModelFilterForm form_class = YourPluginModelForm queryset = YourPluginModel . objects . all () serializer_class = serializers . YourPluginModelSerializer table_class = YourPluginModelTable","title":"NautobotUIViewSet"},{"location":"plugins/development.html#setting-viewset-attributes","text":"One caveat of using the NautobotUIViewSet is that the queryset , serializer_class and table_class attribute of the YourPluginModelUIViewSet has to be set before most of the NautobotUIViewSet functionalities will become available. By default the URL patterns generated by a NautobotUIViewSet are based on the model's slug ( /model-name/<slug>/ for the detail view, /model-name/<slug>/edit/ for the edit view, etc.). If your model lacks a slug field, or if you otherwise need to use a different field to look up an object, just override the default lookup_field in your ViewSet attributes: from nautobot.core.views.viewsets import NautobotUIViewset class YourPluginModelUIViewSet ( NautobotUIViewSet ): ... lookup_field = \"pk\" ...","title":"Setting ViewSet Attributes"},{"location":"plugins/development.html#view-template-context","text":"Templates can benefit from a very rich context passed down from the views and renderer, including forms, tables, as well as any other information that may be helpful for rendering templates. The keys it provides are as follows: content_type : The ContentType object for the associated model filter_form : The FilterForm object for the associated model form : A Form object for the associated model if relevant ( None for list and detail/retrieve views) object : An instance of the associated mode if available ( None for list and bulk operation views) permissions : Summary of user permissions for the given model return_url : The relevant return URL table : A Table object for the associated model if relevant ( None for detail/retrieve and update views) table_config_form : A TableConfigForm object for the associated table , providing the ability to customize the table verbose_name : The singular form of the model's name verbose_name_plural : The plural form of the model's name An example from editing a Provider object: { 'content_type' : < ContentType : circuits | provider > , 'filter_form' : < ProviderFilterForm bound = True , valid = Unknown , fields = ( region ; site ; location ; q ; asn ; tag ) > , 'form' : < ProviderForm bound = False , valid = Unknown , fields = ( name ; slug ; asn ; account ; portal_url ; noc_contact ; admin_contact ; comments ; tags ; object_note ) > , 'object' : < Provider : NautobotProvider > , 'permissions' : { 'add' : True , 'change' : True , 'delete' : True , 'view' : True }, 'return_url' : '/circuits/providers/nautobotprovider' , 'table' : None , 'table_config_form' : None , 'verbose_name' : 'provider' , 'verbose_name_plural' : 'providers' } Other context keys may be available for certain views: editing : Provided for create and update views to help the template determine if this is a new or existing object action_buttons : Provided for the list view for the top of table buttons (such as \"Add\" and \"Export\") You may see other context keys as well, but any not documented above should not be relied upon as they may be removed in a future release. Some examples of those are: changelog_url : This can now be retrieved from the object itself, via object.get_changelog_url , if the object supports change-logging obj : Please use object instead obj_type : Please use verbose_name instead obj_type_plural : Please use verbose_name_plural instead","title":"View Template Context"},{"location":"plugins/development.html#excluding-viewmixins-from-nautobotuiviewset","text":"For plugin models that do not require certain views, simply inherit directly from the ViewMixins available in nautobot.core.views.mixins instead of NautobotUIViewSet . Concrete examples for excluding ViewMixins , checkout CircuitTerminationUIViewSet and CircuitTypeUIViewSet in nautobot.circuits.views . ## A plugin model viewset that does not support bulk views and operations from nautobot.core.views import mixins as view_mixins class YourPluginModelUIViewSet ( view_mixins . ObjectListViewMixin , view_mixins . ObjectDetailViewMixin , view_mixins . ObjectEditViewMixin , view_mixins . ObjectDestroyViewMixin , ): filterset_class = YourPluginModelFilterSet filterset_form_class = YourPluginModelFilterForm form_class = YourPluginModelForm queryset = YourPluginModel . objects . all () serializer_class = serializers . YourPluginModelSerializer table_class = YourPluginModelTable # You do not need to specify attributes that are not needed. Excluding unwanted urls from NautobotUIViewSetRouter is done for you at the ViewSet level. If you do not inherit the unwanted ViewMixins, the corresponding route from the router will not be published. # urls.py # All the urls correspond to BulkViewMixins will not be published when you register your ViewSet with the router. router . register ( \"yourpluginmodel\" , views . YourPluginModelUIViewSet )","title":"Excluding ViewMixins from NautobotUIViewSet"},{"location":"plugins/development.html#template-naming-for-nautobotuiviewset","text":"Template naming is very intuitive in NautobotUIViewSet. In templates/yourpluginmodel folder, name your templates following this convention {app_label}/{model_name}_{self.action}.html . ViewMixins self.action ObjectListViewMixin list ObjectDetailViewMixin retrieve ObjectEditViewMixin create/update ObjectDestroyViewMixin destroy ObjectBulkDestroyViewMixin bulk_destroy ObjectBulkCreateViewMixin bulk_create ObjectBulkUpdateViewMixin bulk_update For example, for a DetailView template for YourPluginModel , the template name will be yourplugin/yourpluginmodel_retrieve.html , for a BulkCreateView template for yourpluginmodel , the template name will be yourplugin/yourpluginmodel_bulk_create.html and etc. If you do not provide your own templates in the yourplugin/templates/yourplugin folder, NautobotUIViewSet will fall back to generic/object_{self.action}.html . Since in many cases the create and update templates for a model will be identical, you are not required to create both. If you provide a {app_label}/{model_opts.model_name}_create.html file but not a {app_label}/{model_opts.model_name}_update.html file, then when you update an object, it will fall back to {app_label}/{model_opts.model_name}_create.html and vice versa.","title":"Template Naming for NautobotUIViewSet"},{"location":"plugins/development.html#nautobotuiviewsetrouter","text":"With NautobotUIViewSet as the base UI ViewSet for YourPluginModel , it is required to register your urls with the help of NautobotUIViewSetRouter . For a concrete example on how to use NautobotUIViewSetRouter , see nautobot.circuits.urls . Below is a theoretical urls.py file for YourPluginModel : from django.urls import path from nautobot.core.views.routers import NautobotUIViewSetRouter from nautobot.plugins import views router = NautobotUIViewSetRouter () router . register ( \"yourpluginmodel\" , views . YourPluginModelUIViewSet ) urlpatterns = [ # Extra urls that do not follow the patterns of `NautobotUIViewSetRouter` go here. # changelog, notes and etc. ... path ( \"yourpluginmodels/<slug:slug>/changelog/\" , ObjectChangeLogView . as_view (), name = \"yourpluginmodel_changelog\" , kwargs = { \"model\" : yourpluginmodel }, ), path ( \"yourpluginmodels/<slug:slug>/notes/\" , ObjectNotesView . as_view (), name = \"yourpluginmodel_notes\" , kwargs = { \"model\" : yourpluginmodel }, ), ... ] urlpatterns += router . urls","title":"NautobotUIViewSetRouter"},{"location":"plugins/development.html#adding-rest-api-endpoints","text":"Plugins can declare custom endpoints on Nautobot's REST API to retrieve or manipulate models or other data. These behave very similarly to views, except that instead of rendering arbitrary content using a template, data is returned in JSON format using a serializer. Nautobot uses the Django REST Framework , which makes writing API serializers and views very simple. First, create a serializer for the Animal model, in api/serializers.py : # api/serializers.py from rest_framework.serializers import ModelSerializer from nautobot_animal_sounds.models import Animal class AnimalSerializer ( ModelSerializer ): \"\"\"API serializer for interacting with Animal objects.\"\"\" class Meta : model = Animal fields = ( 'id' , 'name' , 'sound' ) Next, create a generic API view set that allows basic CRUD (create, read, update, and delete) operations for Animal instances. This is defined in api/views.py : # api/views.py from rest_framework.viewsets import ModelViewSet from nautobot_animal_sounds.models import Animal from .serializers import AnimalSerializer class AnimalViewSet ( ModelViewSet ): \"\"\"API viewset for interacting with Animal objects.\"\"\" queryset = Animal . objects . all () serializer_class = AnimalSerializer Finally, register a URL for our endpoint in api/urls.py . This file must define a variable named urlpatterns . # api/urls.py from rest_framework import routers from .views import AnimalViewSet router = routers . DefaultRouter () router . register ( 'animals' , AnimalViewSet ) urlpatterns = router . urls With these three components in place, we can request /api/plugins/animal-sounds/animals/ to retrieve a list of all Animal objects defined. Warning This example is provided as a minimal reference implementation only. It does not address authentication, performance, or the myriad of other concerns that plugin authors should have.","title":"Adding REST API Endpoints"},{"location":"plugins/development.html#adding-help-documentation","text":"If you are using the generic.ObjectEditView from Nautobot for your object, the form can automatically include a help icon with a link to that object's documentation. For this to happen, Nautobot must be able to find the documentation for this object in a specific directory tree within your plugin: plugin_name/ # \"nautobot_animal_sounds\" - static/ - plugin_name/ # \"nautobot_animal_sounds\" - docs/ - index.html - models/ - object_model.html # \"animal.html\"","title":"Adding Help Documentation"},{"location":"plugins/development.html#overriding-existing-functionality","text":"","title":"Overriding Existing Functionality"},{"location":"plugins/development.html#replacing-views","text":"Added in version 1.4.0 You may override any of the core or plugin views by providing an override_views dict in a plugin's views.py file. To override a view, you must specify the view's fully qualified name as the dict key which consists of the app name followed by the view's name separated by a colon, for instance dcim:device . The dict value should be the overriding view function. A simple example to override the device detail view: # views.py from django.shortcuts import HttpResponse from nautobot.core.views import generic class DeviceViewOverride ( generic . View ): def get ( self , request , * args , ** kwargs ): return HttpResponse (( \"Hello world! I'm a view which \" \"overrides the device object detail view.\" )) override_views = { \"dcim:device\" : DeviceViewOverride . as_view (), }","title":"Replacing Views"},{"location":"plugins/development.html#note-url-endpoint","text":"Added in version 1.4.0 Models that inherit from PrimaryModel and OrganizationalModel can have notes associated. In order to utilize this new feature you will need to add the endpoint to urls.py . Here is an option to be able to support both 1.4+ and older versions of Nautobot: urlpatterns = [ path ( 'random/' , views . RandomAnimalView . as_view (), name = 'random_animal' ), ] try : from nautobot.extras.views import ObjectNotesView urlpatterns . append ( path ( 'random/<slug:slug>/notes/), ObjectNotesView . as_view (), name = \"random_notes\" , kwargs = { \"model\" : Random }, ) ) except ImportError : pass","title":"Note URL Endpoint"},{"location":"plugins/porting-from-netbox.html","text":"Porting NetBox Plugins to Nautobot \u00b6 Given an existing NetBox plugin, it will range from straightforward to very complicated to create a port of this plugin that's compatible with Nautobot, though in general it should be easier than developing a comparable plugin entirely from scratch. Of course, it would be impossible to provide a generalized, step-by-step guide that would cover all possibilities, but this document at least documents some known tips and tricks for this purpose. Updating Python module import paths \u00b6 The most likely first issue you will encounter will be a module import problem, and in most cases a simple change to the name of imported modules will suffice: circuits.* -> nautobot.circuits.* dcim.* -> nautobot.dcim.* extras.* -> nautobot.extras.* ipam.* -> nautobot.ipam.* netbox.* -> nautobot.core.* tenancy.* -> nautobot.tenancy.* utilities.* -> nautobot.utilities.* virtualization.* -> nautobot.virtualization.* Regenerating database migrations \u00b6 In general, your migrations files will not port over easily; you will probably want to delete and re-generate them ( nautobot-server makemigrations <plugin-name> ) instead.","title":"Porting NetBox Plugins to Nautobot"},{"location":"plugins/porting-from-netbox.html#porting-netbox-plugins-to-nautobot","text":"Given an existing NetBox plugin, it will range from straightforward to very complicated to create a port of this plugin that's compatible with Nautobot, though in general it should be easier than developing a comparable plugin entirely from scratch. Of course, it would be impossible to provide a generalized, step-by-step guide that would cover all possibilities, but this document at least documents some known tips and tricks for this purpose.","title":"Porting NetBox Plugins to Nautobot"},{"location":"plugins/porting-from-netbox.html#updating-python-module-import-paths","text":"The most likely first issue you will encounter will be a module import problem, and in most cases a simple change to the name of imported modules will suffice: circuits.* -> nautobot.circuits.* dcim.* -> nautobot.dcim.* extras.* -> nautobot.extras.* ipam.* -> nautobot.ipam.* netbox.* -> nautobot.core.* tenancy.* -> nautobot.tenancy.* utilities.* -> nautobot.utilities.* virtualization.* -> nautobot.virtualization.*","title":"Updating Python module import paths"},{"location":"plugins/porting-from-netbox.html#regenerating-database-migrations","text":"In general, your migrations files will not port over easily; you will probably want to delete and re-generate them ( nautobot-server makemigrations <plugin-name> ) instead.","title":"Regenerating database migrations"},{"location":"release-notes/index.html","text":"Release Notes \u00b6 All the published release notes can be found via the navigation menu. All patch releases are included in the same minor release (e.g. Version 1.4 ) document.","title":"Overview"},{"location":"release-notes/index.html#release-notes","text":"All the published release notes can be found via the navigation menu. All patch releases are included in the same minor release (e.g. Version 1.4 ) document.","title":"Release Notes"},{"location":"release-notes/version-1.0.html","text":"Nautobot v1.0 \u00b6 This document describes all new features and changes in Nautobot 1.0, a divergent fork of NetBox 2.10. For the launch of Nautobot 1.0 and for the purpose of this document, all \u201cnew\u201d features or \u201cchanges\u201d are referring to the features and changes comparing Nautobot 1.0 coming from NetBox 2.10. All future release notes will only refer to features and changes relative to prior releases of Nautobot. Users migrating from NetBox to Nautobot should also refer to the \"Migrating from NetBox\" documentation as well. Release Overview \u00b6 Added \u00b6 Configuration Context Association to Device Types \u00b6 Config contexts can now be associated to (filtered by) Device Types, in addition to all other previously supported associations. Custom Fields on All Models \u00b6 Custom fields allow user-defined fields, or attributes, on specific data models such as sites or devices. Historically, custom fields have been supported only on \u201cprimary\u201d models (Site, Device, Rack, Virtual Machine, etc.) but not on \u201corganizational\u201d models (Region, Device Platform, Rack Group, etc.) or on \u201cdevice component\u201d models like interfaces. As of Nautobot 1.0, custom fields are now supported on every model, including interfaces. Once created the name or data type of the custom field cannot be modified. Choices for custom fields are now stored as discrete database objects. Choices that are in active use cannot be deleted. Customizable Statuses \u00b6 A new \"Status\" model has been added, allowing users to define additional permitted values for the \"status\" field on any or all of the models that have such a field (Cable, Circuit, Device, IPAddress, PowerFeed, Prefix, Rack, Site, VirtualMachine, VLAN). The default sets of statuses permitted for each model remain the same as in NetBox 2.10, but you are now free to define additional status values as suit your needs and workflows. One example application for custom statuses would be in defining additional values to apply to a Device as part of an automation workflow, with statuses such as upgrading or rebooting to reflect the progress of each device through the workflow, allowing automation to identify the appropriate next action to take for each status. Data Validation Plugin API \u00b6 Data quality assurance in Nautobot becomes easier with the new data validation plugin API . This makes it possible to codify organizational standards. Using a data validation plugin , an organization can ensure all data stored in Nautobot meets its specific standards, such as enforcing device naming standards, ensuring certain prefixes are never used, asserting that VLANs always have a name, requiring interfaces to always have a description, etc. The ability to ensure a high quality of data becomes much more streamlined; error-prone, manual process becomes automated; and there is no more need to actively run reports to check data quality. Detail Views for more Models \u00b6 Detailed view pages are now provided for models including ClusterGroup, ClusterType, DeviceRole, Manufacturer, Platform, and RackRole. Docker-Based Development Environment \u00b6 In addition to the previously available virtual-environment-based developer workflow, Nautobot now additionally supports a development environment based around Docker as an alternative. Git Integration as a Data Source \u00b6 Git integration offers users an option to integrate into a more traditional NetDevOps pipeline for managing Python modules, Jinja templates, and YAML/JSON data. There are several use cases that have historically required users to either manage Python modules on the filesystem or use Jinja2 templates within the GUI. With this new feature, users can add a Git repository from the UI or REST API, the contents of which will be synchronized into Nautobot immediately and can be later refreshed on-demand. This allows users to more easily update and manage: Jobs - store your Python modules that define Jobs (formerly known as Custom Scripts and/or Reports) in a Git repository Export Templates - store your Jinja templates used to create an export template in a Git repository Config Contexts - store your YAML/JSON data used within a config context in a Git repository Arbitrary Files - usable by custom plugins and apps Not only does this integration and feature simplify management of these features in Nautobot, it offers users the ability to use Git workflows for the management of the jobs, templates, and data ensuring there has been proper review and approval before updating them on the system. GraphQL Support \u00b6 Nautobot now provides an HTTP API endpoint supporting GraphQL . This feature adds a tremendous amount of flexibility in querying data from Nautobot. It offers the ability to query for specific datasets across multiple models in a single query. Historically, if you wanted to retrieve the list of devices, all of their interfaces, and all of their neighbors, this would require numerous REST API calls. GraphQL gives the flexibility to get all the data desired and nothing unnecessary, all in a single API call. For more details, please refer to the GraphQL website, as well as to the Nautobot GraphQL documentation. Installable Python Package \u00b6 Nautobot is now installable as a self-contained Python package via pip install nautobot . Packages are released to PyPI with every Nautobot update. nautobot-server command \u00b6 Nautobot now includes a dedicated administrative CLI command, nautobot-server . Plugin API Enhancements \u00b6 Plugins can now provide custom data validation logic. Plugins can now include executable Jobs (formerly known as Custom Scripts and Reports) that will automatically be added to the list of available Jobs for a user to execute. Additional data models defined by a plugin are automatically made available in GraphQL . Plugins can now define additional Django apps that they require and these dependencies will be automatically enabled when the plugin is activated. Nautobot now allows and encourages plugins to make use of the generic view classes and page templates provided in nautobot.core.views.generic and nautobot/core/templates/generic/ respectively. Single Sign-On / Social Authentication Support \u00b6 Nautobot now supports single sign on as an authentication option using OAuth2, OpenID, SAML, and others, using the social-auth-app-django module. For more details please refer to the guide on SSO authentication . User-Defined Relationships \u00b6 User-Defined, or \"custom\", relationships allow users to create their own relationships between models in Nautobot to best suit the needs of their specific network design. For example, a VLAN is mapped to a Site by default. After a VLAN is created today, you then assign that VLAN to an Interface on a Device. This Device should be within the initial mapped Site. However, many networks today have different requirements and relationships for VLANs (and many other models): VLANs may be limited to racks in Layer 3 DC fabrics; VLANs may be mapped to multiple buildings in a campus; they may span sites. Relationships allow you to express these additional requirements and relationships without requiring code changes to Nautobot itself. Other use cases include circuits, ASNs, or IP addressing -- just to name a few -- allowing users to define the exact relationships required for their network. Changed \u00b6 Code Reorganization \u00b6 All of the individual Django apps in NetBox ( dcim , extras , ipam , etc.) have been moved into a common nautobot Python package namespace. The netbox application namespace has been moved to nautobot.core . This will require updates when porting NetBox custom scripts and reports to Nautobot jobs, as well as when porting NetBox plugins to Nautobot. Packaging Changes \u00b6 Nautobot is now packaged using Poetry and builds as an installable Python package. setup.py and requirements.txt have been replaced with pyproject.toml . Releases of Nautobot are now published to PyPI , the Python Package Index, and therefore can now be installed using pip install nautobot . Installation and Startup \u00b6 Because Nautobot may be installed using pip , we have replaced manage.py with a dedicated nautobot-server CLI command used to adminster the server. It works exactly as manage.py does, but does not require you to be within the project root directory. Configuration and Settings \u00b6 Nautobot has done away with the requirement to duplicate or modify files anywhere in the source code. The configuration.py file has been replaced with a nautobot_config.py file that may be read from anywhere on your system. It is also much easier to add custom settings or overload nearly any default setting. To facilitate this, many automatically generated settings have been removed, and replaced with their underlying static configurations. We feel this affords a greater amount of flexibility in deployment patterns, with a tradeoff of slightly more initial configuration. To make things a little easier, you may generate a new configuration with sane defaults using the nautobot-server init command! The configuration file defaults to ~/.nautobot/nautobot_config.py but using the nautobot-server --config argument, you may name or place the file anywhere you choose. You may also defined a NAUTOBOT_CONFIG variable to tell Nautobot where to find the file so that you don't need to always pass the --config argument. For details see Configuring Nautobot . Consolidating Custom Scripts and Reports into Jobs \u00b6 Nautobot has consolidated NetBox's \"custom scripts\" and \"reports\" into what is now called Jobs . The job history ( results ) table on the home page now shows metadata on each job such as the timestamp and the user that executed the job. Additionally, jobs can be defined and executed by the system and by plugins, and when they are, users can see their results in the history too. UI views have been added for viewing the details of a given job result, and the JobResult model now provides standard APIs for Jobs to log their status and results in a consistent way. Job result history is now retained indefinitely unless intentionally deleted. Historically only the most recent result for each custom script or report was retained and all older records were deleted. Python modules that define jobs can now be stored in Git and easily added to Nautobot via the UI as documented above in Git Integration as a Data Source . Custom User Model \u00b6 A new custom model has been created for User data. This has allowed Nautobot to use a UUID as a primary key for the User model, and to prepare for future use-cases not support by the default Django model. This has also meant UserConfig no longer exists as a separate model. UserConfig is now a property on the custom User class. Hiding UI Elements based on Permissions \u00b6 Historically, a user viewing the home page and navigation menu would see a list of all model types and menu items in the system, with a \u201clock\u201d icon on items that they were not granted access to view in detail. As an option , administrators can now choose to instead hide un-permitted items altogether from the home page and the navigation menu, providing a simpler interface for limited-access users. The prior behavior remains as the default. IPAM Network Fields to VARBINARY \u00b6 To enable future support of databases other than PostgreSQL, the network fields inside of IPAM needed to be changed. cidr and inet field types have been replaced with a database agnostic field type. For this purpose varbinary was chosen because it can safely and efficiently store packed binary integers. More details about the impact of this and other changes can be found in the Migration documentation . Navigation Menu Changes \u00b6 The \"Other\" menu has been renamed to \"Extensibility\" and many new items have been added to this menu. Status records have been added to the \"Organization\" menu. New Name and Logo \u00b6 \"NetBox\" has been changed to \"Nautobot\" throughout the code, UI, and documentation, and Nautobot has a new logo and icon. User-Defined Custom Links \u00b6 Historically the custom links feature was restricted so that only administrators could define and manage custom links to add to various built-in data views. In Nautobot the management of custom links has been moved into the main user interface, accessible to any user who has been granted appropriate access permissions. User-Defined Export Templates \u00b6 Historically the custom data export templates feature was restricted such that only administrators could define and edit these templates. In Nautobot this has been moved into the main user interface, accessible to any user who has been granted appropriate access permissions. User-Defined Webhooks \u00b6 Historically the webhooks feature was restricted such that only administrators could define and manage webhooks, HTTP callbacks that are triggered automatically when a specified data model(s) are created, updated, and/or deleted. In Nautobot this has been moved into the main user interface, accessible to any user who has been granted appropriate access permissions. UUID Primary Database Keys \u00b6 Database keys are now defined as Universally Unique Identifiers (UUIDs) instead of integers, protecting against certain classes of data-traversal attacks. uWSGI \u00b6 Nautobot has replaced Gunicorn with uWSGI. In most cases uWSGI is faster, more stable and easier to setup making it ideal to use over Gunicorn. Our recommendation is to use uWSGI in production. Removed \u00b6 Secrets \u00b6 Secrets storage and management has been removed from Nautobot. Related Devices \u00b6 The \"Related Devices\" table has been removed from the detailed Device view. v1.0.3 (2021-06-21) \u00b6 Added \u00b6 #143 - Added \"copy\" button on hover to Device detail view for name, primary IP addresses, and serial number. #183 - Implemented a baseline integration test suite using Selenium #505 - Added example of Okta OAuth2 integration to the docs. #523 - Added instructions for using LDAP TLS Options to SSO documentation #576 - JobResult detail views now support custom links and plugin template extensions Changed \u00b6 #537 - To mitigate CVE-2021-31542, the minimum supported Django version is now 3.1.12. Fixed \u00b6 #220 - Added a troubleshooting section to the development guide for issues encountered when using the multi-threaded development server #342 - Fixed inconsistent behavior in Site.time_zone to emit and accept input as a null field if not set when using API #389 - Fixed incorrect TaggedItem base class that caused tag issues on MySQL. #421 - Fixed git: Reference at 'refs/heads/master' does not exist by improving error-handling displaying a warning when a user tries to use an empty repo or a branch that does not exist upstream. #452 - Fixed api/dcim/cables OPTIONS response not including the status field. #476 - Fixed incorrect handling of /31 and /127 networks in Aggregate , Prefix , and IPAddress models. #490 - Fixed incorrect VLAN count displayed in VLANGroup detail views. #499 - Fixed object's changelog showing incorrect information about its tags on partial (PATCH) updates using API #501 - Fixed missing prepopulation of address/prefix value into the form when adding an address or prefix under a parent prefix. #508 - Fixed typo in 500.html page template. #512 - Fixed ServerError when cloning a record with exactly one Tag applied to it. #513 - Fixed inadvertent omission of \"Search\" box from ReadTheDocs. #528 - Fixed an ordering issue in the test_EXTERNAL_AUTH_DEFAULT_groups test case. #530 - Fixed incorrect/confusing docstring in nautobot.core.api.serializers.WritableNestedSerializer #540 - Fixed intermittent CI failures due to DockerHub rate limits. #542 - Fixed incorrect documentation for running nautobot-server test commands. #562 - Fixed inability to use a Git repository to define a ConfigContext mapped to a specific DeviceType . #564 - Fixed incorrect docstring on nautobot.utilities.tables.ButtonsColumn . #570 - Fixed inability to import ExportTemplates for the VLAN model via Git. #583 - Fixed incorrect rejection of various forms when explicitly selecting a null option. (Port of NetBox #5704 ) Security \u00b6 #418 - Removed unused JQuery-UI component flagged by vulnerability scanner (CVE-2020-7729) v1.0.2 (2021-05-27) \u00b6 Added \u00b6 #14 - Plugins are now officially permitted to use the generic view classes defined in nautobot.core.views.generic and corresponding base templates defined in nautobot/core/templates/generic/ . #162 - Added Invoke tasks dumpdata and loaddata for database backup/restoration in the development environment. #430 - GraphQL ip_addresses now includes an assigned_object field #438 - Config contexts can now be assigned to individual DeviceTypes. #442 - Added warning when mixing @extras_features(\"graphql\") with explicitly declared GraphQL types #450 - GraphQL ip_addresses now includes interface and vminterface fields; GraphQL interfaces and similar models now include connected_endpoint and path fields #451 - Added static GraphQL type for VirtualMachine model #456 - Added mkdocs-include-markdown-plugin #465 - Added Virtual Chassis to the Home Page Changed \u00b6 #423 - Clarified reference to /config_contexts/ folder in Git user guide #448 - nautobot-server init no longer provides an option to overwrite the existing configuration files. #474 - The dummy_plugin has been moved to a new examples directory in the Git repository and now serves as an example of implementing various plugin features. Fixed \u00b6 #309 - Fixed erroneous termination display when cables are connected to power feeds. #396 - Fixed ValidationError not being raised when Relationship filters are invalid #397 - Fixed Git repository sync failure when token contains special characters #415 - Fixed incorrect handling of Unicode in view test cases #417 - Fixed incorrect link to Docker docs from installation docs #428 - Fixed GraphQL error when handling ASNs greater than 2147483647 #430 - Fixed missing ContentType foreign keys in GraphQL #436 - Fixed Redis Cacheops error when using newly generated nautobot_config.py file #454 - Fixed inability to create IPv6 addresses via REST API. #459 - Fixed issue with Job forms not respecting field_order #461 - Fixed NAUTOBOT_DB_TIMEOUT read as string in default config #482 - Fixed FieldError from being raised when a JobResult references a model with no name field #486 - Fixed failing Docker builds due to do missing examples development dependency #488 - Fix migrations in MySQL by hard-coding the VarbinaryIPField to use varbinary(16) Removed \u00b6 #456 - Removed markdown-include v1.0.1 (2021-05-06) \u00b6 Added \u00b6 #242 - Added a production-ready Dockerfile for clustered deployment #356 - Added a new nautobot-server startplugin management command to ease plugin development #366 - Added GraphQL filter tests for interfaces queries and added missing unit tests for Interface filtersets Changed \u00b6 #362 - Updated sample code in plugin development guide to inherit from BaseModel Fixed \u00b6 #15 - Added documentation for plugins using generic models to get change logging using ChangeLoggedModel #336 - Fixed nautobot.utilities.api.get_serializer_for_model to now support the plugins namespace #337 - Fixed nautobot.extras.plugins.api.views.PluginsAPIRootView no longer creates null entries when PluginConfig does not define a base_url #365 - Fixed incorrect field types on GraphQL ID fields #382 - Fixed choices returned from OPTIONS requests returning mixed use of display and display_name fields. #393 - Fixed creating a VirtualChassis with a master device changes the master device's vc_position #398 - Fixed VirtualChassis edit view to now show \"Update\" button vs. \"Create\" #399 - Fixed nautobot.utilities.utils.get_filterset_for_model to now support the plugins namespace #400 - Fixed the class_path format for Jobs API usage documentation not being clear enough #402 - Docs build requirements will now install markdown-include version from PyPI instead of GitHub #409 - Fixed misspelling: \"Datbase\" --> \"Database\" in nautobot_config.py.j2 v1.0.0 (2021-04-26) \u00b6 Added \u00b6 #290 - Added REST API endpoint for triggering a Git repository sync Changed \u00b6 #333 - Relationships now display the name of the related object type as well as the count #358 - Updated Python dependencies to their latest patch versions Fixed \u00b6 #276 - Fixed 500 error when creating Rack Reservation with invalid units #277 - Fixed 500 error when editing/updating IPAM Services with invalid ports #332 - Fixed UI allowing creation of multiple RelationshipAssociations for \" one_to_* \" relationships #334 - Fixed missing \"Bulk Create\" option when creating an IP Address #357 - Fixed error when plugins attempted to use ButtonsColumn #359 - Fixed incorrect GraphQL filtering of cables by site #361 - Fixed duplicate \"tags\" field when creating a cable connection v1.0.0b4 (2021-04-19) \u00b6 Added \u00b6 #96 - Implemented user guide documentation for GraphQL #97 - Implemented user guide documentation for Git as a Data Source Changed \u00b6 #150 - Revised all documentation referencing objects with status fields #175 - Revised plugin development guide to use Poetry #211 - Travis CI build improvements to simplify entry points and make tests fail faster #217 - Replaced JSONB aggregation with custom cross-database implementation that supports PG and MySQL #245 - Replaced PG-specific \"advisory locks\" with cross-database distributed Redis lock #252 - Revised and clarified install instructions for CentOS #262 - Revised Nautobot upgrade and NetBox migration guides #273 - Update to jQuery 3.6.0 #289 - Updated natural unicode-aware sorting for interface/device names to support MySQL Fixed \u00b6 #167 - Fix to enable to query ip_addresses by parent in GraphQL #212 - Allow plugins to use built-in buttons #232 - Fix to enable inclusion of custom fields in queries in GraphQL #233 - Fix to enable filtering by booleans in GraphQL #247 - Fix to enable filtering by custom field values in GraphQL #260 - Fix cable path tracing by not coercing UUID values to version 4 #264 - Fix missing parenthesis in datasources example #265 - Fix 500 crash in API when posting ports as strings to IPAM services #269 - Fix NoneType error when searching for /31 prefixes #272 - Fix invalid f-string in invoke createsuperuser #278 - Fix crash when sorting IPAM objects in list view by network address in web UI #285 - Refactor GraphQL filter argument generation to emit the correct types for each field #286 - Fix NoneType error when seraching for IPs without a prefix #287 - Fix IP addresses not showing in search results #288 - Fix display of parent prefixes from IPAddress detail view #293 - Allow DynamicModel[Multiple]ChoiceField to work with plugin model #300 - Fix AttributeError when assigning an IP to a device interface #304 - Fix for IPAM network objects clean() checks not working as intended #305 - Fix Status rendering to always preserve capitalization of Status.name #306 - Fix custom relationship display fields for all models #307 - Fix the ability to CSV export power connections if connected to a PowerFeed #308 - Fix missing template error when viewing a PowerFeed connected to a PowerPort on a Device. #318 - Fix TypeError when creating any IPAM network object with prefix of /0 #320 - Fix issue causing model validation to fail on all IPAM network objects #324 - Fix unit test execution on MySQL by changing subquery limiting to list slicing #325 - Fix to allow relationship associations to be unset in the web UI #326 - Fix 404 error when attempting to delete a RelationshipAssociation from the list view #373 - Fix missing \"Bulk Add IP Addresses\" tab v1.0.0b3 (2021-04-05) \u00b6 Warning v1.0.0b3 introduces several database changes that are not backwards-compatible with v1.0.0b2 and earlier. There is no direct upgrade path from v1.0.0b2 to v1.0.0b3 - you must create a new database when installing v1.0.0b3! Added \u00b6 #100 - Added detailed documentation of the nautobot-server command #105 - Added tooltip with detailed information to utilization graph bars. #109 - Docker development environment build now automatically installs from any present local_requirements.txt file #121 - Added \"Data Model Changes\" section to the \"Migrating from NetBox\" documentation #141 - Custom Link UI now includes example usage hints #227 - Add QFSP+ (64GFC) FiberChannel interface type #236 - Add post_upgrade to developer docs and add invoke post-upgrade Changed \u00b6 Major backwards-incompatible database changes were included in this beta release that are intended are to pave the way for us to support MySQL as a database backend in a future update. Of those changes, these are the most notable: All IPAM objects with network field types ( ipam.Aggregate , ipam.IPAddress , and ipam.Prefix ) are no longer hard-coded to use PostgreSQL-only inet or cidr field types and are now using a custom implementation leveraging SQL-standard varbinary field types The users.User model has been completely replaced with a custom implementation that no longer requires the use of a secondary database table for storing user configuration. Custom Fields have been overhauled for asserting data integrity and improving user experience Custom Fields can no longer be renamed or have their type changed after they have been created. Choices for Custom Fields are now stored as discrete database objects. Choices that are in active use cannot be deleted. Other changes: #78 - Replaced PostgreSQL-specific IP network/address fields with more generic field types #83 - Custom user model added; UserConfig model merged into User model #84 - Revised developer documentation for clarity and current workflows #98 - Simplify MultipleContentTypeField boilerplate #119 - Various documentation improvements #120 - Revise development release checklist document for new processes #128 - Overview of usage for the nautobot-netbox-importer plugin could be mistaken for full instructions #122 - Improved installation flow for creating nautobot user and virtualenv #131 - Replaced PostgreSQL-specific ArrayField with a more generic JSONArrayField #137 - Explicitly disallow Custom Field Name Changes #142 - Converted various config validation checks into proper Django checks #149 - Unify optional settings documentation for REMOTE_AUTH*/SOCIAL_AUTH* #159 - Update documentation for external authentication SSO Backend to get a proper install #180 - Revised available Invoke tasks for simplicity and maintainability #208 - Custom fields model refactor #216 - Update install docs to address inconsistent experience w/ $PATH #235 - Update restart docs to include worker #241 - Swap contrib.postgres.fields.JSONField for db.models.JSONField Removed \u00b6 #124 - Removed incorrect statement from feature request template #161 - Removed leftover references in documentation to RQ_DEFAULT_TIMEOUT #188 - Remove CSRF_TRUSTED_ORIGINS from core settings #189 - Remove all references to settings.BASE_PATH Fixed \u00b6 #26 - nautobot-server runserver does not work using poetry run #58 - GraphQL Device Query - Role filter issue #76 - Cable paths could not be traced through circuits #95 - Plugin load errors under Gunicorn #127 - SSL error: decryption failed or bad record mac & SSL SYSCALL error: EOF detected #132 - Generated nautobot_config.py did not include a trailing newline #134 - Missing venv activation step in install guide #135 - Custom field Selection value name change causes data inconsistency #147 - Login failed when BASE_PATH is set #153 - Editing an existing user token shows \"create\" buttons instead of \"update\" #154 - Some tests were failing when run in the development Docker container #155 - NAPALM driver string not displayed in Platform detail view #166 - Contrib directory is missing (including the apache.conf) #168 - Incorrect AUTHENTICATION_BACKENDS example in remote authentication documentation #170 - GraphQL filtering failure returned all objects instead of none #172 - Incorrect whitespace in some HTML template tags #181 - Incorrect UI reference in Webhook documentation #185 - Possible infinite loop in cable tracing algorithm #186 - Example Jobs are not updated for Nautobot #201 - Custom Fields cannot filter by name for content_types #205 - API Documentation shows numeric id instead of UUID #213 - Programming Error Exception Value: relation \"social_auth_usersocialauth\" does not exist #224 - Edit view for IPAM network objects does not emit the current network address value #255 - Update docs edit_uri to point to correct path v1.0.0b2 (2021-03-08) \u00b6 Added \u00b6 #35 - Documentation for troubleshooting Nautobot's interaction with SELinux. #47 - Basic user documentation for Relationships feature. #48 - Additional unit testing and bug fixes for Relationships feature. #99 - Add BASE_PATH to development/nautobot_config.py . #101 - Complete documentation of NAUTOBOT_ROOT #107 - Add nautobot-server post_upgrade command Changed \u00b6 #52 - Disabled Poetry's \"parallel installation\" feature for CI and development builds. #61 - Updated pull request template contents for clarity. #74 - Refactor install instructions to be more streamlined and more intuitive. Renamed nautobot-rq service to nautobot-worker Replaced BASE_STORAGE_DIR configuration setting with NAUTOBOT_ROOT ; this new setting also influences the default value of DEFAULT_CONFIG_PATH . #88 - Replace Gunicorn w/ uWSGI #89 - Development workflow improvements Replace pycodestyle with flake8 for linting. Add invoke black and invoke tests commands Improve speed of development Docker container rebuilds django-debug-toolbar is now a development dependency rather than a production dependency for Nautobot. #106 - Revise deployment docs to use $PATH instead of venv activate #108 - Document special workflow for development using containers on remote servers Removed \u00b6 #72 - Removed issue template for \"Documentation Change\"; use \"Bug\" or \"Feature Request\" issue templates instead. Fixed \u00b6 #36 - Broken links to ReadTheDocs pages. #41 - Incorrect field name in CustomLink Admin page. #42 - Incorrect link to nautobot-plugin-golden-config GitHub repository #45 - Incorrect button labels when creating/editing an Interface record. #43 - Incorrect commands in documentation for adding optional dependencies to local_requirements.txt #51 - Incorrect functioning of \"development container\" in VSCode integration. #57 - Incorrect AUTHENTICATION_BACKENDS example in authentication/ldap.md #63 - Incorrect help text for \"Destination Label\" field when creating/editing Relationship records. #64 - Incorrect absolute link to ReadTheDocs page. #69 - More incorrect links to ReadTheDocs pages. #79 - Incorrect internal documentation link to STORAGE_BACKEND optional setting. #81 - Unable to change Device rack position after creation. #93 - Bug when setting CACHEOPS_DEFAULTS timeout value to 0 . v1.0.0b1 (2021-02-24) \u00b6 Initial public beta release. Fixed \u00b6 Fixed a bug, inherited from NetBox 2.10, in which object permissions were not filtered correctly in the admin interface. Fixed a bug, inherited from NetBox 2.10, in which the UI would report an exception if the database contains ChangeLog entries that reference a nonexistent ContentType.","title":"Version 1.0"},{"location":"release-notes/version-1.0.html#nautobot-v10","text":"This document describes all new features and changes in Nautobot 1.0, a divergent fork of NetBox 2.10. For the launch of Nautobot 1.0 and for the purpose of this document, all \u201cnew\u201d features or \u201cchanges\u201d are referring to the features and changes comparing Nautobot 1.0 coming from NetBox 2.10. All future release notes will only refer to features and changes relative to prior releases of Nautobot. Users migrating from NetBox to Nautobot should also refer to the \"Migrating from NetBox\" documentation as well.","title":"Nautobot v1.0"},{"location":"release-notes/version-1.0.html#release-overview","text":"","title":"Release Overview"},{"location":"release-notes/version-1.0.html#added","text":"","title":"Added"},{"location":"release-notes/version-1.0.html#configuration-context-association-to-device-types","text":"Config contexts can now be associated to (filtered by) Device Types, in addition to all other previously supported associations.","title":"Configuration Context Association to Device Types"},{"location":"release-notes/version-1.0.html#custom-fields-on-all-models","text":"Custom fields allow user-defined fields, or attributes, on specific data models such as sites or devices. Historically, custom fields have been supported only on \u201cprimary\u201d models (Site, Device, Rack, Virtual Machine, etc.) but not on \u201corganizational\u201d models (Region, Device Platform, Rack Group, etc.) or on \u201cdevice component\u201d models like interfaces. As of Nautobot 1.0, custom fields are now supported on every model, including interfaces. Once created the name or data type of the custom field cannot be modified. Choices for custom fields are now stored as discrete database objects. Choices that are in active use cannot be deleted.","title":"Custom Fields on All Models"},{"location":"release-notes/version-1.0.html#customizable-statuses","text":"A new \"Status\" model has been added, allowing users to define additional permitted values for the \"status\" field on any or all of the models that have such a field (Cable, Circuit, Device, IPAddress, PowerFeed, Prefix, Rack, Site, VirtualMachine, VLAN). The default sets of statuses permitted for each model remain the same as in NetBox 2.10, but you are now free to define additional status values as suit your needs and workflows. One example application for custom statuses would be in defining additional values to apply to a Device as part of an automation workflow, with statuses such as upgrading or rebooting to reflect the progress of each device through the workflow, allowing automation to identify the appropriate next action to take for each status.","title":"Customizable Statuses"},{"location":"release-notes/version-1.0.html#data-validation-plugin-api","text":"Data quality assurance in Nautobot becomes easier with the new data validation plugin API . This makes it possible to codify organizational standards. Using a data validation plugin , an organization can ensure all data stored in Nautobot meets its specific standards, such as enforcing device naming standards, ensuring certain prefixes are never used, asserting that VLANs always have a name, requiring interfaces to always have a description, etc. The ability to ensure a high quality of data becomes much more streamlined; error-prone, manual process becomes automated; and there is no more need to actively run reports to check data quality.","title":"Data Validation Plugin API"},{"location":"release-notes/version-1.0.html#detail-views-for-more-models","text":"Detailed view pages are now provided for models including ClusterGroup, ClusterType, DeviceRole, Manufacturer, Platform, and RackRole.","title":"Detail Views for more Models"},{"location":"release-notes/version-1.0.html#docker-based-development-environment","text":"In addition to the previously available virtual-environment-based developer workflow, Nautobot now additionally supports a development environment based around Docker as an alternative.","title":"Docker-Based Development Environment"},{"location":"release-notes/version-1.0.html#git-integration-as-a-data-source","text":"Git integration offers users an option to integrate into a more traditional NetDevOps pipeline for managing Python modules, Jinja templates, and YAML/JSON data. There are several use cases that have historically required users to either manage Python modules on the filesystem or use Jinja2 templates within the GUI. With this new feature, users can add a Git repository from the UI or REST API, the contents of which will be synchronized into Nautobot immediately and can be later refreshed on-demand. This allows users to more easily update and manage: Jobs - store your Python modules that define Jobs (formerly known as Custom Scripts and/or Reports) in a Git repository Export Templates - store your Jinja templates used to create an export template in a Git repository Config Contexts - store your YAML/JSON data used within a config context in a Git repository Arbitrary Files - usable by custom plugins and apps Not only does this integration and feature simplify management of these features in Nautobot, it offers users the ability to use Git workflows for the management of the jobs, templates, and data ensuring there has been proper review and approval before updating them on the system.","title":"Git Integration as a Data Source"},{"location":"release-notes/version-1.0.html#graphql-support","text":"Nautobot now provides an HTTP API endpoint supporting GraphQL . This feature adds a tremendous amount of flexibility in querying data from Nautobot. It offers the ability to query for specific datasets across multiple models in a single query. Historically, if you wanted to retrieve the list of devices, all of their interfaces, and all of their neighbors, this would require numerous REST API calls. GraphQL gives the flexibility to get all the data desired and nothing unnecessary, all in a single API call. For more details, please refer to the GraphQL website, as well as to the Nautobot GraphQL documentation.","title":"GraphQL Support"},{"location":"release-notes/version-1.0.html#installable-python-package","text":"Nautobot is now installable as a self-contained Python package via pip install nautobot . Packages are released to PyPI with every Nautobot update.","title":"Installable Python Package"},{"location":"release-notes/version-1.0.html#nautobot-server-command","text":"Nautobot now includes a dedicated administrative CLI command, nautobot-server .","title":"nautobot-server command"},{"location":"release-notes/version-1.0.html#plugin-api-enhancements","text":"Plugins can now provide custom data validation logic. Plugins can now include executable Jobs (formerly known as Custom Scripts and Reports) that will automatically be added to the list of available Jobs for a user to execute. Additional data models defined by a plugin are automatically made available in GraphQL . Plugins can now define additional Django apps that they require and these dependencies will be automatically enabled when the plugin is activated. Nautobot now allows and encourages plugins to make use of the generic view classes and page templates provided in nautobot.core.views.generic and nautobot/core/templates/generic/ respectively.","title":"Plugin API Enhancements"},{"location":"release-notes/version-1.0.html#single-sign-on-social-authentication-support","text":"Nautobot now supports single sign on as an authentication option using OAuth2, OpenID, SAML, and others, using the social-auth-app-django module. For more details please refer to the guide on SSO authentication .","title":"Single Sign-On / Social Authentication Support"},{"location":"release-notes/version-1.0.html#user-defined-relationships","text":"User-Defined, or \"custom\", relationships allow users to create their own relationships between models in Nautobot to best suit the needs of their specific network design. For example, a VLAN is mapped to a Site by default. After a VLAN is created today, you then assign that VLAN to an Interface on a Device. This Device should be within the initial mapped Site. However, many networks today have different requirements and relationships for VLANs (and many other models): VLANs may be limited to racks in Layer 3 DC fabrics; VLANs may be mapped to multiple buildings in a campus; they may span sites. Relationships allow you to express these additional requirements and relationships without requiring code changes to Nautobot itself. Other use cases include circuits, ASNs, or IP addressing -- just to name a few -- allowing users to define the exact relationships required for their network.","title":"User-Defined Relationships"},{"location":"release-notes/version-1.0.html#changed","text":"","title":"Changed"},{"location":"release-notes/version-1.0.html#code-reorganization","text":"All of the individual Django apps in NetBox ( dcim , extras , ipam , etc.) have been moved into a common nautobot Python package namespace. The netbox application namespace has been moved to nautobot.core . This will require updates when porting NetBox custom scripts and reports to Nautobot jobs, as well as when porting NetBox plugins to Nautobot.","title":"Code Reorganization"},{"location":"release-notes/version-1.0.html#packaging-changes","text":"Nautobot is now packaged using Poetry and builds as an installable Python package. setup.py and requirements.txt have been replaced with pyproject.toml . Releases of Nautobot are now published to PyPI , the Python Package Index, and therefore can now be installed using pip install nautobot .","title":"Packaging Changes"},{"location":"release-notes/version-1.0.html#installation-and-startup","text":"Because Nautobot may be installed using pip , we have replaced manage.py with a dedicated nautobot-server CLI command used to adminster the server. It works exactly as manage.py does, but does not require you to be within the project root directory.","title":"Installation and Startup"},{"location":"release-notes/version-1.0.html#configuration-and-settings","text":"Nautobot has done away with the requirement to duplicate or modify files anywhere in the source code. The configuration.py file has been replaced with a nautobot_config.py file that may be read from anywhere on your system. It is also much easier to add custom settings or overload nearly any default setting. To facilitate this, many automatically generated settings have been removed, and replaced with their underlying static configurations. We feel this affords a greater amount of flexibility in deployment patterns, with a tradeoff of slightly more initial configuration. To make things a little easier, you may generate a new configuration with sane defaults using the nautobot-server init command! The configuration file defaults to ~/.nautobot/nautobot_config.py but using the nautobot-server --config argument, you may name or place the file anywhere you choose. You may also defined a NAUTOBOT_CONFIG variable to tell Nautobot where to find the file so that you don't need to always pass the --config argument. For details see Configuring Nautobot .","title":"Configuration and Settings"},{"location":"release-notes/version-1.0.html#consolidating-custom-scripts-and-reports-into-jobs","text":"Nautobot has consolidated NetBox's \"custom scripts\" and \"reports\" into what is now called Jobs . The job history ( results ) table on the home page now shows metadata on each job such as the timestamp and the user that executed the job. Additionally, jobs can be defined and executed by the system and by plugins, and when they are, users can see their results in the history too. UI views have been added for viewing the details of a given job result, and the JobResult model now provides standard APIs for Jobs to log their status and results in a consistent way. Job result history is now retained indefinitely unless intentionally deleted. Historically only the most recent result for each custom script or report was retained and all older records were deleted. Python modules that define jobs can now be stored in Git and easily added to Nautobot via the UI as documented above in Git Integration as a Data Source .","title":"Consolidating Custom Scripts and Reports into Jobs"},{"location":"release-notes/version-1.0.html#custom-user-model","text":"A new custom model has been created for User data. This has allowed Nautobot to use a UUID as a primary key for the User model, and to prepare for future use-cases not support by the default Django model. This has also meant UserConfig no longer exists as a separate model. UserConfig is now a property on the custom User class.","title":"Custom User Model"},{"location":"release-notes/version-1.0.html#hiding-ui-elements-based-on-permissions","text":"Historically, a user viewing the home page and navigation menu would see a list of all model types and menu items in the system, with a \u201clock\u201d icon on items that they were not granted access to view in detail. As an option , administrators can now choose to instead hide un-permitted items altogether from the home page and the navigation menu, providing a simpler interface for limited-access users. The prior behavior remains as the default.","title":"Hiding UI Elements based on Permissions"},{"location":"release-notes/version-1.0.html#ipam-network-fields-to-varbinary","text":"To enable future support of databases other than PostgreSQL, the network fields inside of IPAM needed to be changed. cidr and inet field types have been replaced with a database agnostic field type. For this purpose varbinary was chosen because it can safely and efficiently store packed binary integers. More details about the impact of this and other changes can be found in the Migration documentation .","title":"IPAM Network Fields to VARBINARY"},{"location":"release-notes/version-1.0.html#navigation-menu-changes","text":"The \"Other\" menu has been renamed to \"Extensibility\" and many new items have been added to this menu. Status records have been added to the \"Organization\" menu.","title":"Navigation Menu Changes"},{"location":"release-notes/version-1.0.html#new-name-and-logo","text":"\"NetBox\" has been changed to \"Nautobot\" throughout the code, UI, and documentation, and Nautobot has a new logo and icon.","title":"New Name and Logo"},{"location":"release-notes/version-1.0.html#user-defined-custom-links","text":"Historically the custom links feature was restricted so that only administrators could define and manage custom links to add to various built-in data views. In Nautobot the management of custom links has been moved into the main user interface, accessible to any user who has been granted appropriate access permissions.","title":"User-Defined Custom Links"},{"location":"release-notes/version-1.0.html#user-defined-export-templates","text":"Historically the custom data export templates feature was restricted such that only administrators could define and edit these templates. In Nautobot this has been moved into the main user interface, accessible to any user who has been granted appropriate access permissions.","title":"User-Defined Export Templates"},{"location":"release-notes/version-1.0.html#user-defined-webhooks","text":"Historically the webhooks feature was restricted such that only administrators could define and manage webhooks, HTTP callbacks that are triggered automatically when a specified data model(s) are created, updated, and/or deleted. In Nautobot this has been moved into the main user interface, accessible to any user who has been granted appropriate access permissions.","title":"User-Defined Webhooks"},{"location":"release-notes/version-1.0.html#uuid-primary-database-keys","text":"Database keys are now defined as Universally Unique Identifiers (UUIDs) instead of integers, protecting against certain classes of data-traversal attacks.","title":"UUID Primary Database Keys"},{"location":"release-notes/version-1.0.html#uwsgi","text":"Nautobot has replaced Gunicorn with uWSGI. In most cases uWSGI is faster, more stable and easier to setup making it ideal to use over Gunicorn. Our recommendation is to use uWSGI in production.","title":"uWSGI"},{"location":"release-notes/version-1.0.html#removed","text":"","title":"Removed"},{"location":"release-notes/version-1.0.html#secrets","text":"Secrets storage and management has been removed from Nautobot.","title":"Secrets"},{"location":"release-notes/version-1.0.html#related-devices","text":"The \"Related Devices\" table has been removed from the detailed Device view.","title":"Related Devices"},{"location":"release-notes/version-1.0.html#v103-2021-06-21","text":"","title":"v1.0.3 (2021-06-21)"},{"location":"release-notes/version-1.0.html#added_1","text":"#143 - Added \"copy\" button on hover to Device detail view for name, primary IP addresses, and serial number. #183 - Implemented a baseline integration test suite using Selenium #505 - Added example of Okta OAuth2 integration to the docs. #523 - Added instructions for using LDAP TLS Options to SSO documentation #576 - JobResult detail views now support custom links and plugin template extensions","title":"Added"},{"location":"release-notes/version-1.0.html#changed_1","text":"#537 - To mitigate CVE-2021-31542, the minimum supported Django version is now 3.1.12.","title":"Changed"},{"location":"release-notes/version-1.0.html#fixed","text":"#220 - Added a troubleshooting section to the development guide for issues encountered when using the multi-threaded development server #342 - Fixed inconsistent behavior in Site.time_zone to emit and accept input as a null field if not set when using API #389 - Fixed incorrect TaggedItem base class that caused tag issues on MySQL. #421 - Fixed git: Reference at 'refs/heads/master' does not exist by improving error-handling displaying a warning when a user tries to use an empty repo or a branch that does not exist upstream. #452 - Fixed api/dcim/cables OPTIONS response not including the status field. #476 - Fixed incorrect handling of /31 and /127 networks in Aggregate , Prefix , and IPAddress models. #490 - Fixed incorrect VLAN count displayed in VLANGroup detail views. #499 - Fixed object's changelog showing incorrect information about its tags on partial (PATCH) updates using API #501 - Fixed missing prepopulation of address/prefix value into the form when adding an address or prefix under a parent prefix. #508 - Fixed typo in 500.html page template. #512 - Fixed ServerError when cloning a record with exactly one Tag applied to it. #513 - Fixed inadvertent omission of \"Search\" box from ReadTheDocs. #528 - Fixed an ordering issue in the test_EXTERNAL_AUTH_DEFAULT_groups test case. #530 - Fixed incorrect/confusing docstring in nautobot.core.api.serializers.WritableNestedSerializer #540 - Fixed intermittent CI failures due to DockerHub rate limits. #542 - Fixed incorrect documentation for running nautobot-server test commands. #562 - Fixed inability to use a Git repository to define a ConfigContext mapped to a specific DeviceType . #564 - Fixed incorrect docstring on nautobot.utilities.tables.ButtonsColumn . #570 - Fixed inability to import ExportTemplates for the VLAN model via Git. #583 - Fixed incorrect rejection of various forms when explicitly selecting a null option. (Port of NetBox #5704 )","title":"Fixed"},{"location":"release-notes/version-1.0.html#security","text":"#418 - Removed unused JQuery-UI component flagged by vulnerability scanner (CVE-2020-7729)","title":"Security"},{"location":"release-notes/version-1.0.html#v102-2021-05-27","text":"","title":"v1.0.2 (2021-05-27)"},{"location":"release-notes/version-1.0.html#added_2","text":"#14 - Plugins are now officially permitted to use the generic view classes defined in nautobot.core.views.generic and corresponding base templates defined in nautobot/core/templates/generic/ . #162 - Added Invoke tasks dumpdata and loaddata for database backup/restoration in the development environment. #430 - GraphQL ip_addresses now includes an assigned_object field #438 - Config contexts can now be assigned to individual DeviceTypes. #442 - Added warning when mixing @extras_features(\"graphql\") with explicitly declared GraphQL types #450 - GraphQL ip_addresses now includes interface and vminterface fields; GraphQL interfaces and similar models now include connected_endpoint and path fields #451 - Added static GraphQL type for VirtualMachine model #456 - Added mkdocs-include-markdown-plugin #465 - Added Virtual Chassis to the Home Page","title":"Added"},{"location":"release-notes/version-1.0.html#changed_2","text":"#423 - Clarified reference to /config_contexts/ folder in Git user guide #448 - nautobot-server init no longer provides an option to overwrite the existing configuration files. #474 - The dummy_plugin has been moved to a new examples directory in the Git repository and now serves as an example of implementing various plugin features.","title":"Changed"},{"location":"release-notes/version-1.0.html#fixed_1","text":"#309 - Fixed erroneous termination display when cables are connected to power feeds. #396 - Fixed ValidationError not being raised when Relationship filters are invalid #397 - Fixed Git repository sync failure when token contains special characters #415 - Fixed incorrect handling of Unicode in view test cases #417 - Fixed incorrect link to Docker docs from installation docs #428 - Fixed GraphQL error when handling ASNs greater than 2147483647 #430 - Fixed missing ContentType foreign keys in GraphQL #436 - Fixed Redis Cacheops error when using newly generated nautobot_config.py file #454 - Fixed inability to create IPv6 addresses via REST API. #459 - Fixed issue with Job forms not respecting field_order #461 - Fixed NAUTOBOT_DB_TIMEOUT read as string in default config #482 - Fixed FieldError from being raised when a JobResult references a model with no name field #486 - Fixed failing Docker builds due to do missing examples development dependency #488 - Fix migrations in MySQL by hard-coding the VarbinaryIPField to use varbinary(16)","title":"Fixed"},{"location":"release-notes/version-1.0.html#removed_1","text":"#456 - Removed markdown-include","title":"Removed"},{"location":"release-notes/version-1.0.html#v101-2021-05-06","text":"","title":"v1.0.1 (2021-05-06)"},{"location":"release-notes/version-1.0.html#added_3","text":"#242 - Added a production-ready Dockerfile for clustered deployment #356 - Added a new nautobot-server startplugin management command to ease plugin development #366 - Added GraphQL filter tests for interfaces queries and added missing unit tests for Interface filtersets","title":"Added"},{"location":"release-notes/version-1.0.html#changed_3","text":"#362 - Updated sample code in plugin development guide to inherit from BaseModel","title":"Changed"},{"location":"release-notes/version-1.0.html#fixed_2","text":"#15 - Added documentation for plugins using generic models to get change logging using ChangeLoggedModel #336 - Fixed nautobot.utilities.api.get_serializer_for_model to now support the plugins namespace #337 - Fixed nautobot.extras.plugins.api.views.PluginsAPIRootView no longer creates null entries when PluginConfig does not define a base_url #365 - Fixed incorrect field types on GraphQL ID fields #382 - Fixed choices returned from OPTIONS requests returning mixed use of display and display_name fields. #393 - Fixed creating a VirtualChassis with a master device changes the master device's vc_position #398 - Fixed VirtualChassis edit view to now show \"Update\" button vs. \"Create\" #399 - Fixed nautobot.utilities.utils.get_filterset_for_model to now support the plugins namespace #400 - Fixed the class_path format for Jobs API usage documentation not being clear enough #402 - Docs build requirements will now install markdown-include version from PyPI instead of GitHub #409 - Fixed misspelling: \"Datbase\" --> \"Database\" in nautobot_config.py.j2","title":"Fixed"},{"location":"release-notes/version-1.0.html#v100-2021-04-26","text":"","title":"v1.0.0 (2021-04-26)"},{"location":"release-notes/version-1.0.html#added_4","text":"#290 - Added REST API endpoint for triggering a Git repository sync","title":"Added"},{"location":"release-notes/version-1.0.html#changed_4","text":"#333 - Relationships now display the name of the related object type as well as the count #358 - Updated Python dependencies to their latest patch versions","title":"Changed"},{"location":"release-notes/version-1.0.html#fixed_3","text":"#276 - Fixed 500 error when creating Rack Reservation with invalid units #277 - Fixed 500 error when editing/updating IPAM Services with invalid ports #332 - Fixed UI allowing creation of multiple RelationshipAssociations for \" one_to_* \" relationships #334 - Fixed missing \"Bulk Create\" option when creating an IP Address #357 - Fixed error when plugins attempted to use ButtonsColumn #359 - Fixed incorrect GraphQL filtering of cables by site #361 - Fixed duplicate \"tags\" field when creating a cable connection","title":"Fixed"},{"location":"release-notes/version-1.0.html#v100b4-2021-04-19","text":"","title":"v1.0.0b4 (2021-04-19)"},{"location":"release-notes/version-1.0.html#added_5","text":"#96 - Implemented user guide documentation for GraphQL #97 - Implemented user guide documentation for Git as a Data Source","title":"Added"},{"location":"release-notes/version-1.0.html#changed_5","text":"#150 - Revised all documentation referencing objects with status fields #175 - Revised plugin development guide to use Poetry #211 - Travis CI build improvements to simplify entry points and make tests fail faster #217 - Replaced JSONB aggregation with custom cross-database implementation that supports PG and MySQL #245 - Replaced PG-specific \"advisory locks\" with cross-database distributed Redis lock #252 - Revised and clarified install instructions for CentOS #262 - Revised Nautobot upgrade and NetBox migration guides #273 - Update to jQuery 3.6.0 #289 - Updated natural unicode-aware sorting for interface/device names to support MySQL","title":"Changed"},{"location":"release-notes/version-1.0.html#fixed_4","text":"#167 - Fix to enable to query ip_addresses by parent in GraphQL #212 - Allow plugins to use built-in buttons #232 - Fix to enable inclusion of custom fields in queries in GraphQL #233 - Fix to enable filtering by booleans in GraphQL #247 - Fix to enable filtering by custom field values in GraphQL #260 - Fix cable path tracing by not coercing UUID values to version 4 #264 - Fix missing parenthesis in datasources example #265 - Fix 500 crash in API when posting ports as strings to IPAM services #269 - Fix NoneType error when searching for /31 prefixes #272 - Fix invalid f-string in invoke createsuperuser #278 - Fix crash when sorting IPAM objects in list view by network address in web UI #285 - Refactor GraphQL filter argument generation to emit the correct types for each field #286 - Fix NoneType error when seraching for IPs without a prefix #287 - Fix IP addresses not showing in search results #288 - Fix display of parent prefixes from IPAddress detail view #293 - Allow DynamicModel[Multiple]ChoiceField to work with plugin model #300 - Fix AttributeError when assigning an IP to a device interface #304 - Fix for IPAM network objects clean() checks not working as intended #305 - Fix Status rendering to always preserve capitalization of Status.name #306 - Fix custom relationship display fields for all models #307 - Fix the ability to CSV export power connections if connected to a PowerFeed #308 - Fix missing template error when viewing a PowerFeed connected to a PowerPort on a Device. #318 - Fix TypeError when creating any IPAM network object with prefix of /0 #320 - Fix issue causing model validation to fail on all IPAM network objects #324 - Fix unit test execution on MySQL by changing subquery limiting to list slicing #325 - Fix to allow relationship associations to be unset in the web UI #326 - Fix 404 error when attempting to delete a RelationshipAssociation from the list view #373 - Fix missing \"Bulk Add IP Addresses\" tab","title":"Fixed"},{"location":"release-notes/version-1.0.html#v100b3-2021-04-05","text":"Warning v1.0.0b3 introduces several database changes that are not backwards-compatible with v1.0.0b2 and earlier. There is no direct upgrade path from v1.0.0b2 to v1.0.0b3 - you must create a new database when installing v1.0.0b3!","title":"v1.0.0b3 (2021-04-05)"},{"location":"release-notes/version-1.0.html#added_6","text":"#100 - Added detailed documentation of the nautobot-server command #105 - Added tooltip with detailed information to utilization graph bars. #109 - Docker development environment build now automatically installs from any present local_requirements.txt file #121 - Added \"Data Model Changes\" section to the \"Migrating from NetBox\" documentation #141 - Custom Link UI now includes example usage hints #227 - Add QFSP+ (64GFC) FiberChannel interface type #236 - Add post_upgrade to developer docs and add invoke post-upgrade","title":"Added"},{"location":"release-notes/version-1.0.html#changed_6","text":"Major backwards-incompatible database changes were included in this beta release that are intended are to pave the way for us to support MySQL as a database backend in a future update. Of those changes, these are the most notable: All IPAM objects with network field types ( ipam.Aggregate , ipam.IPAddress , and ipam.Prefix ) are no longer hard-coded to use PostgreSQL-only inet or cidr field types and are now using a custom implementation leveraging SQL-standard varbinary field types The users.User model has been completely replaced with a custom implementation that no longer requires the use of a secondary database table for storing user configuration. Custom Fields have been overhauled for asserting data integrity and improving user experience Custom Fields can no longer be renamed or have their type changed after they have been created. Choices for Custom Fields are now stored as discrete database objects. Choices that are in active use cannot be deleted. Other changes: #78 - Replaced PostgreSQL-specific IP network/address fields with more generic field types #83 - Custom user model added; UserConfig model merged into User model #84 - Revised developer documentation for clarity and current workflows #98 - Simplify MultipleContentTypeField boilerplate #119 - Various documentation improvements #120 - Revise development release checklist document for new processes #128 - Overview of usage for the nautobot-netbox-importer plugin could be mistaken for full instructions #122 - Improved installation flow for creating nautobot user and virtualenv #131 - Replaced PostgreSQL-specific ArrayField with a more generic JSONArrayField #137 - Explicitly disallow Custom Field Name Changes #142 - Converted various config validation checks into proper Django checks #149 - Unify optional settings documentation for REMOTE_AUTH*/SOCIAL_AUTH* #159 - Update documentation for external authentication SSO Backend to get a proper install #180 - Revised available Invoke tasks for simplicity and maintainability #208 - Custom fields model refactor #216 - Update install docs to address inconsistent experience w/ $PATH #235 - Update restart docs to include worker #241 - Swap contrib.postgres.fields.JSONField for db.models.JSONField","title":"Changed"},{"location":"release-notes/version-1.0.html#removed_2","text":"#124 - Removed incorrect statement from feature request template #161 - Removed leftover references in documentation to RQ_DEFAULT_TIMEOUT #188 - Remove CSRF_TRUSTED_ORIGINS from core settings #189 - Remove all references to settings.BASE_PATH","title":"Removed"},{"location":"release-notes/version-1.0.html#fixed_5","text":"#26 - nautobot-server runserver does not work using poetry run #58 - GraphQL Device Query - Role filter issue #76 - Cable paths could not be traced through circuits #95 - Plugin load errors under Gunicorn #127 - SSL error: decryption failed or bad record mac & SSL SYSCALL error: EOF detected #132 - Generated nautobot_config.py did not include a trailing newline #134 - Missing venv activation step in install guide #135 - Custom field Selection value name change causes data inconsistency #147 - Login failed when BASE_PATH is set #153 - Editing an existing user token shows \"create\" buttons instead of \"update\" #154 - Some tests were failing when run in the development Docker container #155 - NAPALM driver string not displayed in Platform detail view #166 - Contrib directory is missing (including the apache.conf) #168 - Incorrect AUTHENTICATION_BACKENDS example in remote authentication documentation #170 - GraphQL filtering failure returned all objects instead of none #172 - Incorrect whitespace in some HTML template tags #181 - Incorrect UI reference in Webhook documentation #185 - Possible infinite loop in cable tracing algorithm #186 - Example Jobs are not updated for Nautobot #201 - Custom Fields cannot filter by name for content_types #205 - API Documentation shows numeric id instead of UUID #213 - Programming Error Exception Value: relation \"social_auth_usersocialauth\" does not exist #224 - Edit view for IPAM network objects does not emit the current network address value #255 - Update docs edit_uri to point to correct path","title":"Fixed"},{"location":"release-notes/version-1.0.html#v100b2-2021-03-08","text":"","title":"v1.0.0b2 (2021-03-08)"},{"location":"release-notes/version-1.0.html#added_7","text":"#35 - Documentation for troubleshooting Nautobot's interaction with SELinux. #47 - Basic user documentation for Relationships feature. #48 - Additional unit testing and bug fixes for Relationships feature. #99 - Add BASE_PATH to development/nautobot_config.py . #101 - Complete documentation of NAUTOBOT_ROOT #107 - Add nautobot-server post_upgrade command","title":"Added"},{"location":"release-notes/version-1.0.html#changed_7","text":"#52 - Disabled Poetry's \"parallel installation\" feature for CI and development builds. #61 - Updated pull request template contents for clarity. #74 - Refactor install instructions to be more streamlined and more intuitive. Renamed nautobot-rq service to nautobot-worker Replaced BASE_STORAGE_DIR configuration setting with NAUTOBOT_ROOT ; this new setting also influences the default value of DEFAULT_CONFIG_PATH . #88 - Replace Gunicorn w/ uWSGI #89 - Development workflow improvements Replace pycodestyle with flake8 for linting. Add invoke black and invoke tests commands Improve speed of development Docker container rebuilds django-debug-toolbar is now a development dependency rather than a production dependency for Nautobot. #106 - Revise deployment docs to use $PATH instead of venv activate #108 - Document special workflow for development using containers on remote servers","title":"Changed"},{"location":"release-notes/version-1.0.html#removed_3","text":"#72 - Removed issue template for \"Documentation Change\"; use \"Bug\" or \"Feature Request\" issue templates instead.","title":"Removed"},{"location":"release-notes/version-1.0.html#fixed_6","text":"#36 - Broken links to ReadTheDocs pages. #41 - Incorrect field name in CustomLink Admin page. #42 - Incorrect link to nautobot-plugin-golden-config GitHub repository #45 - Incorrect button labels when creating/editing an Interface record. #43 - Incorrect commands in documentation for adding optional dependencies to local_requirements.txt #51 - Incorrect functioning of \"development container\" in VSCode integration. #57 - Incorrect AUTHENTICATION_BACKENDS example in authentication/ldap.md #63 - Incorrect help text for \"Destination Label\" field when creating/editing Relationship records. #64 - Incorrect absolute link to ReadTheDocs page. #69 - More incorrect links to ReadTheDocs pages. #79 - Incorrect internal documentation link to STORAGE_BACKEND optional setting. #81 - Unable to change Device rack position after creation. #93 - Bug when setting CACHEOPS_DEFAULTS timeout value to 0 .","title":"Fixed"},{"location":"release-notes/version-1.0.html#v100b1-2021-02-24","text":"Initial public beta release.","title":"v1.0.0b1 (2021-02-24)"},{"location":"release-notes/version-1.0.html#fixed_7","text":"Fixed a bug, inherited from NetBox 2.10, in which object permissions were not filtered correctly in the admin interface. Fixed a bug, inherited from NetBox 2.10, in which the UI would report an exception if the database contains ChangeLog entries that reference a nonexistent ContentType.","title":"Fixed"},{"location":"release-notes/version-1.1.html","text":"Nautobot v1.1 \u00b6 This document describes all new features and changes in Nautobot 1.1. If you are a user migrating from NetBox to Nautobot, please refer to the \"Migrating from NetBox\" documentation. Release Overview \u00b6 Added \u00b6 Computed Fields ( #4 ) \u00b6 Computed fields offers users the ability to create read-only custom fields using existing data already stored in the database. Users define Jinja2 templates that populate the value of these fields. Computed fields are available on all data models that currently support custom fields. Config Context JSON Schemas ( #274 ) \u00b6 While config contexts allow for arbitrary data structures to be stored within Nautobot, at scale it is desirable to apply validation constraints to that data to ensure its consistency and to avoid data entry errors. To service this need, Nautobot supports optionally backing config contexts with JSON Schemas for validation. These schema are managed via the config context schema model and are optionally linked to config context instances, in addition to devices and virtual machines for the purpose of validating their local context data. Please see the docs for more details. Just like config contexts, config context JSON schemas can optionally be managed via a Git repository . Dynamic Navigation Menus ( #12 ) \u00b6 Applications and plugins can now define tabs, groups, items and buttons in the navigation menu. Using navigation objects a developer can add items to any section of the navigation using key names and weight values. Please see Application Registry for more details. MySQL Database Support ( #17 ) \u00b6 MySQL 8.x is now fully supported as a database backend! The installation and configuration guides have been revised to include MySQL. If you prefer MySQL or it is more easily supported in your environment, configuring Nautobot to use MySQL is as easy as changing value of ENGINE in your DATABASES setting to point to django.db.backends.mysql and installing the MySQL Python driver using pip3 install nautobot[mysql] . A new NAUTOBOT_DB_ENGINE environment variable has been added to allow for specifying the desired database engine at runtime without needing to modify your nautobot_config.py . Please see the configuration guide on DATABASES for more details on how to configure Nautobot to use MySQL. Please see the MySQL setup guides for Ubuntu and CentOS to get started. Plugin Defined Jinja2 Filters \u00b6 Plugins can now define custom Jinja2 filters to be used when rendering templates defined within computed fields or custom links. To register your own filters, you may add a jinja_filters.py to your plugin and any filters defined there will be automatically registered and globally usable. Please see the plugin development documentation on including Jinja2 filters to get started. Read Only Jobs ( #200 ) \u00b6 Jobs may be optionally marked as read only by setting the read_only = True meta attribute. This prevents the job from making any changes to nautobot data and suppresses certain log messages. Read only jobs can be a great way to safely develop new jobs, and for working with reporting use cases. Please see the Jobs documentation for more details. Saved GraphQL Queries ( #3 ) \u00b6 Saved GraphQL queries offers a new model where reusable queries can be stored in Nautobot. New views for managing saved queries are available; additionally, the GraphiQL interface has been augmented to allow populating the interface from a saved query, editing and saving new queries. Saved queries can easily be imported into the GraphiQL interface by using the new navigation tab located on the right side of the navbar. Inside the new tab are also buttons for editing and saving queries directly into Nautobot's databases. Additionally, two new GraphQL utility functions have been added to allow easy access to the GraphQL system from source code. Both functions can be imported from nautobot.core.graphql : execute_query() : Runs string as a query against GraphQL. execute_saved_query() : Execute a saved query from Nautobot database. Changed \u00b6 Background Tasks now use Celery ( #223 ) \u00b6 Celery has been introduced to eventually replace RQ for executing background tasks within Nautobot. All Nautobot core usage of RQ has been migrated to use Celery. Note Custom background tasks implemented by plugins are not part of Nautobot core functions Prior to version 1.1.0, Nautobot utilized RQ as the primary background task worker. As of Nautobot 1.1.0, RQ is now deprecated . RQ and the @job decorator for custom tasks are still supported for now, but will no longer be documented, and support for RQ will be removed in a future release. RQ support for custom tasks was not removed in order to give plugin authors time to migrate, however, to continue to utilize advanced Nautobot features such as Git repository synchronization, webhooks, jobs, etc. you must migrate your nautobot-worker deployment from RQ to Celery. Please see the section on migrating to Celery from RQ for more information on how to easily migrate your deployment. Warning If you are running plugins that use background tasks requiring the RQ worker, you will need to run both the RQ and Celery workers concurrently until the plugins are converted to use the Celery worker. See the Migrating to Celery from RQ for details. Fixed \u00b6 HTTP \"Remote end closed connection\" errors ( #725 ) \u00b6 The example uwsgi.ini provided in earlier versions of the Nautobot documentation was missing a recommendation to include the configuration http-keepalive = 1 which enables support for HTTP/1.1 keep-alive headers. Warning If you are upgrading from an earlier version of Nautobot (including 1.1.0) you should check your uwsgi.ini and ensure that it contains this important configuration line. v1.1.6 (2021-12-03) \u00b6 Fixed \u00b6 #1093 - Improved REST API performance by adding caching of serializer \"opt-in fields\". v1.1.5 (2021-11-11) \u00b6 Added \u00b6 #263 - Added a link in the Docker deployment guide to the Nautobot Docker Compose repository. #931 - Added support for direct upload of CSV files as an alternative to copy-pasting CSV text into a form field. (Partially based on NetBox #6561 ) #953 - Added option to use MySQL in the docker-compose development environment #954 - Added documentation for migrating from PostgreSQL to MySQL, improved documentation as to recommended MySQL database configuration. #989 - Added id and name fields to NestedJobResultSerializer for the REST API. #1019 - Added GitHub action to redeploy the Nautobot sandbox on pushes to main , develop , and next . #1025 - Added reference documentation for how to hide navigation menu items with no permissions. #1031 - Added a troubleshooting note around the combination of RedHat/CentOS, uWSGI, and static files. #1057 - Added GitHub action to automatically push Docker images to ghcr.io . Fixed \u00b6 #555 - Fixed Status.DoesNotExist during nautobot-server loaddata . #567 - Fixed incorrect GraphQL schema generation for _custom_field_data when certain plugins are installed. #733 - A Job erroring out early in initialization could result in its associated JobResult staying in Pending state indefinitely. #816 - Fixed AttributeError reported when viewing a Rack with certain associated power configurations. #948 - Fixed advanced logging example to use EXTRA_MIDDLEWARE instead of MIDDLEWARE.append() . #970 - Clarified documentation around config context definition in Git repositories. #981 - Fixed incorrect handling of missing custom fields in the fix_custom_fields management command. #986 - Fixed TemplateDoesNotExist exception when running a Job containing a FileVar variable. #991 - Fixed incorrect logging when importing ConfigContextSchemas from Git. #993 - Fixed incorrect git command when refreshing a previously checked out repository. #1023 - Removed invalid link in \"Deploying Nautobot\" documentation. Security \u00b6 #998 - Update mkdocs dependency to avoid a potential path-traversal vulnerability; note that mkdocs is only used in development and is not a production deployment dependency of Nautobot. v1.1.4 (2021-10-04) \u00b6 Added \u00b6 #623 - Git repository sync logs now include the commit hash that was synchronized to. #728 - Added SOCIAL_AUTH_BACKEND_PREFIX configuration setting to support custom authentication backends. #861 - Bulk editing of devices can now update their site, rack, and rack-group assignments. #949 - Added documentation note about using MAINTENANCE_MODE in combination with LDAP. Changed \u00b6 #956 - Switched CI from Travis to GitHub Actions. #964 - Updated README.md build status badge to show GitHub status. Fixed \u00b6 #944 - Jobs that commit changes to the database could not be invoked successfully from the nautobot-server runjob command. #955 - REST API endpoint for syncing Git repositories was still checking for RQ workers instead of Celery workers. #969 - IPv6 prefixes such as ::1/128 were not being treated correctly. Security \u00b6 #939 - Nautobot views now default to X-Frame-Options: DENY rather than X-Frame-Options: SAMEORIGIN , with the exception of the rack-elevation API view ( /api/dcim/rack-elevation/ ) which specifically requires X-Frame-Options: SAMEORIGIN for functional reasons. v1.1.3 (2021-09-13) \u00b6 Added \u00b6 #11 - Added tests to verify that plugin models can support webhooks if appropriately decorated with @extras_features(\"webhooks\") #652 - Jobs REST API run endpoint now can look up ObjectVar references via a dictionary of parameters. #755 - Added example showing how to use django-request-logging middleware to log the user associated with inbound requests. #791 - Added support for NAUTOBOT_DOCKER_SKIP_INIT variable to allow running the Docker container with a read-only database. #841 - Added more detailed documentation around defining Relationship filters. #850 - Added developer documentation around the installation and use of mkdocs to locally preview documentation changes. #856 - Added more detailed user documentation on how to create an API token. Changed \u00b6 #601 - Developer documentation for advanced docker-compose use cases is now a separate file. #709 - Computed fields can now have a blank fallback_value . #812 - In the GraphiQL interface, the \"Queries\" dropdown now appears alongside the other GraphiQL interface buttons instead of appearing in the main Nautobot navigation bar. #832 - Plugin installation documentation now recommends nautobot-server post_upgrade instead of separately running nautobot-server migrate and nautobot-server collectstatic . Fixed \u00b6 #464 - Fixed GraphQL schema generation error when certain custom field names are used. #651 - Fixed Jobs validation enforce schema consistently across UI and API. #670 - Clarified Jobs documentation regarding how to fail or abort a Job. #715 - Fixed display of GraphiQL interface in narrow browser windows. #718 - Fixed rendering of long template values in Computed Field detail view. #731 - Config context schemas including format properties (such as \"format\": \"ipv4\" ) are now correctly enforced. #779 - Fixed incorrect Tenant display in Prefix \"Duplicate Prefixes\" table. (Port of two fixes originally from NetBox) #809 - Fixed docker-compose file version values to work correctly with older versions of docker-compose. #818 - Database health-check now reports as healthy even when in MAINTENANCE_MODE . #825 - Removed unnecessary -B flag from development Celery worker invocation. #830 - Fixed incorrect database migration introduced by #818. #845 - Clarified documentation around nautobot-server init and NAUTOBOT_ROOT . #848 - Fixed stale links to NAPALM documentation Security \u00b6 #893 - Bump Pillow dependency version from 8.2.0 to 8.2.3 to address numerous critical CVE advisories v1.1.2 (2021-08-10) \u00b6 Added \u00b6 #758 - Added documentation about the Job class_path concept. #771 - Added examples of various possible logging configurations. #773 - Added documentation around enabling Prometheus metrics for database and caching backends. Changed \u00b6 #742 - The development environment now respects the setting of the NAUTOBOT_DEBUG environment variable if present. Fixed \u00b6 #723 - Fixed power draw not providing a UtilizationData type for use in graphing power draw utilization #782 - Corrected documentation regarding the use of docker-compose.override.yml #785 - Fixed plugin loading error when using final Docker image. #786 - Fixed Unknown command: 'post_upgrade' when using final Docker image. #789 - Avoid a NoReverseMatch exception at startup time if an app or plugin defines a nav menu item with an invalid link reference. v1.1.1 (2021-08-05) \u00b6 Added \u00b6 #506 - nautobot-server now detects and rejects the misconfiguration of setting MAINTENANCE_MODE while using database-backed session storage ( django.contrib.sessions.backends.db ) #681 - Added an example guide on how to use AWS S3 for hosting static files in production. Changed \u00b6 #738 - Added *.env (except dev.env ) to .gitignore to prevent local environment variable files from accidentally being committed to Git Fixed \u00b6 #683 - Fixed slug auto-construction when defining a new ComputedField. #725 - Added missing http-keepalive = 1 to recommended uswgi.ini configuration. #727 - Fixed broken REST API endpoint ( /api/extras/graphql-queries/<uuid>/run/ ) for running saved GraphQL queries. #744 - Fixed missing Celery Django fixup that could cause assorted errors when multiple background tasks were run concurrently. #746 - Fixed data serialization error when running Jobs that used IPAddressVar , IPAddressWithMaskVar , and/or IPNetworkVar variables. #759 - Corrected backwards add/import links for Power Feed and Power Panel in navigation bar v1.1.0 (2021-07-20) \u00b6 Added \u00b6 #372 - Added support for displaying custom fields in tables used in object list views #620 - Config context schemas can now be managed via Git repositories. Changed \u00b6 #675 - Update MySQL unicode settings docs to be more visible #684 - Renamed ?opt_in_fields= query param to ?include= #691 - Clarify documentation on RQ to Celery worker migration and running both workers in parallel to help ease migration #692 - Clarify plugin development docs on naming of file for custom Jinja2 filters #697 - Added CELERY_TASK_SOFT_TIME_LIMIT to settings.py and lowered the default CELERY_TASK_TIME_LIMIT configuration. Fixed \u00b6 #363 - Fixed using S3 django-storages backend requires USE_TZ=False #466 - Fixed improper GraphQL schema generation on fields that can be blank but not null (such as Interface.mode ) #663 - Fixed kombu.exceptions.EncodeError when trying to execute Jobs using (Multi)ObjectVar objects with nested relationships #672 - Fixed inheritance of Celery broker/results URL settings for dev/template configs (they can now be defined using Redis env. vars) #677 - Revise LDAPS outdated documentation for ignoring TLS cert errors #680 - Removed unnecessary warning message when both RQ and Celery workers are present #686 - Fixed incorrect permission name for Tags list view in nav menu #690 - Fixed Jinja2 dependency version to remain backwards-compatible with Nautobot 1.0.x #696 - Fixed inheritance of VRF and Tenant assignment when creating an IPAddress or Prefix under a parent Prefix. (Port of NetBox #5703 and NetBox #6012 ) #698 - Fixed cloning of a computed field object to now carry over required non-unique fields #699 - Exceptions such as TypeError are now caught and handled correctly when rendering a computed field. #702 - GraphiQL view no longer requires internet access to load libraries. #703 - Fixed direct execution of saved GraphQL queries containing double quotes #705 - Fixed missing description field from detail view for computed fields Security \u00b6 #717 - Bump Pillow dependency version from 8.1.2 to 8.2.0 to address numerous critical CVE advisories v1.1.0b2 (2021-07-09) \u00b6 Added \u00b6 #599 - Custom fields are now supported on JobResult objects #637 - Implemented a nautobot-server fix_custom_fields command to manually purge stale custom field data Changed \u00b6 #634 - Documentation on plugin capabilities has been clarified. Fixed \u00b6 #495 - Fixed search for partial IPv4 prefixes/aggregates not finding all matching objects #533 - Custom field tasks are now run atomically to avoid stale field data from being saved on objects. #554 - Fixed search for partial IPv6 prefixes/aggregates not finding all matching objects #569 - Change minimum/maximum allowed values for integer type in Custom Fields to 64-bit BigIntegerField types (64-bit) #600 - The invoke migrate step is now included in the development getting started guide for Docker workflows #617 - Added extra comments to uwsgi.ini config to help with load balancer deployments in Nautobot services documentation #626 - Added prefix NAUTOBOT_ in override.env example inside of docker-entrypoint.sh #645 - Updated services troubleshooting docs to include \"incorrect string value\" fix when using Unicode emojis with MySQL as a database backend #653 - Fixed systemd unit file for nautobot-worker to correctly start/stop/restart #661 - Fixed computed_fields key not being included in API response for devices when using include (for opt-in fields) #667 - Fixed various outdated/incorrect places in the documentation for v1.1.0 release. v1.1.0b1 (2021-07-02) \u00b6 Added \u00b6 #3 - GraphQL queries can now be saved for later execution #10 - Added a new \"Getting Started in the Web UI\" section to the documentation to help new users begin learning how to use Nautobot. #17 - MySQL 8.x is now fully supported as a database backend #200 - Jobs can be marked as read-only #274 - Added config context schemas to optionally validate config and local context data against JSON Schemas #297 - Added an anonymous health-checking endpoint at /health/ using, also introducing a nautobot-server health_check command. #485 - Applications can define navbar properties through navigation.py #557 - Prefix records can now be created using /32 (IPv4) and /128 (IPv6) networks. (Port of NetBox #6545 ) #561 - Added autodetection of mime_type on export_templates provided by Git datasources #636 - Added custom fields to JobResult model, with minor changes to job result detail page Changed \u00b6 #431 - ConfigContext and ExportTemplate records now must have unique name values. This was always the case in NetBox, but was inadvertently un-enforced in earlier versions of Nautobot. Fixed \u00b6 #460 - Deleting a record now deletes any associated RelationshipAssociation records #494 - Objects with status fields now emit limited choices correctly when performing OPTIONS metadata API requests #602 - Fixed incorrect requirement to install toml Python library before running invoke tasks #618 - Fixed typo in release-notes","title":"Version 1.1"},{"location":"release-notes/version-1.1.html#nautobot-v11","text":"This document describes all new features and changes in Nautobot 1.1. If you are a user migrating from NetBox to Nautobot, please refer to the \"Migrating from NetBox\" documentation.","title":"Nautobot v1.1"},{"location":"release-notes/version-1.1.html#release-overview","text":"","title":"Release Overview"},{"location":"release-notes/version-1.1.html#added","text":"","title":"Added"},{"location":"release-notes/version-1.1.html#computed-fields-4","text":"Computed fields offers users the ability to create read-only custom fields using existing data already stored in the database. Users define Jinja2 templates that populate the value of these fields. Computed fields are available on all data models that currently support custom fields.","title":"Computed Fields (#4)"},{"location":"release-notes/version-1.1.html#config-context-json-schemas-274","text":"While config contexts allow for arbitrary data structures to be stored within Nautobot, at scale it is desirable to apply validation constraints to that data to ensure its consistency and to avoid data entry errors. To service this need, Nautobot supports optionally backing config contexts with JSON Schemas for validation. These schema are managed via the config context schema model and are optionally linked to config context instances, in addition to devices and virtual machines for the purpose of validating their local context data. Please see the docs for more details. Just like config contexts, config context JSON schemas can optionally be managed via a Git repository .","title":"Config Context JSON Schemas (#274)"},{"location":"release-notes/version-1.1.html#dynamic-navigation-menus-12","text":"Applications and plugins can now define tabs, groups, items and buttons in the navigation menu. Using navigation objects a developer can add items to any section of the navigation using key names and weight values. Please see Application Registry for more details.","title":"Dynamic Navigation Menus (#12)"},{"location":"release-notes/version-1.1.html#mysql-database-support-17","text":"MySQL 8.x is now fully supported as a database backend! The installation and configuration guides have been revised to include MySQL. If you prefer MySQL or it is more easily supported in your environment, configuring Nautobot to use MySQL is as easy as changing value of ENGINE in your DATABASES setting to point to django.db.backends.mysql and installing the MySQL Python driver using pip3 install nautobot[mysql] . A new NAUTOBOT_DB_ENGINE environment variable has been added to allow for specifying the desired database engine at runtime without needing to modify your nautobot_config.py . Please see the configuration guide on DATABASES for more details on how to configure Nautobot to use MySQL. Please see the MySQL setup guides for Ubuntu and CentOS to get started.","title":"MySQL Database Support (#17)"},{"location":"release-notes/version-1.1.html#plugin-defined-jinja2-filters","text":"Plugins can now define custom Jinja2 filters to be used when rendering templates defined within computed fields or custom links. To register your own filters, you may add a jinja_filters.py to your plugin and any filters defined there will be automatically registered and globally usable. Please see the plugin development documentation on including Jinja2 filters to get started.","title":"Plugin Defined Jinja2 Filters"},{"location":"release-notes/version-1.1.html#read-only-jobs-200","text":"Jobs may be optionally marked as read only by setting the read_only = True meta attribute. This prevents the job from making any changes to nautobot data and suppresses certain log messages. Read only jobs can be a great way to safely develop new jobs, and for working with reporting use cases. Please see the Jobs documentation for more details.","title":"Read Only Jobs (#200)"},{"location":"release-notes/version-1.1.html#saved-graphql-queries-3","text":"Saved GraphQL queries offers a new model where reusable queries can be stored in Nautobot. New views for managing saved queries are available; additionally, the GraphiQL interface has been augmented to allow populating the interface from a saved query, editing and saving new queries. Saved queries can easily be imported into the GraphiQL interface by using the new navigation tab located on the right side of the navbar. Inside the new tab are also buttons for editing and saving queries directly into Nautobot's databases. Additionally, two new GraphQL utility functions have been added to allow easy access to the GraphQL system from source code. Both functions can be imported from nautobot.core.graphql : execute_query() : Runs string as a query against GraphQL. execute_saved_query() : Execute a saved query from Nautobot database.","title":"Saved GraphQL Queries (#3)"},{"location":"release-notes/version-1.1.html#changed","text":"","title":"Changed"},{"location":"release-notes/version-1.1.html#background-tasks-now-use-celery-223","text":"Celery has been introduced to eventually replace RQ for executing background tasks within Nautobot. All Nautobot core usage of RQ has been migrated to use Celery. Note Custom background tasks implemented by plugins are not part of Nautobot core functions Prior to version 1.1.0, Nautobot utilized RQ as the primary background task worker. As of Nautobot 1.1.0, RQ is now deprecated . RQ and the @job decorator for custom tasks are still supported for now, but will no longer be documented, and support for RQ will be removed in a future release. RQ support for custom tasks was not removed in order to give plugin authors time to migrate, however, to continue to utilize advanced Nautobot features such as Git repository synchronization, webhooks, jobs, etc. you must migrate your nautobot-worker deployment from RQ to Celery. Please see the section on migrating to Celery from RQ for more information on how to easily migrate your deployment. Warning If you are running plugins that use background tasks requiring the RQ worker, you will need to run both the RQ and Celery workers concurrently until the plugins are converted to use the Celery worker. See the Migrating to Celery from RQ for details.","title":"Background Tasks now use Celery (#223)"},{"location":"release-notes/version-1.1.html#fixed","text":"","title":"Fixed"},{"location":"release-notes/version-1.1.html#http-remote-end-closed-connection-errors-725","text":"The example uwsgi.ini provided in earlier versions of the Nautobot documentation was missing a recommendation to include the configuration http-keepalive = 1 which enables support for HTTP/1.1 keep-alive headers. Warning If you are upgrading from an earlier version of Nautobot (including 1.1.0) you should check your uwsgi.ini and ensure that it contains this important configuration line.","title":"HTTP \"Remote end closed connection\" errors (#725)"},{"location":"release-notes/version-1.1.html#v116-2021-12-03","text":"","title":"v1.1.6 (2021-12-03)"},{"location":"release-notes/version-1.1.html#fixed_1","text":"#1093 - Improved REST API performance by adding caching of serializer \"opt-in fields\".","title":"Fixed"},{"location":"release-notes/version-1.1.html#v115-2021-11-11","text":"","title":"v1.1.5 (2021-11-11)"},{"location":"release-notes/version-1.1.html#added_1","text":"#263 - Added a link in the Docker deployment guide to the Nautobot Docker Compose repository. #931 - Added support for direct upload of CSV files as an alternative to copy-pasting CSV text into a form field. (Partially based on NetBox #6561 ) #953 - Added option to use MySQL in the docker-compose development environment #954 - Added documentation for migrating from PostgreSQL to MySQL, improved documentation as to recommended MySQL database configuration. #989 - Added id and name fields to NestedJobResultSerializer for the REST API. #1019 - Added GitHub action to redeploy the Nautobot sandbox on pushes to main , develop , and next . #1025 - Added reference documentation for how to hide navigation menu items with no permissions. #1031 - Added a troubleshooting note around the combination of RedHat/CentOS, uWSGI, and static files. #1057 - Added GitHub action to automatically push Docker images to ghcr.io .","title":"Added"},{"location":"release-notes/version-1.1.html#fixed_2","text":"#555 - Fixed Status.DoesNotExist during nautobot-server loaddata . #567 - Fixed incorrect GraphQL schema generation for _custom_field_data when certain plugins are installed. #733 - A Job erroring out early in initialization could result in its associated JobResult staying in Pending state indefinitely. #816 - Fixed AttributeError reported when viewing a Rack with certain associated power configurations. #948 - Fixed advanced logging example to use EXTRA_MIDDLEWARE instead of MIDDLEWARE.append() . #970 - Clarified documentation around config context definition in Git repositories. #981 - Fixed incorrect handling of missing custom fields in the fix_custom_fields management command. #986 - Fixed TemplateDoesNotExist exception when running a Job containing a FileVar variable. #991 - Fixed incorrect logging when importing ConfigContextSchemas from Git. #993 - Fixed incorrect git command when refreshing a previously checked out repository. #1023 - Removed invalid link in \"Deploying Nautobot\" documentation.","title":"Fixed"},{"location":"release-notes/version-1.1.html#security","text":"#998 - Update mkdocs dependency to avoid a potential path-traversal vulnerability; note that mkdocs is only used in development and is not a production deployment dependency of Nautobot.","title":"Security"},{"location":"release-notes/version-1.1.html#v114-2021-10-04","text":"","title":"v1.1.4 (2021-10-04)"},{"location":"release-notes/version-1.1.html#added_2","text":"#623 - Git repository sync logs now include the commit hash that was synchronized to. #728 - Added SOCIAL_AUTH_BACKEND_PREFIX configuration setting to support custom authentication backends. #861 - Bulk editing of devices can now update their site, rack, and rack-group assignments. #949 - Added documentation note about using MAINTENANCE_MODE in combination with LDAP.","title":"Added"},{"location":"release-notes/version-1.1.html#changed_1","text":"#956 - Switched CI from Travis to GitHub Actions. #964 - Updated README.md build status badge to show GitHub status.","title":"Changed"},{"location":"release-notes/version-1.1.html#fixed_3","text":"#944 - Jobs that commit changes to the database could not be invoked successfully from the nautobot-server runjob command. #955 - REST API endpoint for syncing Git repositories was still checking for RQ workers instead of Celery workers. #969 - IPv6 prefixes such as ::1/128 were not being treated correctly.","title":"Fixed"},{"location":"release-notes/version-1.1.html#security_1","text":"#939 - Nautobot views now default to X-Frame-Options: DENY rather than X-Frame-Options: SAMEORIGIN , with the exception of the rack-elevation API view ( /api/dcim/rack-elevation/ ) which specifically requires X-Frame-Options: SAMEORIGIN for functional reasons.","title":"Security"},{"location":"release-notes/version-1.1.html#v113-2021-09-13","text":"","title":"v1.1.3 (2021-09-13)"},{"location":"release-notes/version-1.1.html#added_3","text":"#11 - Added tests to verify that plugin models can support webhooks if appropriately decorated with @extras_features(\"webhooks\") #652 - Jobs REST API run endpoint now can look up ObjectVar references via a dictionary of parameters. #755 - Added example showing how to use django-request-logging middleware to log the user associated with inbound requests. #791 - Added support for NAUTOBOT_DOCKER_SKIP_INIT variable to allow running the Docker container with a read-only database. #841 - Added more detailed documentation around defining Relationship filters. #850 - Added developer documentation around the installation and use of mkdocs to locally preview documentation changes. #856 - Added more detailed user documentation on how to create an API token.","title":"Added"},{"location":"release-notes/version-1.1.html#changed_2","text":"#601 - Developer documentation for advanced docker-compose use cases is now a separate file. #709 - Computed fields can now have a blank fallback_value . #812 - In the GraphiQL interface, the \"Queries\" dropdown now appears alongside the other GraphiQL interface buttons instead of appearing in the main Nautobot navigation bar. #832 - Plugin installation documentation now recommends nautobot-server post_upgrade instead of separately running nautobot-server migrate and nautobot-server collectstatic .","title":"Changed"},{"location":"release-notes/version-1.1.html#fixed_4","text":"#464 - Fixed GraphQL schema generation error when certain custom field names are used. #651 - Fixed Jobs validation enforce schema consistently across UI and API. #670 - Clarified Jobs documentation regarding how to fail or abort a Job. #715 - Fixed display of GraphiQL interface in narrow browser windows. #718 - Fixed rendering of long template values in Computed Field detail view. #731 - Config context schemas including format properties (such as \"format\": \"ipv4\" ) are now correctly enforced. #779 - Fixed incorrect Tenant display in Prefix \"Duplicate Prefixes\" table. (Port of two fixes originally from NetBox) #809 - Fixed docker-compose file version values to work correctly with older versions of docker-compose. #818 - Database health-check now reports as healthy even when in MAINTENANCE_MODE . #825 - Removed unnecessary -B flag from development Celery worker invocation. #830 - Fixed incorrect database migration introduced by #818. #845 - Clarified documentation around nautobot-server init and NAUTOBOT_ROOT . #848 - Fixed stale links to NAPALM documentation","title":"Fixed"},{"location":"release-notes/version-1.1.html#security_2","text":"#893 - Bump Pillow dependency version from 8.2.0 to 8.2.3 to address numerous critical CVE advisories","title":"Security"},{"location":"release-notes/version-1.1.html#v112-2021-08-10","text":"","title":"v1.1.2 (2021-08-10)"},{"location":"release-notes/version-1.1.html#added_4","text":"#758 - Added documentation about the Job class_path concept. #771 - Added examples of various possible logging configurations. #773 - Added documentation around enabling Prometheus metrics for database and caching backends.","title":"Added"},{"location":"release-notes/version-1.1.html#changed_3","text":"#742 - The development environment now respects the setting of the NAUTOBOT_DEBUG environment variable if present.","title":"Changed"},{"location":"release-notes/version-1.1.html#fixed_5","text":"#723 - Fixed power draw not providing a UtilizationData type for use in graphing power draw utilization #782 - Corrected documentation regarding the use of docker-compose.override.yml #785 - Fixed plugin loading error when using final Docker image. #786 - Fixed Unknown command: 'post_upgrade' when using final Docker image. #789 - Avoid a NoReverseMatch exception at startup time if an app or plugin defines a nav menu item with an invalid link reference.","title":"Fixed"},{"location":"release-notes/version-1.1.html#v111-2021-08-05","text":"","title":"v1.1.1 (2021-08-05)"},{"location":"release-notes/version-1.1.html#added_5","text":"#506 - nautobot-server now detects and rejects the misconfiguration of setting MAINTENANCE_MODE while using database-backed session storage ( django.contrib.sessions.backends.db ) #681 - Added an example guide on how to use AWS S3 for hosting static files in production.","title":"Added"},{"location":"release-notes/version-1.1.html#changed_4","text":"#738 - Added *.env (except dev.env ) to .gitignore to prevent local environment variable files from accidentally being committed to Git","title":"Changed"},{"location":"release-notes/version-1.1.html#fixed_6","text":"#683 - Fixed slug auto-construction when defining a new ComputedField. #725 - Added missing http-keepalive = 1 to recommended uswgi.ini configuration. #727 - Fixed broken REST API endpoint ( /api/extras/graphql-queries/<uuid>/run/ ) for running saved GraphQL queries. #744 - Fixed missing Celery Django fixup that could cause assorted errors when multiple background tasks were run concurrently. #746 - Fixed data serialization error when running Jobs that used IPAddressVar , IPAddressWithMaskVar , and/or IPNetworkVar variables. #759 - Corrected backwards add/import links for Power Feed and Power Panel in navigation bar","title":"Fixed"},{"location":"release-notes/version-1.1.html#v110-2021-07-20","text":"","title":"v1.1.0 (2021-07-20)"},{"location":"release-notes/version-1.1.html#added_6","text":"#372 - Added support for displaying custom fields in tables used in object list views #620 - Config context schemas can now be managed via Git repositories.","title":"Added"},{"location":"release-notes/version-1.1.html#changed_5","text":"#675 - Update MySQL unicode settings docs to be more visible #684 - Renamed ?opt_in_fields= query param to ?include= #691 - Clarify documentation on RQ to Celery worker migration and running both workers in parallel to help ease migration #692 - Clarify plugin development docs on naming of file for custom Jinja2 filters #697 - Added CELERY_TASK_SOFT_TIME_LIMIT to settings.py and lowered the default CELERY_TASK_TIME_LIMIT configuration.","title":"Changed"},{"location":"release-notes/version-1.1.html#fixed_7","text":"#363 - Fixed using S3 django-storages backend requires USE_TZ=False #466 - Fixed improper GraphQL schema generation on fields that can be blank but not null (such as Interface.mode ) #663 - Fixed kombu.exceptions.EncodeError when trying to execute Jobs using (Multi)ObjectVar objects with nested relationships #672 - Fixed inheritance of Celery broker/results URL settings for dev/template configs (they can now be defined using Redis env. vars) #677 - Revise LDAPS outdated documentation for ignoring TLS cert errors #680 - Removed unnecessary warning message when both RQ and Celery workers are present #686 - Fixed incorrect permission name for Tags list view in nav menu #690 - Fixed Jinja2 dependency version to remain backwards-compatible with Nautobot 1.0.x #696 - Fixed inheritance of VRF and Tenant assignment when creating an IPAddress or Prefix under a parent Prefix. (Port of NetBox #5703 and NetBox #6012 ) #698 - Fixed cloning of a computed field object to now carry over required non-unique fields #699 - Exceptions such as TypeError are now caught and handled correctly when rendering a computed field. #702 - GraphiQL view no longer requires internet access to load libraries. #703 - Fixed direct execution of saved GraphQL queries containing double quotes #705 - Fixed missing description field from detail view for computed fields","title":"Fixed"},{"location":"release-notes/version-1.1.html#security_3","text":"#717 - Bump Pillow dependency version from 8.1.2 to 8.2.0 to address numerous critical CVE advisories","title":"Security"},{"location":"release-notes/version-1.1.html#v110b2-2021-07-09","text":"","title":"v1.1.0b2 (2021-07-09)"},{"location":"release-notes/version-1.1.html#added_7","text":"#599 - Custom fields are now supported on JobResult objects #637 - Implemented a nautobot-server fix_custom_fields command to manually purge stale custom field data","title":"Added"},{"location":"release-notes/version-1.1.html#changed_6","text":"#634 - Documentation on plugin capabilities has been clarified.","title":"Changed"},{"location":"release-notes/version-1.1.html#fixed_8","text":"#495 - Fixed search for partial IPv4 prefixes/aggregates not finding all matching objects #533 - Custom field tasks are now run atomically to avoid stale field data from being saved on objects. #554 - Fixed search for partial IPv6 prefixes/aggregates not finding all matching objects #569 - Change minimum/maximum allowed values for integer type in Custom Fields to 64-bit BigIntegerField types (64-bit) #600 - The invoke migrate step is now included in the development getting started guide for Docker workflows #617 - Added extra comments to uwsgi.ini config to help with load balancer deployments in Nautobot services documentation #626 - Added prefix NAUTOBOT_ in override.env example inside of docker-entrypoint.sh #645 - Updated services troubleshooting docs to include \"incorrect string value\" fix when using Unicode emojis with MySQL as a database backend #653 - Fixed systemd unit file for nautobot-worker to correctly start/stop/restart #661 - Fixed computed_fields key not being included in API response for devices when using include (for opt-in fields) #667 - Fixed various outdated/incorrect places in the documentation for v1.1.0 release.","title":"Fixed"},{"location":"release-notes/version-1.1.html#v110b1-2021-07-02","text":"","title":"v1.1.0b1 (2021-07-02)"},{"location":"release-notes/version-1.1.html#added_8","text":"#3 - GraphQL queries can now be saved for later execution #10 - Added a new \"Getting Started in the Web UI\" section to the documentation to help new users begin learning how to use Nautobot. #17 - MySQL 8.x is now fully supported as a database backend #200 - Jobs can be marked as read-only #274 - Added config context schemas to optionally validate config and local context data against JSON Schemas #297 - Added an anonymous health-checking endpoint at /health/ using, also introducing a nautobot-server health_check command. #485 - Applications can define navbar properties through navigation.py #557 - Prefix records can now be created using /32 (IPv4) and /128 (IPv6) networks. (Port of NetBox #6545 ) #561 - Added autodetection of mime_type on export_templates provided by Git datasources #636 - Added custom fields to JobResult model, with minor changes to job result detail page","title":"Added"},{"location":"release-notes/version-1.1.html#changed_7","text":"#431 - ConfigContext and ExportTemplate records now must have unique name values. This was always the case in NetBox, but was inadvertently un-enforced in earlier versions of Nautobot.","title":"Changed"},{"location":"release-notes/version-1.1.html#fixed_9","text":"#460 - Deleting a record now deletes any associated RelationshipAssociation records #494 - Objects with status fields now emit limited choices correctly when performing OPTIONS metadata API requests #602 - Fixed incorrect requirement to install toml Python library before running invoke tasks #618 - Fixed typo in release-notes","title":"Fixed"},{"location":"release-notes/version-1.2.html","text":"Nautobot v1.2 \u00b6 This document describes all new features and changes in Nautobot 1.2. If you are a user migrating from NetBox to Nautobot, please refer to the \"Migrating from NetBox\" documentation. Release Overview \u00b6 Added \u00b6 Admin Configuration UI ( #370 ) \u00b6 The Nautobot Admin UI now includes a \"Configuration\" page that can be used to dynamically customize a number of optional settings as an alternative to editing nautobot_config.py and restarting the Nautobot processes. If upgrading from a previous Nautobot version where these settings were defined in your nautobot_config.py , you must remove those definitions in order to use this feature, as explicit configuration in nautobot_config.py takes precedence over values configured in the Admin UI. Common Base Template for Object Detail Views ( #479 , #585 ) \u00b6 All \"object detail\" views (pages displaying details of a single Nautobot record) now inherit from a common base template, providing improved UI consistency, reducing the amount of boilerplate code needed to create a new detail view, and fixing a number of bugs in various views. Plugin developers are encouraged to make use of this new template ( generic/object_detail.html ) to take advantage of these improvements. Views based on this template now include a new \"Advanced\" tab - currently this tab includes the UUID and slug (if any) of the object being viewed, but may be extended in the future to include additional information not relevant to the basic object detail view. Custom Fields are now User Configurable ( #229 ) \u00b6 Creation and management of Custom Field definitions can now be performed by any user with appropriate permissions. (Previously, only admin users were able to manage Custom Fields.) Custom Field Webhooks ( #519 ) \u00b6 Webhooks can now be triggered when creating/updating/deleting CustomField and CustomFieldChoice definition records. Database Ready Signal ( #13 ) \u00b6 After running nautobot-server migrate or nautobot-server post_upgrade , Nautobot now emits a custom signal, nautobot_database_ready . This signal is designed for plugins to connect to in order to perform automatic database population (such as defining custom fields, relationships, webhooks, etc.) at install/upgrade time. For more details, refer to the plugin development documentation . GraphQL Filters at All Levels ( #248 ) \u00b6 The GraphQL API now supports query filter parameters at any level of a query. For example: query { sites(name: \"ams\") { devices(role: \"edge\") { name interfaces(type: \"virtual\") { name } } } } GraphQL Query Optimizations ( #171 ) \u00b6 Complex GraphQL queries have been greatly optimized thanks to integration of graphene-django-optimizer into Nautobot! In our internal testing and benchmarking the number of SQL queries generated per GraphQL query have been drastically reduced, resulting in much quicker response times and less strain on the database. For in depth details on our benchmarks, please see the comment thread on the issue . Installed Plugins List and Detail Views, Plugin Config and Home Views ( #935 ) \u00b6 The Plugins menu now includes an \"Installed Plugins\" menu item which provides a list view of information about all installed and enabled plugins, similar to a formerly administrator-only view. Additionally, when viewing this list, each plugin can now be clicked on for a detail view, which provides an in-depth look at the capabilities of the plugin, including whether it makes use of each or all of the various Nautobot features available to be used by plugins. Additionally, plugins now have the option of registering specific \"home\" and/or \"configuration\" views, which will be linked and accessible directly from the installed-plugins list and detail views. Please refer to the plugin development documentation for more details about this functionality. IPAM custom lookups for filtering ( #982 ) \u00b6 Nautobot now again supports custom lookup filters on the IPAddress , Prefix , and Aggregate models, such as address__net_contained , network__net_contains_or_equals , etc. Refer to the REST API filtering documentation for more specifics and examples. Job Approval ( #125 ) \u00b6 Jobs can now be optionally defined as approval_required = True , in which case the Job will not be executed immediately upon submission, but will instead be placed into an approval queue; any user other than the submitter can approve or deny a queued Job, at which point it will then be executed as normal. Job Scheduling ( #374 ) \u00b6 Jobs can now be scheduled for execution at a future date and time (such as during a planned maintenance window), and can also be scheduled for repeated execution on an hourly, daily, or weekly recurring cadence. Note Execution of scheduled jobs is dependent on Celery Beat ; enablement of this system service is a new requirement in Nautobot 1.2. Please see the documentation on enabling the Celery Beat scheduler service to get started! Networking Template Filters ( #1082 ) \u00b6 Template rendering with Django and/or Jinja2 now supports by default all filters provided by the netutils library. These filters can be used in page templates, computed fields, custom links, export templates, etc. For details, please refer to the filters documentation. Organizational Branding ( #859 ) \u00b6 Organizations may provide custom branding assets to change the logo, icons, and footer URLs to help Nautobot fit within their environments and user communities. Please see the configuration documenation for details on how to specify the location and usage of custom branding assets. Plugin Banners ( #534 ) \u00b6 Each plugin is now able to optionally inject a custom banner into any of the Nautobot core views. Please refer to the plugin development documentation for more details about this functionality. Same-Type and Symmetric Relationships ( #157 ) \u00b6 The Relationships feature has been extended in two ways: Relationships between the same object type (e.g. device-to-device) are now permitted and supported. For same-object-type relationships specifically, symmetric (peer-to-peer rather than source-to-destination) relationships are now an option. For more details, refer to the Relationships documentation. Secrets Integration ( #541 ) \u00b6 Nautobot can now read secret values (such as device or Git repository access credentials) on demand from a variety of external sources, including environment variables and text files, and extensible via plugins to support additional secrets providers such as Hashicorp Vault and AWS Secrets Manager. Both the NAPALM device integration and the Git repository integration can now make use of these secrets, and plugins and jobs can do so as well. For more details, please refer to the Secrets documentation. Software-Defined Home Page ( #674 , #716 ) \u00b6 Nautobot core applications and plugins can now both define panels, groups, and items to populate the Nautobot home page. The home page now dynamically reflows to accommodate available content. Plugin developers can add to existing panels or groups or define entirely new panels as needed. For more details, see Populating the Home Page . Changed \u00b6 Admin Site Changes ( #900 ) \u00b6 The Admin sub-site within Nautobot ( /admin/ and its child pages) has been revamped in appearance and functionality. It has been re-skinned to resemble the rest of the Nautobot UI, and has been slimmed down to only include those models and features that are still exclusive to admin users, such as user/group/permission management. JobLogEntry Data Model ( #1030 ) \u00b6 Job log messages are now stored in a separate database table as a separate JobLogEntry data model, instead of being stored as JSON on the JobResult model/table. This provides faster and more robust rendering of JobResult -related views and lays groundwork for future enhancements of the Jobs feature. Note If you are executing Jobs inside your tests, there are some changes you will need to make for your tests to support this feature correctly. Refer to the Jobs documentation for details. Note Because JobLogEntry records reference their associated JobResult , the pattern job.job_result = JobResult() (creating only an in-memory JobResult object, rather than a database entry) will no longer work. Instead you will need to create a proper JobResult database object job.job_result = JobResult.objects.create(...) . Slug fields are now Optional in CSV import, REST API and ORM ( #493 ) \u00b6 All models that have slug fields now use AutoSlugField from the django-extensions package. This means that when creating a record via the REST API, CSV import, or direct ORM Python calls, the slug field is now fully optional; if unspecified, it will be automatically assigned a unique value, just as how a slug is auto-populated in the UI when creating a new record. Just as with the UI, the slug can still always be explicitly set if desired. v1.2.11 (2022-04-04) \u00b6 Added \u00b6 #1123 - Add validation for IPAddress assigned_object_type and assigned_object_id. #1146 - Added change date filtering lookup expressions to GraphQL. #1495 - Added full coverage of cable termination types to Graphene. #1501 - Add IP field to CSV export of device. #1529 - Added list of standard hex colors to the Tags documentation. Changed \u00b6 #1536 - Removed the ServiceUnavailable exception when no primary_ip is available for a device, but other connection options are available. #1581 - Changed MultipleChoiceJSONField to accept choices as a callable, fixing Datasource Contents provided by plugins are not accepted as valid choice by REST API. #1584 - Replaced links in docs to celeryproject.org with celeryq.dev Fixed \u00b6 #1313 - Fixed GraphQL query error on OneToOneFields such as IPAddress.primary_ip4_for #1408 - Fixed incorrect HTML in the Devices detail views. #1467 - Fixed an issue where at certain browser widths the nav bar would cover the top of the page content. #1523 - Fixed primary IP being unset after creating/updating different interface #1548 - Pin Jinja2 version for mkdocs requirements to fix RTD docs builds related to API deprecation in Jinja2 >= 3.1.0 #1583 - Fixed Nautobot service definition in PostgreSQL-backed development environment. #1599 - Bump mkdocs version for Snyk report. v1.2.10 (2022-03-21) \u00b6 Added \u00b6 #1492 - Added note in the Jobs documentation about the use of AbortTransaction to end the job and force rollback. #1517 - Added password filtering example to advanced logging section in docs. Changed \u00b6 #1514 - Simplified switching between PostgreSQL and MySQL database backends in the developer environment. #1518 - Updated GitHub Pull Request template to include detail section, todo list. Fixed \u00b6 #1511 - Fixed left column of Read The Docs being cut off. #1522 - Fixed link name attribute name in developer docs. v1.2.9 (2022-03-14) \u00b6 Fixed \u00b6 #1431 - Fixed potential failure of extras.0017_joblog_data_migration migration when the job logs contain messages mistakenly logged as object references. #1459 - Fixed incorrect display of related devices and VMs in the Cluster Type and Cluster Group detail views. #1469 - Fixed incorrect CSV export for devices Security \u00b6 Danger It is highly recommended that users of Python 3.6 prioritize upgrading to a newer version of Python. Nautobot will be removing support for Python 3.6 in a future update. Important For users remaining on Python 3.6, please know that upgrading to Nautobot v1.2.9 will not resolve these CVEs for your installation . The only remedy at this time is to upgrade your systems to utilize Python 3.7 or later. #1487 - Implemented fixes for CVE-2022-22817 , CVE-2022-24303 , and potential infinite loop by requiring Pillow >=9.0.1 for Python version >=3.7. For Python version <3.7 (e.g. 3.6), it is recommended that you prioritize upgrading your environment to use Python 3.7 or higher. Support for Python 3.6 will be removed in a future update. v1.2.8 (2022-03-07) \u00b6 Added \u00b6 #839 - Add CODE_OF_CONDUCT.md to repository. #1242 - Add MAJOR.MINOR tags to Docker images upon release. #1299 - Add SECURITY.md to repository. #1388 - Added beta version of GitHub Issue Form style for feature request. #1419 - Add documentation for specifying a CA cert file for LDAP authentication backend. #1446 - Apply title labels to Docker images. Changed \u00b6 #1348 - Pin Selenium Grid container version to match Python Client version. #1432 - Update django-redis to 5.2.x to address 5.1.x blocking redis 4.x versions. #1447 - Minor nit on Github Issue Form styling. #1452 - Changed GitHub release workflow to not run on prerelease releases. #1453 - Changed feature request to use GitHub Issue Form. Fixed \u00b6 #1301 - Fixed window history handling for views with tabs in Safari/Firefox. #1302 - Fixed missing Advanced tab on Virtual Machine detail view. #1398 - Fixed missing safeguard for removing master from Virtual Chassis via API. #1399 - Fixed not being able to set master to null on Virtual Chassis API. #1405 - Fixed incorrect import in 'startplugin' template code. #1412 - Fixed not being able to query for prefix family via GraphQL. #1442 - Fixed missing Advanced tab on Job Result, Git Repository, and Config Context Schema detail views. v1.2.7 (2022-02-22) \u00b6 Changed \u00b6 #1403 - Changes the GitHub Action on Release version template variable name. v1.2.6 (2022-02-22) \u00b6 Added \u00b6 #1279 - Circuit terminations now render custom relationships on the circuit detail page. #1353 - Added UI for deleting previously uploaded images when editing a DeviceType. Changed \u00b6 #1386 - Updated release schedule in docs for patch releases, now every two weeks. Fixed \u00b6 #1249 - Fixed a timing issue where after creating a custom field with a default value and immediately assigning values to this custom field on individual objects, the custom field values could be automatically reverted to the default value. #1280 - Added missing get_absolute_url method to the CircuitTermination model, fixing a UI error that could occur when relationships involve CircuitTerminations. #1283 - Update Sentinel docs to have 3 hosts (minimum per Redis docs), and change CELERY_BROKER_URL to a multiline string instead of a Tuple (tuple is invalid, and raises an exception when job completes). #1312 - Fixed a bug where a Prefix filter matching zero records would instead show all records in the UI. #1327 - Fixes the broken dependencies from the Release action. #1328 - Fixed an error in the Job class-path documentation . #1332 - Fixed a regression in which the REST API did not default to pagination based on the configured PAGINATE_COUNT setting but instead defaulted to full unpaginated results. #1335 - Fixed an issue with the Secret create/edit form that caused problems when defining AWS secrets using the nautobot-secrets-providers plugin. #1346 - Fixed an error in the periodic execution of Celery's built-in celery.backend_cleanup task. #1360 - Fixed an issue in the development environment that could cause Selenium integration tests to error out. #1390 - Pinned transitive dependency MarkupSafe to version 2.0.1 as later versions are incompatible with Nautobot's current Jinja2 dependency. v1.2.5 (2022-02-02) \u00b6 Changed \u00b6 #1293 - Reorganized the developer documents somewhat to reduce duplication of information, added diagrams for issue intake process. Fixed \u00b6 #371 - Fixed a server error that could occur when importing cables via CSV. #1161 - The description field for device component templates is now correctly propagated to device components created from these templates. #1233 - Prevented a job aborting when an optional ObjectVar is provided with a value of None #1272 - Fixed GitHub Actions syntax and Slack payload for release CI workflow #1282 - Fixed a server error when editing User accounts. #1308 - Fixed another server error that could occur when importing cables via CSV. v1.2.4 (2022-01-13) \u00b6 Added \u00b6 #1113 - Added documentation about using Redis Sentinel with Nautobot. #1251 - Added workflow_call to the GitHub Actions CI workflow so that it may be called by other GHA workflows. Changed \u00b6 #616 - The REST API now no longer permits setting non-string values for text-type custom fields. #1243 - Github CI action no longer runs for pull requests that don't impact Nautobot code, such as documentation, examples, etc. Fixed \u00b6 #1053 - Fixed error when removing an IP address from an interface when it was previously the parent device's primary IP. #1140 - Fixed incorrect UI widgets in the updated Admin UI. #1253 - Fixed missing code that prevented switching between tabs in the device-type detail view. Security \u00b6 Danger It is highly recommended that users of Python 3.6 prioritize upgrading to a newer version of Python. Nautobot will be removing support for Python 3.6 in a future update. Important For users remaining on Python 3.6, please know that upgrading to Nautobot v1.2.4 will not resolve these CVEs for your installation . The only remedy at this time is to upgrade your systems utilize Python 3.7 or later. #1267 - Implemented fixes for CVE-2022-22815 , CVE-2022-22816 , and CVE-2022-22817 to require Pillow >=9.0.0 for Python version >=3.7. For Python version <3.7 (e.g. 3.6), it is recommended that you prioritize upgrading your environment to use Python 3.7 or higher. Support for Python 3.6 will be removed in a future update. v1.2.3 (2022-01-07) \u00b6 Added \u00b6 #1037 - Added documentation about how to successfully use the nautobot-server dumpdata and nautobot-server loaddata commands. Fixed \u00b6 #313 - REST API documentation now correctly shows that status is a required field. #477 - Model TextField s are now correctly mapped to MultiValueCharFilter in filter classes. #734 - Requests to nonexistent /api/ URLs now correctly return a JSON 404 response rather than an HTML 404 response. #1127 - Fixed incorrect rendering of the navbar at certain browser window sizes. #1203 - Fixed maximum recursion depth error when filtering GraphQL queries by device_types . #1220 - Fixed an inconsistency in the breadcrumbs seen in various Admin pages. #1228 - Fixed a case where a GraphQL query for objects associated by Relationships could potentially throw an exception. #1229 - Fixed a template rendering error in the login page. #1234 - Fixed missing changelog support for Custom Fields. Security \u00b6 Danger It is highly recommended that users of Python 3.6 prioritize upgrading to a newer version of Python. Nautobot will be removing support for Python 3.6 in a future update. Important For users remaining on Python 3.6, please know that upgrading to Nautobot v1.2.3 will not resolve this CVE for your installation . The only remedy at this time is to upgrade your systems utilize Python 3.7 or later. #1238 - Implemented fix for CVE-2021-23727 to require Celery >=5.2.2 for Python version >=3.7. For Python version <3.7 (e.g. 3.6), it is recommended that you prioritize upgrading your environment to use Python 3.7 or higher. Support for Python 3.6 will be removed in a future update. v1.2.2 (2021-12-27) \u00b6 Added \u00b6 #1152 - Added REST API and GraphQL for JobLogEntry objects. Changed \u00b6 #650 - Job Results UI now render job log messages immediately Fixed \u00b6 #1181 - Avoid throwing a 500 error in the case where users have deleted a required Status value. (Preventing the user from doing this will need to be a later fix.) #1186 - Corrected an error in the docs regarding developing secrets providers in plugins. #1188 - Corrected some errors in the developer documentation about our branch management approach. #1193 - Fixed JobResult page may fail to list JobLogEntries in chronological order #1195 - Job log entries now again correctly render inline Markdown formatting. v1.2.1 (2021-12-16) \u00b6 Added \u00b6 #1110 - Added GraphQL support for the ObjectChange model. Changed \u00b6 #1106 - Updating Docker health checks to be more robust and greatly reduce performance impact. Fixed \u00b6 #1170 - Fixed bug in renamed column of JobResultTable where rename was not made to the Meta . #1173 - Fixed official Docker image: v1.2.0 tagged images fail to load with ImportError: libxml2.so.2 . Removed \u00b6 Security \u00b6 #1077 - Updated graphiql to 1.5.16 as well as updating the associated Javascript libraries used in the GraphiQL UI to address a reported security flaw in older versions of GraphiQL. To the best of our understanding, the Nautobot implementation of GraphiQL was not vulnerable to said flaw. v1.2.0 (2021-12-15) \u00b6 Added \u00b6 #843 - Added more information about Celery in the Upgrading Nautobot docs. #876 - Added option to apply a validation regex when defining CustomFieldChoices. #965 - Added example script for performing group sync from AzureAD. #982 - Added IPAM custom lookup database functions. #1002 - Added URM-P2 , URM-P4 , and URM-P8 port types. #1041 - Add passing of **kwargs to Celery tasks when using JobResult.enqueue_job() to execute a Job . #1080 - Added documentation around using LDAP with multiple search groups. #1082 - Added netutils template filters for both Django and Jinja2 template rendering. #1104 - Added documentation and context on filtering execution of unit tests using labels #1124 - Added documentation on generating SECRET_KEY before Nautobot is configured. #1143 - Added documentation on using LDAP with multiple LDAP servers. #1159 - Add family field to IPAddressType for GraphQL API enable filtering of IPAddress objects by family . Changed \u00b6 #1068 - Docker images now include optional Nautobot dependencies by default. #1095 - Refined Admin Configuration UI. #1105 - Reverted minimum Python 3.6 version to 3.6.0 rather than 3.6.2. Fixed \u00b6 #453 - Fixed potential ValueError when rendering JobResult detail view with non-standard JobResult.data contents. #864 - Fixed inconsistent JobResult detail view page templates. #888 - Addressed FIXME comment in LDAP documentation. #926 - Fixed inability to pass multiple values for a MultiObjectVar as query parameters. #958 - Fixed Job REST API handling of ObjectVars specified by query parameters. #992 - Improved loading/rendering time of the JobResult table/list view. #1043 - Fixed AttributeError when bulk-adding interfaces to virtual machines. #1078 - Fixed missing support for filtering several models by their custom fields and/or created/updated stamps. #1093 - Improved REST API performance by adding caching of serializer \"opt-in fields\". #1098 - Fixed 404 error when creating a circuit termination for circuit and other edge cases resulting in 404 errors #1112 - Fixed broken single-object GraphQL query endpoints. #1116 - Fixed UnboundLocalError when using device NAPALM integration #1121 - Fixed issue with handling of relationships referencing no-longer-present model classes. #1133 - Fixed some incorrect documentation about the Docker image build/publish process. #1141 - Improved reloading of changed Job files. (Port of NetBox #7820 ) #1154 - Fixed inability to save changes in Admin Configuration UI. #1162 - Fixed error when creating a NavMenuItem without specifying the buttons argument. Removed \u00b6 #1094 - Removed leftover custom field management views from Admin UI v1.2.0b1 (2021-11-19) \u00b6 Added \u00b6 #13 - Added nautobot_database_ready signal #125 - Added support for approval_required = True on Jobs #157 - Added support for same-object-type and symmetric Relationships #171 - GraphQL queries have been greatly optimized by integration with graphene-django-optimizer #229 - Added user-facing views for Custom Field management #248 - Added support for filtering GraphQL queries at all levels #370 - Added support for server configuration via the Admin UI. #374 - Added ability to schedule Jobs for future and/or recurring execution #478 - CustomFieldChoice model now supports GraphQL. #479 - Added shared generic template for all object detail views #519 - Added webhook support for CustomField and CustomFieldChoice models. #534 - Added ability to inject a banner from a plugin #541 - Added Secrets integration #580 - Added ability for plugins to register \"home\" and \"configuration\" views. #585 - Added \"Advanced\" tab to object detail views including UUID and slug information. #642 - Added documentation of the GIT_SSL_NO_VERIFY environment variable for using self-signed Git repositories #674 - Plugins can now add items to the Nautobot home page #716 - Nautobot home page content is now dynamically populated based on installed apps and plugins. #866 - Added support for organizational custom branding for the logo and icons #866 - Added documentation for job scheduling and approvals #879 - Added API testing for job scheduling and approvals #908 - Added UI testing for job scheduling and approvals #935 - Added Installed Plugins list view and detail view #937 - Added bulk-delete option for scheduled jobs #938 - Added titles to job approval UI buttons #947 - Added DISABLE_PREFIX_LIST_HIERARCHY setting to render IPAM Prefix list view as a flat list #953 - Added option to use MySQL in docker-compose development environment Changed \u00b6 #222 - Changed wildcard imports to explicitly enumerated imports and enabled associated Flake8 linter rules. #472 - JobResult lists now show the associated Job's name (if available) instead of the Job's class_path . #493 - All slug fields are now optional when creating records via the REST API, ORM, or CSV import. Slugs will be automatically assigned if unspecified. #877 - Hid unused \"Social Auth\" section from Django admin page. #900 - Admin site has been revised and re-skinned to more closely match the core Nautobot UI. Fixed \u00b6 #852 - Fixed missing \"Change Log\" tab on certain object detail views #853 - Fixed AttributeError on certain object detail views #891 - Fixed custom field select/multiselect not handled by new UI and added integration tests #966 - Fixed missing \"Advanced\" tab on Device detail views #1060 - Fixed documentation incorrectly indicating that the Admin UI was the only way to manage custom field definitions. Security \u00b6 #1017 - Custom field descriptions no longer potentially render as arbitrary HTML in object edit forms; Markdown format is now supported as a less dangerous option.","title":"Version 1.2"},{"location":"release-notes/version-1.2.html#nautobot-v12","text":"This document describes all new features and changes in Nautobot 1.2. If you are a user migrating from NetBox to Nautobot, please refer to the \"Migrating from NetBox\" documentation.","title":"Nautobot v1.2"},{"location":"release-notes/version-1.2.html#release-overview","text":"","title":"Release Overview"},{"location":"release-notes/version-1.2.html#added","text":"","title":"Added"},{"location":"release-notes/version-1.2.html#admin-configuration-ui-370","text":"The Nautobot Admin UI now includes a \"Configuration\" page that can be used to dynamically customize a number of optional settings as an alternative to editing nautobot_config.py and restarting the Nautobot processes. If upgrading from a previous Nautobot version where these settings were defined in your nautobot_config.py , you must remove those definitions in order to use this feature, as explicit configuration in nautobot_config.py takes precedence over values configured in the Admin UI.","title":"Admin Configuration UI (#370)"},{"location":"release-notes/version-1.2.html#common-base-template-for-object-detail-views-479-585","text":"All \"object detail\" views (pages displaying details of a single Nautobot record) now inherit from a common base template, providing improved UI consistency, reducing the amount of boilerplate code needed to create a new detail view, and fixing a number of bugs in various views. Plugin developers are encouraged to make use of this new template ( generic/object_detail.html ) to take advantage of these improvements. Views based on this template now include a new \"Advanced\" tab - currently this tab includes the UUID and slug (if any) of the object being viewed, but may be extended in the future to include additional information not relevant to the basic object detail view.","title":"Common Base Template for Object Detail Views (#479, #585)"},{"location":"release-notes/version-1.2.html#custom-fields-are-now-user-configurable-229","text":"Creation and management of Custom Field definitions can now be performed by any user with appropriate permissions. (Previously, only admin users were able to manage Custom Fields.)","title":"Custom Fields are now User Configurable (#229)"},{"location":"release-notes/version-1.2.html#custom-field-webhooks-519","text":"Webhooks can now be triggered when creating/updating/deleting CustomField and CustomFieldChoice definition records.","title":"Custom Field Webhooks (#519)"},{"location":"release-notes/version-1.2.html#database-ready-signal-13","text":"After running nautobot-server migrate or nautobot-server post_upgrade , Nautobot now emits a custom signal, nautobot_database_ready . This signal is designed for plugins to connect to in order to perform automatic database population (such as defining custom fields, relationships, webhooks, etc.) at install/upgrade time. For more details, refer to the plugin development documentation .","title":"Database Ready Signal (#13)"},{"location":"release-notes/version-1.2.html#graphql-filters-at-all-levels-248","text":"The GraphQL API now supports query filter parameters at any level of a query. For example: query { sites(name: \"ams\") { devices(role: \"edge\") { name interfaces(type: \"virtual\") { name } } } }","title":"GraphQL Filters at All Levels (#248)"},{"location":"release-notes/version-1.2.html#graphql-query-optimizations-171","text":"Complex GraphQL queries have been greatly optimized thanks to integration of graphene-django-optimizer into Nautobot! In our internal testing and benchmarking the number of SQL queries generated per GraphQL query have been drastically reduced, resulting in much quicker response times and less strain on the database. For in depth details on our benchmarks, please see the comment thread on the issue .","title":"GraphQL Query Optimizations (#171)"},{"location":"release-notes/version-1.2.html#installed-plugins-list-and-detail-views-plugin-config-and-home-views-935","text":"The Plugins menu now includes an \"Installed Plugins\" menu item which provides a list view of information about all installed and enabled plugins, similar to a formerly administrator-only view. Additionally, when viewing this list, each plugin can now be clicked on for a detail view, which provides an in-depth look at the capabilities of the plugin, including whether it makes use of each or all of the various Nautobot features available to be used by plugins. Additionally, plugins now have the option of registering specific \"home\" and/or \"configuration\" views, which will be linked and accessible directly from the installed-plugins list and detail views. Please refer to the plugin development documentation for more details about this functionality.","title":"Installed Plugins List and Detail Views, Plugin Config and Home Views (#935)"},{"location":"release-notes/version-1.2.html#ipam-custom-lookups-for-filtering-982","text":"Nautobot now again supports custom lookup filters on the IPAddress , Prefix , and Aggregate models, such as address__net_contained , network__net_contains_or_equals , etc. Refer to the REST API filtering documentation for more specifics and examples.","title":"IPAM custom lookups for filtering (#982)"},{"location":"release-notes/version-1.2.html#job-approval-125","text":"Jobs can now be optionally defined as approval_required = True , in which case the Job will not be executed immediately upon submission, but will instead be placed into an approval queue; any user other than the submitter can approve or deny a queued Job, at which point it will then be executed as normal.","title":"Job Approval (#125)"},{"location":"release-notes/version-1.2.html#job-scheduling-374","text":"Jobs can now be scheduled for execution at a future date and time (such as during a planned maintenance window), and can also be scheduled for repeated execution on an hourly, daily, or weekly recurring cadence. Note Execution of scheduled jobs is dependent on Celery Beat ; enablement of this system service is a new requirement in Nautobot 1.2. Please see the documentation on enabling the Celery Beat scheduler service to get started!","title":"Job Scheduling (#374)"},{"location":"release-notes/version-1.2.html#networking-template-filters-1082","text":"Template rendering with Django and/or Jinja2 now supports by default all filters provided by the netutils library. These filters can be used in page templates, computed fields, custom links, export templates, etc. For details, please refer to the filters documentation.","title":"Networking Template Filters (#1082)"},{"location":"release-notes/version-1.2.html#organizational-branding-859","text":"Organizations may provide custom branding assets to change the logo, icons, and footer URLs to help Nautobot fit within their environments and user communities. Please see the configuration documenation for details on how to specify the location and usage of custom branding assets.","title":"Organizational Branding (#859)"},{"location":"release-notes/version-1.2.html#plugin-banners-534","text":"Each plugin is now able to optionally inject a custom banner into any of the Nautobot core views. Please refer to the plugin development documentation for more details about this functionality.","title":"Plugin Banners (#534)"},{"location":"release-notes/version-1.2.html#same-type-and-symmetric-relationships-157","text":"The Relationships feature has been extended in two ways: Relationships between the same object type (e.g. device-to-device) are now permitted and supported. For same-object-type relationships specifically, symmetric (peer-to-peer rather than source-to-destination) relationships are now an option. For more details, refer to the Relationships documentation.","title":"Same-Type and Symmetric Relationships (#157)"},{"location":"release-notes/version-1.2.html#secrets-integration-541","text":"Nautobot can now read secret values (such as device or Git repository access credentials) on demand from a variety of external sources, including environment variables and text files, and extensible via plugins to support additional secrets providers such as Hashicorp Vault and AWS Secrets Manager. Both the NAPALM device integration and the Git repository integration can now make use of these secrets, and plugins and jobs can do so as well. For more details, please refer to the Secrets documentation.","title":"Secrets Integration (#541)"},{"location":"release-notes/version-1.2.html#software-defined-home-page-674-716","text":"Nautobot core applications and plugins can now both define panels, groups, and items to populate the Nautobot home page. The home page now dynamically reflows to accommodate available content. Plugin developers can add to existing panels or groups or define entirely new panels as needed. For more details, see Populating the Home Page .","title":"Software-Defined Home Page (#674, #716)"},{"location":"release-notes/version-1.2.html#changed","text":"","title":"Changed"},{"location":"release-notes/version-1.2.html#admin-site-changes-900","text":"The Admin sub-site within Nautobot ( /admin/ and its child pages) has been revamped in appearance and functionality. It has been re-skinned to resemble the rest of the Nautobot UI, and has been slimmed down to only include those models and features that are still exclusive to admin users, such as user/group/permission management.","title":"Admin Site Changes (#900)"},{"location":"release-notes/version-1.2.html#joblogentry-data-model-1030","text":"Job log messages are now stored in a separate database table as a separate JobLogEntry data model, instead of being stored as JSON on the JobResult model/table. This provides faster and more robust rendering of JobResult -related views and lays groundwork for future enhancements of the Jobs feature. Note If you are executing Jobs inside your tests, there are some changes you will need to make for your tests to support this feature correctly. Refer to the Jobs documentation for details. Note Because JobLogEntry records reference their associated JobResult , the pattern job.job_result = JobResult() (creating only an in-memory JobResult object, rather than a database entry) will no longer work. Instead you will need to create a proper JobResult database object job.job_result = JobResult.objects.create(...) .","title":"JobLogEntry Data Model (#1030)"},{"location":"release-notes/version-1.2.html#slug-fields-are-now-optional-in-csv-import-rest-api-and-orm-493","text":"All models that have slug fields now use AutoSlugField from the django-extensions package. This means that when creating a record via the REST API, CSV import, or direct ORM Python calls, the slug field is now fully optional; if unspecified, it will be automatically assigned a unique value, just as how a slug is auto-populated in the UI when creating a new record. Just as with the UI, the slug can still always be explicitly set if desired.","title":"Slug fields are now Optional in CSV import, REST API and ORM (#493)"},{"location":"release-notes/version-1.2.html#v1211-2022-04-04","text":"","title":"v1.2.11 (2022-04-04)"},{"location":"release-notes/version-1.2.html#added_1","text":"#1123 - Add validation for IPAddress assigned_object_type and assigned_object_id. #1146 - Added change date filtering lookup expressions to GraphQL. #1495 - Added full coverage of cable termination types to Graphene. #1501 - Add IP field to CSV export of device. #1529 - Added list of standard hex colors to the Tags documentation.","title":"Added"},{"location":"release-notes/version-1.2.html#changed_1","text":"#1536 - Removed the ServiceUnavailable exception when no primary_ip is available for a device, but other connection options are available. #1581 - Changed MultipleChoiceJSONField to accept choices as a callable, fixing Datasource Contents provided by plugins are not accepted as valid choice by REST API. #1584 - Replaced links in docs to celeryproject.org with celeryq.dev","title":"Changed"},{"location":"release-notes/version-1.2.html#fixed","text":"#1313 - Fixed GraphQL query error on OneToOneFields such as IPAddress.primary_ip4_for #1408 - Fixed incorrect HTML in the Devices detail views. #1467 - Fixed an issue where at certain browser widths the nav bar would cover the top of the page content. #1523 - Fixed primary IP being unset after creating/updating different interface #1548 - Pin Jinja2 version for mkdocs requirements to fix RTD docs builds related to API deprecation in Jinja2 >= 3.1.0 #1583 - Fixed Nautobot service definition in PostgreSQL-backed development environment. #1599 - Bump mkdocs version for Snyk report.","title":"Fixed"},{"location":"release-notes/version-1.2.html#v1210-2022-03-21","text":"","title":"v1.2.10 (2022-03-21)"},{"location":"release-notes/version-1.2.html#added_2","text":"#1492 - Added note in the Jobs documentation about the use of AbortTransaction to end the job and force rollback. #1517 - Added password filtering example to advanced logging section in docs.","title":"Added"},{"location":"release-notes/version-1.2.html#changed_2","text":"#1514 - Simplified switching between PostgreSQL and MySQL database backends in the developer environment. #1518 - Updated GitHub Pull Request template to include detail section, todo list.","title":"Changed"},{"location":"release-notes/version-1.2.html#fixed_1","text":"#1511 - Fixed left column of Read The Docs being cut off. #1522 - Fixed link name attribute name in developer docs.","title":"Fixed"},{"location":"release-notes/version-1.2.html#v129-2022-03-14","text":"","title":"v1.2.9 (2022-03-14)"},{"location":"release-notes/version-1.2.html#fixed_2","text":"#1431 - Fixed potential failure of extras.0017_joblog_data_migration migration when the job logs contain messages mistakenly logged as object references. #1459 - Fixed incorrect display of related devices and VMs in the Cluster Type and Cluster Group detail views. #1469 - Fixed incorrect CSV export for devices","title":"Fixed"},{"location":"release-notes/version-1.2.html#security","text":"Danger It is highly recommended that users of Python 3.6 prioritize upgrading to a newer version of Python. Nautobot will be removing support for Python 3.6 in a future update. Important For users remaining on Python 3.6, please know that upgrading to Nautobot v1.2.9 will not resolve these CVEs for your installation . The only remedy at this time is to upgrade your systems to utilize Python 3.7 or later. #1487 - Implemented fixes for CVE-2022-22817 , CVE-2022-24303 , and potential infinite loop by requiring Pillow >=9.0.1 for Python version >=3.7. For Python version <3.7 (e.g. 3.6), it is recommended that you prioritize upgrading your environment to use Python 3.7 or higher. Support for Python 3.6 will be removed in a future update.","title":"Security"},{"location":"release-notes/version-1.2.html#v128-2022-03-07","text":"","title":"v1.2.8 (2022-03-07)"},{"location":"release-notes/version-1.2.html#added_3","text":"#839 - Add CODE_OF_CONDUCT.md to repository. #1242 - Add MAJOR.MINOR tags to Docker images upon release. #1299 - Add SECURITY.md to repository. #1388 - Added beta version of GitHub Issue Form style for feature request. #1419 - Add documentation for specifying a CA cert file for LDAP authentication backend. #1446 - Apply title labels to Docker images.","title":"Added"},{"location":"release-notes/version-1.2.html#changed_3","text":"#1348 - Pin Selenium Grid container version to match Python Client version. #1432 - Update django-redis to 5.2.x to address 5.1.x blocking redis 4.x versions. #1447 - Minor nit on Github Issue Form styling. #1452 - Changed GitHub release workflow to not run on prerelease releases. #1453 - Changed feature request to use GitHub Issue Form.","title":"Changed"},{"location":"release-notes/version-1.2.html#fixed_3","text":"#1301 - Fixed window history handling for views with tabs in Safari/Firefox. #1302 - Fixed missing Advanced tab on Virtual Machine detail view. #1398 - Fixed missing safeguard for removing master from Virtual Chassis via API. #1399 - Fixed not being able to set master to null on Virtual Chassis API. #1405 - Fixed incorrect import in 'startplugin' template code. #1412 - Fixed not being able to query for prefix family via GraphQL. #1442 - Fixed missing Advanced tab on Job Result, Git Repository, and Config Context Schema detail views.","title":"Fixed"},{"location":"release-notes/version-1.2.html#v127-2022-02-22","text":"","title":"v1.2.7 (2022-02-22)"},{"location":"release-notes/version-1.2.html#changed_4","text":"#1403 - Changes the GitHub Action on Release version template variable name.","title":"Changed"},{"location":"release-notes/version-1.2.html#v126-2022-02-22","text":"","title":"v1.2.6 (2022-02-22)"},{"location":"release-notes/version-1.2.html#added_4","text":"#1279 - Circuit terminations now render custom relationships on the circuit detail page. #1353 - Added UI for deleting previously uploaded images when editing a DeviceType.","title":"Added"},{"location":"release-notes/version-1.2.html#changed_5","text":"#1386 - Updated release schedule in docs for patch releases, now every two weeks.","title":"Changed"},{"location":"release-notes/version-1.2.html#fixed_4","text":"#1249 - Fixed a timing issue where after creating a custom field with a default value and immediately assigning values to this custom field on individual objects, the custom field values could be automatically reverted to the default value. #1280 - Added missing get_absolute_url method to the CircuitTermination model, fixing a UI error that could occur when relationships involve CircuitTerminations. #1283 - Update Sentinel docs to have 3 hosts (minimum per Redis docs), and change CELERY_BROKER_URL to a multiline string instead of a Tuple (tuple is invalid, and raises an exception when job completes). #1312 - Fixed a bug where a Prefix filter matching zero records would instead show all records in the UI. #1327 - Fixes the broken dependencies from the Release action. #1328 - Fixed an error in the Job class-path documentation . #1332 - Fixed a regression in which the REST API did not default to pagination based on the configured PAGINATE_COUNT setting but instead defaulted to full unpaginated results. #1335 - Fixed an issue with the Secret create/edit form that caused problems when defining AWS secrets using the nautobot-secrets-providers plugin. #1346 - Fixed an error in the periodic execution of Celery's built-in celery.backend_cleanup task. #1360 - Fixed an issue in the development environment that could cause Selenium integration tests to error out. #1390 - Pinned transitive dependency MarkupSafe to version 2.0.1 as later versions are incompatible with Nautobot's current Jinja2 dependency.","title":"Fixed"},{"location":"release-notes/version-1.2.html#v125-2022-02-02","text":"","title":"v1.2.5 (2022-02-02)"},{"location":"release-notes/version-1.2.html#changed_6","text":"#1293 - Reorganized the developer documents somewhat to reduce duplication of information, added diagrams for issue intake process.","title":"Changed"},{"location":"release-notes/version-1.2.html#fixed_5","text":"#371 - Fixed a server error that could occur when importing cables via CSV. #1161 - The description field for device component templates is now correctly propagated to device components created from these templates. #1233 - Prevented a job aborting when an optional ObjectVar is provided with a value of None #1272 - Fixed GitHub Actions syntax and Slack payload for release CI workflow #1282 - Fixed a server error when editing User accounts. #1308 - Fixed another server error that could occur when importing cables via CSV.","title":"Fixed"},{"location":"release-notes/version-1.2.html#v124-2022-01-13","text":"","title":"v1.2.4 (2022-01-13)"},{"location":"release-notes/version-1.2.html#added_5","text":"#1113 - Added documentation about using Redis Sentinel with Nautobot. #1251 - Added workflow_call to the GitHub Actions CI workflow so that it may be called by other GHA workflows.","title":"Added"},{"location":"release-notes/version-1.2.html#changed_7","text":"#616 - The REST API now no longer permits setting non-string values for text-type custom fields. #1243 - Github CI action no longer runs for pull requests that don't impact Nautobot code, such as documentation, examples, etc.","title":"Changed"},{"location":"release-notes/version-1.2.html#fixed_6","text":"#1053 - Fixed error when removing an IP address from an interface when it was previously the parent device's primary IP. #1140 - Fixed incorrect UI widgets in the updated Admin UI. #1253 - Fixed missing code that prevented switching between tabs in the device-type detail view.","title":"Fixed"},{"location":"release-notes/version-1.2.html#security_1","text":"Danger It is highly recommended that users of Python 3.6 prioritize upgrading to a newer version of Python. Nautobot will be removing support for Python 3.6 in a future update. Important For users remaining on Python 3.6, please know that upgrading to Nautobot v1.2.4 will not resolve these CVEs for your installation . The only remedy at this time is to upgrade your systems utilize Python 3.7 or later. #1267 - Implemented fixes for CVE-2022-22815 , CVE-2022-22816 , and CVE-2022-22817 to require Pillow >=9.0.0 for Python version >=3.7. For Python version <3.7 (e.g. 3.6), it is recommended that you prioritize upgrading your environment to use Python 3.7 or higher. Support for Python 3.6 will be removed in a future update.","title":"Security"},{"location":"release-notes/version-1.2.html#v123-2022-01-07","text":"","title":"v1.2.3 (2022-01-07)"},{"location":"release-notes/version-1.2.html#added_6","text":"#1037 - Added documentation about how to successfully use the nautobot-server dumpdata and nautobot-server loaddata commands.","title":"Added"},{"location":"release-notes/version-1.2.html#fixed_7","text":"#313 - REST API documentation now correctly shows that status is a required field. #477 - Model TextField s are now correctly mapped to MultiValueCharFilter in filter classes. #734 - Requests to nonexistent /api/ URLs now correctly return a JSON 404 response rather than an HTML 404 response. #1127 - Fixed incorrect rendering of the navbar at certain browser window sizes. #1203 - Fixed maximum recursion depth error when filtering GraphQL queries by device_types . #1220 - Fixed an inconsistency in the breadcrumbs seen in various Admin pages. #1228 - Fixed a case where a GraphQL query for objects associated by Relationships could potentially throw an exception. #1229 - Fixed a template rendering error in the login page. #1234 - Fixed missing changelog support for Custom Fields.","title":"Fixed"},{"location":"release-notes/version-1.2.html#security_2","text":"Danger It is highly recommended that users of Python 3.6 prioritize upgrading to a newer version of Python. Nautobot will be removing support for Python 3.6 in a future update. Important For users remaining on Python 3.6, please know that upgrading to Nautobot v1.2.3 will not resolve this CVE for your installation . The only remedy at this time is to upgrade your systems utilize Python 3.7 or later. #1238 - Implemented fix for CVE-2021-23727 to require Celery >=5.2.2 for Python version >=3.7. For Python version <3.7 (e.g. 3.6), it is recommended that you prioritize upgrading your environment to use Python 3.7 or higher. Support for Python 3.6 will be removed in a future update.","title":"Security"},{"location":"release-notes/version-1.2.html#v122-2021-12-27","text":"","title":"v1.2.2 (2021-12-27)"},{"location":"release-notes/version-1.2.html#added_7","text":"#1152 - Added REST API and GraphQL for JobLogEntry objects.","title":"Added"},{"location":"release-notes/version-1.2.html#changed_8","text":"#650 - Job Results UI now render job log messages immediately","title":"Changed"},{"location":"release-notes/version-1.2.html#fixed_8","text":"#1181 - Avoid throwing a 500 error in the case where users have deleted a required Status value. (Preventing the user from doing this will need to be a later fix.) #1186 - Corrected an error in the docs regarding developing secrets providers in plugins. #1188 - Corrected some errors in the developer documentation about our branch management approach. #1193 - Fixed JobResult page may fail to list JobLogEntries in chronological order #1195 - Job log entries now again correctly render inline Markdown formatting.","title":"Fixed"},{"location":"release-notes/version-1.2.html#v121-2021-12-16","text":"","title":"v1.2.1 (2021-12-16)"},{"location":"release-notes/version-1.2.html#added_8","text":"#1110 - Added GraphQL support for the ObjectChange model.","title":"Added"},{"location":"release-notes/version-1.2.html#changed_9","text":"#1106 - Updating Docker health checks to be more robust and greatly reduce performance impact.","title":"Changed"},{"location":"release-notes/version-1.2.html#fixed_9","text":"#1170 - Fixed bug in renamed column of JobResultTable where rename was not made to the Meta . #1173 - Fixed official Docker image: v1.2.0 tagged images fail to load with ImportError: libxml2.so.2 .","title":"Fixed"},{"location":"release-notes/version-1.2.html#removed","text":"","title":"Removed"},{"location":"release-notes/version-1.2.html#security_3","text":"#1077 - Updated graphiql to 1.5.16 as well as updating the associated Javascript libraries used in the GraphiQL UI to address a reported security flaw in older versions of GraphiQL. To the best of our understanding, the Nautobot implementation of GraphiQL was not vulnerable to said flaw.","title":"Security"},{"location":"release-notes/version-1.2.html#v120-2021-12-15","text":"","title":"v1.2.0 (2021-12-15)"},{"location":"release-notes/version-1.2.html#added_9","text":"#843 - Added more information about Celery in the Upgrading Nautobot docs. #876 - Added option to apply a validation regex when defining CustomFieldChoices. #965 - Added example script for performing group sync from AzureAD. #982 - Added IPAM custom lookup database functions. #1002 - Added URM-P2 , URM-P4 , and URM-P8 port types. #1041 - Add passing of **kwargs to Celery tasks when using JobResult.enqueue_job() to execute a Job . #1080 - Added documentation around using LDAP with multiple search groups. #1082 - Added netutils template filters for both Django and Jinja2 template rendering. #1104 - Added documentation and context on filtering execution of unit tests using labels #1124 - Added documentation on generating SECRET_KEY before Nautobot is configured. #1143 - Added documentation on using LDAP with multiple LDAP servers. #1159 - Add family field to IPAddressType for GraphQL API enable filtering of IPAddress objects by family .","title":"Added"},{"location":"release-notes/version-1.2.html#changed_10","text":"#1068 - Docker images now include optional Nautobot dependencies by default. #1095 - Refined Admin Configuration UI. #1105 - Reverted minimum Python 3.6 version to 3.6.0 rather than 3.6.2.","title":"Changed"},{"location":"release-notes/version-1.2.html#fixed_10","text":"#453 - Fixed potential ValueError when rendering JobResult detail view with non-standard JobResult.data contents. #864 - Fixed inconsistent JobResult detail view page templates. #888 - Addressed FIXME comment in LDAP documentation. #926 - Fixed inability to pass multiple values for a MultiObjectVar as query parameters. #958 - Fixed Job REST API handling of ObjectVars specified by query parameters. #992 - Improved loading/rendering time of the JobResult table/list view. #1043 - Fixed AttributeError when bulk-adding interfaces to virtual machines. #1078 - Fixed missing support for filtering several models by their custom fields and/or created/updated stamps. #1093 - Improved REST API performance by adding caching of serializer \"opt-in fields\". #1098 - Fixed 404 error when creating a circuit termination for circuit and other edge cases resulting in 404 errors #1112 - Fixed broken single-object GraphQL query endpoints. #1116 - Fixed UnboundLocalError when using device NAPALM integration #1121 - Fixed issue with handling of relationships referencing no-longer-present model classes. #1133 - Fixed some incorrect documentation about the Docker image build/publish process. #1141 - Improved reloading of changed Job files. (Port of NetBox #7820 ) #1154 - Fixed inability to save changes in Admin Configuration UI. #1162 - Fixed error when creating a NavMenuItem without specifying the buttons argument.","title":"Fixed"},{"location":"release-notes/version-1.2.html#removed_1","text":"#1094 - Removed leftover custom field management views from Admin UI","title":"Removed"},{"location":"release-notes/version-1.2.html#v120b1-2021-11-19","text":"","title":"v1.2.0b1 (2021-11-19)"},{"location":"release-notes/version-1.2.html#added_10","text":"#13 - Added nautobot_database_ready signal #125 - Added support for approval_required = True on Jobs #157 - Added support for same-object-type and symmetric Relationships #171 - GraphQL queries have been greatly optimized by integration with graphene-django-optimizer #229 - Added user-facing views for Custom Field management #248 - Added support for filtering GraphQL queries at all levels #370 - Added support for server configuration via the Admin UI. #374 - Added ability to schedule Jobs for future and/or recurring execution #478 - CustomFieldChoice model now supports GraphQL. #479 - Added shared generic template for all object detail views #519 - Added webhook support for CustomField and CustomFieldChoice models. #534 - Added ability to inject a banner from a plugin #541 - Added Secrets integration #580 - Added ability for plugins to register \"home\" and \"configuration\" views. #585 - Added \"Advanced\" tab to object detail views including UUID and slug information. #642 - Added documentation of the GIT_SSL_NO_VERIFY environment variable for using self-signed Git repositories #674 - Plugins can now add items to the Nautobot home page #716 - Nautobot home page content is now dynamically populated based on installed apps and plugins. #866 - Added support for organizational custom branding for the logo and icons #866 - Added documentation for job scheduling and approvals #879 - Added API testing for job scheduling and approvals #908 - Added UI testing for job scheduling and approvals #935 - Added Installed Plugins list view and detail view #937 - Added bulk-delete option for scheduled jobs #938 - Added titles to job approval UI buttons #947 - Added DISABLE_PREFIX_LIST_HIERARCHY setting to render IPAM Prefix list view as a flat list #953 - Added option to use MySQL in docker-compose development environment","title":"Added"},{"location":"release-notes/version-1.2.html#changed_11","text":"#222 - Changed wildcard imports to explicitly enumerated imports and enabled associated Flake8 linter rules. #472 - JobResult lists now show the associated Job's name (if available) instead of the Job's class_path . #493 - All slug fields are now optional when creating records via the REST API, ORM, or CSV import. Slugs will be automatically assigned if unspecified. #877 - Hid unused \"Social Auth\" section from Django admin page. #900 - Admin site has been revised and re-skinned to more closely match the core Nautobot UI.","title":"Changed"},{"location":"release-notes/version-1.2.html#fixed_11","text":"#852 - Fixed missing \"Change Log\" tab on certain object detail views #853 - Fixed AttributeError on certain object detail views #891 - Fixed custom field select/multiselect not handled by new UI and added integration tests #966 - Fixed missing \"Advanced\" tab on Device detail views #1060 - Fixed documentation incorrectly indicating that the Admin UI was the only way to manage custom field definitions.","title":"Fixed"},{"location":"release-notes/version-1.2.html#security_4","text":"#1017 - Custom field descriptions no longer potentially render as arbitrary HTML in object edit forms; Markdown format is now supported as a less dangerous option.","title":"Security"},{"location":"release-notes/version-1.3.html","text":"Nautobot v1.3 \u00b6 This document describes all new features and changes in Nautobot 1.3. If you are a user migrating from NetBox to Nautobot, please refer to the \"Migrating from NetBox\" documentation. Release Overview \u00b6 Added \u00b6 Dynamic Group Model ( #896 ) \u00b6 A new data model for representing dynamic groups of objects has been implemented. Dynamic groups can be used to organize objects together by matching criteria such as their site location or region, for example, and are dynamically updated whenever new matching objects are created, or existing objects are updated. For the initial release only dynamic groups of Device and VirtualMachine objects are supported. Extend FilterSets and Filter Forms via Plugins ( #1470 ) \u00b6 Plugins can now extend existing FilterSets and Filter Forms. This allows plugins to provide alternative lookup methods or custom queries in the UI or API that may not already exist today. You can refer to the plugin development guide on how to create new filters and fields. GraphQL Pagination ( #1109 ) \u00b6 GraphQL list queries can now be paginated by specifying the filter parameters limit and offset . Refer to the GraphQL user guide for examples. Job Database Model ( #1001 ) \u00b6 Installed Jobs are now represented by a data model in the Nautobot database. This allows for new functionality including: The Jobs listing UI view can now be filtered and searched like most other Nautobot table/list views. Job attributes (name, description, approval requirements, etc.) can now be managed via the Nautobot UI by an administrator or user with appropriate permissions to customize or override the attributes defined in the Job source code. Jobs can now be identified by a slug as well as by their class_path . A new set of REST API endpoints have been added to /api/extras/jobs/<uuid>/ . The existing /api/extras/jobs/<class_path>/ REST API endpoints continue to work but should be considered as deprecated. A new version of the REST API /api/extras/jobs/ list endpoint has been implemented as well, but by default this endpoint continues to demonstrate the pre-1.3 behavior unless the REST API client explicitly requests API version=1.3 . See the section on REST API versioning, below, for more details. As a minor security measure, newly installed Jobs default to enabled = False , preventing them from being run until an administrator or user with appropriate permissions updates them to be enabled for running. Note As a convenience measure, when initially upgrading to Nautobot 1.3.x, any existing Jobs that have been run or scheduled previously (i.e., have at least one associated JobResult and/or ScheduledJob record) will instead default to enabled = True so that they may continue to be run without requiring changes. For more details please refer to the Jobs feature documentation as well as the Job data model documentation . Jobs With Sensitive Parameters ( #2091 ) \u00b6 Jobs model now includes a has_sensitive_variables field which by default prevents the job's input parameters from being saved to the database. Review whether each job's input parameters include sensitive data such as passwords or other user credentials before setting this to False for any given job. JSON Type for Custom Fields ( #897 ) \u00b6 Custom fields can now have a type of \"json\". Fields of this type can be used to store arbitrary JSON data. Natural Indexing for Common Lookups ( #1638 ) \u00b6 Many fields have had indexing added to them as well as index togethers on ObjectChange fields. This should provide a noticeable performance improvement when filtering and doing lookups. Note This is going to perform several migrations to add all of the indexes. On MySQL databases and tables with 1M+ records this can take a few minutes. Every environment is different but it should be expected for this upgrade to take some time. Overlapping/Multiple NAT Support ( #630 ) \u00b6 IP addresses can now be associated with multiple outside NAT IP addresses. To do this, set more than one IP Address to have the same NAT inside IP address. A new version of the REST API /api/ipam/ip-addresses/* endpoints have been implemented as well, but by default this endpoint continues to demonstrate the pre-1.3 behavior unless the REST API client explicitly requests API version=1.3 . See the section on REST API versioning, below, for more details. Note There are some guardrails on this feature to support backwards compatibility. If you consume the REST API without specifying the version header or query argument and start associating multiple IPs to have the same NAT inside IP address, an error will be reported, because the existing REST API schema returns nat_outside as a single object, where as 1.3 and beyond will return this as a list. Provider Network Model ( #724 ) \u00b6 A data model has been added to support representing the termination of a circuit to an external provider's network. Python 3.10 Support ( #1255 ) \u00b6 Python 3.10 is officially supported by Nautobot now, and we are building and publishing Docker images with Python 3.10 now. Regular Expression Support in API Filtering ( #1525 ) \u00b6 New lookup expressions for using regular expressions to filter objects by string (char) fields in the API have been added to all core filters. The expressions re (regex), nre (negated regex), ire (case-insensitive regex), and nire (negated case-insensitive regex) lookup expressions are now dynamically-generated for filter fields inherited by subclasses of nautobot.utilities.filters.BaseFilterSet . Remove Stale Scheduled Jobs ( #2091 ) \u00b6 remove_stale_scheduled_jobs management command has been added to delete non-recurring scheduled jobs that were scheduled to run more than a specified days ago. REST API Token Provisioning ( #1374 ) \u00b6 Nautobot now has an /api/users/tokens/ REST API endpoint where a user can provision a new REST API token. This allows a user to gain REST API access without needing to first create a token via the web UI. $ curl -X POST \\ -H \"Accept: application/json; indent=4\" \\ -u \"hankhill:I<3C3H8\" \\ https://nautobot/api/users/tokens/ This endpoint specifically supports Basic Authentication in addition to the other REST API authentication methods. REST API Versioning ( #1465 ) \u00b6 Nautobot's REST API now supports multiple versions, which may be requested by modifying the HTTP Accept header on any requests sent by a REST API client. Details are in the REST API documentation , but in brief: The REST API endpoints that are versioned in the 1.3.0 release are /api/extras/jobs/ listing endpoint /api/extras/tags/ create/put/patch endpoints all /api/ipam/ip-addresses/ endpoints All other REST API endpoints are currently non-versioned. However, over time more versioned REST APIs will be developed, so this is important to understand for all REST API consumers. If a REST API client does not request a specific REST API version (in other words, requests Accept: application/json rather than Accept: application/json; version=1.3 ) the API behavior will be compatible with Nautobot 1.2, at a minimum for the remainder of the Nautobot 1.x release cycle. The API behavior may change to a newer default version in a Nautobot major release (such as 2.0). To request an updated (non-backwards-compatible) API endpoint, an API version must be requested corresponding at a minimum to the Nautobot major.minor version where the updated API endpoint was introduced (so to interact with the updated REST API endpoints mentioned above, Accept: application/json; version=1.3 ). Tip As a best practice, when developing a Nautobot REST API integration, your client should always request the current API version it is being developed against, rather than relying on the default API behavior (which may change with a new Nautobot major release, as noted, and which also may not include the latest and greatest API endpoints already available but not yet made default in the current release). Webhook Pre/Post-change Data Added to Request Body ( #330 ) \u00b6 Webhooks now provide a snapshot of data before and after a change, as well as the differences between the old and new data. See the default request body section in the webhook docs . Changed \u00b6 Docker Images Now Default to Python 3.7 ( #1252 ) \u00b6 As Python 3.6 has reached end-of-life, the default Docker images published for this release (i.e. 1.3.0 , stable , latest ) have been updated to use Python 3.7 instead. Job Approval Now Controlled By extras.approve_job Permission ( #1490 ) \u00b6 Similar to the existing extras.run_job permission, a new extras.approve_job permission is now enforced by the UI and the REST API when approving scheduled jobs. Only users with this permission can approve or deny approval requests; additionally such users also now require the extras.view_scheduledjob , extras.change_scheduledjob , and extras.delete_scheduledjob permissions as well. OpenAPI 3.0 REST API documentation ( #595 ) \u00b6 The online REST API Swagger documentation ( /api/docs/ ) has been updated from OpenAPI 2.0 format to OpenAPI 3.0 format and now supports Nautobot's REST API versioning as described above. Try /api/docs/?api_version=1.3 as an example. Tag restriction by content-type ( #872 ) \u00b6 When created, a Tag can be associated to one or more model content-types using a many-to-many relationship. The tag will then apply only to models belonging to those associated content-types. For users migrating from an earlier Nautobot release, any existing tags will default to being enabled for all content-types for compatibility purposes. Individual tags may subsequently edited to remove any content-types that they do not need to apply to. Note that a Tag created programmatically via the ORM without assigning any content_types will not be applicable to any model until content-types are assigned to it. Update Jinja2 to 3.x ( #1474 ) \u00b6 We've updated the Jinja2 dependency from version 2.11 to version 3.0.3. This may affect the syntax of any nautobot.extras.models.ComputedField objects in your database... Specifically, the template attribute, which is parsed as a Jinja2 template. Please refer to Jinja2 3.0.x's release notes to check if any changes might be required in your computed fields' templates. Virtual Chassis Master Device Interfaces List \u00b6 The device column will now show on a device's interfaces list if this device is the master in a virtual chassis. And will conversely not appear if the device is not a master on a virtual chassis. It is no longer possible to connect an interface to itself in the cable connect form. Removed \u00b6 Python 3.6 No Longer Supported ( #1268 ) \u00b6 As Python 3.6 has reached end-of-life, and many of Nautobot's dependencies have already dropped support for Python 3.6 as a consequence, Nautobot 1.3 and later do not support installation under Python 3.6. v1.3.10 (2022-08-08) \u00b6 Added \u00b6 #1226 - Added custom job intervals package management. #2073 - Added --local option to nautobot-server runjob command. #2080 - Added --data parameter to nautobot-server runjob command. #2091 - Added remove_stale_scheduled_jobs management command which removes all stale scheduled jobs and also added has_sensitive_variables field to Job model which prevents the job's input parameters from being saved to the database. #2143 - Scheduled Job detail view now includes details of any custom interval. Changed \u00b6 #2025 - Tweak Renovate config for automated package management. #2114 - Home page now redirects to the login page when HIDE_RESTRICTED_UI is enabled and user is not authenticated. #2115 - Patch updates to mkdocs , svgwrite . Fixed \u00b6 #1739 - Fixed paginator not enforcing max_page_size setting in web ui views. #2060 - Fixed relationship peer_id filter non-existent error on relationship association page. #2095 - Fixed health check failing with Redis Sentinel, TLS configuration. #2119 - Fixed flaky integration test for cable connection UI. Security \u00b6 Important With introducing the has_sensitive_variables flag on Job classes and model (see: #2091 ), jobs can be prevented from storing their inputs in the database. Due to the nature of queuing or scheduling jobs, the desired inputs must be stored for future use. New safe-default behavior will only permit jobs to be executed immediately, as has_sensitive_variables defaults to True . This value can be overridden by the Job class itself or the Job model edit page. Values entered for jobs executing immediately go straight to the Celery message bus and are cleaned up on completion of execution. Scheduling jobs or requiring approval necessitates those values to be stored in the database until they have been sent to the Celery message bus for execution. During installation of v1.3.10 , a migration is applied to set the has_sensitive_variables value to True to all existing Jobs. However to maintain backwards-compatibility, past scheduled jobs are permitted to keep their schedule. New schedules cannot be made until an administrator has overridden the has_sensitive_variables for the desired Job. A new management command exists ( remove_stale_scheduled_jobs ) which will aid in cleaning up schedules to past jobs which may still have sensitive data stored in the database. This command is not exhaustive nor intended to clean up sensitive values stored in the database. You should review the extras_scheduledjob table for any further cleanup. Note: Leveraging the Secrets and Secret Groups features in Jobs does not need to be considered a sensitive variable. Secrets are retrieved by reference at run time, which means no secret value is stored directly in the database. v1.3.9 (2022-07-25) \u00b6 Added \u00b6 #860 - Added documentation that adding device component to device type does not modify existing device instances #1595 - Add ability to specify uWSGI buffer size via environment variable. #1757 - Added nullable face, position to Device bulk edit form to provided desired behavior to bulk assigning to a new rack. Changed \u00b6 #386 - Clarified messaging in API for rack position occupied. #1356 - Virtual chassis master device interface list is less confusing. #2045 - Clarified Job authoring around proper class inheritance. Fixed \u00b6 #1035 - Fix assertion raised if SLAAC Status is missing when creating IPAddress objects #1694 - Fixed CablePath not found error when disconnects/delete action performed on a cable #1795 - Corrected relationship source/destination filter help text from queryset Filter to filterset Filter and updated documentations. #1839 - Fixed staff users with auth > group permissions unable to view groups in admin UI. #1937 - Solved _custom_field_data do not fully delete when using CustomFieldBulkDeleteView. #1947 - Fixed unbound local error by initializing template variable before conditional logic statements. #2036 - Fixed outdated UI navigation references in documentation. #2039 - Fixed IntegerVar with default set to 0 on Job evaluating to False. #2057 - Fixed RIR changelog route being in VRF name prefix. #2077 - Fixed an error when viewing object detail pages after uninstalling a plugin but still having RelationshipAssociations involving the plugin's models. #2081 - Fixed error raised if status connected not found when creating a cable v1.3.8 (2022-07-11) \u00b6 Added \u00b6 #1464 - Added \"Continue with SSO\" link on login page. Changed \u00b6 #1407 - Changed custom field export column headings to prefix with cf_ . #1603 - Changed GraphQL schema generation to call time for GraphQL API. #1977 - Updated Renovate config to batch updates (additional PRs included to further refine config). #2020 - Updated celery >= 5.2.7 , django-jinja >= 2.10.2 , and mysqlclient >= 2.1.1 versions in lock file (patch updates). Fixed \u00b6 #1838 - Fixed job result to show latest not oldest. #1874 - Fixed Git repo sync issue with Sentinel with deprecated rq_count check. Security \u00b6 Important CVE in Django versions >= 3.2, < 3.2.14 . This update upgrades Django to 3.2.14 . #2004 - Bump Django from 3.2.13 to 3.2.14 for for CVE-2022-34265 . v1.3.7 (2022-06-27) \u00b6 Added \u00b6 #1896 - Added Renovate Bot configuration, targeting next . #1900 - Added ability to filter Git repository table based on provided contents. Changed \u00b6 #1645 - Hide search bar for unauthenticated users if HIDE_RESTRICTED_UI is True #1946 - Increase character limit on FileAttachment.mimetype to 255 to allow for all mime types to be used. #1948 - Switched Renovate Bot configuration to bump lock-file only on patch releases instead of bumping in pyproject.toml . Fixed \u00b6 #1677 - Fixed default values of custom fields on device components (such as Interface) not being applied upon Device creation. #1769 - Resolve missing menu 'General / Installed Plugins' in navbar if HIDE_RESTRICTED_UI is activated #1836 - Fixed incorrect pre-population of custom field filters in table views. #1870 - Fixed cable _abs_length validation error. #1941 - Fixes uWSGI config example, development environment links in Docker section of docs. v1.3.6 (2022-06-13) \u00b6 Changed \u00b6 #207 - Update permissions documentation to add assigning permissions section. #1763 - Job testing documentation updated to include details around enabling jobs. Job logs database added to TransactionTestCase . #1829 - Change Docker build GitHub Action to cache with matrix awareness. #1856 - Updated links to Slack community. Fixed \u00b6 #1409 - Fixed page title on device status (NAPALM) page template. #1524 - Fixed valid \"None\" option removed from search field upon display. #1649 - Changed the incorrect view permission ( circuits.view_vrf to ipam.view_vrf ) #1750 - Fixed incorrect display of boolean value in Virtual Chassis display. #1759 - Fixed TypeError on webhook REST API PATCH. #1787 - Fix scheduled jobs failing when scheduled from REST API. #1841 - Fixed incorrect display of boolean values in Git Repository view. #1848 - Fix Poetry cache issue in CI causing version tests to fail in next . #1850 - Added {{block.super}} to negate the override from the js block in rack.html. This change fixed the issue of unable to navigate away from rack changelog tab. #1868 - Updated link to advanced Docker compose use in getting started guide. v1.3.5 (2022-05-30) \u00b6 Added \u00b6 #1606 - Added best practices for working with FilterSet classes to developer documentation. #1796 - Added documentation for using Git Repositories behind/via proxies. #1811 - Added developer Docker container for running mkdocs instead of locally. Changed \u00b6 #1818 - Changed README.md to link to correct build status workflows. Fixed \u00b6 #895 - Fixed validation when creating Interface and VMInterface objects via the REST API while specifying untagged_vlan without mode also set in the payload. A 400 error will now be raised as expected. #1289 - Fixed issue where job result live pagination would reset to page 1 on refresh. The currently selected page will now persist until the job run completes. #1290 - Fix NAPALM enable password argument for devices using the eos NAPALM driver. #1427 - Fix NoReverseMatch exception when related views for action_buttons don't exist. #1428 - Fix IPAM prefix utilization sometimes showing greater than 100 percent for IPv4 prefixes. #1604 - Fix missing filter restriction enforcement on relationship association. #1771 - Fix exception raised for RelationshipAssociation when updating source. #1772 - Fix RelationshipAssociationSerializer not triggering model clean method. #1784 - Fix nautobot-server dumpdata not working due to django_rq update. Updated documentation. #1805 - Fix git pre-commit hook incompatibility with dash shell and add warning on skipped tests. Security \u00b6 Attention PyJWT - Nautobot does not directly depend on PyJWT so your upgrading Nautobot via pip or other package management tools may not pick up the patched version (we are not pinning this dependency). However some tools support an \"eager\" upgrade policy as an option. For example, pip install --upgrade --upgrade-strategy eager nautobot will upgrade Nautobot and all it's dependencies to their latest compatible version. This may not work for all use cases so it may be safer to update Nautobot then perform pip install --upgrade PyJWT . Docker containers published with this build will have PyJWT upgraded. #1808 - Bump PyJWT from 2.3.0 to 2.4.0 v1.3.4 (2022-05-16) \u00b6 Added \u00b6 #1766 - Added configuration for downloaded filename branding. #1752 - Added a new SearchFilter that is now used on all core filtersets to provide the q= search parameter for basic searching in list view of objects. Changed \u00b6 #1744 - Updated REST API token provisioning docs to include added in version. #1751 - Updated secrets documentation advisory notes. Fixed \u00b6 #1263 - Rack device image toggle added back to detail UI. #1449 - Fixed a performance bug in /api/dcim/devices/ and /api/virtualization/virtual-machines/ relating to configuration contexts. #1652 - Unicode now renders correctly on uses of json.dumps and yaml.dump throughout the code base. #1712 - Fixed circuit termination detail view getting 500 response when it's a provider network. #1755 - Fixed \"Select All\" helper widget from taking full UI height. #1761 - Fixed typo in upgrading documentation. Security \u00b6 #1715 - Add SANITIZER_PATTERNS optional setting and nautobot.utilities.logging.sanitize function and use it for redaction of Job log entries. v1.3.3 (2022-05-02) \u00b6 Added \u00b6 #1481 - Pre-Generate Docs, Add Support for Plugin-Provided Docs #1617 - Added run_job_for_testing helper method for testing Jobs in plugins, internally. Changed \u00b6 #1481 - Docs link in footer now opens link to bundled documentation instead of Read the Docs. #1680 - Bump netutils dependency to 1.1.0. #1700 - Revert vendoring drf-spectacular . Fixed \u00b6 #473 - Fix get_return_url for plugin reverse URLs. #1430 - Fix not being able to print Job results, related IPs. #1503 - SSO users can no longer interact with or see the change password form. #1515 - Further fixes for slow/unresponsive jobs results display. #1538 - Fix incorrect page title alignment on the \"Device Type Import\" page. #1678 - Custom fields with 'json' type no longer raise TypeError when filtering on an object list URL #1679 - Fix a data migration error when upgrading to 1.3.x with pre-existing JobResults that reference Jobs with names exceeding 100 characters in length. #1685 - Fix Hadolint issue of docker/Dockerfile . #1692 - Fix duplicate tags in search list results. #1697 - Fix docs incorrectly stating Celerey Redis URLs defaulting from CACHES. #1701 - Fix static file serving of drf-spectacular-sidecar assets when using alternative STATICFILES_STORAGE settings. #1705 - Fix NestedVMInterfaceSerializer referencing the wrong model. v1.3.2 (2022-04-22) \u00b6 Added \u00b6 #1219 - Add ARM64 support (alpha). #1426 - Added plugin development documentation around using ObjectListView. #1674 - Added flag in Dockerfile, tasks.py to enable Poetry install parallelization. Changed \u00b6 #1667 - Updated README.md screenshots. #1670 - Configure drf-spectacular schema to more closely match drf-yasg (related to: nautobot-ansible#135 ). Fixed \u00b6 #1659 - Added some missing test/lint commands to the development getting-started documentation, and made invoke cli parameters match invoke start/stop . #1666 - Fixed errors in documentation with incomplete import statements. #1682 - Fixed Nautobot health checks failing if Redis Sentinel password is required. Security \u00b6 Important Critical CVEs in Django versions >= 3.2, < 3.2.13 . This update upgrades Django to 3.2.13 . #1686 - Implemented fixes for CVE-2022-28347 and CVE-2022-28346 to require Django >=3.2.13. v1.3.1 (2022-04-19) \u00b6 Changed \u00b6 #1647 - Changed class inheritance of JobViewSet to be simpler and more self-consistent. Fixed \u00b6 #1278 - Fixed several different errors that could be raised when working with RelationshipAssociations. #1662 - Fixed nat_outside prefetch on Device API view, and displaying multiple nat_outside entries on VM detail view. v1.3.0 (2022-04-18) \u00b6 Added \u00b6 #630 - Added support for multiple NAT outside IP addresses. #872 - Added ability to scope tags to content types. #896 - Implemented support for Dynamic Groups objects. #897 - Added JSON type for custom fields. #1374 - Added REST API Token Provisioning. (Port of NetBox #6592 and subsequent fixes) #1385 - Added MarkdownLint validation and enforcement to CI. #1465 - Implemented REST API versioning. #1525 - Implemented support for regex lookup expressions for BaseFilterSet filter fields in the API. #1638 - Implemented numerous indexes on models natural lookup fields as well as some index togethers for ObjectChange . Changed \u00b6 #595 - Migrated from drf-yasg (OpenAPI 2.0) to drf-spectacular (OpenAPI 3.0) for REST API interactive Swagger documentation. #792 - Poetry-installed dependencies are now identical between dev and final images. #814 - Extended documentation for configuring Celery for use Redis Sentinel clustering. #1225 - Relaxed uniqueness constraint on Webhook creation, allowing multiple webhooks to send to the same target address so long as their content-type(s) and action(s) do not overlap. #1417 - CI scope improvements for streamlined performance. #1478 - ScheduledJob REST API endpoints now enforce extras.approve_job permissions as appropriate. #1479 - Updated Jobs documentation regarding the concrete Job database model. #1502 Finalized Dynamic Groups implementation for 1.3 release (including documentation and integration tests). #1521 - Consolidated Job REST API endpoints, taking advantage of REST API versioning. #1556 - Cleaned up typos and formatting issues across docs, few code spots. Fixed \u00b6 #794 - Fixed health check issue when using Redis Sentinel for caching with Cacheops. The Redis health check backend is now aware of Redis Sentinel. #1311 - Fixed a where it was not possible to set the rack height to 0 when performing a bulk edit of device types. #1476 - Fixed a bug wherein a Job run via the REST API with a missing schedule would allow approval_required to be bypassed. #1504 - Fixed an error that could be encountered when migrating from Nautobot 1.1 or earlier with JobResults with very long log entries. #1515 - Fix Job Result rendering performance issue causing Bad Gateway errors. #1516 - Fixed MySQL unit tests running in Docker environment and revised recommended MySQL encoding settings #1562 - Fixed JobResult filter form UI pointing to the wrong endpoint. #1563 - Fixed UI crash when trying to execute Jobs provided by disabled plugins. A friendly error message will now be displayed. #1582 - Fixed a timing issue with editing a record while its custom field(s) are in the process of being cleaned up by a background task. #1632 - Fixed issue accessing request attributes when request may be None. #1637 - Fixed warnings logged during REST API schema generation. v1.3.0b1 (2022-03-11) \u00b6 Added \u00b6 #5 - Added the option to perform a \"dry run\" of Git repository syncing. #330 - Added pre-/post-change data to WebHooks leveraging snapshots. #498 - Added custom-validator support to the RelationshipAssociation model. #724 - Added Provider Network data model. (Partially based on NetBox #5986 .) #795 - Added ability to filter objects missing custom field values by using null . #803 - Added a render_boolean template filter, which renders computed boolean values as HTML in a consistent manner. #863 - Added the ability to hide a job in the UI by setting hidden = True in the Job's inner Meta class. #881 - Improved the UX of the main Jobs list by adding accordion style interface that can collapse/expand jobs provided by each module. #885 - Added the ability to define a soft_time_limit and time_limit in seconds as attributes of a Job's Meta . #894 - Added the ability to view computed fields in an object list. #898 - Added support for moving a CustomField, Relationship or ComputedField from the main tab of an object's detail page in the UI to the \"Advanced\" tab. #1001 - Added Job database model and associated functionality. #1109 - Added pagination support for GraphQL list queries. #1255 - Added Python 3.10 support. #1350 - Added missing methods on Circuit Termination detail view. #1411 - Added concrete Job database model; added database signals to populate Job records in the database; added detail, edit, and delete views for Job records. #1457 - Added new Jobs REST API, added control logic to use JobModel rather than JobClass where appropriate; improved permissions enforcement for Jobs. #1470 - Added plugin framework for extending FilterSets and Filter Forms. Changed \u00b6 #368 - Added nautobot.extras.forms.NautobotModelForm and nautobot.extras.filters.NautobotFilterSet base classes. All form classes which inherited from all three of ( BootstrapMixin , CustomFieldModelForm , and RelationshipModelForm ) now inherit from NautobotModelForm as their base class. All filterset classes which inherited from all three of ( BaseFilterSet , CreatedUpdatedFilterSet , and CustomFieldModelFilterSet ) now inherit from NautobotFilterSet as their base class. #443 - The provided \"Dummy Plugin\" has been renamed to \"Example Plugin\". #591 - All uses of type() are now refactored to use isinstance() where applicable. #880 - Jobs menu items now form their own top-level menu instead of a sub-section under the Extensibility menu. #909 - Device, InventoryItem, and Rack serial numbers can now be up to 255 characters in length. #916 - A Job.Meta.description can now contain markdown-formatted multi-line text. #1107 - Circuit Provider account numbers can now be up to 100 characters in length. #1252 - As Python 3.6 has reached end-of-life, the default Docker images published for this release (i.e. 1.3.0 , stable , latest ) have been updated to use Python 3.7 instead. #1277 - Updated Django dependency to 3.2.X LTS. #1307 - Updated various Python package dependencies to their latest compatible versions. #1314 - Updated various development-only Python package dependencies to their latest compatible versions. #1321 - Updates to various browser package dependencies. This includes updating from Material Design Icons 5.x to 6.x, which has a potential impact on plugins: a small number of icons have been removed or renamed as a result of this change. #1367 - Extracted Job-related models to submodule nautobot.extras.models.jobs ; refined Job testing best practices. #1391 - Updated Jinja2 dependency to 3.0.X. #1435 - Update to Selenium 4.X. Fixed \u00b6 #1440 - Handle models missing serializer methods, dependent from adding pre-/post-change data to WebHooks. Removed \u00b6 #1268 - Drop Support for Python 3.6.","title":"Version 1.3"},{"location":"release-notes/version-1.3.html#nautobot-v13","text":"This document describes all new features and changes in Nautobot 1.3. If you are a user migrating from NetBox to Nautobot, please refer to the \"Migrating from NetBox\" documentation.","title":"Nautobot v1.3"},{"location":"release-notes/version-1.3.html#release-overview","text":"","title":"Release Overview"},{"location":"release-notes/version-1.3.html#added","text":"","title":"Added"},{"location":"release-notes/version-1.3.html#dynamic-group-model-896","text":"A new data model for representing dynamic groups of objects has been implemented. Dynamic groups can be used to organize objects together by matching criteria such as their site location or region, for example, and are dynamically updated whenever new matching objects are created, or existing objects are updated. For the initial release only dynamic groups of Device and VirtualMachine objects are supported.","title":"Dynamic Group Model (#896)"},{"location":"release-notes/version-1.3.html#extend-filtersets-and-filter-forms-via-plugins-1470","text":"Plugins can now extend existing FilterSets and Filter Forms. This allows plugins to provide alternative lookup methods or custom queries in the UI or API that may not already exist today. You can refer to the plugin development guide on how to create new filters and fields.","title":"Extend FilterSets and Filter Forms via Plugins (#1470)"},{"location":"release-notes/version-1.3.html#graphql-pagination-1109","text":"GraphQL list queries can now be paginated by specifying the filter parameters limit and offset . Refer to the GraphQL user guide for examples.","title":"GraphQL Pagination (#1109)"},{"location":"release-notes/version-1.3.html#job-database-model-1001","text":"Installed Jobs are now represented by a data model in the Nautobot database. This allows for new functionality including: The Jobs listing UI view can now be filtered and searched like most other Nautobot table/list views. Job attributes (name, description, approval requirements, etc.) can now be managed via the Nautobot UI by an administrator or user with appropriate permissions to customize or override the attributes defined in the Job source code. Jobs can now be identified by a slug as well as by their class_path . A new set of REST API endpoints have been added to /api/extras/jobs/<uuid>/ . The existing /api/extras/jobs/<class_path>/ REST API endpoints continue to work but should be considered as deprecated. A new version of the REST API /api/extras/jobs/ list endpoint has been implemented as well, but by default this endpoint continues to demonstrate the pre-1.3 behavior unless the REST API client explicitly requests API version=1.3 . See the section on REST API versioning, below, for more details. As a minor security measure, newly installed Jobs default to enabled = False , preventing them from being run until an administrator or user with appropriate permissions updates them to be enabled for running. Note As a convenience measure, when initially upgrading to Nautobot 1.3.x, any existing Jobs that have been run or scheduled previously (i.e., have at least one associated JobResult and/or ScheduledJob record) will instead default to enabled = True so that they may continue to be run without requiring changes. For more details please refer to the Jobs feature documentation as well as the Job data model documentation .","title":"Job Database Model (#1001)"},{"location":"release-notes/version-1.3.html#jobs-with-sensitive-parameters-2091","text":"Jobs model now includes a has_sensitive_variables field which by default prevents the job's input parameters from being saved to the database. Review whether each job's input parameters include sensitive data such as passwords or other user credentials before setting this to False for any given job.","title":"Jobs With Sensitive Parameters (#2091)"},{"location":"release-notes/version-1.3.html#json-type-for-custom-fields-897","text":"Custom fields can now have a type of \"json\". Fields of this type can be used to store arbitrary JSON data.","title":"JSON Type for Custom Fields (#897)"},{"location":"release-notes/version-1.3.html#natural-indexing-for-common-lookups-1638","text":"Many fields have had indexing added to them as well as index togethers on ObjectChange fields. This should provide a noticeable performance improvement when filtering and doing lookups. Note This is going to perform several migrations to add all of the indexes. On MySQL databases and tables with 1M+ records this can take a few minutes. Every environment is different but it should be expected for this upgrade to take some time.","title":"Natural Indexing for Common Lookups (#1638)"},{"location":"release-notes/version-1.3.html#overlappingmultiple-nat-support-630","text":"IP addresses can now be associated with multiple outside NAT IP addresses. To do this, set more than one IP Address to have the same NAT inside IP address. A new version of the REST API /api/ipam/ip-addresses/* endpoints have been implemented as well, but by default this endpoint continues to demonstrate the pre-1.3 behavior unless the REST API client explicitly requests API version=1.3 . See the section on REST API versioning, below, for more details. Note There are some guardrails on this feature to support backwards compatibility. If you consume the REST API without specifying the version header or query argument and start associating multiple IPs to have the same NAT inside IP address, an error will be reported, because the existing REST API schema returns nat_outside as a single object, where as 1.3 and beyond will return this as a list.","title":"Overlapping/Multiple NAT Support (#630)"},{"location":"release-notes/version-1.3.html#provider-network-model-724","text":"A data model has been added to support representing the termination of a circuit to an external provider's network.","title":"Provider Network Model (#724)"},{"location":"release-notes/version-1.3.html#python-310-support-1255","text":"Python 3.10 is officially supported by Nautobot now, and we are building and publishing Docker images with Python 3.10 now.","title":"Python 3.10 Support (#1255)"},{"location":"release-notes/version-1.3.html#regular-expression-support-in-api-filtering-1525","text":"New lookup expressions for using regular expressions to filter objects by string (char) fields in the API have been added to all core filters. The expressions re (regex), nre (negated regex), ire (case-insensitive regex), and nire (negated case-insensitive regex) lookup expressions are now dynamically-generated for filter fields inherited by subclasses of nautobot.utilities.filters.BaseFilterSet .","title":"Regular Expression Support in API Filtering (#1525)"},{"location":"release-notes/version-1.3.html#remove-stale-scheduled-jobs-2091","text":"remove_stale_scheduled_jobs management command has been added to delete non-recurring scheduled jobs that were scheduled to run more than a specified days ago.","title":"Remove Stale Scheduled Jobs (#2091)"},{"location":"release-notes/version-1.3.html#rest-api-token-provisioning-1374","text":"Nautobot now has an /api/users/tokens/ REST API endpoint where a user can provision a new REST API token. This allows a user to gain REST API access without needing to first create a token via the web UI. $ curl -X POST \\ -H \"Accept: application/json; indent=4\" \\ -u \"hankhill:I<3C3H8\" \\ https://nautobot/api/users/tokens/ This endpoint specifically supports Basic Authentication in addition to the other REST API authentication methods.","title":"REST API Token Provisioning (#1374)"},{"location":"release-notes/version-1.3.html#rest-api-versioning-1465","text":"Nautobot's REST API now supports multiple versions, which may be requested by modifying the HTTP Accept header on any requests sent by a REST API client. Details are in the REST API documentation , but in brief: The REST API endpoints that are versioned in the 1.3.0 release are /api/extras/jobs/ listing endpoint /api/extras/tags/ create/put/patch endpoints all /api/ipam/ip-addresses/ endpoints All other REST API endpoints are currently non-versioned. However, over time more versioned REST APIs will be developed, so this is important to understand for all REST API consumers. If a REST API client does not request a specific REST API version (in other words, requests Accept: application/json rather than Accept: application/json; version=1.3 ) the API behavior will be compatible with Nautobot 1.2, at a minimum for the remainder of the Nautobot 1.x release cycle. The API behavior may change to a newer default version in a Nautobot major release (such as 2.0). To request an updated (non-backwards-compatible) API endpoint, an API version must be requested corresponding at a minimum to the Nautobot major.minor version where the updated API endpoint was introduced (so to interact with the updated REST API endpoints mentioned above, Accept: application/json; version=1.3 ). Tip As a best practice, when developing a Nautobot REST API integration, your client should always request the current API version it is being developed against, rather than relying on the default API behavior (which may change with a new Nautobot major release, as noted, and which also may not include the latest and greatest API endpoints already available but not yet made default in the current release).","title":"REST API Versioning (#1465)"},{"location":"release-notes/version-1.3.html#webhook-prepost-change-data-added-to-request-body-330","text":"Webhooks now provide a snapshot of data before and after a change, as well as the differences between the old and new data. See the default request body section in the webhook docs .","title":"Webhook Pre/Post-change Data Added to Request Body (#330)"},{"location":"release-notes/version-1.3.html#changed","text":"","title":"Changed"},{"location":"release-notes/version-1.3.html#docker-images-now-default-to-python-37-1252","text":"As Python 3.6 has reached end-of-life, the default Docker images published for this release (i.e. 1.3.0 , stable , latest ) have been updated to use Python 3.7 instead.","title":"Docker Images Now Default to Python 3.7 (#1252)"},{"location":"release-notes/version-1.3.html#job-approval-now-controlled-by-extrasapprove_job-permission-1490","text":"Similar to the existing extras.run_job permission, a new extras.approve_job permission is now enforced by the UI and the REST API when approving scheduled jobs. Only users with this permission can approve or deny approval requests; additionally such users also now require the extras.view_scheduledjob , extras.change_scheduledjob , and extras.delete_scheduledjob permissions as well.","title":"Job Approval Now Controlled By extras.approve_job Permission (#1490)"},{"location":"release-notes/version-1.3.html#openapi-30-rest-api-documentation-595","text":"The online REST API Swagger documentation ( /api/docs/ ) has been updated from OpenAPI 2.0 format to OpenAPI 3.0 format and now supports Nautobot's REST API versioning as described above. Try /api/docs/?api_version=1.3 as an example.","title":"OpenAPI 3.0 REST API documentation (#595)"},{"location":"release-notes/version-1.3.html#tag-restriction-by-content-type-872","text":"When created, a Tag can be associated to one or more model content-types using a many-to-many relationship. The tag will then apply only to models belonging to those associated content-types. For users migrating from an earlier Nautobot release, any existing tags will default to being enabled for all content-types for compatibility purposes. Individual tags may subsequently edited to remove any content-types that they do not need to apply to. Note that a Tag created programmatically via the ORM without assigning any content_types will not be applicable to any model until content-types are assigned to it.","title":"Tag restriction by content-type (#872)"},{"location":"release-notes/version-1.3.html#update-jinja2-to-3x-1474","text":"We've updated the Jinja2 dependency from version 2.11 to version 3.0.3. This may affect the syntax of any nautobot.extras.models.ComputedField objects in your database... Specifically, the template attribute, which is parsed as a Jinja2 template. Please refer to Jinja2 3.0.x's release notes to check if any changes might be required in your computed fields' templates.","title":"Update Jinja2 to 3.x (#1474)"},{"location":"release-notes/version-1.3.html#virtual-chassis-master-device-interfaces-list","text":"The device column will now show on a device's interfaces list if this device is the master in a virtual chassis. And will conversely not appear if the device is not a master on a virtual chassis. It is no longer possible to connect an interface to itself in the cable connect form.","title":"Virtual Chassis Master Device Interfaces List"},{"location":"release-notes/version-1.3.html#removed","text":"","title":"Removed"},{"location":"release-notes/version-1.3.html#python-36-no-longer-supported-1268","text":"As Python 3.6 has reached end-of-life, and many of Nautobot's dependencies have already dropped support for Python 3.6 as a consequence, Nautobot 1.3 and later do not support installation under Python 3.6.","title":"Python 3.6 No Longer Supported (#1268)"},{"location":"release-notes/version-1.3.html#v1310-2022-08-08","text":"","title":"v1.3.10 (2022-08-08)"},{"location":"release-notes/version-1.3.html#added_1","text":"#1226 - Added custom job intervals package management. #2073 - Added --local option to nautobot-server runjob command. #2080 - Added --data parameter to nautobot-server runjob command. #2091 - Added remove_stale_scheduled_jobs management command which removes all stale scheduled jobs and also added has_sensitive_variables field to Job model which prevents the job's input parameters from being saved to the database. #2143 - Scheduled Job detail view now includes details of any custom interval.","title":"Added"},{"location":"release-notes/version-1.3.html#changed_1","text":"#2025 - Tweak Renovate config for automated package management. #2114 - Home page now redirects to the login page when HIDE_RESTRICTED_UI is enabled and user is not authenticated. #2115 - Patch updates to mkdocs , svgwrite .","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed","text":"#1739 - Fixed paginator not enforcing max_page_size setting in web ui views. #2060 - Fixed relationship peer_id filter non-existent error on relationship association page. #2095 - Fixed health check failing with Redis Sentinel, TLS configuration. #2119 - Fixed flaky integration test for cable connection UI.","title":"Fixed"},{"location":"release-notes/version-1.3.html#security","text":"Important With introducing the has_sensitive_variables flag on Job classes and model (see: #2091 ), jobs can be prevented from storing their inputs in the database. Due to the nature of queuing or scheduling jobs, the desired inputs must be stored for future use. New safe-default behavior will only permit jobs to be executed immediately, as has_sensitive_variables defaults to True . This value can be overridden by the Job class itself or the Job model edit page. Values entered for jobs executing immediately go straight to the Celery message bus and are cleaned up on completion of execution. Scheduling jobs or requiring approval necessitates those values to be stored in the database until they have been sent to the Celery message bus for execution. During installation of v1.3.10 , a migration is applied to set the has_sensitive_variables value to True to all existing Jobs. However to maintain backwards-compatibility, past scheduled jobs are permitted to keep their schedule. New schedules cannot be made until an administrator has overridden the has_sensitive_variables for the desired Job. A new management command exists ( remove_stale_scheduled_jobs ) which will aid in cleaning up schedules to past jobs which may still have sensitive data stored in the database. This command is not exhaustive nor intended to clean up sensitive values stored in the database. You should review the extras_scheduledjob table for any further cleanup. Note: Leveraging the Secrets and Secret Groups features in Jobs does not need to be considered a sensitive variable. Secrets are retrieved by reference at run time, which means no secret value is stored directly in the database.","title":"Security"},{"location":"release-notes/version-1.3.html#v139-2022-07-25","text":"","title":"v1.3.9 (2022-07-25)"},{"location":"release-notes/version-1.3.html#added_2","text":"#860 - Added documentation that adding device component to device type does not modify existing device instances #1595 - Add ability to specify uWSGI buffer size via environment variable. #1757 - Added nullable face, position to Device bulk edit form to provided desired behavior to bulk assigning to a new rack.","title":"Added"},{"location":"release-notes/version-1.3.html#changed_2","text":"#386 - Clarified messaging in API for rack position occupied. #1356 - Virtual chassis master device interface list is less confusing. #2045 - Clarified Job authoring around proper class inheritance.","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed_1","text":"#1035 - Fix assertion raised if SLAAC Status is missing when creating IPAddress objects #1694 - Fixed CablePath not found error when disconnects/delete action performed on a cable #1795 - Corrected relationship source/destination filter help text from queryset Filter to filterset Filter and updated documentations. #1839 - Fixed staff users with auth > group permissions unable to view groups in admin UI. #1937 - Solved _custom_field_data do not fully delete when using CustomFieldBulkDeleteView. #1947 - Fixed unbound local error by initializing template variable before conditional logic statements. #2036 - Fixed outdated UI navigation references in documentation. #2039 - Fixed IntegerVar with default set to 0 on Job evaluating to False. #2057 - Fixed RIR changelog route being in VRF name prefix. #2077 - Fixed an error when viewing object detail pages after uninstalling a plugin but still having RelationshipAssociations involving the plugin's models. #2081 - Fixed error raised if status connected not found when creating a cable","title":"Fixed"},{"location":"release-notes/version-1.3.html#v138-2022-07-11","text":"","title":"v1.3.8 (2022-07-11)"},{"location":"release-notes/version-1.3.html#added_3","text":"#1464 - Added \"Continue with SSO\" link on login page.","title":"Added"},{"location":"release-notes/version-1.3.html#changed_3","text":"#1407 - Changed custom field export column headings to prefix with cf_ . #1603 - Changed GraphQL schema generation to call time for GraphQL API. #1977 - Updated Renovate config to batch updates (additional PRs included to further refine config). #2020 - Updated celery >= 5.2.7 , django-jinja >= 2.10.2 , and mysqlclient >= 2.1.1 versions in lock file (patch updates).","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed_2","text":"#1838 - Fixed job result to show latest not oldest. #1874 - Fixed Git repo sync issue with Sentinel with deprecated rq_count check.","title":"Fixed"},{"location":"release-notes/version-1.3.html#security_1","text":"Important CVE in Django versions >= 3.2, < 3.2.14 . This update upgrades Django to 3.2.14 . #2004 - Bump Django from 3.2.13 to 3.2.14 for for CVE-2022-34265 .","title":"Security"},{"location":"release-notes/version-1.3.html#v137-2022-06-27","text":"","title":"v1.3.7 (2022-06-27)"},{"location":"release-notes/version-1.3.html#added_4","text":"#1896 - Added Renovate Bot configuration, targeting next . #1900 - Added ability to filter Git repository table based on provided contents.","title":"Added"},{"location":"release-notes/version-1.3.html#changed_4","text":"#1645 - Hide search bar for unauthenticated users if HIDE_RESTRICTED_UI is True #1946 - Increase character limit on FileAttachment.mimetype to 255 to allow for all mime types to be used. #1948 - Switched Renovate Bot configuration to bump lock-file only on patch releases instead of bumping in pyproject.toml .","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed_3","text":"#1677 - Fixed default values of custom fields on device components (such as Interface) not being applied upon Device creation. #1769 - Resolve missing menu 'General / Installed Plugins' in navbar if HIDE_RESTRICTED_UI is activated #1836 - Fixed incorrect pre-population of custom field filters in table views. #1870 - Fixed cable _abs_length validation error. #1941 - Fixes uWSGI config example, development environment links in Docker section of docs.","title":"Fixed"},{"location":"release-notes/version-1.3.html#v136-2022-06-13","text":"","title":"v1.3.6 (2022-06-13)"},{"location":"release-notes/version-1.3.html#changed_5","text":"#207 - Update permissions documentation to add assigning permissions section. #1763 - Job testing documentation updated to include details around enabling jobs. Job logs database added to TransactionTestCase . #1829 - Change Docker build GitHub Action to cache with matrix awareness. #1856 - Updated links to Slack community.","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed_4","text":"#1409 - Fixed page title on device status (NAPALM) page template. #1524 - Fixed valid \"None\" option removed from search field upon display. #1649 - Changed the incorrect view permission ( circuits.view_vrf to ipam.view_vrf ) #1750 - Fixed incorrect display of boolean value in Virtual Chassis display. #1759 - Fixed TypeError on webhook REST API PATCH. #1787 - Fix scheduled jobs failing when scheduled from REST API. #1841 - Fixed incorrect display of boolean values in Git Repository view. #1848 - Fix Poetry cache issue in CI causing version tests to fail in next . #1850 - Added {{block.super}} to negate the override from the js block in rack.html. This change fixed the issue of unable to navigate away from rack changelog tab. #1868 - Updated link to advanced Docker compose use in getting started guide.","title":"Fixed"},{"location":"release-notes/version-1.3.html#v135-2022-05-30","text":"","title":"v1.3.5 (2022-05-30)"},{"location":"release-notes/version-1.3.html#added_5","text":"#1606 - Added best practices for working with FilterSet classes to developer documentation. #1796 - Added documentation for using Git Repositories behind/via proxies. #1811 - Added developer Docker container for running mkdocs instead of locally.","title":"Added"},{"location":"release-notes/version-1.3.html#changed_6","text":"#1818 - Changed README.md to link to correct build status workflows.","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed_5","text":"#895 - Fixed validation when creating Interface and VMInterface objects via the REST API while specifying untagged_vlan without mode also set in the payload. A 400 error will now be raised as expected. #1289 - Fixed issue where job result live pagination would reset to page 1 on refresh. The currently selected page will now persist until the job run completes. #1290 - Fix NAPALM enable password argument for devices using the eos NAPALM driver. #1427 - Fix NoReverseMatch exception when related views for action_buttons don't exist. #1428 - Fix IPAM prefix utilization sometimes showing greater than 100 percent for IPv4 prefixes. #1604 - Fix missing filter restriction enforcement on relationship association. #1771 - Fix exception raised for RelationshipAssociation when updating source. #1772 - Fix RelationshipAssociationSerializer not triggering model clean method. #1784 - Fix nautobot-server dumpdata not working due to django_rq update. Updated documentation. #1805 - Fix git pre-commit hook incompatibility with dash shell and add warning on skipped tests.","title":"Fixed"},{"location":"release-notes/version-1.3.html#security_2","text":"Attention PyJWT - Nautobot does not directly depend on PyJWT so your upgrading Nautobot via pip or other package management tools may not pick up the patched version (we are not pinning this dependency). However some tools support an \"eager\" upgrade policy as an option. For example, pip install --upgrade --upgrade-strategy eager nautobot will upgrade Nautobot and all it's dependencies to their latest compatible version. This may not work for all use cases so it may be safer to update Nautobot then perform pip install --upgrade PyJWT . Docker containers published with this build will have PyJWT upgraded. #1808 - Bump PyJWT from 2.3.0 to 2.4.0","title":"Security"},{"location":"release-notes/version-1.3.html#v134-2022-05-16","text":"","title":"v1.3.4 (2022-05-16)"},{"location":"release-notes/version-1.3.html#added_6","text":"#1766 - Added configuration for downloaded filename branding. #1752 - Added a new SearchFilter that is now used on all core filtersets to provide the q= search parameter for basic searching in list view of objects.","title":"Added"},{"location":"release-notes/version-1.3.html#changed_7","text":"#1744 - Updated REST API token provisioning docs to include added in version. #1751 - Updated secrets documentation advisory notes.","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed_6","text":"#1263 - Rack device image toggle added back to detail UI. #1449 - Fixed a performance bug in /api/dcim/devices/ and /api/virtualization/virtual-machines/ relating to configuration contexts. #1652 - Unicode now renders correctly on uses of json.dumps and yaml.dump throughout the code base. #1712 - Fixed circuit termination detail view getting 500 response when it's a provider network. #1755 - Fixed \"Select All\" helper widget from taking full UI height. #1761 - Fixed typo in upgrading documentation.","title":"Fixed"},{"location":"release-notes/version-1.3.html#security_3","text":"#1715 - Add SANITIZER_PATTERNS optional setting and nautobot.utilities.logging.sanitize function and use it for redaction of Job log entries.","title":"Security"},{"location":"release-notes/version-1.3.html#v133-2022-05-02","text":"","title":"v1.3.3 (2022-05-02)"},{"location":"release-notes/version-1.3.html#added_7","text":"#1481 - Pre-Generate Docs, Add Support for Plugin-Provided Docs #1617 - Added run_job_for_testing helper method for testing Jobs in plugins, internally.","title":"Added"},{"location":"release-notes/version-1.3.html#changed_8","text":"#1481 - Docs link in footer now opens link to bundled documentation instead of Read the Docs. #1680 - Bump netutils dependency to 1.1.0. #1700 - Revert vendoring drf-spectacular .","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed_7","text":"#473 - Fix get_return_url for plugin reverse URLs. #1430 - Fix not being able to print Job results, related IPs. #1503 - SSO users can no longer interact with or see the change password form. #1515 - Further fixes for slow/unresponsive jobs results display. #1538 - Fix incorrect page title alignment on the \"Device Type Import\" page. #1678 - Custom fields with 'json' type no longer raise TypeError when filtering on an object list URL #1679 - Fix a data migration error when upgrading to 1.3.x with pre-existing JobResults that reference Jobs with names exceeding 100 characters in length. #1685 - Fix Hadolint issue of docker/Dockerfile . #1692 - Fix duplicate tags in search list results. #1697 - Fix docs incorrectly stating Celerey Redis URLs defaulting from CACHES. #1701 - Fix static file serving of drf-spectacular-sidecar assets when using alternative STATICFILES_STORAGE settings. #1705 - Fix NestedVMInterfaceSerializer referencing the wrong model.","title":"Fixed"},{"location":"release-notes/version-1.3.html#v132-2022-04-22","text":"","title":"v1.3.2 (2022-04-22)"},{"location":"release-notes/version-1.3.html#added_8","text":"#1219 - Add ARM64 support (alpha). #1426 - Added plugin development documentation around using ObjectListView. #1674 - Added flag in Dockerfile, tasks.py to enable Poetry install parallelization.","title":"Added"},{"location":"release-notes/version-1.3.html#changed_9","text":"#1667 - Updated README.md screenshots. #1670 - Configure drf-spectacular schema to more closely match drf-yasg (related to: nautobot-ansible#135 ).","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed_8","text":"#1659 - Added some missing test/lint commands to the development getting-started documentation, and made invoke cli parameters match invoke start/stop . #1666 - Fixed errors in documentation with incomplete import statements. #1682 - Fixed Nautobot health checks failing if Redis Sentinel password is required.","title":"Fixed"},{"location":"release-notes/version-1.3.html#security_4","text":"Important Critical CVEs in Django versions >= 3.2, < 3.2.13 . This update upgrades Django to 3.2.13 . #1686 - Implemented fixes for CVE-2022-28347 and CVE-2022-28346 to require Django >=3.2.13.","title":"Security"},{"location":"release-notes/version-1.3.html#v131-2022-04-19","text":"","title":"v1.3.1 (2022-04-19)"},{"location":"release-notes/version-1.3.html#changed_10","text":"#1647 - Changed class inheritance of JobViewSet to be simpler and more self-consistent.","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed_9","text":"#1278 - Fixed several different errors that could be raised when working with RelationshipAssociations. #1662 - Fixed nat_outside prefetch on Device API view, and displaying multiple nat_outside entries on VM detail view.","title":"Fixed"},{"location":"release-notes/version-1.3.html#v130-2022-04-18","text":"","title":"v1.3.0 (2022-04-18)"},{"location":"release-notes/version-1.3.html#added_9","text":"#630 - Added support for multiple NAT outside IP addresses. #872 - Added ability to scope tags to content types. #896 - Implemented support for Dynamic Groups objects. #897 - Added JSON type for custom fields. #1374 - Added REST API Token Provisioning. (Port of NetBox #6592 and subsequent fixes) #1385 - Added MarkdownLint validation and enforcement to CI. #1465 - Implemented REST API versioning. #1525 - Implemented support for regex lookup expressions for BaseFilterSet filter fields in the API. #1638 - Implemented numerous indexes on models natural lookup fields as well as some index togethers for ObjectChange .","title":"Added"},{"location":"release-notes/version-1.3.html#changed_11","text":"#595 - Migrated from drf-yasg (OpenAPI 2.0) to drf-spectacular (OpenAPI 3.0) for REST API interactive Swagger documentation. #792 - Poetry-installed dependencies are now identical between dev and final images. #814 - Extended documentation for configuring Celery for use Redis Sentinel clustering. #1225 - Relaxed uniqueness constraint on Webhook creation, allowing multiple webhooks to send to the same target address so long as their content-type(s) and action(s) do not overlap. #1417 - CI scope improvements for streamlined performance. #1478 - ScheduledJob REST API endpoints now enforce extras.approve_job permissions as appropriate. #1479 - Updated Jobs documentation regarding the concrete Job database model. #1502 Finalized Dynamic Groups implementation for 1.3 release (including documentation and integration tests). #1521 - Consolidated Job REST API endpoints, taking advantage of REST API versioning. #1556 - Cleaned up typos and formatting issues across docs, few code spots.","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed_10","text":"#794 - Fixed health check issue when using Redis Sentinel for caching with Cacheops. The Redis health check backend is now aware of Redis Sentinel. #1311 - Fixed a where it was not possible to set the rack height to 0 when performing a bulk edit of device types. #1476 - Fixed a bug wherein a Job run via the REST API with a missing schedule would allow approval_required to be bypassed. #1504 - Fixed an error that could be encountered when migrating from Nautobot 1.1 or earlier with JobResults with very long log entries. #1515 - Fix Job Result rendering performance issue causing Bad Gateway errors. #1516 - Fixed MySQL unit tests running in Docker environment and revised recommended MySQL encoding settings #1562 - Fixed JobResult filter form UI pointing to the wrong endpoint. #1563 - Fixed UI crash when trying to execute Jobs provided by disabled plugins. A friendly error message will now be displayed. #1582 - Fixed a timing issue with editing a record while its custom field(s) are in the process of being cleaned up by a background task. #1632 - Fixed issue accessing request attributes when request may be None. #1637 - Fixed warnings logged during REST API schema generation.","title":"Fixed"},{"location":"release-notes/version-1.3.html#v130b1-2022-03-11","text":"","title":"v1.3.0b1 (2022-03-11)"},{"location":"release-notes/version-1.3.html#added_10","text":"#5 - Added the option to perform a \"dry run\" of Git repository syncing. #330 - Added pre-/post-change data to WebHooks leveraging snapshots. #498 - Added custom-validator support to the RelationshipAssociation model. #724 - Added Provider Network data model. (Partially based on NetBox #5986 .) #795 - Added ability to filter objects missing custom field values by using null . #803 - Added a render_boolean template filter, which renders computed boolean values as HTML in a consistent manner. #863 - Added the ability to hide a job in the UI by setting hidden = True in the Job's inner Meta class. #881 - Improved the UX of the main Jobs list by adding accordion style interface that can collapse/expand jobs provided by each module. #885 - Added the ability to define a soft_time_limit and time_limit in seconds as attributes of a Job's Meta . #894 - Added the ability to view computed fields in an object list. #898 - Added support for moving a CustomField, Relationship or ComputedField from the main tab of an object's detail page in the UI to the \"Advanced\" tab. #1001 - Added Job database model and associated functionality. #1109 - Added pagination support for GraphQL list queries. #1255 - Added Python 3.10 support. #1350 - Added missing methods on Circuit Termination detail view. #1411 - Added concrete Job database model; added database signals to populate Job records in the database; added detail, edit, and delete views for Job records. #1457 - Added new Jobs REST API, added control logic to use JobModel rather than JobClass where appropriate; improved permissions enforcement for Jobs. #1470 - Added plugin framework for extending FilterSets and Filter Forms.","title":"Added"},{"location":"release-notes/version-1.3.html#changed_12","text":"#368 - Added nautobot.extras.forms.NautobotModelForm and nautobot.extras.filters.NautobotFilterSet base classes. All form classes which inherited from all three of ( BootstrapMixin , CustomFieldModelForm , and RelationshipModelForm ) now inherit from NautobotModelForm as their base class. All filterset classes which inherited from all three of ( BaseFilterSet , CreatedUpdatedFilterSet , and CustomFieldModelFilterSet ) now inherit from NautobotFilterSet as their base class. #443 - The provided \"Dummy Plugin\" has been renamed to \"Example Plugin\". #591 - All uses of type() are now refactored to use isinstance() where applicable. #880 - Jobs menu items now form their own top-level menu instead of a sub-section under the Extensibility menu. #909 - Device, InventoryItem, and Rack serial numbers can now be up to 255 characters in length. #916 - A Job.Meta.description can now contain markdown-formatted multi-line text. #1107 - Circuit Provider account numbers can now be up to 100 characters in length. #1252 - As Python 3.6 has reached end-of-life, the default Docker images published for this release (i.e. 1.3.0 , stable , latest ) have been updated to use Python 3.7 instead. #1277 - Updated Django dependency to 3.2.X LTS. #1307 - Updated various Python package dependencies to their latest compatible versions. #1314 - Updated various development-only Python package dependencies to their latest compatible versions. #1321 - Updates to various browser package dependencies. This includes updating from Material Design Icons 5.x to 6.x, which has a potential impact on plugins: a small number of icons have been removed or renamed as a result of this change. #1367 - Extracted Job-related models to submodule nautobot.extras.models.jobs ; refined Job testing best practices. #1391 - Updated Jinja2 dependency to 3.0.X. #1435 - Update to Selenium 4.X.","title":"Changed"},{"location":"release-notes/version-1.3.html#fixed_11","text":"#1440 - Handle models missing serializer methods, dependent from adding pre-/post-change data to WebHooks.","title":"Fixed"},{"location":"release-notes/version-1.3.html#removed_1","text":"#1268 - Drop Support for Python 3.6.","title":"Removed"},{"location":"release-notes/version-1.4.html","text":"Nautobot v1.4 \u00b6 This document describes all new features and changes in Nautobot 1.4. If you are a user migrating from NetBox to Nautobot, please refer to the \"Migrating from NetBox\" documentation. Release Overview \u00b6 Added \u00b6 Custom Field Extended Filtering ( #1498 ) \u00b6 Objects with custom fields now support filter lookup expressions for filtering by custom field values, such as cf_date_field__gte=2022-08-11 to select objects whose date_field custom field has a date of 2022-08-11 or later. Custom Field Slugs ( #1962 ) \u00b6 Custom fields now have a distinct slug field. The custom field name attribute should be considered deprecated, and will be removed in a future major release (see also #824 .) Additionally, the label attribute, while currently optional in the database, will become mandatory in that same future release as a consequence. When migrating from an earlier Nautobot release to version 1.4 or later, the slug and label for all existing custom fields will be automatically populated if not previously defined. A new version of the /api/extras/custom-fields/ REST API endpoints has been implemented. By default this endpoint continues to demonstrate the pre-1.4 behavior ( name required, slug and label optional; if unspecified, the slug and label will receive default values based on the provided name ). A REST API client can request API version 1.4, in which case the updated API will require slug and label parameters in place of name . Additionally, REST API serialization of custom field data is itself now versioned. For all object endpoints that include custom field data under the custom_fields key, REST API versions 1.3 and earlier will continue the previous behavior of indexing the custom_fields dictionary by fields' name values, but when REST API version 1.4 or later is requested, the custom_fields data will be indexed by slug instead. For technical reasons of backwards-compatibility, the database (ORM) and GraphQL interfaces continue to access and store object custom field data exclusively under the name key; this will change to use the slug in a future major release. Again, watch #824 for plans in that regard. Custom Tabs in Object Detail Views ( #1000 ) \u00b6 A plugin may now define extra tabs which will be appended to the object view's list of tabs. You can refer to the plugin development guide on how to add tabs to existing object detail views. Custom Template (CSS, HTML, JavaScript) on Job Forms ( #1865 ) \u00b6 Jobs can now specify a template_name property and provide a custom template with additional JavaScript and CSS to help with user input on the Job submission form. You can refer to the Job class metadata attribute documentation on how to build and define this template. Dynamic Groups Support Additional Models ( #2200 ) \u00b6 Cluster, IP Address, Prefix, and Rack models can now be filtered on in Dynamic Groups and can also support nested or groups of Dynamic Groups. Some fields have been excluded from filtering until a sensible widget can be provided. Dark Mode UI ( #729 ) \u00b6 Nautobot's UI now supports dark mode, both explicitly and via browser preference selection. The \"Theme\" link in the footer provides a modal popup to select the preferred theme. This preference is saved per browser via localStorage . Improved Filter Coverage for DCIM and Virtualization Models \u00b6 DCIM: #1729 Virtualization: #1735 The DCIM, Virtualization FilterSets have been updated with over 150 new filters, including hybrid filters that support filtering on both pk and slug (or pk and name where slug is not available). A new filter class NaturalKeyOrPKMultipleChoiceFilter was added to nautobot.utilities.filters to support filtering on multiple fields of a related object. Please see the documentation on best practices for mapping model fields to filters for more information. Job Hooks ( #1878 ) \u00b6 Jobs can now be configured to run automatically when a change event occurs on a Nautobot object. Job hooks associate jobs to content types and actions to run jobs when a create, update or delete action occurs on the selected content type. A new job base class JobHookReceiver was introduced that jobs must subclass to be associated with a job hook. Please see the documentation on Job Hooks for more information. Job Re-Runs ( #1875 ) \u00b6 JobResult records now save the arguments with which the Job was called, allowing for easy re-execution of the Job with the same arguments as before. A \"re-run\" button has been added to the JobResult list view and detail view. Location Data Model ( #1052 ) \u00b6 To locate network information more precisely than a Site defines, you can now define a hierarchy of Location Types (for example, Building \u2190 Floor \u2190 Room ) and then create Locations corresponding to these types within each Site. Data objects such as devices, prefixes, VLAN groups, etc. can thus be mapped or assigned to Location representing a specific building, wing, floor, room, etc. as appropriate to your needs. Info At present, Locations fill the conceptual space between the more abstract Region and Site models and the more concrete Rack Group model. In a future Nautobot release, some or all of these other models may be collapsed into Locations. That is to say, in the future you might not deal with Regions and Sites as distinct models, but instead your Location Type hierarchy might include these higher-level categories, becoming something like Country \u2190 City \u2190 Site \u2190 Building \u2190 Floor \u2190 Room. Parent Interfaces and Bridge Interfaces ( #1455 ) \u00b6 Interface and VMInterface models now have parent_interface and bridge keys. An interface of type Virtual can now associate to a parent physical interface on the same device, virtual chassis, or virtual machine, and an interface of any type can specify another interface as its associated bridge interface. (A new Bridge interface type has also been added, but the bridge interface property is not restricted to interfaces of this type.) Rackview UI - Add Option to Truncate Device Name ( #1119 ) \u00b6 Users can now toggle device full name and truncated name in the rack elevation view. The truncating function is customizable in nautobot_config.py via defining UI_RACK_VIEW_TRUNCATE_FUNCTION . Default behavior is to split on . and return the first item in the list. \"Save SVG\" link presents the same view as what is currently displayed on screen Current preferred toggle state is preserved across tabs (requires refresh) and persists in-browser until local storage is cleared. This presents a consistent behavior when browsing between multiple racks. REST API Enhancements ( #1463 ) \u00b6 For all models that support Relationships, their corresponding list and detail REST API endpoints now include the option to include data on their associated Relationships and related objects by specifying include=relationships as a query parameter. Relationship associations on a model can be edited by a PATCH to the appropriate nested value, such as \"relationships\" -> <relationship-slug> -> \"source\" or \"relationships\" -> <relationship-slug> -> \"destination\" . For implementers of REST API serializers (core and/or plugins), a new nautobot.extras.api.serializers.NautobotModelSerializer base class has been added. Using this class guarantees support for relationships, custom fields, and computed fields on the serializer, and provides for a streamlined developer experience. Status Field on Interface, VMInterface Models ( #984 ) \u00b6 Interface and VMInterface models now support a status. Default statuses that are available to be set are: Active, Planned, Maintenance, Failed, and Decommissioned. During migration all existing interfaces will be set to the status of \"Active\". A new version of the /dcim/interfaces/* REST API endpoints have been implemented. By default this endpoint continues to demonstrate the pre-1.4 behavior unless the REST API client explicitly requests API version=1.4. If you continue to use the pre-1.4 API endpoints, status is defaulted to \"Active\". Visit the documentation on REST API versioning for more information on using the versioned APIs. NautobotUIViewSet ( #1812 ) \u00b6 New in Nautobot 1.4 is the debut of NautobotUIViewSet : A powerful plugin development tool that can save plugin developer hundreds of lines of code compared to using legacy generic.views . Using it to gain access to default functionalities previous provided by generic.views such as create() , bulk_create() , update() , partial_update() , bulk_update() , destroy() , bulk_destroy() , retrieve() and list() actions. Note that this ViewSet is catered specifically to the UI, not the API. Concrete examples on how to use NautobotUIViewSet resides in nautobot.circuits.views . Please visit the plugin development guide on NautobotViewSet for more information. Notes ( #767 ) \u00b6 Primary and Organizational models now support notes. A notes tab has been added to the Object Detail view for all models that inherit the Primary or Organizational base abstract models. Warning Any plugin that inherits from one of these two models and uses the ViewTestCases.PrimaryObjectViewTestCase or ViewTestCases.OrganizationalObjectViewTestCase for their test will need to add the NotesObjectView to the objects URLs. Notes can also be used via the REST API at endpoint /api/extras/notes/ or per object detail endpoint at the object's nested /notes/ endpoint. Info For implementers of REST API views (core and/or plugins), a new nautobot.extras.api.views.NautobotModelViewSet base class has been added. Use of this class ensures that all features from PrimaryModel or OrganizationalModel are accessible through the API. This includes custom fields and notes. Please see the on plugin development guide on Notes for more details. Changed \u00b6 Dynamic Groups of Dynamic Groups ( #1614 ) \u00b6 Dynamic Groups may now be nested in parent/child relationships. The Dynamic Group edit view now has a \"Child Groups\" tab that allows one to make other Dynamic Groups of the same content type children of the parent group. Any filters provided by the child groups are used to filter the members from the parent group using one of three operators: \"Restrict (AND)\", \"Include (OR)\", or \"Exclude (NOT)\". This allows for logical parenthetical grouping of nested groups by the operator you choose for that child group association to the parent. Warning The default behavior of Dynamic Groups with an empty filter ( {} ) has been inverted to include all objects matching the content type by default instead of matching no objects. This was necessary to implement the progressive layering of child filters similarly to how we use filters to reduce desired objects from basic list view filters. Please see the greatly-expanded documentation on Dynamic Groups for more information. Renamed Mixin Classes ( #2135 ) \u00b6 A number of mixin classes have been renamed for improved self-consistency and clarity of usage. The former names of these mixins are still available for now as aliases, but inheriting from these mixins will raise a DeprecationWarning , and these aliases will be removed in a future major release. Former Name New Name AddRemoveTagsForm TagsBulkEditFormMixin CustomFieldBulkCreateForm CustomFieldModelBulkEditFormMixin CustomFieldBulkEditForm CustomFieldModelBulkEditFormMixin CustomFieldFilterForm CustomFieldModelFilterFormMixin CustomFieldModelForm CustomFieldModelFormMixin RelationshipModelForm RelationshipModelFormMixin StatusBulkEditFormMixin StatusModelBulkEditFormMixin StatusFilterFormMixin StatusModelFilterFormMixin Strict Filter Validation by Default ( #1736 ) \u00b6 Filtering of object lists in the UI and in the REST API will now report an error if an unknown or unrecognized filter parameter is specified. This is a behavior change from previous Nautobot releases, in which unknown filter parameters would be silently discarded and ignored. A new configuration setting, STRICT_FILTERING has been added. It defaults to True , enabling strict validation of filter parameters, but can be set to False to disable this validation. Warning Setting STRICT_FILTERING to False can result in unexpected filtering results in the case of user error, for example a request to /api/dcim/devices/?has_primry_ip=false (note the typo primry ) will result in a list of all devices, rather than the intended list of only devices that lack a primary IP address. In the case of Jobs or external automation making use of such a filter, this could have wide-ranging consequences. Moved Registry Template Context ( #1945 ) \u00b6 The settings_and_registry default context processor was changed to purely settings - the (large) Nautobot application registry dictionary is no longer provided as part of the render context for all templates by default. Added a new registry template tag that can be invoked by specific templates to provide this variable where needed. v1.4.5 (2022-10-03) \u00b6 Added \u00b6 #2330 - Added created and last_updated fields to Device Component and ComponentTemplate models. Changed \u00b6 #2490 - Added change log fragment checkbox to Github pull request template. #2508 - Updated dark mode theme to be less...dark. #2537 - Updated django-extensions to 3.2.1 , drf-spectacular to 0.24.2 , drf-yasg to 1.21.4 . #2544 - Updated towncrier template to convert multi-line change fragments into multiple release notes entries. Fixed \u00b6 #2326 - Fixed 500 error in Circuit Termination swap. #2330 - Fixed missing Change Log tab on device component detail views. #2445 - Fixed invalid Renovate config. #2466 - Fixed Jobs misleading sensitive variables or requiring approval error message. #2509 - Fixed template lookup logic in ObjectNotesView, ObjectDynamicGroupsView and ObjectChangeLogView. #2526 - Fixed error in rendering the example plugin's AnotherExampleModel list view by adding a get_absolute_url() method to the AnotherExampleModel class and adding an AnotherExampleModel detail view template. #2533 - Fixed test_list_objects_unknown_filter_no_strict_filtering failure if a filterset couldn't be found for a given model. #2539 - Updated links from nautobot.readthedocs.io to docs.nautobot.com. v1.4.4 (2022-09-26) \u00b6 Added \u00b6 #2023 - Added reusable GitHub Action workflow for use by plugins to test against next , develop routinely. Changed \u00b6 #2153 - Updated optional settings documentation to provide clarity on Nautobot vs Django settings. #2235 - Added ancestor information to Location and LocationType display names to provide additional context in the UI. #2457 - Moved towncrier_template.j2 from root to develop directory. #2468 - Upgraded CI gh-action-setup-poetry-environment action to v3 Fixed \u00b6 #2353 - Fixed MultiSelect CustomField being emptied unintentionally after bulk update. #2375 - Fixed error in Cable list view filtering when color and type filters are not set. #2461 - Fixed an exception during OpenAPI schema generation when certain Nautobot apps (including nautobot-firewall-models ) were installed. #2496 - Fixed failing CI due to #2468 v1.4.3 (2022-09-19) \u00b6 Added \u00b6 #2327 - Added help text to the Job scheduling datetime picker to indicate the applicable time zone. #2362 - Added documentation and automation for Nautobot Github project to use towncrier for changelog fragments. #2431 - Add section to the custom field documentation on ORM filtering. Changed \u00b6 #1619 - Updated drf-spectacular dependency to version 0.24. #2223 - Augment get_route_for_model() to support REST API routes. #2340 - Improved rendering of badges, labels, buttons, and color selection menus in dark mode. #2383 - Updated documentation link for Nautobot ChatOps plugin. #2392 - Un-group Renovate next updates to address code changes per package. #2400 - Improved formatting of version changes in the documentation. #2417 - Reworked Docker builds in CI to publish an intermediate \"dependencies-only\" image to speed up rebuild times. #2447 - Moved Dynamic Groups tab on object detail view to it's own view as a generic ObjectDynamicGroupsView . Fixed \u00b6 #138 - Fixed lack of user-facing message when an exception occurs while discovering Jobs from a Git repository. #950 - Fixed database concurrency issues with uWSGI pre-forking. #1619 - Improved accuracy of OpenAPI schema for bulk REST API operations. #2299 - Remove render_filter() method and filter field from table columns #2309 - Fixed 404 on ScheduledJobView, job_class no longer found behavior. #2324 - Fixed errors encountered when a job model is deleted while a job is running. #2338 - Fixed whitespace issue with Text File secrets and they are now stripped of leading/trailing whitespace and newlines. #2364 - Allow invoke tasks to be run even if rich is not installed. #2378 - Fix Job Result redirection on submit. #2382 - Removed extraneous cache and temporary files from the dev and final-dev Docker images to reduce image size. #2389 - Removed extraneous inspect.getsource() call from Job class. #2407 - Corrected SSO Backend reference for Azure AD Tenant. #2449 - CI: Moved dependency build to be a job, not a step. v1.4.2 (2022-09-05) \u00b6 Added \u00b6 #983 - Added functionalities to specify args and kwargs to NavMenuItem . #2250 - Added \"Stats\" and \"Rack Groups\" to Location detail view, added \"Locations\" to Site detail view. #2273 - Added custom markdownlint rule to check for invalid relative links in the documentation. #2307 - Added dynamic_groups field in GraphQL on objects that can belong to dynamic groups. #2314 - Added pylint to linting suite and CI. #2339 - Enabled and addressed additional pylint checkers. #2360 - Added Django natural key to extras.Tag . Changed \u00b6 #2011 - replaced all .format() strings and C format strings with fstrings. #2293 - Updated GitHub bug report template. #2296 - Updated netutils dependency from 1.1.x to 1.2.x. #2347 - Revamped documentation look and feel. #2349 - Docker images are now built with Poetry 1.2.0. #2360 - Django natural key for Status is now name rather than slug . #2363 - Update app icons for consistency #2365 - Update Network to Code branding name #2367 - Remove coming soon from projects that exists Fixed \u00b6 #449 - Improved error checking and reporting when syncing Git repositories. #1227 - The NAUTOBOT_DOCKER_SKIP_INIT environment variable can now be set to \"false\" (case-insensitive), #1807 - Fixed post_run method fails to add exceptions to job log. #2085 - The log entries table on a job result page can now be filtered by log level or message and hitting the return key has no effect. #2107 - Fixed a TypeError when a view defines action_buttons = None . #2237 - Fixed several issues with permissions enforcement for Note creation and viewing. #2268 - Fixed broken links in documentation. #2269 - Fixed missing JS code causing rendering errors on GraphQL Query and Rack Reservation detail views. #2278 - Fixed incorrect permissions check on \"Installed Plugins\" menu item. #2290 - Fixed inheritance of ObjectListViewMixin for CircuitTypeUIViewSet. #2311 - Fixed autopopulation of \"Parent\" selection when editing an existing Location. #2341 - Fixed omission of docs from published Python packages. #2342 - Reduced file size of nautobot-dev Docker images by clearing Poetry cache #2350 - Fixed potential Redis deadlock if Nautobot server restarts at an unfortunate time. v1.4.1 (2022-08-22) \u00b6 Added \u00b6 #1809 - Added Django natural key to extras.Status to simplify exporting and importing of database dumps for Status objects. #2202 - Added validate_models management command to validate each instance in the database. #2213 - Added a new --pull parameter for invoke build to tell Docker to pull images when building containers. Changed \u00b6 #2206 - Changed Run button on Job Result to always be displayed, \"Re-Run\" if available. #2252 - Updated Poetry install command for Development Getting Started guide. Fixed \u00b6 #2209 - Fixed lack of dark-mode support in GraphiQL page. #2215 - Fixed error seen in migration from 1.3.x if certain default Statuses had been modified. #2218 - Fixed typos/links in release notes and Dynamic Groups docs. #2219 - Fixed broken pagination in Dynamic Group detail \"Members\" tab. #2220 - Narrowed scope of auto-formatting in VSCode to only apply to Python files. #2222 - Fixed missing app headings in Swagger UI. #2229 - Fixed render_form.html include template to not render a duplicate object_note field. #2232 - Fixed incorrect API URLs and incorrect inclusion of Circuits UI URLs in Swagger UI. #2241 - Fixed DynamicGroup.objects.get_for_model() to support nested Dynamic Groups. #2259 - Fixed footer not bound to bottom of Device View. v1.4.0 (2022-08-15) \u00b6 Added \u00b6 #1812 - Added NautobotViewSet and accompanying helper methods, documentation. #2173 - Added flake8 linting and black formatting settings to vscode workspace settings. #2105 - Added support for Notes in NautobotBulkEditForm and NautobotEditForm. #2200 - Added Dynamic Groups support for Cluster, IP Address, Prefix, and Rack. Changed \u00b6 #1812 - Changed Circuit app models to use NautobotViewSet s. #2029 - Updated optional settings docs to call out environment variable only settings. #2176 - Update invoke task output to use rich formatting, print full Docker Compose commands. #2183 - Update dependency django to ~3.2.15. #2193 - Updated Postgres/MySQL dumpdata docs to exclude django_rq exports. #2200 - Group of dynamic group membership links now link to the group's membership table view. Fixed \u00b6 #1304 - Fixed incorrect display of connection counts on home page. #1845 - Fixed not being able to schedule job with 'immediate' schedule via API. #1996 - Fixed Menu Item link_text render on top of buttons. #2178 - Fixed \"invalid filter\" error when filtering JobResults in the UI. #2184 - Fixed job re-run not honoring has_sensitive_variables . #2190 - Fixed tags missing from Location forms. #2191 - Fix widget for boolean filters fields when generating filter form for a Dynamic Group #2192 - Fixed job.request removed from job instance in v1.4.0b1 . #2197 - Fixed some display issues in the Dynamic Groups detail view. v1.4.0rc1 (2022-08-10) \u00b6 Added \u00b6 #767 - Added notes field to Primary and Organizational models. #1498 - Added extended lookup expression filters to custom fields. #1962 - Added slug field to Custom Field model, added 1.4 REST API version of the api/extras/custom-fields/ endpoints. #2106 - Added support for listing/creating Notes via REST API. Changed \u00b6 #2156 - Update network automation apps listed on overview of docs. #2168 - Added model toggle to skip adding missing Dynamic Group filter fields for use in easing integration of new models into Dynamic Groups. Fixed \u00b6 #2090 - Fixed an issue where a REST API PATCH of a Tag could inadvertently reset its associated content-types. #2150 - Fixed unit tests performance degradation. #2132 - Updated job hooks to use slugs in urls instead of pk. #2133 - Update documentation for job hooks, make it reachable from the Nautobot UI. #2135 - Fixed ImportError on RelationshipModelForm , renamed other mixins and added aliases for backwards compatibility. #2137 - Fixed incorrect parameter name in NaturalKeyOrPKMultipleChoiceFilter documentation. #2142 - Fixed incorrect URL field in REST API nested relationship representation. #2165 - Fix up relationship-association API test issue. v1.4.0b1 (2022-07-30) \u00b6 Added \u00b6 #1463 - Added REST API support for opt-in relationships data on model endpoints; added NautobotModelSerializer base class. #1614 - Added support for nesting of Dynamic Groups, allowing inclusion/exclusion rules of sub-group members. #1735 - Added missing filters to model FilterSets for Virtualization models. #1865 - Added support for a custom template on Job forms. #1875 - Add ability to quickly re-submit a previously run Job with the same parameters. #1877 - Add new job base class JobHookReceiver to support triggering job execution from change events. #1878 - Add job hooks feature. #1883 - Add ability to filter objects by their relationships into the existing FilterSet. #1884 - Add ability to set the relationship filter via the filter form. #2035 - Added change source context to object change context manager. #2051 - Add changelog url for Relationships. #2061 - Add draggable child groups to Dynamic Groups edit view in UI, recompute and hide weights. #2072 - Expand on query_params for ObjectVar in Jobs documentation. Changed \u00b6 #2049 - Moved get_changelog_url to a method on objects that support changelogs, updated template context. #2116 - Updated package dependencies: Pillow ~9.1.1 -> ~9.2.0 , black ~22.3.0 -> ~22.6.0 , coverage 6.4.1 -> 6.4.2 , django-cacheops 6.0 -> 6.1 , django-cryptography 1.0 -> 1.1 , django-debug-toolbar ~3.4.0 -> ~3.5.0 , django-extensions ~3.1.5 -> ~3.2.0 , drf-yasg ~1.20.0 -> ^1.20.0 , importlib-metadata ~4.4 -> ^4.4.0 , jsonschema ~4.4.0 -> ~4.8.0 , mkdocs 1.3.0 -> 1.3.1 , mkdocs ==1.3.0 -> ==1.3.1 , mkdocs-include-markdown-plugin ~3.2.3 -> ~3.6.0 , mkdocs-include-markdown-plugin ==3.2.3 -> ==3.6.1 , social-auth-core ~4.2.0 -> ~4.3.0 , svgwrite 1.4.2 -> 1.4.3 Fixed \u00b6 #1710 - Fixed invalid CSS when clicking \"Add another\" row buttons for formsets on Secrets Groups, Dynamic Groups edit view in the UI. #2069 - Addressed numerous UX improvements for Dynamic Groups of Dynamic Groups feature to ease usability of this feature. #2109 - Fixed Relationship Filters are not Applied with \"And\" Operator. #2111 - Fixed Invalid filter error thrown for __source with message: \"\" is not a valid UUID. v1.4.0a2 (2022-07-11) \u00b6 Attention The next and develop branches introduced conflicting migration numbers during the release cycle. This necessitates reordering the migration in next . If you installed v1.4.0a1 , you will need to roll back a migration before upgrading/installing v1.4.0a2 and newer. If you have not installed v1.4.0a this will not be an issue. Before upgrading, run: nautobot-server migrate extras 0033_add__optimized_indexing . This will revert the reordered migration 0034_configcontextschema__remove_name_unique__create_constraint_unique_name_owner , which is now number 0035 . Perform the Nautobot upgrade as usual and proceed with post-installation migration. No data loss is expected as the reordered migration only modified indexing on existing fields. Added \u00b6 #1000 - Object detail views can now have extra UI tabs which are defined by a plugin. #1052 - Initial prototype implementation of Location data model. #1318 - Added nautobot.extras.forms.NautobotBulkEditForm base class. All bulk-edit forms for models that support both custom fields and relationships now inherit from this class. #1466 - Plugins can now override views. #1729 - Add new filter class NaturalKeyOrPKMultipleChoiceFilter to nautobot.utilities.filters . #1729 - Add 137 new filters to nautobot.dcim.filters FilterSets. #1729 - Add cable_terminations to the model_features registry. #1893 - Added an object detail view for Relationships. #1949 - Added TestCaseMixin for Helper Functions across all test case bases. Changed \u00b6 #1908 - Update dependency Markdown to ~3.3.7 #1909 - Update dependency MarkupSafe to ~2.1.1 #1912 - Update dependency celery to ~5.2.7 #1913 - Update dependency django-jinja to ~2.10.2 #1915 - Update dependency invoke to ~1.7.1 #1917 - Update dependency svgwrite to ~1.4.2 #1919 - Update dependency Pillow to ~9.1.1 #1920 - Update dependency coverage to ~6.4.1 #1921 - Update dependency django-auth-ldap to ~4.1.0 #1924 - Update dependency django-cors-headers to ~3.13.0 #1925 - Update dependency django-debug-toolbar to ~3.4.0 #1928 - Update dependency napalm to ~3.4.1 #1929 - Update dependency selenium to ~4.2.0 #1945 - Change the settings_and_registry default context processor to purely settings , moving registry dictionary to be accessible via registry template tag. Fixed \u00b6 #1898 - Browsable API is now properly styled as the rest of the app. Removed \u00b6 #1462 - Removed job source tab from Job and Job Result view. #2002 - Removed rqworker container from default Docker development environment. v1.4.0a1 (2022-06-13) \u00b6 Added \u00b6 #729 - Added UI dark mode. #984 - Added status field to Interface, VMInterface models. #1119 - Added truncated device name functionality to Rackview UI. #1455 - Added parent_interface and bridge fields to Interface and VMInterface models. #1833 - Added hyperlinked_object template filter to consistently reference objects in templates. Changed \u00b6 #1736 - STRICT_FILTERING setting is added and enabled by default. #1793 - Added index notes to fields from analysis, relaxed ConfigContextSchema constraint (unique on name , owner_content_type , owner_object_id instead of just name ). Fixed \u00b6 #1815 - Fix theme link style in footer. #1831 - Fixed missing parent_interface and bridge from 1.4 serializer of Interfaces. #1831 - Fix job from with approval_required=True and has_sensitive_variables=True can be scheduled. .","title":"Version 1.4"},{"location":"release-notes/version-1.4.html#nautobot-v14","text":"This document describes all new features and changes in Nautobot 1.4. If you are a user migrating from NetBox to Nautobot, please refer to the \"Migrating from NetBox\" documentation.","title":"Nautobot v1.4"},{"location":"release-notes/version-1.4.html#release-overview","text":"","title":"Release Overview"},{"location":"release-notes/version-1.4.html#added","text":"","title":"Added"},{"location":"release-notes/version-1.4.html#custom-field-extended-filtering-1498","text":"Objects with custom fields now support filter lookup expressions for filtering by custom field values, such as cf_date_field__gte=2022-08-11 to select objects whose date_field custom field has a date of 2022-08-11 or later.","title":"Custom Field Extended Filtering (#1498)"},{"location":"release-notes/version-1.4.html#custom-field-slugs-1962","text":"Custom fields now have a distinct slug field. The custom field name attribute should be considered deprecated, and will be removed in a future major release (see also #824 .) Additionally, the label attribute, while currently optional in the database, will become mandatory in that same future release as a consequence. When migrating from an earlier Nautobot release to version 1.4 or later, the slug and label for all existing custom fields will be automatically populated if not previously defined. A new version of the /api/extras/custom-fields/ REST API endpoints has been implemented. By default this endpoint continues to demonstrate the pre-1.4 behavior ( name required, slug and label optional; if unspecified, the slug and label will receive default values based on the provided name ). A REST API client can request API version 1.4, in which case the updated API will require slug and label parameters in place of name . Additionally, REST API serialization of custom field data is itself now versioned. For all object endpoints that include custom field data under the custom_fields key, REST API versions 1.3 and earlier will continue the previous behavior of indexing the custom_fields dictionary by fields' name values, but when REST API version 1.4 or later is requested, the custom_fields data will be indexed by slug instead. For technical reasons of backwards-compatibility, the database (ORM) and GraphQL interfaces continue to access and store object custom field data exclusively under the name key; this will change to use the slug in a future major release. Again, watch #824 for plans in that regard.","title":"Custom Field Slugs (#1962)"},{"location":"release-notes/version-1.4.html#custom-tabs-in-object-detail-views-1000","text":"A plugin may now define extra tabs which will be appended to the object view's list of tabs. You can refer to the plugin development guide on how to add tabs to existing object detail views.","title":"Custom Tabs in Object Detail Views (#1000)"},{"location":"release-notes/version-1.4.html#custom-template-css-html-javascript-on-job-forms-1865","text":"Jobs can now specify a template_name property and provide a custom template with additional JavaScript and CSS to help with user input on the Job submission form. You can refer to the Job class metadata attribute documentation on how to build and define this template.","title":"Custom Template (CSS, HTML, JavaScript) on Job Forms (#1865)"},{"location":"release-notes/version-1.4.html#dynamic-groups-support-additional-models-2200","text":"Cluster, IP Address, Prefix, and Rack models can now be filtered on in Dynamic Groups and can also support nested or groups of Dynamic Groups. Some fields have been excluded from filtering until a sensible widget can be provided.","title":"Dynamic Groups Support Additional Models (#2200)"},{"location":"release-notes/version-1.4.html#dark-mode-ui-729","text":"Nautobot's UI now supports dark mode, both explicitly and via browser preference selection. The \"Theme\" link in the footer provides a modal popup to select the preferred theme. This preference is saved per browser via localStorage .","title":"Dark Mode UI (#729)"},{"location":"release-notes/version-1.4.html#improved-filter-coverage-for-dcim-and-virtualization-models","text":"DCIM: #1729 Virtualization: #1735 The DCIM, Virtualization FilterSets have been updated with over 150 new filters, including hybrid filters that support filtering on both pk and slug (or pk and name where slug is not available). A new filter class NaturalKeyOrPKMultipleChoiceFilter was added to nautobot.utilities.filters to support filtering on multiple fields of a related object. Please see the documentation on best practices for mapping model fields to filters for more information.","title":"Improved Filter Coverage for DCIM and Virtualization Models"},{"location":"release-notes/version-1.4.html#job-hooks-1878","text":"Jobs can now be configured to run automatically when a change event occurs on a Nautobot object. Job hooks associate jobs to content types and actions to run jobs when a create, update or delete action occurs on the selected content type. A new job base class JobHookReceiver was introduced that jobs must subclass to be associated with a job hook. Please see the documentation on Job Hooks for more information.","title":"Job Hooks (#1878)"},{"location":"release-notes/version-1.4.html#job-re-runs-1875","text":"JobResult records now save the arguments with which the Job was called, allowing for easy re-execution of the Job with the same arguments as before. A \"re-run\" button has been added to the JobResult list view and detail view.","title":"Job Re-Runs (#1875)"},{"location":"release-notes/version-1.4.html#location-data-model-1052","text":"To locate network information more precisely than a Site defines, you can now define a hierarchy of Location Types (for example, Building \u2190 Floor \u2190 Room ) and then create Locations corresponding to these types within each Site. Data objects such as devices, prefixes, VLAN groups, etc. can thus be mapped or assigned to Location representing a specific building, wing, floor, room, etc. as appropriate to your needs. Info At present, Locations fill the conceptual space between the more abstract Region and Site models and the more concrete Rack Group model. In a future Nautobot release, some or all of these other models may be collapsed into Locations. That is to say, in the future you might not deal with Regions and Sites as distinct models, but instead your Location Type hierarchy might include these higher-level categories, becoming something like Country \u2190 City \u2190 Site \u2190 Building \u2190 Floor \u2190 Room.","title":"Location Data Model (#1052)"},{"location":"release-notes/version-1.4.html#parent-interfaces-and-bridge-interfaces-1455","text":"Interface and VMInterface models now have parent_interface and bridge keys. An interface of type Virtual can now associate to a parent physical interface on the same device, virtual chassis, or virtual machine, and an interface of any type can specify another interface as its associated bridge interface. (A new Bridge interface type has also been added, but the bridge interface property is not restricted to interfaces of this type.)","title":"Parent Interfaces and Bridge Interfaces (#1455)"},{"location":"release-notes/version-1.4.html#rackview-ui-add-option-to-truncate-device-name-1119","text":"Users can now toggle device full name and truncated name in the rack elevation view. The truncating function is customizable in nautobot_config.py via defining UI_RACK_VIEW_TRUNCATE_FUNCTION . Default behavior is to split on . and return the first item in the list. \"Save SVG\" link presents the same view as what is currently displayed on screen Current preferred toggle state is preserved across tabs (requires refresh) and persists in-browser until local storage is cleared. This presents a consistent behavior when browsing between multiple racks.","title":"Rackview UI - Add Option to Truncate Device Name (#1119)"},{"location":"release-notes/version-1.4.html#rest-api-enhancements-1463","text":"For all models that support Relationships, their corresponding list and detail REST API endpoints now include the option to include data on their associated Relationships and related objects by specifying include=relationships as a query parameter. Relationship associations on a model can be edited by a PATCH to the appropriate nested value, such as \"relationships\" -> <relationship-slug> -> \"source\" or \"relationships\" -> <relationship-slug> -> \"destination\" . For implementers of REST API serializers (core and/or plugins), a new nautobot.extras.api.serializers.NautobotModelSerializer base class has been added. Using this class guarantees support for relationships, custom fields, and computed fields on the serializer, and provides for a streamlined developer experience.","title":"REST API Enhancements (#1463)"},{"location":"release-notes/version-1.4.html#status-field-on-interface-vminterface-models-984","text":"Interface and VMInterface models now support a status. Default statuses that are available to be set are: Active, Planned, Maintenance, Failed, and Decommissioned. During migration all existing interfaces will be set to the status of \"Active\". A new version of the /dcim/interfaces/* REST API endpoints have been implemented. By default this endpoint continues to demonstrate the pre-1.4 behavior unless the REST API client explicitly requests API version=1.4. If you continue to use the pre-1.4 API endpoints, status is defaulted to \"Active\". Visit the documentation on REST API versioning for more information on using the versioned APIs.","title":"Status Field on Interface, VMInterface Models (#984)"},{"location":"release-notes/version-1.4.html#nautobotuiviewset-1812","text":"New in Nautobot 1.4 is the debut of NautobotUIViewSet : A powerful plugin development tool that can save plugin developer hundreds of lines of code compared to using legacy generic.views . Using it to gain access to default functionalities previous provided by generic.views such as create() , bulk_create() , update() , partial_update() , bulk_update() , destroy() , bulk_destroy() , retrieve() and list() actions. Note that this ViewSet is catered specifically to the UI, not the API. Concrete examples on how to use NautobotUIViewSet resides in nautobot.circuits.views . Please visit the plugin development guide on NautobotViewSet for more information.","title":"NautobotUIViewSet (#1812)"},{"location":"release-notes/version-1.4.html#notes-767","text":"Primary and Organizational models now support notes. A notes tab has been added to the Object Detail view for all models that inherit the Primary or Organizational base abstract models. Warning Any plugin that inherits from one of these two models and uses the ViewTestCases.PrimaryObjectViewTestCase or ViewTestCases.OrganizationalObjectViewTestCase for their test will need to add the NotesObjectView to the objects URLs. Notes can also be used via the REST API at endpoint /api/extras/notes/ or per object detail endpoint at the object's nested /notes/ endpoint. Info For implementers of REST API views (core and/or plugins), a new nautobot.extras.api.views.NautobotModelViewSet base class has been added. Use of this class ensures that all features from PrimaryModel or OrganizationalModel are accessible through the API. This includes custom fields and notes. Please see the on plugin development guide on Notes for more details.","title":"Notes (#767)"},{"location":"release-notes/version-1.4.html#changed","text":"","title":"Changed"},{"location":"release-notes/version-1.4.html#dynamic-groups-of-dynamic-groups-1614","text":"Dynamic Groups may now be nested in parent/child relationships. The Dynamic Group edit view now has a \"Child Groups\" tab that allows one to make other Dynamic Groups of the same content type children of the parent group. Any filters provided by the child groups are used to filter the members from the parent group using one of three operators: \"Restrict (AND)\", \"Include (OR)\", or \"Exclude (NOT)\". This allows for logical parenthetical grouping of nested groups by the operator you choose for that child group association to the parent. Warning The default behavior of Dynamic Groups with an empty filter ( {} ) has been inverted to include all objects matching the content type by default instead of matching no objects. This was necessary to implement the progressive layering of child filters similarly to how we use filters to reduce desired objects from basic list view filters. Please see the greatly-expanded documentation on Dynamic Groups for more information.","title":"Dynamic Groups of Dynamic Groups (#1614)"},{"location":"release-notes/version-1.4.html#renamed-mixin-classes-2135","text":"A number of mixin classes have been renamed for improved self-consistency and clarity of usage. The former names of these mixins are still available for now as aliases, but inheriting from these mixins will raise a DeprecationWarning , and these aliases will be removed in a future major release. Former Name New Name AddRemoveTagsForm TagsBulkEditFormMixin CustomFieldBulkCreateForm CustomFieldModelBulkEditFormMixin CustomFieldBulkEditForm CustomFieldModelBulkEditFormMixin CustomFieldFilterForm CustomFieldModelFilterFormMixin CustomFieldModelForm CustomFieldModelFormMixin RelationshipModelForm RelationshipModelFormMixin StatusBulkEditFormMixin StatusModelBulkEditFormMixin StatusFilterFormMixin StatusModelFilterFormMixin","title":"Renamed Mixin Classes (#2135)"},{"location":"release-notes/version-1.4.html#strict-filter-validation-by-default-1736","text":"Filtering of object lists in the UI and in the REST API will now report an error if an unknown or unrecognized filter parameter is specified. This is a behavior change from previous Nautobot releases, in which unknown filter parameters would be silently discarded and ignored. A new configuration setting, STRICT_FILTERING has been added. It defaults to True , enabling strict validation of filter parameters, but can be set to False to disable this validation. Warning Setting STRICT_FILTERING to False can result in unexpected filtering results in the case of user error, for example a request to /api/dcim/devices/?has_primry_ip=false (note the typo primry ) will result in a list of all devices, rather than the intended list of only devices that lack a primary IP address. In the case of Jobs or external automation making use of such a filter, this could have wide-ranging consequences.","title":"Strict Filter Validation by Default (#1736)"},{"location":"release-notes/version-1.4.html#moved-registry-template-context-1945","text":"The settings_and_registry default context processor was changed to purely settings - the (large) Nautobot application registry dictionary is no longer provided as part of the render context for all templates by default. Added a new registry template tag that can be invoked by specific templates to provide this variable where needed.","title":"Moved Registry Template Context (#1945)"},{"location":"release-notes/version-1.4.html#v145-2022-10-03","text":"","title":"v1.4.5 (2022-10-03)"},{"location":"release-notes/version-1.4.html#added_1","text":"#2330 - Added created and last_updated fields to Device Component and ComponentTemplate models.","title":"Added"},{"location":"release-notes/version-1.4.html#changed_1","text":"#2490 - Added change log fragment checkbox to Github pull request template. #2508 - Updated dark mode theme to be less...dark. #2537 - Updated django-extensions to 3.2.1 , drf-spectacular to 0.24.2 , drf-yasg to 1.21.4 . #2544 - Updated towncrier template to convert multi-line change fragments into multiple release notes entries.","title":"Changed"},{"location":"release-notes/version-1.4.html#fixed","text":"#2326 - Fixed 500 error in Circuit Termination swap. #2330 - Fixed missing Change Log tab on device component detail views. #2445 - Fixed invalid Renovate config. #2466 - Fixed Jobs misleading sensitive variables or requiring approval error message. #2509 - Fixed template lookup logic in ObjectNotesView, ObjectDynamicGroupsView and ObjectChangeLogView. #2526 - Fixed error in rendering the example plugin's AnotherExampleModel list view by adding a get_absolute_url() method to the AnotherExampleModel class and adding an AnotherExampleModel detail view template. #2533 - Fixed test_list_objects_unknown_filter_no_strict_filtering failure if a filterset couldn't be found for a given model. #2539 - Updated links from nautobot.readthedocs.io to docs.nautobot.com.","title":"Fixed"},{"location":"release-notes/version-1.4.html#v144-2022-09-26","text":"","title":"v1.4.4 (2022-09-26)"},{"location":"release-notes/version-1.4.html#added_2","text":"#2023 - Added reusable GitHub Action workflow for use by plugins to test against next , develop routinely.","title":"Added"},{"location":"release-notes/version-1.4.html#changed_2","text":"#2153 - Updated optional settings documentation to provide clarity on Nautobot vs Django settings. #2235 - Added ancestor information to Location and LocationType display names to provide additional context in the UI. #2457 - Moved towncrier_template.j2 from root to develop directory. #2468 - Upgraded CI gh-action-setup-poetry-environment action to v3","title":"Changed"},{"location":"release-notes/version-1.4.html#fixed_1","text":"#2353 - Fixed MultiSelect CustomField being emptied unintentionally after bulk update. #2375 - Fixed error in Cable list view filtering when color and type filters are not set. #2461 - Fixed an exception during OpenAPI schema generation when certain Nautobot apps (including nautobot-firewall-models ) were installed. #2496 - Fixed failing CI due to #2468","title":"Fixed"},{"location":"release-notes/version-1.4.html#v143-2022-09-19","text":"","title":"v1.4.3 (2022-09-19)"},{"location":"release-notes/version-1.4.html#added_3","text":"#2327 - Added help text to the Job scheduling datetime picker to indicate the applicable time zone. #2362 - Added documentation and automation for Nautobot Github project to use towncrier for changelog fragments. #2431 - Add section to the custom field documentation on ORM filtering.","title":"Added"},{"location":"release-notes/version-1.4.html#changed_3","text":"#1619 - Updated drf-spectacular dependency to version 0.24. #2223 - Augment get_route_for_model() to support REST API routes. #2340 - Improved rendering of badges, labels, buttons, and color selection menus in dark mode. #2383 - Updated documentation link for Nautobot ChatOps plugin. #2392 - Un-group Renovate next updates to address code changes per package. #2400 - Improved formatting of version changes in the documentation. #2417 - Reworked Docker builds in CI to publish an intermediate \"dependencies-only\" image to speed up rebuild times. #2447 - Moved Dynamic Groups tab on object detail view to it's own view as a generic ObjectDynamicGroupsView .","title":"Changed"},{"location":"release-notes/version-1.4.html#fixed_2","text":"#138 - Fixed lack of user-facing message when an exception occurs while discovering Jobs from a Git repository. #950 - Fixed database concurrency issues with uWSGI pre-forking. #1619 - Improved accuracy of OpenAPI schema for bulk REST API operations. #2299 - Remove render_filter() method and filter field from table columns #2309 - Fixed 404 on ScheduledJobView, job_class no longer found behavior. #2324 - Fixed errors encountered when a job model is deleted while a job is running. #2338 - Fixed whitespace issue with Text File secrets and they are now stripped of leading/trailing whitespace and newlines. #2364 - Allow invoke tasks to be run even if rich is not installed. #2378 - Fix Job Result redirection on submit. #2382 - Removed extraneous cache and temporary files from the dev and final-dev Docker images to reduce image size. #2389 - Removed extraneous inspect.getsource() call from Job class. #2407 - Corrected SSO Backend reference for Azure AD Tenant. #2449 - CI: Moved dependency build to be a job, not a step.","title":"Fixed"},{"location":"release-notes/version-1.4.html#v142-2022-09-05","text":"","title":"v1.4.2 (2022-09-05)"},{"location":"release-notes/version-1.4.html#added_4","text":"#983 - Added functionalities to specify args and kwargs to NavMenuItem . #2250 - Added \"Stats\" and \"Rack Groups\" to Location detail view, added \"Locations\" to Site detail view. #2273 - Added custom markdownlint rule to check for invalid relative links in the documentation. #2307 - Added dynamic_groups field in GraphQL on objects that can belong to dynamic groups. #2314 - Added pylint to linting suite and CI. #2339 - Enabled and addressed additional pylint checkers. #2360 - Added Django natural key to extras.Tag .","title":"Added"},{"location":"release-notes/version-1.4.html#changed_4","text":"#2011 - replaced all .format() strings and C format strings with fstrings. #2293 - Updated GitHub bug report template. #2296 - Updated netutils dependency from 1.1.x to 1.2.x. #2347 - Revamped documentation look and feel. #2349 - Docker images are now built with Poetry 1.2.0. #2360 - Django natural key for Status is now name rather than slug . #2363 - Update app icons for consistency #2365 - Update Network to Code branding name #2367 - Remove coming soon from projects that exists","title":"Changed"},{"location":"release-notes/version-1.4.html#fixed_3","text":"#449 - Improved error checking and reporting when syncing Git repositories. #1227 - The NAUTOBOT_DOCKER_SKIP_INIT environment variable can now be set to \"false\" (case-insensitive), #1807 - Fixed post_run method fails to add exceptions to job log. #2085 - The log entries table on a job result page can now be filtered by log level or message and hitting the return key has no effect. #2107 - Fixed a TypeError when a view defines action_buttons = None . #2237 - Fixed several issues with permissions enforcement for Note creation and viewing. #2268 - Fixed broken links in documentation. #2269 - Fixed missing JS code causing rendering errors on GraphQL Query and Rack Reservation detail views. #2278 - Fixed incorrect permissions check on \"Installed Plugins\" menu item. #2290 - Fixed inheritance of ObjectListViewMixin for CircuitTypeUIViewSet. #2311 - Fixed autopopulation of \"Parent\" selection when editing an existing Location. #2341 - Fixed omission of docs from published Python packages. #2342 - Reduced file size of nautobot-dev Docker images by clearing Poetry cache #2350 - Fixed potential Redis deadlock if Nautobot server restarts at an unfortunate time.","title":"Fixed"},{"location":"release-notes/version-1.4.html#v141-2022-08-22","text":"","title":"v1.4.1 (2022-08-22)"},{"location":"release-notes/version-1.4.html#added_5","text":"#1809 - Added Django natural key to extras.Status to simplify exporting and importing of database dumps for Status objects. #2202 - Added validate_models management command to validate each instance in the database. #2213 - Added a new --pull parameter for invoke build to tell Docker to pull images when building containers.","title":"Added"},{"location":"release-notes/version-1.4.html#changed_5","text":"#2206 - Changed Run button on Job Result to always be displayed, \"Re-Run\" if available. #2252 - Updated Poetry install command for Development Getting Started guide.","title":"Changed"},{"location":"release-notes/version-1.4.html#fixed_4","text":"#2209 - Fixed lack of dark-mode support in GraphiQL page. #2215 - Fixed error seen in migration from 1.3.x if certain default Statuses had been modified. #2218 - Fixed typos/links in release notes and Dynamic Groups docs. #2219 - Fixed broken pagination in Dynamic Group detail \"Members\" tab. #2220 - Narrowed scope of auto-formatting in VSCode to only apply to Python files. #2222 - Fixed missing app headings in Swagger UI. #2229 - Fixed render_form.html include template to not render a duplicate object_note field. #2232 - Fixed incorrect API URLs and incorrect inclusion of Circuits UI URLs in Swagger UI. #2241 - Fixed DynamicGroup.objects.get_for_model() to support nested Dynamic Groups. #2259 - Fixed footer not bound to bottom of Device View.","title":"Fixed"},{"location":"release-notes/version-1.4.html#v140-2022-08-15","text":"","title":"v1.4.0 (2022-08-15)"},{"location":"release-notes/version-1.4.html#added_6","text":"#1812 - Added NautobotViewSet and accompanying helper methods, documentation. #2173 - Added flake8 linting and black formatting settings to vscode workspace settings. #2105 - Added support for Notes in NautobotBulkEditForm and NautobotEditForm. #2200 - Added Dynamic Groups support for Cluster, IP Address, Prefix, and Rack.","title":"Added"},{"location":"release-notes/version-1.4.html#changed_6","text":"#1812 - Changed Circuit app models to use NautobotViewSet s. #2029 - Updated optional settings docs to call out environment variable only settings. #2176 - Update invoke task output to use rich formatting, print full Docker Compose commands. #2183 - Update dependency django to ~3.2.15. #2193 - Updated Postgres/MySQL dumpdata docs to exclude django_rq exports. #2200 - Group of dynamic group membership links now link to the group's membership table view.","title":"Changed"},{"location":"release-notes/version-1.4.html#fixed_5","text":"#1304 - Fixed incorrect display of connection counts on home page. #1845 - Fixed not being able to schedule job with 'immediate' schedule via API. #1996 - Fixed Menu Item link_text render on top of buttons. #2178 - Fixed \"invalid filter\" error when filtering JobResults in the UI. #2184 - Fixed job re-run not honoring has_sensitive_variables . #2190 - Fixed tags missing from Location forms. #2191 - Fix widget for boolean filters fields when generating filter form for a Dynamic Group #2192 - Fixed job.request removed from job instance in v1.4.0b1 . #2197 - Fixed some display issues in the Dynamic Groups detail view.","title":"Fixed"},{"location":"release-notes/version-1.4.html#v140rc1-2022-08-10","text":"","title":"v1.4.0rc1 (2022-08-10)"},{"location":"release-notes/version-1.4.html#added_7","text":"#767 - Added notes field to Primary and Organizational models. #1498 - Added extended lookup expression filters to custom fields. #1962 - Added slug field to Custom Field model, added 1.4 REST API version of the api/extras/custom-fields/ endpoints. #2106 - Added support for listing/creating Notes via REST API.","title":"Added"},{"location":"release-notes/version-1.4.html#changed_7","text":"#2156 - Update network automation apps listed on overview of docs. #2168 - Added model toggle to skip adding missing Dynamic Group filter fields for use in easing integration of new models into Dynamic Groups.","title":"Changed"},{"location":"release-notes/version-1.4.html#fixed_6","text":"#2090 - Fixed an issue where a REST API PATCH of a Tag could inadvertently reset its associated content-types. #2150 - Fixed unit tests performance degradation. #2132 - Updated job hooks to use slugs in urls instead of pk. #2133 - Update documentation for job hooks, make it reachable from the Nautobot UI. #2135 - Fixed ImportError on RelationshipModelForm , renamed other mixins and added aliases for backwards compatibility. #2137 - Fixed incorrect parameter name in NaturalKeyOrPKMultipleChoiceFilter documentation. #2142 - Fixed incorrect URL field in REST API nested relationship representation. #2165 - Fix up relationship-association API test issue.","title":"Fixed"},{"location":"release-notes/version-1.4.html#v140b1-2022-07-30","text":"","title":"v1.4.0b1 (2022-07-30)"},{"location":"release-notes/version-1.4.html#added_8","text":"#1463 - Added REST API support for opt-in relationships data on model endpoints; added NautobotModelSerializer base class. #1614 - Added support for nesting of Dynamic Groups, allowing inclusion/exclusion rules of sub-group members. #1735 - Added missing filters to model FilterSets for Virtualization models. #1865 - Added support for a custom template on Job forms. #1875 - Add ability to quickly re-submit a previously run Job with the same parameters. #1877 - Add new job base class JobHookReceiver to support triggering job execution from change events. #1878 - Add job hooks feature. #1883 - Add ability to filter objects by their relationships into the existing FilterSet. #1884 - Add ability to set the relationship filter via the filter form. #2035 - Added change source context to object change context manager. #2051 - Add changelog url for Relationships. #2061 - Add draggable child groups to Dynamic Groups edit view in UI, recompute and hide weights. #2072 - Expand on query_params for ObjectVar in Jobs documentation.","title":"Added"},{"location":"release-notes/version-1.4.html#changed_8","text":"#2049 - Moved get_changelog_url to a method on objects that support changelogs, updated template context. #2116 - Updated package dependencies: Pillow ~9.1.1 -> ~9.2.0 , black ~22.3.0 -> ~22.6.0 , coverage 6.4.1 -> 6.4.2 , django-cacheops 6.0 -> 6.1 , django-cryptography 1.0 -> 1.1 , django-debug-toolbar ~3.4.0 -> ~3.5.0 , django-extensions ~3.1.5 -> ~3.2.0 , drf-yasg ~1.20.0 -> ^1.20.0 , importlib-metadata ~4.4 -> ^4.4.0 , jsonschema ~4.4.0 -> ~4.8.0 , mkdocs 1.3.0 -> 1.3.1 , mkdocs ==1.3.0 -> ==1.3.1 , mkdocs-include-markdown-plugin ~3.2.3 -> ~3.6.0 , mkdocs-include-markdown-plugin ==3.2.3 -> ==3.6.1 , social-auth-core ~4.2.0 -> ~4.3.0 , svgwrite 1.4.2 -> 1.4.3","title":"Changed"},{"location":"release-notes/version-1.4.html#fixed_7","text":"#1710 - Fixed invalid CSS when clicking \"Add another\" row buttons for formsets on Secrets Groups, Dynamic Groups edit view in the UI. #2069 - Addressed numerous UX improvements for Dynamic Groups of Dynamic Groups feature to ease usability of this feature. #2109 - Fixed Relationship Filters are not Applied with \"And\" Operator. #2111 - Fixed Invalid filter error thrown for __source with message: \"\" is not a valid UUID.","title":"Fixed"},{"location":"release-notes/version-1.4.html#v140a2-2022-07-11","text":"Attention The next and develop branches introduced conflicting migration numbers during the release cycle. This necessitates reordering the migration in next . If you installed v1.4.0a1 , you will need to roll back a migration before upgrading/installing v1.4.0a2 and newer. If you have not installed v1.4.0a this will not be an issue. Before upgrading, run: nautobot-server migrate extras 0033_add__optimized_indexing . This will revert the reordered migration 0034_configcontextschema__remove_name_unique__create_constraint_unique_name_owner , which is now number 0035 . Perform the Nautobot upgrade as usual and proceed with post-installation migration. No data loss is expected as the reordered migration only modified indexing on existing fields.","title":"v1.4.0a2 (2022-07-11)"},{"location":"release-notes/version-1.4.html#added_9","text":"#1000 - Object detail views can now have extra UI tabs which are defined by a plugin. #1052 - Initial prototype implementation of Location data model. #1318 - Added nautobot.extras.forms.NautobotBulkEditForm base class. All bulk-edit forms for models that support both custom fields and relationships now inherit from this class. #1466 - Plugins can now override views. #1729 - Add new filter class NaturalKeyOrPKMultipleChoiceFilter to nautobot.utilities.filters . #1729 - Add 137 new filters to nautobot.dcim.filters FilterSets. #1729 - Add cable_terminations to the model_features registry. #1893 - Added an object detail view for Relationships. #1949 - Added TestCaseMixin for Helper Functions across all test case bases.","title":"Added"},{"location":"release-notes/version-1.4.html#changed_9","text":"#1908 - Update dependency Markdown to ~3.3.7 #1909 - Update dependency MarkupSafe to ~2.1.1 #1912 - Update dependency celery to ~5.2.7 #1913 - Update dependency django-jinja to ~2.10.2 #1915 - Update dependency invoke to ~1.7.1 #1917 - Update dependency svgwrite to ~1.4.2 #1919 - Update dependency Pillow to ~9.1.1 #1920 - Update dependency coverage to ~6.4.1 #1921 - Update dependency django-auth-ldap to ~4.1.0 #1924 - Update dependency django-cors-headers to ~3.13.0 #1925 - Update dependency django-debug-toolbar to ~3.4.0 #1928 - Update dependency napalm to ~3.4.1 #1929 - Update dependency selenium to ~4.2.0 #1945 - Change the settings_and_registry default context processor to purely settings , moving registry dictionary to be accessible via registry template tag.","title":"Changed"},{"location":"release-notes/version-1.4.html#fixed_8","text":"#1898 - Browsable API is now properly styled as the rest of the app.","title":"Fixed"},{"location":"release-notes/version-1.4.html#removed","text":"#1462 - Removed job source tab from Job and Job Result view. #2002 - Removed rqworker container from default Docker development environment.","title":"Removed"},{"location":"release-notes/version-1.4.html#v140a1-2022-06-13","text":"","title":"v1.4.0a1 (2022-06-13)"},{"location":"release-notes/version-1.4.html#added_10","text":"#729 - Added UI dark mode. #984 - Added status field to Interface, VMInterface models. #1119 - Added truncated device name functionality to Rackview UI. #1455 - Added parent_interface and bridge fields to Interface and VMInterface models. #1833 - Added hyperlinked_object template filter to consistently reference objects in templates.","title":"Added"},{"location":"release-notes/version-1.4.html#changed_10","text":"#1736 - STRICT_FILTERING setting is added and enabled by default. #1793 - Added index notes to fields from analysis, relaxed ConfigContextSchema constraint (unique on name , owner_content_type , owner_object_id instead of just name ).","title":"Changed"},{"location":"release-notes/version-1.4.html#fixed_9","text":"#1815 - Fix theme link style in footer. #1831 - Fixed missing parent_interface and bridge from 1.4 serializer of Interfaces. #1831 - Fix job from with approval_required=True and has_sensitive_variables=True can be scheduled. .","title":"Fixed"},{"location":"rest-api/authentication.html","text":"REST API Authentication \u00b6 The Nautobot REST API primarily employs token-based authentication. For convenience, cookie-based authentication can also be used when navigating the browseable API. Tokens \u00b6 A token is a unique identifier mapped to a Nautobot user account. Each user may have one or more tokens which he or she can use for authentication when making REST API requests. To create a token, navigate to the API tokens page under your user profile. Sign into Nautobot On the upper right hand corner, select your username, then Profile On the left hand side, under User Profile, select API Tokens Select +Add a token Leave Key blank to automatically create a token, or fill one in for yourself Check or uncheck \"Write enabled\", as desired (Optional) Set an expiration date for this token (Optional) Add a description Note The creation and modification of API tokens can be restricted per user by an administrator. If you don't see an option to create an API token, ask an administrator to grant you access. Each token contains a 160-bit key represented as 40 hexadecimal characters. When creating a token, you'll typically leave the key field blank so that a random key will be automatically generated. However, Nautobot allows you to specify a key in case you need to restore a previously deleted token to operation. By default, a token can be used to perform all actions via the API that a user would be permitted to do via the web UI. Deselecting the \"write enabled\" option will restrict API requests made with the token to read operations (e.g. GET) only. Additionally, a token can be set to expire at a specific time. This can be useful if an external client needs to be granted temporary access to Nautobot. Authenticating to the API \u00b6 An authentication token is attached to a request by setting the Authorization header to the string Token followed by a space and the user's token: $ curl -H \"Authorization: Token $TOKEN \" \\ -H \"Accept: application/json; indent=4\" \\ http://nautobot/api/dcim/sites/ { \"count\" : 10 , \"next\" : null , \"previous\" : null , \"results\" : [ ... ] } A token is not required for read-only operations which have been exempted from permissions enforcement (using the EXEMPT_VIEW_PERMISSIONS configuration parameter). However, if a token is required but not present in a request, the API will return a 403 (Forbidden) response: $ curl http://nautobot/api/dcim/sites/ { \"detail\" : \"Authentication credentials were not provided.\" } Initial Token Provisioning \u00b6 Added in version 1.3.0 Ideally, each user should provision his or her own REST API token(s) via the web UI. However, you may encounter where a token must be created by a user via the REST API itself. Nautobot provides a special endpoint to provision tokens using a valid username and password combination. To provision a token via the REST API, make a POST request to the /api/users/tokens/ endpoint: $ curl -X POST \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; indent=4\" \\ -u \"hankhill:I<3C3H8\" \\ https://nautobot/api/users/tokens/ Note that we are not passing an existing REST API token with this request. If the supplied credentials are valid, a new REST API token will be automatically created for the user. Note that the key will be automatically generated, and write ability will be enabled. { \"id\" : \"e87e6ee9-1ab2-46c6-ad7f-3d4697c33d13\" , \"url\" : \"https://nautobot/api/users/tokens/e87e6ee9-1ab2-46c6-ad7f-3d4697c33d13/\" , \"display\" : \"3c9cb9 (hankhill)\" , \"created\" : \"2021-06-11T20:09:13.339367Z\" , \"expires\" : null , \"key\" : \"9fc9b897abec9ada2da6aec9dbc34596293c9cb9\" , \"write_enabled\" : true , \"description\" : \"\" }","title":"Authentication"},{"location":"rest-api/authentication.html#rest-api-authentication","text":"The Nautobot REST API primarily employs token-based authentication. For convenience, cookie-based authentication can also be used when navigating the browseable API.","title":"REST API Authentication"},{"location":"rest-api/authentication.html#tokens","text":"A token is a unique identifier mapped to a Nautobot user account. Each user may have one or more tokens which he or she can use for authentication when making REST API requests. To create a token, navigate to the API tokens page under your user profile. Sign into Nautobot On the upper right hand corner, select your username, then Profile On the left hand side, under User Profile, select API Tokens Select +Add a token Leave Key blank to automatically create a token, or fill one in for yourself Check or uncheck \"Write enabled\", as desired (Optional) Set an expiration date for this token (Optional) Add a description Note The creation and modification of API tokens can be restricted per user by an administrator. If you don't see an option to create an API token, ask an administrator to grant you access. Each token contains a 160-bit key represented as 40 hexadecimal characters. When creating a token, you'll typically leave the key field blank so that a random key will be automatically generated. However, Nautobot allows you to specify a key in case you need to restore a previously deleted token to operation. By default, a token can be used to perform all actions via the API that a user would be permitted to do via the web UI. Deselecting the \"write enabled\" option will restrict API requests made with the token to read operations (e.g. GET) only. Additionally, a token can be set to expire at a specific time. This can be useful if an external client needs to be granted temporary access to Nautobot.","title":"Tokens"},{"location":"rest-api/authentication.html#authenticating-to-the-api","text":"An authentication token is attached to a request by setting the Authorization header to the string Token followed by a space and the user's token: $ curl -H \"Authorization: Token $TOKEN \" \\ -H \"Accept: application/json; indent=4\" \\ http://nautobot/api/dcim/sites/ { \"count\" : 10 , \"next\" : null , \"previous\" : null , \"results\" : [ ... ] } A token is not required for read-only operations which have been exempted from permissions enforcement (using the EXEMPT_VIEW_PERMISSIONS configuration parameter). However, if a token is required but not present in a request, the API will return a 403 (Forbidden) response: $ curl http://nautobot/api/dcim/sites/ { \"detail\" : \"Authentication credentials were not provided.\" }","title":"Authenticating to the API"},{"location":"rest-api/authentication.html#initial-token-provisioning","text":"Added in version 1.3.0 Ideally, each user should provision his or her own REST API token(s) via the web UI. However, you may encounter where a token must be created by a user via the REST API itself. Nautobot provides a special endpoint to provision tokens using a valid username and password combination. To provision a token via the REST API, make a POST request to the /api/users/tokens/ endpoint: $ curl -X POST \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; indent=4\" \\ -u \"hankhill:I<3C3H8\" \\ https://nautobot/api/users/tokens/ Note that we are not passing an existing REST API token with this request. If the supplied credentials are valid, a new REST API token will be automatically created for the user. Note that the key will be automatically generated, and write ability will be enabled. { \"id\" : \"e87e6ee9-1ab2-46c6-ad7f-3d4697c33d13\" , \"url\" : \"https://nautobot/api/users/tokens/e87e6ee9-1ab2-46c6-ad7f-3d4697c33d13/\" , \"display\" : \"3c9cb9 (hankhill)\" , \"created\" : \"2021-06-11T20:09:13.339367Z\" , \"expires\" : null , \"key\" : \"9fc9b897abec9ada2da6aec9dbc34596293c9cb9\" , \"write_enabled\" : true , \"description\" : \"\" }","title":"Initial Token Provisioning"},{"location":"rest-api/filtering.html","text":"REST API Filtering \u00b6 Filtering Objects \u00b6 The objects returned by an API list endpoint can be filtered by attaching one or more query parameters to the request URL. For example, GET /api/dcim/sites/?status=active will return only sites with a status of \"active.\" Multiple parameters can be joined to further narrow results. For example, GET /api/dcim/sites/?status=active&region=europe will return only active sites within the Europe region. Generally, passing multiple values for a single parameter will result in a logical OR operation. For example, GET /api/dcim/sites/?region=north-america&region=south-america will return sites in North America or South America. However, a logical AND operation will be used in instances where a field may have multiple values, such as tags. For example, GET /api/dcim/sites/?tag=foo&tag=bar will return only sites which have both the \"foo\" and \"bar\" tags applied. Changed in version 1.4.0 If STRICT_FILTERING is True (its default value), unrecognized filter parameters now result in a 400 Bad Request response instead of being silently ignored. Filtering by Choice Field \u00b6 Some models have fields which are limited to specific choices, such as the status field on the Prefix model. To find all available choices for this field, make an authenticated OPTIONS request to the model's list endpoint, and use jq to extract the relevant parameters: $ curl -s -X OPTIONS \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ http://nautobot/api/ipam/prefixes/ | jq \".actions.POST.status.choices\" [ { \"value\": \"container\", \"display\": \"Container\" }, { \"value\": \"active\", \"display\": \"Active\" }, { \"value\": \"reserved\", \"display\": \"Reserved\" }, { \"value\": \"deprecated\", \"display\": \"Deprecated\" } ] Note The above works only if the API token used to authenticate the request has permission to make a POST request to this endpoint. Filtering by Custom Field \u00b6 To filter results by a custom field value, prepend cf_ to the custom field name. For example, the following query will return only sites where a custom field named foo is equal to 123: GET /api/dcim/sites/?cf_foo=123 Note For custom field filters, due to historical details of implementation, only a single filter value can be specified when matching a given field. In other words, in the above example, you could not add &cf_foo=456 to the query in order to get all sites where custom field foo is 123 or 456; instead you would need to run two separate queries. This restriction does not apply to custom field filters using lookup expressions (next section) and will likely be changed in a future major version of Nautobot. Custom fields can be mixed with built-in fields to further narrow results. When creating a custom string field, the type of filtering selected (loose versus exact) determines whether partial or full matching is used. Added in version 1.4.0 Custom fields can use the lookup expressions listed in the next section by prepending cf_ to the custom field name (and not the slug ) followed by the required lookup type (see below). Lookup Expressions \u00b6 Certain model fields (including, in Nautobot 1.4.0 and later, custom fields of type text , url , select , integer , and date ) also support filtering using additional lookup expressions. This allows for negation and other context-specific filtering. These lookup expressions can be applied by adding a suffix to the desired field's name, e.g. mac_address__n . In this case, the filter expression is for negation and it is separated by two underscores. Below are the lookup expressions that are supported across different field types. Numeric Fields \u00b6 Numeric-based fields (ASN, VLAN ID, etc.) support these lookup expressions: n - not equal to (negation) lt - less than lte - less than or equal gt - greater than gte - greater than or equal String Fields \u00b6 String-based (char) fields (Name, Address, etc.) support these lookup expressions: n - not equal to (negation) ic - case-insensitive contains nic - negated case-insensitive contains isw - case-insensitive starts-with nisw - negated case-insensitive starts-with iew - case-insensitive ends-with niew - negated case-insensitive ends-with ie - case-insensitive exact match nie - negated case-insensitive exact match Added in version 1.3.0 re - case-sensitive regular expression match nre - negated case-sensitive regular expression match ire - case-insensitive regular expression match nire - negated case-insensitive regular expression match Foreign Keys & Other Fields \u00b6 Certain other fields, namely foreign key relationships support just the negation expression: n . Network and Host Fields \u00b6 There are Custom Lookups built for the VarbinaryIPField field types. While the VarbinaryIPField is applied to fields for network, host, and broadcast, the below filters only apply to network and host. The design makes an assumption that there is in fact a broadcast (of type VarbinaryIPField ) and prefix_length (of type Integer ) within the same model. This assumption is used to understand the relevant scope of the network in question and is important to note when extending the Nautobot core or plugin data model. ** exact - An exact match of an IP or network address, e.g. host__exact=\"10.0.0.1\" ** iexact - An exact match of an IP or network address, e.g. host__iexact=\"10.0.0.1\" ** startswith - Determine if IP or network starts with the value provided, e.g. host__startswith=\"10.0.0.\" ** istartswith - Determine if IP or network starts with the value provided, e.g. host__istartswith=\"10.0.0.\" ** endswith - Determine if IP or network ends with the value provided, e.g. host__endswith=\"0.1\" ** iendswith - Determine if IP or network ends with the value provided, e.g. host__iendswith=\"0.1\" ** regex - Determine if IP or network matches the pattern provided, e.g. host__regex=r\"10\\.(.*)\\.1 ** iregex - Determine if IP or network matches the pattern provided, e.g. host__iregex=r\"10\\.(.*)\\.1 net_contained - Given a network, determine which networks are contained within the provided e.g. network__net_contained=\"192.0.0.0/8\" would include 192.168.0.0/24 in the result net_contained_or_equal - Given a network, determine which networks are contained or is within the provided e.g. network__net_contained_or_equal=\"192.0.0.0/8\" would include 192.168.0.0/24 and 192.0.0.0/8 in the result net_contains - Given a network, determine which networks contain the provided network e.g. network__net_contains=\"192.168.0.0/16\" would include 192.0.0.0/8 in the result net_contains_or_equals - Given a network, determine which networks contain or is the provided network e.g. network__net_contains=\"192.168.0.0/16\" would include 192.0.0.0/8 and 192.168.0.0/16 in the result net_equals - Given a network, determine which which networks are an exact match. e.g. network__net_equals=\"192.168.0.0/16\" would include only 192.168.0.0/16 in the result net_host - Determine which networks are parent of the provided IP, e.g. host__net_host=\"10.0.0.1\" would include 10.0.0.1/32 and 10.0.0.0/24 in the result net_host_contained - Given a network, select IPs whose host address (regardless of its subnet mask) falls within that network , e.g. host__net_host_contained=\"10.0.0.0/24\" would include hosts 10.0.0.1/8 and 10.0.0.254/32 in the result net_in - Given a list of networks, select addresses (regardless of their subnet masks) within those networks, e.g. host__net_in=[\"10.0.0.0/24\", \"2001:db8::/64\"] would include hosts 10.0.0.1/16 and 2001:db8::1/65 in the result family - Given an IP address family of 4 or 6, provide hosts or networks that are that IP version type, e.g. host__family=6 would include 2001:db8::1 in the result Note: The fields denoted with ** are only supported in the MySQL dialect (and not Postgresql) at the current time.","title":"Filtering"},{"location":"rest-api/filtering.html#rest-api-filtering","text":"","title":"REST API Filtering"},{"location":"rest-api/filtering.html#filtering-objects","text":"The objects returned by an API list endpoint can be filtered by attaching one or more query parameters to the request URL. For example, GET /api/dcim/sites/?status=active will return only sites with a status of \"active.\" Multiple parameters can be joined to further narrow results. For example, GET /api/dcim/sites/?status=active&region=europe will return only active sites within the Europe region. Generally, passing multiple values for a single parameter will result in a logical OR operation. For example, GET /api/dcim/sites/?region=north-america&region=south-america will return sites in North America or South America. However, a logical AND operation will be used in instances where a field may have multiple values, such as tags. For example, GET /api/dcim/sites/?tag=foo&tag=bar will return only sites which have both the \"foo\" and \"bar\" tags applied. Changed in version 1.4.0 If STRICT_FILTERING is True (its default value), unrecognized filter parameters now result in a 400 Bad Request response instead of being silently ignored.","title":"Filtering Objects"},{"location":"rest-api/filtering.html#filtering-by-choice-field","text":"Some models have fields which are limited to specific choices, such as the status field on the Prefix model. To find all available choices for this field, make an authenticated OPTIONS request to the model's list endpoint, and use jq to extract the relevant parameters: $ curl -s -X OPTIONS \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ http://nautobot/api/ipam/prefixes/ | jq \".actions.POST.status.choices\" [ { \"value\": \"container\", \"display\": \"Container\" }, { \"value\": \"active\", \"display\": \"Active\" }, { \"value\": \"reserved\", \"display\": \"Reserved\" }, { \"value\": \"deprecated\", \"display\": \"Deprecated\" } ] Note The above works only if the API token used to authenticate the request has permission to make a POST request to this endpoint.","title":"Filtering by Choice Field"},{"location":"rest-api/filtering.html#filtering-by-custom-field","text":"To filter results by a custom field value, prepend cf_ to the custom field name. For example, the following query will return only sites where a custom field named foo is equal to 123: GET /api/dcim/sites/?cf_foo=123 Note For custom field filters, due to historical details of implementation, only a single filter value can be specified when matching a given field. In other words, in the above example, you could not add &cf_foo=456 to the query in order to get all sites where custom field foo is 123 or 456; instead you would need to run two separate queries. This restriction does not apply to custom field filters using lookup expressions (next section) and will likely be changed in a future major version of Nautobot. Custom fields can be mixed with built-in fields to further narrow results. When creating a custom string field, the type of filtering selected (loose versus exact) determines whether partial or full matching is used. Added in version 1.4.0 Custom fields can use the lookup expressions listed in the next section by prepending cf_ to the custom field name (and not the slug ) followed by the required lookup type (see below).","title":"Filtering by Custom Field"},{"location":"rest-api/filtering.html#lookup-expressions","text":"Certain model fields (including, in Nautobot 1.4.0 and later, custom fields of type text , url , select , integer , and date ) also support filtering using additional lookup expressions. This allows for negation and other context-specific filtering. These lookup expressions can be applied by adding a suffix to the desired field's name, e.g. mac_address__n . In this case, the filter expression is for negation and it is separated by two underscores. Below are the lookup expressions that are supported across different field types.","title":"Lookup Expressions"},{"location":"rest-api/filtering.html#numeric-fields","text":"Numeric-based fields (ASN, VLAN ID, etc.) support these lookup expressions: n - not equal to (negation) lt - less than lte - less than or equal gt - greater than gte - greater than or equal","title":"Numeric Fields"},{"location":"rest-api/filtering.html#string-fields","text":"String-based (char) fields (Name, Address, etc.) support these lookup expressions: n - not equal to (negation) ic - case-insensitive contains nic - negated case-insensitive contains isw - case-insensitive starts-with nisw - negated case-insensitive starts-with iew - case-insensitive ends-with niew - negated case-insensitive ends-with ie - case-insensitive exact match nie - negated case-insensitive exact match Added in version 1.3.0 re - case-sensitive regular expression match nre - negated case-sensitive regular expression match ire - case-insensitive regular expression match nire - negated case-insensitive regular expression match","title":"String Fields"},{"location":"rest-api/filtering.html#foreign-keys-other-fields","text":"Certain other fields, namely foreign key relationships support just the negation expression: n .","title":"Foreign Keys &amp; Other Fields"},{"location":"rest-api/filtering.html#network-and-host-fields","text":"There are Custom Lookups built for the VarbinaryIPField field types. While the VarbinaryIPField is applied to fields for network, host, and broadcast, the below filters only apply to network and host. The design makes an assumption that there is in fact a broadcast (of type VarbinaryIPField ) and prefix_length (of type Integer ) within the same model. This assumption is used to understand the relevant scope of the network in question and is important to note when extending the Nautobot core or plugin data model. ** exact - An exact match of an IP or network address, e.g. host__exact=\"10.0.0.1\" ** iexact - An exact match of an IP or network address, e.g. host__iexact=\"10.0.0.1\" ** startswith - Determine if IP or network starts with the value provided, e.g. host__startswith=\"10.0.0.\" ** istartswith - Determine if IP or network starts with the value provided, e.g. host__istartswith=\"10.0.0.\" ** endswith - Determine if IP or network ends with the value provided, e.g. host__endswith=\"0.1\" ** iendswith - Determine if IP or network ends with the value provided, e.g. host__iendswith=\"0.1\" ** regex - Determine if IP or network matches the pattern provided, e.g. host__regex=r\"10\\.(.*)\\.1 ** iregex - Determine if IP or network matches the pattern provided, e.g. host__iregex=r\"10\\.(.*)\\.1 net_contained - Given a network, determine which networks are contained within the provided e.g. network__net_contained=\"192.0.0.0/8\" would include 192.168.0.0/24 in the result net_contained_or_equal - Given a network, determine which networks are contained or is within the provided e.g. network__net_contained_or_equal=\"192.0.0.0/8\" would include 192.168.0.0/24 and 192.0.0.0/8 in the result net_contains - Given a network, determine which networks contain the provided network e.g. network__net_contains=\"192.168.0.0/16\" would include 192.0.0.0/8 in the result net_contains_or_equals - Given a network, determine which networks contain or is the provided network e.g. network__net_contains=\"192.168.0.0/16\" would include 192.0.0.0/8 and 192.168.0.0/16 in the result net_equals - Given a network, determine which which networks are an exact match. e.g. network__net_equals=\"192.168.0.0/16\" would include only 192.168.0.0/16 in the result net_host - Determine which networks are parent of the provided IP, e.g. host__net_host=\"10.0.0.1\" would include 10.0.0.1/32 and 10.0.0.0/24 in the result net_host_contained - Given a network, select IPs whose host address (regardless of its subnet mask) falls within that network , e.g. host__net_host_contained=\"10.0.0.0/24\" would include hosts 10.0.0.1/8 and 10.0.0.254/32 in the result net_in - Given a list of networks, select addresses (regardless of their subnet masks) within those networks, e.g. host__net_in=[\"10.0.0.0/24\", \"2001:db8::/64\"] would include hosts 10.0.0.1/16 and 2001:db8::1/65 in the result family - Given an IP address family of 4 or 6, provide hosts or networks that are that IP version type, e.g. host__family=6 would include 2001:db8::1 in the result Note: The fields denoted with ** are only supported in the MySQL dialect (and not Postgresql) at the current time.","title":"Network and Host Fields"},{"location":"rest-api/overview.html","text":"REST API Overview \u00b6 What is a REST API? \u00b6 REST stands for representational state transfer . It's a particular type of API which employs HTTP requests and JavaScript Object Notation (JSON) to facilitate create, retrieve, update, and delete (CRUD) operations on objects within an application. Each type of operation is associated with a particular HTTP verb: GET : Retrieve an object or list of objects POST : Create an object PUT / PATCH : Modify an existing object. PUT requires all mandatory fields to be specified, while PATCH only expects the field that is being modified to be specified. DELETE : Delete an existing object Additionally, the OPTIONS verb can be used to inspect a particular REST API endpoint and return all supported actions and their available parameters. One of the primary benefits of a REST API is its human-friendliness. Because it utilizes HTTP and JSON, it's very easy to interact with Nautobot data on the command line using common tools. For example, we can request an IP address from Nautobot and output the JSON using curl and jq . The following command makes an HTTP GET request for information about a particular IP address, identified by its primary key, and uses jq to present the raw JSON data returned in a more human-friendly format. (Piping the output through jq isn't strictly required but makes it much easier to read.) curl -s http://nautobot/api/ipam/ip-addresses/c557df87-9a63-4555-bfd1-21cea2f6aac3/ | jq '.' { \"id\" : 2954 , \"url\" : \"http://nautobot/api/ipam/ip-addresses/c557df87-9a63-4555-bfd1-21cea2f6aac3/\" , \"family\" : { \"value\" : 4 , \"label\" : \"IPv4\" }, \"address\" : \"192.168.0.42/26\" , \"vrf\" : null , \"tenant\" : null , \"status\" : { \"value\" : \"active\" , \"label\" : \"Active\" }, \"role\" : null , \"assigned_object_type\" : \"dcim.interface\" , \"assigned_object_id\" : \"9fd066d2-135c-4005-b032-e0551cc61cec\" , \"assigned_object\" : { \"id\" : \"9fd066d2-135c-4005-b032-e0551cc61cec\" , \"url\" : \"http://nautobot/api/dcim/interfaces/9fd066d2-135c-4005-b032-e0551cc61cec/\" , \"device\" : { \"id\" : \"6a522ebb-5739-4c5c-922f-ab4a2dc12eb0\" , \"url\" : \"http://nautobot/api/dcim/devices/6a522ebb-5739-4c5c-922f-ab4a2dc12eb0/\" , \"name\" : \"router1\" , \"display\" : \"router1\" }, \"name\" : \"et-0/1/2\" , \"cable\" : null , \"connection_status\" : null }, \"nat_inside\" : null , \"nat_outside\" : null , \"dns_name\" : \"\" , \"description\" : \"Example IP address\" , \"tags\" : [], \"custom_fields\" : {}, \"created\" : \"2020-08-04\" , \"last_updated\" : \"2020-08-04T14:12:39.666885Z\" } Each attribute of the IP address is expressed as an attribute of the JSON object. Fields may include their own nested objects, as in the case of the assigned_object field above. Every object includes a primary key named id which uniquely identifies it in the database. Interactive Documentation \u00b6 Comprehensive, interactive documentation of all REST API endpoints is available on a running Nautobot instance at /api/docs/ . This interface provides a convenient sandbox for researching and experimenting with specific endpoints and request types. The API itself can also be explored using a web browser by navigating to its root at /api/ . Added in version 1.3.0 You can view or explore a specific REST API version by adding the API version as a query parameter, for example /api/docs/?api_version=1.3 or /api/?api_version=1.2 Endpoint Hierarchy \u00b6 Nautobot's entire REST API is housed under the API root at https://<hostname>/api/ . The URL structure is divided at the root level by application: circuits, DCIM, extras, IPAM, plugins, tenancy, users, and virtualization. Within each application exists a separate path for each model. For example, the provider and circuit objects are located under the \"circuits\" application: /api/circuits/providers/ /api/circuits/circuits/ Likewise, the site, rack, and device objects are located under the \"DCIM\" application: /api/dcim/sites/ /api/dcim/racks/ /api/dcim/devices/ The full hierarchy of available endpoints can be viewed by navigating to the API root in a web browser. Each model generally has two views associated with it: a list view and a detail view. The list view is used to retrieve a list of multiple objects and to create new objects. The detail view is used to retrieve, update, or delete an single existing object. All objects are referenced by their UUID primary key ( id ). /api/dcim/devices/ - List existing devices or create a new device /api/dcim/devices/6a522ebb-5739-4c5c-922f-ab4a2dc12eb0/ - Retrieve, update, or delete the device with ID 6a522ebb-5739-4c5c-922f-ab4a2dc12eb0 Lists of objects can be filtered using a set of query parameters. For example, to find all interfaces belonging to the device with ID 6a522ebb-5739-4c5c-922f-ab4a2dc12eb0: GET /api/dcim/interfaces/?device_id=6a522ebb-5739-4c5c-922f-ab4a2dc12eb0 See the filtering documentation for more details. Versioning \u00b6 Added in version 1.3.0 As of Nautobot 1.3, the REST API supports multiple versions. A REST API client may request a given API version by including a major.minor Nautobot version number in its request in one of two ways: A client may include a version in its HTTP Accept header, for example Accept: application/json; version=1.3 A client may include an api_version as a URL query parameter, for example /api/extras/jobs/?api_version=1.3 Generally the former approach is recommended when writing automated API integrations, as it can be set as a general request header alongside the authentication token and re-used across a series of REST API interactions, while the latter approach may be more convenient when initially exploring the REST API via the interactive documentation as described above. Default Versions and Backward Compatibility \u00b6 By default, a REST API request that does not specify an API version number will default to compatibility with a specified Nautobot version. This default REST API version can be expected to remain constant throughout the lifespan of a given Nautobot major release. Note For Nautobot 1.x, the default API behavior is to be compatible with the REST API of Nautobot version 1.2, in other words, for all Nautobot 1.x versions (beginning with Nautobot 1.2.0), Accept: application/json is functionally equivalent to Accept: application/json; version=1.2 . Tip The default REST API version compatibility may change in a subsequent Nautobot major release, so as a best practice, it is recommended that a REST API client should always request the exact Nautobot REST API version that it is compatible with, rather than relying on the default behavior to remain constant. Tip Any successful REST API response will include an API-Version header showing the API version that is in use for the specific API request being handled. Non-Breaking Changes \u00b6 Non-breaking (forward- and backward-compatible) REST API changes may be introduced in major or minor Nautobot releases. Since these changes are non-breaking, they will not correspond to the introduction of a new API version, but will be added seamlessly to the existing API version, and so will immediately be available to existing REST API clients. Examples would include: Addition of new fields in GET responses Added support for new, optional fields in POST/PUT/PATCH requests Deprecation (but not removal) of existing fields Important There is no way to \"opt out\" of backwards-compatible enhancements to the REST API; because they are fully backwards-compatible there should never be a need to do so. Thus, for example, a client requesting API version 1.2 from a Nautobot 1.3 server may actually receive the (updated but still backwards-compatible) 1.3 API version as a response. For this reason, clients should always default to ignoring additional fields in an API response that they do not understand, rather than reporting an error. Breaking Changes \u00b6 Breaking (non-backward-compatible) REST API changes also may be introduced in major or minor Nautobot releases. Examples would include: Removal of deprecated fields Addition of new, required fields in POST/PUT/PATCH requests Changed field types (for example, changing a single value to a list of values) Redesigned API (for example, listing and accessing Job instances by UUID primary-key instead of by class-path string) Per Nautobot's feature-deprecation policy , the previous REST API version will continue to be supported for some time before eventually being removed. Important When breaking changes are introduced in a minor release, for compatibility as described above, the default REST API behavior within the remainder of the current major release cycle will continue to be the previous (unchanged) API version. API clients must \"opt in\" to the new version of the API by explicitly requesting the new API version. Tip This is another reason to always specify the exact major.minor Nautobot REST API version when developing a REST API client integration, as it guarantees that the client will be receiving the latest API feature set available in that release rather than possibly defaulting to an older REST API version that is still default but is now deprecated. Example of API Version Behavior \u00b6 As an example, let us say that Nautobot 1.3 introduced a new, non-backwards-compatible REST API for the /api/extras/jobs/ endpoint, and also introduced a new, backwards-compatible set of additional fields on the /api/dcim/sites/ endpoint. Depending on what API version a REST client interacting with Nautobot 1.3 specified (or didn't specify), it would see the following responses from the server: API endpoint Requested API version Response /api/extras/jobs/ (unspecified) Deprecated 1.2-compatible REST API /api/extras/jobs/ 1.2 Deprecated 1.2-compatible REST API /api/extras/jobs/ 1.3 New/updated 1.3-compatible REST API Important Note again that if not specifying an API version, the client would not receive the latest API version when breaking changes are present. Even though the server had Nautobot version 1.3, the default Jobs REST API behavior would be that of Nautobot 1.2. Only by actually requesting API version 1.3 was the client able to access the new Jobs REST API. API endpoint Requested API version Response /api/dcim/sites/ (unspecified) 1.3-updated, 1.2-compatible REST API /api/dcim/sites/ 1.2 1.3-updated, 1.2-compatible REST API /api/dcim/sites/ 1.3 1.3-updated, 1.2-compatible REST API API endpoint Requested API version Response /api/dcim/racks/ (unspecified) 1.2-compatible REST API (unchanged) /api/dcim/racks/ 1.2 1.2-compatible REST API (unchanged) /api/dcim/racks/ 1.3 1.3-compatible REST API (unchanged from 1.2) APISelect with versioning capability \u00b6 Added in version 1.3.0 The constructor for Nautobot's APISelect / APISelectMultiple UI widgets now includes an optional api_version argument which if set overrides the default API version of the request. Serialization \u00b6 The REST API employs two types of serializers to represent model data: base serializers and nested serializers. The base serializer is used to present the complete view of a model. This includes all database table fields which comprise the model, and may include additional metadata. A base serializer includes relationships to parent objects, but does not include child objects. For example, the VLANSerializer includes a nested representation its parent VLANGroup (if any), but does not include any assigned Prefixes. { \"id\" : 1048 , \"site\" : { \"id\" : \"09c9e21c-e038-44fd-be9a-43aef97bff8f\" , \"url\" : \"http://nautobot/api/dcim/sites/09c9e21c-e038-44fd-be9a-43aef97bff8f/\" , \"name\" : \"Corporate HQ\" , \"slug\" : \"corporate-hq\" }, \"group\" : { \"id\" : \"eccc0964-9fab-43bc-bb77-66b1be08f64b\" , \"url\" : \"http://nautobot/api/ipam/vlan-groups/eccc0964-9fab-43bc-bb77-66b1be08f64b/\" , \"name\" : \"Production\" , \"slug\" : \"production\" }, \"vid\" : 101 , \"name\" : \"Users-Floor1\" , \"tenant\" : null , \"status\" : { \"value\" : \"active\" , \"label\" : \"Active\" }, \"role\" : { \"id\" : \"a1fd5e46-a85e-48c3-a2f4-3c2ec2bb2464\" , \"url\" : \"http://nautobot/api/ipam/roles/a1fd5e46-a85e-48c3-a2f4-3c2ec2bb2464/\" , \"name\" : \"User Access\" , \"slug\" : \"user-access\" }, \"description\" : \"\" , \"display\" : \"101 (Users-Floor1)\" , \"custom_fields\" : {} } Related Objects \u00b6 Related objects (e.g. ForeignKey fields) are represented using nested serializers. A nested serializer provides a minimal representation of an object, including only its direct URL and enough information to display the object to a user. When performing write API actions ( POST , PUT , and PATCH ), related objects may be specified by either UUID (primary key), or by a set of attributes sufficiently unique to return the desired object. For example, when creating a new device, its rack can be specified by Nautobot ID (PK): { \"name\" : \"MyNewDevice\" , \"rack\" : \"7f3ca431-8103-45cc-a9ce-b94c1f784a1d\" , ... } Or by a set of nested attributes which uniquely identify the rack: { \"name\" : \"MyNewDevice\" , \"rack\" : { \"site\" : { \"name\" : \"Equinix DC6\" }, \"name\" : \"R204\" }, ... } Note that if the provided parameters do not return exactly one object, a validation error is raised. Generic Relations \u00b6 Some objects within Nautobot have attributes which can reference an object of multiple types, known as generic relations . For example, an IP address can be assigned to either a device interface or a virtual machine interface. When making this assignment via the REST API, we must specify two attributes: assigned_object_type - The content type of the assigned object, defined as <app>.<model> assigned_object_id - The assigned object's UUID Together, these values identify a unique object in Nautobot. The assigned object (if any) is represented by the assigned_object attribute on the IP address model. curl -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/ipam/ip-addresses/ \\ --data '{ \"address\": \"192.0.2.1/24\", \"assigned_object_type\": \"dcim.interface\", \"assigned_object_id\": \"e824bc29-623f-407e-8aa8-828f4c0b98ee\" }' { \"id\" : \"e2f29f8f-002a-4c4a-9d19-24cc7549e715\" , \"url\" : \"http://nautobot/api/ipam/ip-addresses/56296/\" , \"assigned_object_type\" : \"dcim.interface\" , \"assigned_object_id\" : \"e824bc29-623f-407e-8aa8-828f4c0b98ee\" , \"assigned_object\" : { \"id\" : \"e824bc29-623f-407e-8aa8-828f4c0b98ee\" , \"url\" : \"http://nautobot/api/dcim/interfaces/e824bc29-623f-407e-8aa8-828f4c0b98ee/\" , \"device\" : { \"id\" : \"76816a69-db2c-40e6-812d-115c61156e21\" , \"url\" : \"http://nautobot/api/dcim/devices/76816a69-db2c-40e6-812d-115c61156e21/\" , \"name\" : \"device105\" , \"display\" : \"device105\" }, \"name\" : \"ge-0/0/0\" , \"cable\" : null , \"connection_status\" : null }, ... } If we wanted to assign this IP address to a virtual machine interface instead, we would have set assigned_object_type to virtualization.vminterface and updated the object ID appropriately. Pagination \u00b6 API responses which contain a list of many objects will be paginated for efficiency. The root JSON object returned by a list endpoint contains the following attributes: count : The total number of all objects matching the query next : A hyperlink to the next page of results (if applicable) previous : A hyperlink to the previous page of results (if applicable) results : The list of objects on the current page Here is an example of a paginated response: HTTP 200 OK Allow : GET , POST , PUT , PATCH , DELETE , HEAD , OPTIONS API - Versio n : 1.2 Co ntent - Type : applica t io n /jso n Vary : Accep t { \"count\" : 2861 , \"next\" : \"http://nautobot/api/dcim/devices/?limit=50&offset=50\" , \"previous\" : null , \"results\" : [ { \"id\" : \"fa069c4b-4f6e-4349-88ac-8b6baf9d70c5\" , \"name\" : \"Device1\" , ... }, { \"id\" : \"a37df58c-8bf3-4b97-bad5-301ef3880bea\" , \"name\" : \"Device2\" , ... }, ... ] } The default page is determined by the PAGINATE_COUNT configuration parameter, which defaults to 50. However, this can be overridden per request by specifying the desired offset and limit query parameters. For example, if you wish to retrieve a hundred devices at a time, you would make a request for: http://nautobot/api/dcim/devices/?limit=100 The response will return devices 1 through 100. The URL provided in the next attribute of the response will return devices 101 through 200: { \"count\" : 2861 , \"next\" : \"http://nautobot/api/dcim/devices/?limit=100&offset=100\" , \"previous\" : null , \"results\" : [ ... ] } The maximum number of objects that can be returned is limited by the MAX_PAGE_SIZE configuration parameter, which is 1000 by default. Setting this to 0 or None will remove the maximum limit. An API consumer can then pass ?limit=0 to retrieve all matching objects with a single request. Warning Disabling the page size limit introduces a potential for very resource-intensive requests, since one API request can effectively retrieve an entire table from the database. Interacting with Objects \u00b6 Retrieving Multiple Objects \u00b6 To query Nautobot for a list of objects, make a GET request to the model's list endpoint. Objects are listed under the response object's results parameter. Specifying the Accept header with the Nautobot API version is not required, but is strongly recommended. curl -s -X GET \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/ipam/ip-addresses/ | jq '.' { \"count\" : 42031 , \"next\" : \"http://nautobot/api/ipam/ip-addresses/?limit=50&offset=50\" , \"previous\" : null , \"results\" : [ { \"id\" : \"bd307eca-de34-4bda-9195-d69ca52206d6\" , \"address\" : \"192.0.2.1/24\" , ... }, { \"id\" : \"6c52e918-4f0c-4c50-ae49-6bef22c97fd5\" , \"address\" : \"192.0.2.2/24\" , ... }, { \"id\" : \"b8cde1ee-1b86-4ea4-a884-041c472d8999\" , \"address\" : \"192.0.2.3/24\" , ... }, ... ] } Retrieving a Single Object \u00b6 To query Nautobot for a single object, make a GET request to the model's detail endpoint specifying its UUID. Note Note that the trailing slash is required. Omitting this will return a 302 redirect. curl -s -X GET \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/ipam/ip-addresses/bd307eca-de34-4bda-9195-d69ca52206d6/ | jq '.' { \"id\" : \"bd307eca-de34-4bda-9195-d69ca52206d6\" , \"address\" : \"192.0.2.1/24\" , ... } Brief Format \u00b6 The GET API endpoints support an optional \"brief\" format, which returns only a minimal representation of each object in the response. This is useful when you need only a list of available objects without any related data, such as when populating a drop-down list in a form. As an example, the default (complete) format of an IP address looks like this: GET /api/ipam/prefixes/7d2d24ac-4737-4fc1-a850-b30366618f3d/ { \"id\" : \"7d2d24ac-4737-4fc1-a850-b30366618f3d\" , \"url\" : \"http://nautobot/api/ipam/prefixes/7d2d24ac-4737-4fc1-a850-b30366618f3d/\" , \"family\" : { \"value\" : 4 , \"label\" : \"IPv4\" }, \"prefix\" : \"192.0.2.0/24\" , \"site\" : { \"id\" : \"b9edf2ee-cad9-48be-9921-006294bff730\" , \"url\" : \"http://nautobot/api/dcim/sites/b9edf2ee-cad9-48be-9921-006294bff730/\" , \"name\" : \"Site 23A\" , \"slug\" : \"site-23a\" }, \"vrf\" : null , \"tenant\" : null , \"vlan\" : null , \"status\" : { \"value\" : \"container\" , \"label\" : \"Container\" }, \"role\" : { \"id\" : \"ae1470bc-a858-4ce7-b9ce-dd1cd46333fe\" , \"url\" : \"http://nautobot/api/ipam/roles/ae1470bc-a858-4ce7-b9ce-dd1cd46333fe/\" , \"name\" : \"Staging\" , \"slug\" : \"staging\" }, \"is_pool\" : false , \"description\" : \"Example prefix\" , \"tags\" : [], \"custom_fields\" : {}, \"created\" : \"2018-12-10\" , \"last_updated\" : \"2019-03-01T20:02:46.173540Z\" } The brief format is much more terse: GET /api/ipam/prefixes/7d2d24ac-4737-4fc1-a850-b30366618f3d/?brief=1 { \"id\" : \"7d2d24ac-4737-4fc1-a850-b30366618f3d\" , \"url\" : \"http://nautobot/api/ipam/prefixes/7d2d24ac-4737-4fc1-a850-b30366618f3d/\" , \"family\" : 4 , \"prefix\" : \"10.40.3.0/24\" } The brief format is supported for both lists and individual objects. Retrieving Object Relationships and Relationship Associations \u00b6 Added in version 1.4.0 Objects that are associated with another object by a custom Relationship are also retrievable and modifiable via the REST API. Due to the additional processing overhead involved in retrieving and representing these relationships, they are not included in default REST API GET responses. To include relationships data, pass include=relationships as a query parameter; in this case an additional key, \"relationships\" , will be included in the API response, as seen below: GET /api/dcim/sites/f472bb77-7f56-4e79-ac25-2dc73eb63924/?include=relationships { \"id\" : \"f472bb77-7f56-4e79-ac25-2dc73eb63924\" , \"display\" : \"alpha\" , \"url\" : \"http://nautobot/api/dcim/sites/f472bb77-7f56-4e79-ac25-2dc73eb63924/\" , ... \"relationships\" : { \"site-to-vrf\" : { \"id\" : \"e74cb7f7-15b0-499d-9401-a0f01cb96a9a\" , \"url\" : \"/api/extras/relationships/e74cb7f7-15b0-499d-9401-a0f01cb96a9a/\" , \"name\" : \"Single Site to Single VRF\" , \"type\" : \"one-to-one\" , \"destination\" : { \"label\" : \"VRF\" , \"object_type\" : \"ipam.vrf\" , \"objects\" : [ { \"id\" : \"36641ba0-50d6-43be-b9b5-86aa992402e0\" , \"url\" : \"http://nautobot/api/ipam/vrfs/36641ba0-50d6-43be-b9b5-86aa992402e0/\" , \"name\" : \"red\" , \"rd\" : null , \"display\" : \"red\" } ] } }, \"vrfs-to-sites\" : { \"id\" : \"e39c53e4-78cf-4572-b116-1d8830b81b2e\" , \"url\" : \"/api/extras/relationships/e39c53e4-78cf-4572-b116-1d8830b81b2e/\" , \"name\" : \"VRFs to Sites\" , \"type\" : \"many-to-many\" , \"source\" : { \"label\" : \"VRFs\" , \"object_type\" : \"ipam.vrf\" , \"objects\" : [] } }, } } Under the \"relationships\" key, there will be one key per Relationship that applies to this model, corresponding to the slug of that Relationship. Under each slug key, there will be information about the Relationship itself, plus any of \"source\" , \"destination\" , or \"peer\" keys (depending on the type and directionality of the Relationship). Under the \"source\" , \"destination\" , or \"peer\" keys, there are the following keys: \"label\" - a human-readable description of the related objects \"object_type\" - the content-type of the related objects \"objects\" - a list of all related objects, each represented in nested-serializer form as described under Related Objects above. In the example above we can see that a single VRF, green , is a destination for the site-to-vrf Relationship from this Site, while there are currently no VRFs associated as sources for the vrfs-to-sites Relationship to this Site. Excluding Config Contexts \u00b6 When retrieving devices and virtual machines via the REST API, each will include its rendered configuration context data by default. Users with large amounts of context data will likely observe suboptimal performance when returning multiple objects, particularly with very high page sizes. To combat this, context data may be excluded from the response data by attaching the query parameter ?exclude=config_context to the request. This parameter works for both list and detail views. Creating a New Object \u00b6 To create a new object, make a POST request to the model's list endpoint with JSON data pertaining to the object being created. Note that a REST API token is required for all write operations; see the authentication documentation for more information. Also be sure to set the Content-Type HTTP header to application/json . As always, it's a good practice to also set the Accept HTTP header to include the requested REST API version. curl -s -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/ipam/prefixes/ \\ --data '{\"prefix\": \"192.0.2.0/24\", \"site\": 8df9e629-4338-438b-8ea9-06114f7be08e}' | jq '.' { \"id\" : \"48df6965-0fcb-4155-b5f8-00fe8b9b01af\" , \"url\" : \"http://nautobot/api/ipam/prefixes/48df6965-0fcb-4155-b5f8-00fe8b9b01af/\" , \"family\" : { \"value\" : 4 , \"label\" : \"IPv4\" }, \"prefix\" : \"192.0.2.0/24\" , \"site\" : { \"id\" : \"8df9e629-4338-438b-8ea9-06114f7be08e\" , \"url\" : \"http://nautobot/api/dcim/sites/8df9e629-4338-438b-8ea9-06114f7be08e/\" , \"name\" : \"US-East 4\" , \"slug\" : \"us-east-4\" }, \"vrf\" : null , \"tenant\" : null , \"vlan\" : null , \"status\" : { \"value\" : \"active\" , \"label\" : \"Active\" }, \"role\" : null , \"is_pool\" : false , \"description\" : \"\" , \"tags\" : [], \"custom_fields\" : {}, \"created\" : \"2020-08-04\" , \"last_updated\" : \"2020-08-04T20:08:39.007125Z\" } Creating Multiple Objects \u00b6 To create multiple instances of a model using a single request, make a POST request to the model's list endpoint with a list of JSON objects representing each instance to be created. If successful, the response will contain a list of the newly created instances. The example below illustrates the creation of three new sites. curl -X POST -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/dcim/sites/ \\ --data '[ {\"name\": \"Site 1\", \"slug\": \"site-1\", \"region\": {\"name\": \"United States\"}}, {\"name\": \"Site 2\", \"slug\": \"site-2\", \"region\": {\"name\": \"United States\"}}, {\"name\": \"Site 3\", \"slug\": \"site-3\", \"region\": {\"name\": \"United States\"}} ]' [ { \"id\" : \"0238a4e3-66f2-455a-831f-5f177215de0f\" , \"url\" : \"http://nautobot/api/dcim/sites/0238a4e3-66f2-455a-831f-5f177215de0f/\" , \"name\" : \"Site 1\" , ... }, { \"id\" : \"33ac3a3b-0ee7-49b7-bf2a-244096051dc0\" , \"url\" : \"http://nautobot/api/dcim/sites/33ac3a3b-0ee7-49b7-bf2a-244096051dc0/\" , \"name\" : \"Site 2\" , ... }, { \"id\" : \"10b3134d-960b-4794-ad18-0e73edd357c4\" , \"url\" : \"http://nautobot/api/dcim/sites/10b3134d-960b-4794-ad18-0e73edd357c4/\" , \"name\" : \"Site 3\" , ... } ] Updating an Object \u00b6 To modify an object which has already been created, make a PATCH request to the model's detail endpoint specifying its UUID. Include any data which you wish to update on the object. As with object creation, the Authorization and Content-Type headers must also be specified, and specifying the Accept header is also strongly recommended. curl -s -X PATCH \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/ipam/prefixes/b484b0ac-12e3-484a-84c0-aa17955eaedc/ \\ --data '{\"status\": \"reserved\"}' | jq '.' { \"id\" : \"48df6965-0fcb-4155-b5f8-00fe8b9b01af\" , \"url\" : \"http://nautobot/api/ipam/prefixes/48df6965-0fcb-4155-b5f8-00fe8b9b01af/\" , \"family\" : { \"value\" : 4 , \"label\" : \"IPv4\" }, \"prefix\" : \"192.0.2.0/24\" , \"site\" : { \"id\" : \"8df9e629-4338-438b-8ea9-06114f7be08e\" , \"url\" : \"http://nautobot/api/dcim/sites/8df9e629-4338-438b-8ea9-06114f7be08e/\" , \"name\" : \"US-East 4\" , \"slug\" : \"us-east-4\" }, \"vrf\" : null , \"tenant\" : null , \"vlan\" : null , \"status\" : { \"value\" : \"reserved\" , \"label\" : \"Reserved\" }, \"role\" : null , \"is_pool\" : false , \"description\" : \"\" , \"tags\" : [], \"custom_fields\" : {}, \"created\" : \"2020-08-04\" , \"last_updated\" : \"2020-08-04T20:14:55.709430Z\" } PUT versus PATCH The Nautobot REST API support the use of either PUT or PATCH to modify an existing object. The difference is that a PUT request requires the user to specify a complete representation of the object being modified, whereas a PATCH request need include only the attributes that are being updated. For most purposes, using PATCH is recommended. Updating Relationship Associations \u00b6 Added in version 1.4.0 It is possible to modify the objects associated via Relationship with an object as part of a REST API PATCH request by specifying the \"relationships\" key, any or all of the relevant Relationships, and the list of desired related objects for each such Relationship. Since nested serializers are used for the related objects, they can be identified by ID (primary key) or by one or more attributes in a dictionary. For example, either of the following requests would be valid: { \"relationships\" : { \"site-to-vrf\" : { \"destination\" : { \"objects\" : [ { \"name\" : \"blue\" } ] } }, \"vrfs-to-sites\" : { \"source\" : { \"objects\" : [ { \"name\" : \"green\" }, { \"name\" : \"red\" }, ] } } } } { \"relationships\" : { \"site-to-vrf\" : { \"destination\" : { \"objects\" : [ \"3e3c58f9-4f63-44ba-acee-f0c42430eba7\" ] } } } } Note Relationship slugs can be omitted from the \"relationships\" dictionary, in which case the associations for that Relationship will be left unmodified. In the second example above, the existing association for the \"site-to-vrf\" Relationship would be replaced, but the \"vrfs-to-sites\" Relationship's associations would remain as-is. Updating Multiple Objects \u00b6 Multiple objects can be updated simultaneously by issuing a PUT or PATCH request to a model's list endpoint with a list of dictionaries specifying the UUID of each object to be deleted and the attributes to be updated. For example, to update sites with UUIDs 18de055e-3ea9-4cc3-ba78-b7eef6f0d589 and 1a414273-3d68-4586-ba22-6ae0a5702b8f to a status of \"active\", issue the following request: curl -s -X PATCH \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/dcim/sites/ \\ --data '[{\"id\": \"18de055e-3ea9-4cc3-ba78-b7eef6f0d589\", \"status\": \"active\"}, {\"id\": \"1a414273-3d68-4586-ba22-6ae0a5702b8f\", \"status\": \"active\"}]' Note that there is no requirement for the attributes to be identical among objects. For instance, it's possible to update the status of one site along with the name of another in the same request. Note The bulk update of objects is an all-or-none operation, meaning that if Nautobot fails to successfully update any of the specified objects (e.g. due a validation error), the entire operation will be aborted and none of the objects will be updated. Deleting an Object \u00b6 To delete an object from Nautobot, make a DELETE request to the model's detail endpoint specifying its UUID. The Authorization header must be included to specify an authorization token, however this type of request does not support passing any data in the body. curl -s -X DELETE \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/ipam/prefixes/48df6965-0fcb-4155-b5f8-00fe8b9b01af/ Note that DELETE requests do not return any data: If successful, the API will return a 204 (No Content) response. Note You can run curl with the verbose ( -v ) flag to inspect the HTTP response codes. Deleting Multiple Objects \u00b6 Nautobot supports the simultaneous deletion of multiple objects of the same type by issuing a DELETE request to the model's list endpoint with a list of dictionaries specifying the UUID of each object to be deleted. For example, to delete sites with UUIDs 18de055e-3ea9-4cc3-ba78-b7eef6f0d589, 1a414273-3d68-4586-ba22-6ae0a5702b8f, and c2516019-caf6-41f0-98a6-4276c1a73fa3, issue the following request: curl -s -X DELETE \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/dcim/sites/ \\ --data '[{\"id\": \"18de055e-3ea9-4cc3-ba78-b7eef6f0d589\"}, {\"id\": \"1a414273-3d68-4586-ba22-6ae0a5702b8f\"}, {\"id\": \"c2516019-caf6-41f0-98a6-4276c1a73fa3\"}]' Note The bulk deletion of objects is an all-or-none operation, meaning that if Nautobot fails to delete any of the specified objects (e.g. due a dependency by a related object), the entire operation will be aborted and none of the objects will be deleted.","title":"Overview"},{"location":"rest-api/overview.html#rest-api-overview","text":"","title":"REST API Overview"},{"location":"rest-api/overview.html#what-is-a-rest-api","text":"REST stands for representational state transfer . It's a particular type of API which employs HTTP requests and JavaScript Object Notation (JSON) to facilitate create, retrieve, update, and delete (CRUD) operations on objects within an application. Each type of operation is associated with a particular HTTP verb: GET : Retrieve an object or list of objects POST : Create an object PUT / PATCH : Modify an existing object. PUT requires all mandatory fields to be specified, while PATCH only expects the field that is being modified to be specified. DELETE : Delete an existing object Additionally, the OPTIONS verb can be used to inspect a particular REST API endpoint and return all supported actions and their available parameters. One of the primary benefits of a REST API is its human-friendliness. Because it utilizes HTTP and JSON, it's very easy to interact with Nautobot data on the command line using common tools. For example, we can request an IP address from Nautobot and output the JSON using curl and jq . The following command makes an HTTP GET request for information about a particular IP address, identified by its primary key, and uses jq to present the raw JSON data returned in a more human-friendly format. (Piping the output through jq isn't strictly required but makes it much easier to read.) curl -s http://nautobot/api/ipam/ip-addresses/c557df87-9a63-4555-bfd1-21cea2f6aac3/ | jq '.' { \"id\" : 2954 , \"url\" : \"http://nautobot/api/ipam/ip-addresses/c557df87-9a63-4555-bfd1-21cea2f6aac3/\" , \"family\" : { \"value\" : 4 , \"label\" : \"IPv4\" }, \"address\" : \"192.168.0.42/26\" , \"vrf\" : null , \"tenant\" : null , \"status\" : { \"value\" : \"active\" , \"label\" : \"Active\" }, \"role\" : null , \"assigned_object_type\" : \"dcim.interface\" , \"assigned_object_id\" : \"9fd066d2-135c-4005-b032-e0551cc61cec\" , \"assigned_object\" : { \"id\" : \"9fd066d2-135c-4005-b032-e0551cc61cec\" , \"url\" : \"http://nautobot/api/dcim/interfaces/9fd066d2-135c-4005-b032-e0551cc61cec/\" , \"device\" : { \"id\" : \"6a522ebb-5739-4c5c-922f-ab4a2dc12eb0\" , \"url\" : \"http://nautobot/api/dcim/devices/6a522ebb-5739-4c5c-922f-ab4a2dc12eb0/\" , \"name\" : \"router1\" , \"display\" : \"router1\" }, \"name\" : \"et-0/1/2\" , \"cable\" : null , \"connection_status\" : null }, \"nat_inside\" : null , \"nat_outside\" : null , \"dns_name\" : \"\" , \"description\" : \"Example IP address\" , \"tags\" : [], \"custom_fields\" : {}, \"created\" : \"2020-08-04\" , \"last_updated\" : \"2020-08-04T14:12:39.666885Z\" } Each attribute of the IP address is expressed as an attribute of the JSON object. Fields may include their own nested objects, as in the case of the assigned_object field above. Every object includes a primary key named id which uniquely identifies it in the database.","title":"What is a REST API?"},{"location":"rest-api/overview.html#interactive-documentation","text":"Comprehensive, interactive documentation of all REST API endpoints is available on a running Nautobot instance at /api/docs/ . This interface provides a convenient sandbox for researching and experimenting with specific endpoints and request types. The API itself can also be explored using a web browser by navigating to its root at /api/ . Added in version 1.3.0 You can view or explore a specific REST API version by adding the API version as a query parameter, for example /api/docs/?api_version=1.3 or /api/?api_version=1.2","title":"Interactive Documentation"},{"location":"rest-api/overview.html#endpoint-hierarchy","text":"Nautobot's entire REST API is housed under the API root at https://<hostname>/api/ . The URL structure is divided at the root level by application: circuits, DCIM, extras, IPAM, plugins, tenancy, users, and virtualization. Within each application exists a separate path for each model. For example, the provider and circuit objects are located under the \"circuits\" application: /api/circuits/providers/ /api/circuits/circuits/ Likewise, the site, rack, and device objects are located under the \"DCIM\" application: /api/dcim/sites/ /api/dcim/racks/ /api/dcim/devices/ The full hierarchy of available endpoints can be viewed by navigating to the API root in a web browser. Each model generally has two views associated with it: a list view and a detail view. The list view is used to retrieve a list of multiple objects and to create new objects. The detail view is used to retrieve, update, or delete an single existing object. All objects are referenced by their UUID primary key ( id ). /api/dcim/devices/ - List existing devices or create a new device /api/dcim/devices/6a522ebb-5739-4c5c-922f-ab4a2dc12eb0/ - Retrieve, update, or delete the device with ID 6a522ebb-5739-4c5c-922f-ab4a2dc12eb0 Lists of objects can be filtered using a set of query parameters. For example, to find all interfaces belonging to the device with ID 6a522ebb-5739-4c5c-922f-ab4a2dc12eb0: GET /api/dcim/interfaces/?device_id=6a522ebb-5739-4c5c-922f-ab4a2dc12eb0 See the filtering documentation for more details.","title":"Endpoint Hierarchy"},{"location":"rest-api/overview.html#versioning","text":"Added in version 1.3.0 As of Nautobot 1.3, the REST API supports multiple versions. A REST API client may request a given API version by including a major.minor Nautobot version number in its request in one of two ways: A client may include a version in its HTTP Accept header, for example Accept: application/json; version=1.3 A client may include an api_version as a URL query parameter, for example /api/extras/jobs/?api_version=1.3 Generally the former approach is recommended when writing automated API integrations, as it can be set as a general request header alongside the authentication token and re-used across a series of REST API interactions, while the latter approach may be more convenient when initially exploring the REST API via the interactive documentation as described above.","title":"Versioning"},{"location":"rest-api/overview.html#default-versions-and-backward-compatibility","text":"By default, a REST API request that does not specify an API version number will default to compatibility with a specified Nautobot version. This default REST API version can be expected to remain constant throughout the lifespan of a given Nautobot major release. Note For Nautobot 1.x, the default API behavior is to be compatible with the REST API of Nautobot version 1.2, in other words, for all Nautobot 1.x versions (beginning with Nautobot 1.2.0), Accept: application/json is functionally equivalent to Accept: application/json; version=1.2 . Tip The default REST API version compatibility may change in a subsequent Nautobot major release, so as a best practice, it is recommended that a REST API client should always request the exact Nautobot REST API version that it is compatible with, rather than relying on the default behavior to remain constant. Tip Any successful REST API response will include an API-Version header showing the API version that is in use for the specific API request being handled.","title":"Default Versions and Backward Compatibility"},{"location":"rest-api/overview.html#non-breaking-changes","text":"Non-breaking (forward- and backward-compatible) REST API changes may be introduced in major or minor Nautobot releases. Since these changes are non-breaking, they will not correspond to the introduction of a new API version, but will be added seamlessly to the existing API version, and so will immediately be available to existing REST API clients. Examples would include: Addition of new fields in GET responses Added support for new, optional fields in POST/PUT/PATCH requests Deprecation (but not removal) of existing fields Important There is no way to \"opt out\" of backwards-compatible enhancements to the REST API; because they are fully backwards-compatible there should never be a need to do so. Thus, for example, a client requesting API version 1.2 from a Nautobot 1.3 server may actually receive the (updated but still backwards-compatible) 1.3 API version as a response. For this reason, clients should always default to ignoring additional fields in an API response that they do not understand, rather than reporting an error.","title":"Non-Breaking Changes"},{"location":"rest-api/overview.html#breaking-changes","text":"Breaking (non-backward-compatible) REST API changes also may be introduced in major or minor Nautobot releases. Examples would include: Removal of deprecated fields Addition of new, required fields in POST/PUT/PATCH requests Changed field types (for example, changing a single value to a list of values) Redesigned API (for example, listing and accessing Job instances by UUID primary-key instead of by class-path string) Per Nautobot's feature-deprecation policy , the previous REST API version will continue to be supported for some time before eventually being removed. Important When breaking changes are introduced in a minor release, for compatibility as described above, the default REST API behavior within the remainder of the current major release cycle will continue to be the previous (unchanged) API version. API clients must \"opt in\" to the new version of the API by explicitly requesting the new API version. Tip This is another reason to always specify the exact major.minor Nautobot REST API version when developing a REST API client integration, as it guarantees that the client will be receiving the latest API feature set available in that release rather than possibly defaulting to an older REST API version that is still default but is now deprecated.","title":"Breaking Changes"},{"location":"rest-api/overview.html#example-of-api-version-behavior","text":"As an example, let us say that Nautobot 1.3 introduced a new, non-backwards-compatible REST API for the /api/extras/jobs/ endpoint, and also introduced a new, backwards-compatible set of additional fields on the /api/dcim/sites/ endpoint. Depending on what API version a REST client interacting with Nautobot 1.3 specified (or didn't specify), it would see the following responses from the server: API endpoint Requested API version Response /api/extras/jobs/ (unspecified) Deprecated 1.2-compatible REST API /api/extras/jobs/ 1.2 Deprecated 1.2-compatible REST API /api/extras/jobs/ 1.3 New/updated 1.3-compatible REST API Important Note again that if not specifying an API version, the client would not receive the latest API version when breaking changes are present. Even though the server had Nautobot version 1.3, the default Jobs REST API behavior would be that of Nautobot 1.2. Only by actually requesting API version 1.3 was the client able to access the new Jobs REST API. API endpoint Requested API version Response /api/dcim/sites/ (unspecified) 1.3-updated, 1.2-compatible REST API /api/dcim/sites/ 1.2 1.3-updated, 1.2-compatible REST API /api/dcim/sites/ 1.3 1.3-updated, 1.2-compatible REST API API endpoint Requested API version Response /api/dcim/racks/ (unspecified) 1.2-compatible REST API (unchanged) /api/dcim/racks/ 1.2 1.2-compatible REST API (unchanged) /api/dcim/racks/ 1.3 1.3-compatible REST API (unchanged from 1.2)","title":"Example of API Version Behavior"},{"location":"rest-api/overview.html#apiselect-with-versioning-capability","text":"Added in version 1.3.0 The constructor for Nautobot's APISelect / APISelectMultiple UI widgets now includes an optional api_version argument which if set overrides the default API version of the request.","title":"APISelect with versioning capability"},{"location":"rest-api/overview.html#serialization","text":"The REST API employs two types of serializers to represent model data: base serializers and nested serializers. The base serializer is used to present the complete view of a model. This includes all database table fields which comprise the model, and may include additional metadata. A base serializer includes relationships to parent objects, but does not include child objects. For example, the VLANSerializer includes a nested representation its parent VLANGroup (if any), but does not include any assigned Prefixes. { \"id\" : 1048 , \"site\" : { \"id\" : \"09c9e21c-e038-44fd-be9a-43aef97bff8f\" , \"url\" : \"http://nautobot/api/dcim/sites/09c9e21c-e038-44fd-be9a-43aef97bff8f/\" , \"name\" : \"Corporate HQ\" , \"slug\" : \"corporate-hq\" }, \"group\" : { \"id\" : \"eccc0964-9fab-43bc-bb77-66b1be08f64b\" , \"url\" : \"http://nautobot/api/ipam/vlan-groups/eccc0964-9fab-43bc-bb77-66b1be08f64b/\" , \"name\" : \"Production\" , \"slug\" : \"production\" }, \"vid\" : 101 , \"name\" : \"Users-Floor1\" , \"tenant\" : null , \"status\" : { \"value\" : \"active\" , \"label\" : \"Active\" }, \"role\" : { \"id\" : \"a1fd5e46-a85e-48c3-a2f4-3c2ec2bb2464\" , \"url\" : \"http://nautobot/api/ipam/roles/a1fd5e46-a85e-48c3-a2f4-3c2ec2bb2464/\" , \"name\" : \"User Access\" , \"slug\" : \"user-access\" }, \"description\" : \"\" , \"display\" : \"101 (Users-Floor1)\" , \"custom_fields\" : {} }","title":"Serialization"},{"location":"rest-api/overview.html#related-objects","text":"Related objects (e.g. ForeignKey fields) are represented using nested serializers. A nested serializer provides a minimal representation of an object, including only its direct URL and enough information to display the object to a user. When performing write API actions ( POST , PUT , and PATCH ), related objects may be specified by either UUID (primary key), or by a set of attributes sufficiently unique to return the desired object. For example, when creating a new device, its rack can be specified by Nautobot ID (PK): { \"name\" : \"MyNewDevice\" , \"rack\" : \"7f3ca431-8103-45cc-a9ce-b94c1f784a1d\" , ... } Or by a set of nested attributes which uniquely identify the rack: { \"name\" : \"MyNewDevice\" , \"rack\" : { \"site\" : { \"name\" : \"Equinix DC6\" }, \"name\" : \"R204\" }, ... } Note that if the provided parameters do not return exactly one object, a validation error is raised.","title":"Related Objects"},{"location":"rest-api/overview.html#generic-relations","text":"Some objects within Nautobot have attributes which can reference an object of multiple types, known as generic relations . For example, an IP address can be assigned to either a device interface or a virtual machine interface. When making this assignment via the REST API, we must specify two attributes: assigned_object_type - The content type of the assigned object, defined as <app>.<model> assigned_object_id - The assigned object's UUID Together, these values identify a unique object in Nautobot. The assigned object (if any) is represented by the assigned_object attribute on the IP address model. curl -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/ipam/ip-addresses/ \\ --data '{ \"address\": \"192.0.2.1/24\", \"assigned_object_type\": \"dcim.interface\", \"assigned_object_id\": \"e824bc29-623f-407e-8aa8-828f4c0b98ee\" }' { \"id\" : \"e2f29f8f-002a-4c4a-9d19-24cc7549e715\" , \"url\" : \"http://nautobot/api/ipam/ip-addresses/56296/\" , \"assigned_object_type\" : \"dcim.interface\" , \"assigned_object_id\" : \"e824bc29-623f-407e-8aa8-828f4c0b98ee\" , \"assigned_object\" : { \"id\" : \"e824bc29-623f-407e-8aa8-828f4c0b98ee\" , \"url\" : \"http://nautobot/api/dcim/interfaces/e824bc29-623f-407e-8aa8-828f4c0b98ee/\" , \"device\" : { \"id\" : \"76816a69-db2c-40e6-812d-115c61156e21\" , \"url\" : \"http://nautobot/api/dcim/devices/76816a69-db2c-40e6-812d-115c61156e21/\" , \"name\" : \"device105\" , \"display\" : \"device105\" }, \"name\" : \"ge-0/0/0\" , \"cable\" : null , \"connection_status\" : null }, ... } If we wanted to assign this IP address to a virtual machine interface instead, we would have set assigned_object_type to virtualization.vminterface and updated the object ID appropriately.","title":"Generic Relations"},{"location":"rest-api/overview.html#pagination","text":"API responses which contain a list of many objects will be paginated for efficiency. The root JSON object returned by a list endpoint contains the following attributes: count : The total number of all objects matching the query next : A hyperlink to the next page of results (if applicable) previous : A hyperlink to the previous page of results (if applicable) results : The list of objects on the current page Here is an example of a paginated response: HTTP 200 OK Allow : GET , POST , PUT , PATCH , DELETE , HEAD , OPTIONS API - Versio n : 1.2 Co ntent - Type : applica t io n /jso n Vary : Accep t { \"count\" : 2861 , \"next\" : \"http://nautobot/api/dcim/devices/?limit=50&offset=50\" , \"previous\" : null , \"results\" : [ { \"id\" : \"fa069c4b-4f6e-4349-88ac-8b6baf9d70c5\" , \"name\" : \"Device1\" , ... }, { \"id\" : \"a37df58c-8bf3-4b97-bad5-301ef3880bea\" , \"name\" : \"Device2\" , ... }, ... ] } The default page is determined by the PAGINATE_COUNT configuration parameter, which defaults to 50. However, this can be overridden per request by specifying the desired offset and limit query parameters. For example, if you wish to retrieve a hundred devices at a time, you would make a request for: http://nautobot/api/dcim/devices/?limit=100 The response will return devices 1 through 100. The URL provided in the next attribute of the response will return devices 101 through 200: { \"count\" : 2861 , \"next\" : \"http://nautobot/api/dcim/devices/?limit=100&offset=100\" , \"previous\" : null , \"results\" : [ ... ] } The maximum number of objects that can be returned is limited by the MAX_PAGE_SIZE configuration parameter, which is 1000 by default. Setting this to 0 or None will remove the maximum limit. An API consumer can then pass ?limit=0 to retrieve all matching objects with a single request. Warning Disabling the page size limit introduces a potential for very resource-intensive requests, since one API request can effectively retrieve an entire table from the database.","title":"Pagination"},{"location":"rest-api/overview.html#interacting-with-objects","text":"","title":"Interacting with Objects"},{"location":"rest-api/overview.html#retrieving-multiple-objects","text":"To query Nautobot for a list of objects, make a GET request to the model's list endpoint. Objects are listed under the response object's results parameter. Specifying the Accept header with the Nautobot API version is not required, but is strongly recommended. curl -s -X GET \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/ipam/ip-addresses/ | jq '.' { \"count\" : 42031 , \"next\" : \"http://nautobot/api/ipam/ip-addresses/?limit=50&offset=50\" , \"previous\" : null , \"results\" : [ { \"id\" : \"bd307eca-de34-4bda-9195-d69ca52206d6\" , \"address\" : \"192.0.2.1/24\" , ... }, { \"id\" : \"6c52e918-4f0c-4c50-ae49-6bef22c97fd5\" , \"address\" : \"192.0.2.2/24\" , ... }, { \"id\" : \"b8cde1ee-1b86-4ea4-a884-041c472d8999\" , \"address\" : \"192.0.2.3/24\" , ... }, ... ] }","title":"Retrieving Multiple Objects"},{"location":"rest-api/overview.html#retrieving-a-single-object","text":"To query Nautobot for a single object, make a GET request to the model's detail endpoint specifying its UUID. Note Note that the trailing slash is required. Omitting this will return a 302 redirect. curl -s -X GET \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/ipam/ip-addresses/bd307eca-de34-4bda-9195-d69ca52206d6/ | jq '.' { \"id\" : \"bd307eca-de34-4bda-9195-d69ca52206d6\" , \"address\" : \"192.0.2.1/24\" , ... }","title":"Retrieving a Single Object"},{"location":"rest-api/overview.html#brief-format","text":"The GET API endpoints support an optional \"brief\" format, which returns only a minimal representation of each object in the response. This is useful when you need only a list of available objects without any related data, such as when populating a drop-down list in a form. As an example, the default (complete) format of an IP address looks like this: GET /api/ipam/prefixes/7d2d24ac-4737-4fc1-a850-b30366618f3d/ { \"id\" : \"7d2d24ac-4737-4fc1-a850-b30366618f3d\" , \"url\" : \"http://nautobot/api/ipam/prefixes/7d2d24ac-4737-4fc1-a850-b30366618f3d/\" , \"family\" : { \"value\" : 4 , \"label\" : \"IPv4\" }, \"prefix\" : \"192.0.2.0/24\" , \"site\" : { \"id\" : \"b9edf2ee-cad9-48be-9921-006294bff730\" , \"url\" : \"http://nautobot/api/dcim/sites/b9edf2ee-cad9-48be-9921-006294bff730/\" , \"name\" : \"Site 23A\" , \"slug\" : \"site-23a\" }, \"vrf\" : null , \"tenant\" : null , \"vlan\" : null , \"status\" : { \"value\" : \"container\" , \"label\" : \"Container\" }, \"role\" : { \"id\" : \"ae1470bc-a858-4ce7-b9ce-dd1cd46333fe\" , \"url\" : \"http://nautobot/api/ipam/roles/ae1470bc-a858-4ce7-b9ce-dd1cd46333fe/\" , \"name\" : \"Staging\" , \"slug\" : \"staging\" }, \"is_pool\" : false , \"description\" : \"Example prefix\" , \"tags\" : [], \"custom_fields\" : {}, \"created\" : \"2018-12-10\" , \"last_updated\" : \"2019-03-01T20:02:46.173540Z\" } The brief format is much more terse: GET /api/ipam/prefixes/7d2d24ac-4737-4fc1-a850-b30366618f3d/?brief=1 { \"id\" : \"7d2d24ac-4737-4fc1-a850-b30366618f3d\" , \"url\" : \"http://nautobot/api/ipam/prefixes/7d2d24ac-4737-4fc1-a850-b30366618f3d/\" , \"family\" : 4 , \"prefix\" : \"10.40.3.0/24\" } The brief format is supported for both lists and individual objects.","title":"Brief Format"},{"location":"rest-api/overview.html#retrieving-object-relationships-and-relationship-associations","text":"Added in version 1.4.0 Objects that are associated with another object by a custom Relationship are also retrievable and modifiable via the REST API. Due to the additional processing overhead involved in retrieving and representing these relationships, they are not included in default REST API GET responses. To include relationships data, pass include=relationships as a query parameter; in this case an additional key, \"relationships\" , will be included in the API response, as seen below: GET /api/dcim/sites/f472bb77-7f56-4e79-ac25-2dc73eb63924/?include=relationships { \"id\" : \"f472bb77-7f56-4e79-ac25-2dc73eb63924\" , \"display\" : \"alpha\" , \"url\" : \"http://nautobot/api/dcim/sites/f472bb77-7f56-4e79-ac25-2dc73eb63924/\" , ... \"relationships\" : { \"site-to-vrf\" : { \"id\" : \"e74cb7f7-15b0-499d-9401-a0f01cb96a9a\" , \"url\" : \"/api/extras/relationships/e74cb7f7-15b0-499d-9401-a0f01cb96a9a/\" , \"name\" : \"Single Site to Single VRF\" , \"type\" : \"one-to-one\" , \"destination\" : { \"label\" : \"VRF\" , \"object_type\" : \"ipam.vrf\" , \"objects\" : [ { \"id\" : \"36641ba0-50d6-43be-b9b5-86aa992402e0\" , \"url\" : \"http://nautobot/api/ipam/vrfs/36641ba0-50d6-43be-b9b5-86aa992402e0/\" , \"name\" : \"red\" , \"rd\" : null , \"display\" : \"red\" } ] } }, \"vrfs-to-sites\" : { \"id\" : \"e39c53e4-78cf-4572-b116-1d8830b81b2e\" , \"url\" : \"/api/extras/relationships/e39c53e4-78cf-4572-b116-1d8830b81b2e/\" , \"name\" : \"VRFs to Sites\" , \"type\" : \"many-to-many\" , \"source\" : { \"label\" : \"VRFs\" , \"object_type\" : \"ipam.vrf\" , \"objects\" : [] } }, } } Under the \"relationships\" key, there will be one key per Relationship that applies to this model, corresponding to the slug of that Relationship. Under each slug key, there will be information about the Relationship itself, plus any of \"source\" , \"destination\" , or \"peer\" keys (depending on the type and directionality of the Relationship). Under the \"source\" , \"destination\" , or \"peer\" keys, there are the following keys: \"label\" - a human-readable description of the related objects \"object_type\" - the content-type of the related objects \"objects\" - a list of all related objects, each represented in nested-serializer form as described under Related Objects above. In the example above we can see that a single VRF, green , is a destination for the site-to-vrf Relationship from this Site, while there are currently no VRFs associated as sources for the vrfs-to-sites Relationship to this Site.","title":"Retrieving Object Relationships and Relationship Associations"},{"location":"rest-api/overview.html#excluding-config-contexts","text":"When retrieving devices and virtual machines via the REST API, each will include its rendered configuration context data by default. Users with large amounts of context data will likely observe suboptimal performance when returning multiple objects, particularly with very high page sizes. To combat this, context data may be excluded from the response data by attaching the query parameter ?exclude=config_context to the request. This parameter works for both list and detail views.","title":"Excluding Config Contexts"},{"location":"rest-api/overview.html#creating-a-new-object","text":"To create a new object, make a POST request to the model's list endpoint with JSON data pertaining to the object being created. Note that a REST API token is required for all write operations; see the authentication documentation for more information. Also be sure to set the Content-Type HTTP header to application/json . As always, it's a good practice to also set the Accept HTTP header to include the requested REST API version. curl -s -X POST \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/ipam/prefixes/ \\ --data '{\"prefix\": \"192.0.2.0/24\", \"site\": 8df9e629-4338-438b-8ea9-06114f7be08e}' | jq '.' { \"id\" : \"48df6965-0fcb-4155-b5f8-00fe8b9b01af\" , \"url\" : \"http://nautobot/api/ipam/prefixes/48df6965-0fcb-4155-b5f8-00fe8b9b01af/\" , \"family\" : { \"value\" : 4 , \"label\" : \"IPv4\" }, \"prefix\" : \"192.0.2.0/24\" , \"site\" : { \"id\" : \"8df9e629-4338-438b-8ea9-06114f7be08e\" , \"url\" : \"http://nautobot/api/dcim/sites/8df9e629-4338-438b-8ea9-06114f7be08e/\" , \"name\" : \"US-East 4\" , \"slug\" : \"us-east-4\" }, \"vrf\" : null , \"tenant\" : null , \"vlan\" : null , \"status\" : { \"value\" : \"active\" , \"label\" : \"Active\" }, \"role\" : null , \"is_pool\" : false , \"description\" : \"\" , \"tags\" : [], \"custom_fields\" : {}, \"created\" : \"2020-08-04\" , \"last_updated\" : \"2020-08-04T20:08:39.007125Z\" }","title":"Creating a New Object"},{"location":"rest-api/overview.html#creating-multiple-objects","text":"To create multiple instances of a model using a single request, make a POST request to the model's list endpoint with a list of JSON objects representing each instance to be created. If successful, the response will contain a list of the newly created instances. The example below illustrates the creation of three new sites. curl -X POST -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3; indent=4\" \\ http://nautobot/api/dcim/sites/ \\ --data '[ {\"name\": \"Site 1\", \"slug\": \"site-1\", \"region\": {\"name\": \"United States\"}}, {\"name\": \"Site 2\", \"slug\": \"site-2\", \"region\": {\"name\": \"United States\"}}, {\"name\": \"Site 3\", \"slug\": \"site-3\", \"region\": {\"name\": \"United States\"}} ]' [ { \"id\" : \"0238a4e3-66f2-455a-831f-5f177215de0f\" , \"url\" : \"http://nautobot/api/dcim/sites/0238a4e3-66f2-455a-831f-5f177215de0f/\" , \"name\" : \"Site 1\" , ... }, { \"id\" : \"33ac3a3b-0ee7-49b7-bf2a-244096051dc0\" , \"url\" : \"http://nautobot/api/dcim/sites/33ac3a3b-0ee7-49b7-bf2a-244096051dc0/\" , \"name\" : \"Site 2\" , ... }, { \"id\" : \"10b3134d-960b-4794-ad18-0e73edd357c4\" , \"url\" : \"http://nautobot/api/dcim/sites/10b3134d-960b-4794-ad18-0e73edd357c4/\" , \"name\" : \"Site 3\" , ... } ]","title":"Creating Multiple Objects"},{"location":"rest-api/overview.html#updating-an-object","text":"To modify an object which has already been created, make a PATCH request to the model's detail endpoint specifying its UUID. Include any data which you wish to update on the object. As with object creation, the Authorization and Content-Type headers must also be specified, and specifying the Accept header is also strongly recommended. curl -s -X PATCH \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/ipam/prefixes/b484b0ac-12e3-484a-84c0-aa17955eaedc/ \\ --data '{\"status\": \"reserved\"}' | jq '.' { \"id\" : \"48df6965-0fcb-4155-b5f8-00fe8b9b01af\" , \"url\" : \"http://nautobot/api/ipam/prefixes/48df6965-0fcb-4155-b5f8-00fe8b9b01af/\" , \"family\" : { \"value\" : 4 , \"label\" : \"IPv4\" }, \"prefix\" : \"192.0.2.0/24\" , \"site\" : { \"id\" : \"8df9e629-4338-438b-8ea9-06114f7be08e\" , \"url\" : \"http://nautobot/api/dcim/sites/8df9e629-4338-438b-8ea9-06114f7be08e/\" , \"name\" : \"US-East 4\" , \"slug\" : \"us-east-4\" }, \"vrf\" : null , \"tenant\" : null , \"vlan\" : null , \"status\" : { \"value\" : \"reserved\" , \"label\" : \"Reserved\" }, \"role\" : null , \"is_pool\" : false , \"description\" : \"\" , \"tags\" : [], \"custom_fields\" : {}, \"created\" : \"2020-08-04\" , \"last_updated\" : \"2020-08-04T20:14:55.709430Z\" } PUT versus PATCH The Nautobot REST API support the use of either PUT or PATCH to modify an existing object. The difference is that a PUT request requires the user to specify a complete representation of the object being modified, whereas a PATCH request need include only the attributes that are being updated. For most purposes, using PATCH is recommended.","title":"Updating an Object"},{"location":"rest-api/overview.html#updating-relationship-associations","text":"Added in version 1.4.0 It is possible to modify the objects associated via Relationship with an object as part of a REST API PATCH request by specifying the \"relationships\" key, any or all of the relevant Relationships, and the list of desired related objects for each such Relationship. Since nested serializers are used for the related objects, they can be identified by ID (primary key) or by one or more attributes in a dictionary. For example, either of the following requests would be valid: { \"relationships\" : { \"site-to-vrf\" : { \"destination\" : { \"objects\" : [ { \"name\" : \"blue\" } ] } }, \"vrfs-to-sites\" : { \"source\" : { \"objects\" : [ { \"name\" : \"green\" }, { \"name\" : \"red\" }, ] } } } } { \"relationships\" : { \"site-to-vrf\" : { \"destination\" : { \"objects\" : [ \"3e3c58f9-4f63-44ba-acee-f0c42430eba7\" ] } } } } Note Relationship slugs can be omitted from the \"relationships\" dictionary, in which case the associations for that Relationship will be left unmodified. In the second example above, the existing association for the \"site-to-vrf\" Relationship would be replaced, but the \"vrfs-to-sites\" Relationship's associations would remain as-is.","title":"Updating Relationship Associations"},{"location":"rest-api/overview.html#updating-multiple-objects","text":"Multiple objects can be updated simultaneously by issuing a PUT or PATCH request to a model's list endpoint with a list of dictionaries specifying the UUID of each object to be deleted and the attributes to be updated. For example, to update sites with UUIDs 18de055e-3ea9-4cc3-ba78-b7eef6f0d589 and 1a414273-3d68-4586-ba22-6ae0a5702b8f to a status of \"active\", issue the following request: curl -s -X PATCH \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/dcim/sites/ \\ --data '[{\"id\": \"18de055e-3ea9-4cc3-ba78-b7eef6f0d589\", \"status\": \"active\"}, {\"id\": \"1a414273-3d68-4586-ba22-6ae0a5702b8f\", \"status\": \"active\"}]' Note that there is no requirement for the attributes to be identical among objects. For instance, it's possible to update the status of one site along with the name of another in the same request. Note The bulk update of objects is an all-or-none operation, meaning that if Nautobot fails to successfully update any of the specified objects (e.g. due a validation error), the entire operation will be aborted and none of the objects will be updated.","title":"Updating Multiple Objects"},{"location":"rest-api/overview.html#deleting-an-object","text":"To delete an object from Nautobot, make a DELETE request to the model's detail endpoint specifying its UUID. The Authorization header must be included to specify an authorization token, however this type of request does not support passing any data in the body. curl -s -X DELETE \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/ipam/prefixes/48df6965-0fcb-4155-b5f8-00fe8b9b01af/ Note that DELETE requests do not return any data: If successful, the API will return a 204 (No Content) response. Note You can run curl with the verbose ( -v ) flag to inspect the HTTP response codes.","title":"Deleting an Object"},{"location":"rest-api/overview.html#deleting-multiple-objects","text":"Nautobot supports the simultaneous deletion of multiple objects of the same type by issuing a DELETE request to the model's list endpoint with a list of dictionaries specifying the UUID of each object to be deleted. For example, to delete sites with UUIDs 18de055e-3ea9-4cc3-ba78-b7eef6f0d589, 1a414273-3d68-4586-ba22-6ae0a5702b8f, and c2516019-caf6-41f0-98a6-4276c1a73fa3, issue the following request: curl -s -X DELETE \\ -H \"Authorization: Token $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json; version=1.3\" \\ http://nautobot/api/dcim/sites/ \\ --data '[{\"id\": \"18de055e-3ea9-4cc3-ba78-b7eef6f0d589\"}, {\"id\": \"1a414273-3d68-4586-ba22-6ae0a5702b8f\"}, {\"id\": \"c2516019-caf6-41f0-98a6-4276c1a73fa3\"}]' Note The bulk deletion of objects is an all-or-none operation, meaning that if Nautobot fails to delete any of the specified objects (e.g. due a dependency by a related object), the entire operation will be aborted and none of the objects will be deleted.","title":"Deleting Multiple Objects"},{"location":"user-guides/git-data-source.html","text":"Git as a Data Source \u00b6 The \"Git\u2122 as a Data Source\" feature was developed to provide the ability to populate existing data, templates, scripts, and much more into Nautobot; while leveraging the benefits that tools such as GitHub and GitLab already provide, including issue tracking, discussions, pipelines, and approvals. For example having the ability to have users approve the YAML data that is used for Nautobot config context along with running tests on that data, or having the users approve Jinja2 templates that are used for Nautobot export templates . These examples and more can be accomplished by used Git as a Data Source. For more technical details on how to use this feature, please see the documentation on Git Repositories . Supported Providers \u00b6 The feature uses the concept of a provides field to map a repository to a use case. A list of the supported options is provided below. Core Functionality \u00b6 Name Summary Export Templates Nautobot allows users to define custom templates that can be used when exporting objects. Jobs Jobs are a way for users to execute custom logic on demand from within the Nautobot UI. Jobs can interact directly with Nautobot data to accomplish various data creation, modification, and validation tasks. Config Contexts Config contexts can be used to provide additional data that you can't natively store in Nautobot. Config Context Schemas Schemas enforce data validation on config contexts. Examples of Plugins Defining Additional Providers \u00b6 Additional Git providers can be added by using Nautobot's flexible plugin system. Name Summary Related Plugin Backup Configs Backup configuration data. Golden Config Intended Configs Stores the intended configurations, this grabs Nautobot data and runs through Jinja Templates. Golden Config Jinja Templates Repository that holds Jinja templates to be used to generate intended configs. Golden Config Repository Details \u00b6 This table defines repository parameters that are required to establish a repository connection. Field Explanation Name User friendly name for the repo. Slug Computer-friendly name for the repo. Auto-generated based on the name provided, but you can change it if you wish. Remote URL The URL pointing to the Git repo. Current git url usage is limited to http or https . Branch The branch in the Git repo to use. Defaults to main . Token (Optional) A personal access token for the username provided. For more information on generating a personal access token see the corresponding links below. Username (Optional) The Git username that corresponds with the personal access token above. Note not required for GitHub, but is for GitLab. Secrets Group (Optional) Grouping containing a HTTP token and/or HTTP username as needed to access the repository. Provides Resource type(s) provided by this Git repo. GitHub Personal Access Token GitLab Personal Access Token Bitbucket Personal Access Token Warning Beginning in Nautobot 1.2, there are two ways to define a token and/or username for a Git repository -- either by directly configuring them into the repository definition, or by associating the repository with a secrets group record (this latter approach is new in Nautobot 1.2). The direct-configuration approach should be considered as deprecated, as it is less secure and poses a number of maintainability issues. If at all possible, you should use a secrets group instead. The direct-configuration approach may be removed altogether as an option in a future release of Nautobot. Using Git Data Sources \u00b6 This section will focus on examples and use the user-guide branch on the demo-git-datasources repo: https://github.com/nautobot/demo-git-datasource/tree/user-guide . Export Templates \u00b6 Export Templates allow a user to export Nautobot objects based on a custom template. Export templates can change over time depending on the needs of a user. Allowing export templates to reference a Git repo makes managing templates easier. A template can be used to put objects into a specific format for ingestion into another system, tool, or report. It is possible that different templates are needed depending on specific users or teams. This can lead to sprawl of export templates. To keep accurate templates synced with Nautobot the Git Data Sources extensibility feature can be used. Add a Repository \u00b6 Navigate to the Data Sources Git integration. Extensibility -> Git Repositories . Click [+] or [Add] That loads a default page to add a repository. Note By default only config contexts, export templates, and jobs are available resource types. Others may be added when specific plugins are used. Fill out Repository Details \u00b6 Fill out the details for the Git repository. More information on the inputs can be found in the fields section . As soon as you click on Create & Sync , Nautobot will clone and sync the repository and provide status of the job. Note If you are using a self-signed Git repository, the Server Administrator will need to ensure the GIT_SSL_NO_VERIFY environment variable is set to permit this. The repository will now be displayed on the main Git Repository page. Once the repository is synced each template will now be available in the Export Templates section. Extensibility -> Export Templates . Note If the templates don't populate, make sure the Git directory is named export_templates and the sub-directory and sub-sub-directory names correctly match the Nautobot content type . Example below: \u25b6 tree export_templates export_templates \u2514\u2500\u2500 dcim \u2514\u2500\u2500 device \u251c\u2500\u2500 markdown_export.md \u251c\u2500\u2500 text_export.txt \u2514\u2500\u2500 yaml_export.yml 2 directories, 3 files Modifying a File and Sync Changes \u00b6 Now that the export templates have been loaded into Nautobot they can be utilized as normal. For example navigate to Devices -> Devices and click on Export in the top right corner, the dropdown will now include the templates loaded from the Git repository. The power of having export templates utilizing the Git integration comes with the native source control features that Git comes with. To illustrate a simple Git sync within Nautobot assume the following template needs to be updated. Filename: /export_templates/dcim/device/yaml_export.yml Current contents: --- {% for device in queryset %} {% if device.status %} - {{ device.name }} : {% endif %} {% endfor %} ... The template needs to be modified to provide more information than just a list of hostnames. The site needs to be added. The updated template is now: --- {% for device in queryset %} {% if device.status %} - {{ device.name }} : site: {{ device.site }} {% endif %} {% endfor %} ... Once the contributor updates the Git repository via normal Git processes and it is reviewed and merged into the branch that was used, a sync process from Nautobot needs to be completed. This can be done from the default Git view, or within a specific detailed view of a Git repository. From the default Git repositories view: From the detailed view: Tip Once the repository has been synced it's easy to check the history for the templates. Navigate to Git Repositories and select the repository in question. Once you're in the detailed view you can look through the Synchronization Status or Change Log tabs. Now that the Git repository is linked for export templates it can be controlled via the normal Git operations workflow, which allows users or groups of users to perform code reviews using Pull Requests etc. Jobs \u00b6 Jobs are a way for users to execute custom logic on demand from within the Nautobot UI. Jobs can interact directly with Nautobot data to accomplish various data creation, modification, and validation tasks. For technical details on jobs, please see the documentation on jobs . Jobs allow a user to write scripts in Python. By integrating the scripts with Git, a user can utilize Git workflows to manage source control, versioning, and pipelines. Setting up the repository can be done following the same steps from Export Templates . The only differences is the provides selection changes to jobs . Jobs need to be defined in /jobs/ directory at the root of a Git repository. An example tree for /jobs/ . \u25b6 tree jobs jobs \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 get-device-connection.py 1 directory, 2 files Note As shown in the above example, the /jobs/ directory must contain a file called __init__.py . This may be an empty file, but it must exist. Once the repository is created in Nautobot. Tip The same repository and branch can be used for the different provides methods. Nautobot Git as a data source will look for specific root directory names. Once the scripts have been pushed into the repository, a sync needs to be executed, after which navigating to Jobs via Jobs -> Jobs will show the new jobs loaded from the Git repository. Jobs now shows the job from the Git repository. At this point all changes, and history can be kept using Git. A simple sync operation can be done from Nautobot to pulldown any changes. Config Contexts \u00b6 Detailed information on config contexts in Git Repositories. Config contexts may be provided as JSON or YAML files located in the /config_contexts/ folder, which must be in the root of the Git repository. Config contexts can be used to provide additional details to different automation tooling. For example Ansible variables, or any other data that you can't natively store in Nautobot. It can also be used in the Golden Configuration Nautobot plugin to provide extra details to generate configuration templates. A few simple examples of Configuration Context data might be: DNS Servers NTP Servers ACL Data Routing Information such as BGP ASNs etc. Similar to the other data sources, the repository can be added by navigating to Extensibility -> Git repositories . Click on Add , and fill out the repository details. Once the repository syncs the details can be found in the Synchronization Status tab. For example, the platform specifics were synced: The repository structure is: \u25b6 tree config_contexts config_contexts \u251c\u2500\u2500 devices \u2502 \u251c\u2500\u2500 site-a-bb-01.yml \u2502 \u251c\u2500\u2500 site-a-rtr-01.yml \u2502 \u251c\u2500\u2500 site-a-rtr-02.yml \u2502 \u251c\u2500\u2500 site-a-spine-01.yml \u2502 \u251c\u2500\u2500 site-a-spine-02.yml \u2502 \u251c\u2500\u2500 site-b-bb-01.yml \u2502 \u251c\u2500\u2500 site-b-leaf-01.yml \u2502 \u251c\u2500\u2500 site-b-leaf-02.yml \u2502 \u251c\u2500\u2500 site-b-rtr-01.yml \u2502 \u251c\u2500\u2500 site-b-rtr-02.yml \u2502 \u251c\u2500\u2500 site-b-spine-01.yml \u2502 \u2514\u2500\u2500 site-b-spine-02.yml \u251c\u2500\u2500 platform_eos.yml \u251c\u2500\u2500 platform_junos.yml \u251c\u2500\u2500 platform_nxos.yml \u2514\u2500\u2500 role_spine.yml 1 directory, 16 files Configuration Context details: Follows an inheritance methodology similar to what Ansible implements. Global contexts can be overwritten by local contexts at both a group level, as well as at a device specific level. Nautobot UI provides a simple view to see merged config contexts. It can be visualized by navigating to a device and clicking on the config contexts tab. Here's an example, with some of the details omitted for brevity. There is a huge benefit to having config contexts managed by a Git workflow. This type of data can be modified often, especially platform specifics, or new device roles. Utilizing a standard Git workflow allows for all the proper reviews and approvals to be accomplished before accepting the changes into Nautobot for use. Config Context Schemas \u00b6 Detailed information on config context schemas in Git Repositories. Config context schemas are used to enforce data validation on config contexts. These schema are managed via the config context schema model and are optionally linked to config context instances, in addition to devices and virtual machines for the purpose of validating their local context data. \u25b6 tree config_contexts config_context_schemas \u251c\u2500\u2500 schema_1.yaml \u251c\u2500\u2500 schema_2.json Additional Git Data Sources \u00b6 As seen in Fill out Repository Details , the standard installation of Nautobot will come natively with export templates, jobs, and config contexts. Additional data sources can be incorporated using the Nautobot plugin system. For example, the nautobot-plugin-golden-config plugin implements four additional data sources. Config Contexts Backup Configs Intended Configs Jinja Templates For more information for the Golden Configuration specific data sources, navigate to Nautobot Golden Config Repo . Common Issues and Troubleshooting \u00b6 Repository is linked, but data is not properly loaded into Nautobot. Validate the root directory is set to the proper name. Export Templates -> export_templates . Jobs -> jobs . Config Contexts -> config_contexts . Synchronization Status Failures. Validate branch is correct and exists in the remote repository. Validate the remote url is correct and is the http(s) url. ssh urls are not currently supported. Authentication Issues. Check repository permissions. Ensure the password is the Personal Access Token (PAT) for the username supplied. Ensure the PAT permissions are setup properly. At a minimum the repo option should be checked or access.","title":"Git as a Data Source"},{"location":"user-guides/git-data-source.html#git-as-a-data-source","text":"The \"Git\u2122 as a Data Source\" feature was developed to provide the ability to populate existing data, templates, scripts, and much more into Nautobot; while leveraging the benefits that tools such as GitHub and GitLab already provide, including issue tracking, discussions, pipelines, and approvals. For example having the ability to have users approve the YAML data that is used for Nautobot config context along with running tests on that data, or having the users approve Jinja2 templates that are used for Nautobot export templates . These examples and more can be accomplished by used Git as a Data Source. For more technical details on how to use this feature, please see the documentation on Git Repositories .","title":"Git as a Data Source"},{"location":"user-guides/git-data-source.html#supported-providers","text":"The feature uses the concept of a provides field to map a repository to a use case. A list of the supported options is provided below.","title":"Supported Providers"},{"location":"user-guides/git-data-source.html#core-functionality","text":"Name Summary Export Templates Nautobot allows users to define custom templates that can be used when exporting objects. Jobs Jobs are a way for users to execute custom logic on demand from within the Nautobot UI. Jobs can interact directly with Nautobot data to accomplish various data creation, modification, and validation tasks. Config Contexts Config contexts can be used to provide additional data that you can't natively store in Nautobot. Config Context Schemas Schemas enforce data validation on config contexts.","title":"Core Functionality"},{"location":"user-guides/git-data-source.html#examples-of-plugins-defining-additional-providers","text":"Additional Git providers can be added by using Nautobot's flexible plugin system. Name Summary Related Plugin Backup Configs Backup configuration data. Golden Config Intended Configs Stores the intended configurations, this grabs Nautobot data and runs through Jinja Templates. Golden Config Jinja Templates Repository that holds Jinja templates to be used to generate intended configs. Golden Config","title":"Examples of Plugins Defining Additional Providers"},{"location":"user-guides/git-data-source.html#repository-details","text":"This table defines repository parameters that are required to establish a repository connection. Field Explanation Name User friendly name for the repo. Slug Computer-friendly name for the repo. Auto-generated based on the name provided, but you can change it if you wish. Remote URL The URL pointing to the Git repo. Current git url usage is limited to http or https . Branch The branch in the Git repo to use. Defaults to main . Token (Optional) A personal access token for the username provided. For more information on generating a personal access token see the corresponding links below. Username (Optional) The Git username that corresponds with the personal access token above. Note not required for GitHub, but is for GitLab. Secrets Group (Optional) Grouping containing a HTTP token and/or HTTP username as needed to access the repository. Provides Resource type(s) provided by this Git repo. GitHub Personal Access Token GitLab Personal Access Token Bitbucket Personal Access Token Warning Beginning in Nautobot 1.2, there are two ways to define a token and/or username for a Git repository -- either by directly configuring them into the repository definition, or by associating the repository with a secrets group record (this latter approach is new in Nautobot 1.2). The direct-configuration approach should be considered as deprecated, as it is less secure and poses a number of maintainability issues. If at all possible, you should use a secrets group instead. The direct-configuration approach may be removed altogether as an option in a future release of Nautobot.","title":"Repository Details"},{"location":"user-guides/git-data-source.html#using-git-data-sources","text":"This section will focus on examples and use the user-guide branch on the demo-git-datasources repo: https://github.com/nautobot/demo-git-datasource/tree/user-guide .","title":"Using Git Data Sources"},{"location":"user-guides/git-data-source.html#export-templates","text":"Export Templates allow a user to export Nautobot objects based on a custom template. Export templates can change over time depending on the needs of a user. Allowing export templates to reference a Git repo makes managing templates easier. A template can be used to put objects into a specific format for ingestion into another system, tool, or report. It is possible that different templates are needed depending on specific users or teams. This can lead to sprawl of export templates. To keep accurate templates synced with Nautobot the Git Data Sources extensibility feature can be used.","title":"Export Templates"},{"location":"user-guides/git-data-source.html#add-a-repository","text":"Navigate to the Data Sources Git integration. Extensibility -> Git Repositories . Click [+] or [Add] That loads a default page to add a repository. Note By default only config contexts, export templates, and jobs are available resource types. Others may be added when specific plugins are used.","title":"Add a Repository"},{"location":"user-guides/git-data-source.html#fill-out-repository-details","text":"Fill out the details for the Git repository. More information on the inputs can be found in the fields section . As soon as you click on Create & Sync , Nautobot will clone and sync the repository and provide status of the job. Note If you are using a self-signed Git repository, the Server Administrator will need to ensure the GIT_SSL_NO_VERIFY environment variable is set to permit this. The repository will now be displayed on the main Git Repository page. Once the repository is synced each template will now be available in the Export Templates section. Extensibility -> Export Templates . Note If the templates don't populate, make sure the Git directory is named export_templates and the sub-directory and sub-sub-directory names correctly match the Nautobot content type . Example below: \u25b6 tree export_templates export_templates \u2514\u2500\u2500 dcim \u2514\u2500\u2500 device \u251c\u2500\u2500 markdown_export.md \u251c\u2500\u2500 text_export.txt \u2514\u2500\u2500 yaml_export.yml 2 directories, 3 files","title":"Fill out Repository Details"},{"location":"user-guides/git-data-source.html#modifying-a-file-and-sync-changes","text":"Now that the export templates have been loaded into Nautobot they can be utilized as normal. For example navigate to Devices -> Devices and click on Export in the top right corner, the dropdown will now include the templates loaded from the Git repository. The power of having export templates utilizing the Git integration comes with the native source control features that Git comes with. To illustrate a simple Git sync within Nautobot assume the following template needs to be updated. Filename: /export_templates/dcim/device/yaml_export.yml Current contents: --- {% for device in queryset %} {% if device.status %} - {{ device.name }} : {% endif %} {% endfor %} ... The template needs to be modified to provide more information than just a list of hostnames. The site needs to be added. The updated template is now: --- {% for device in queryset %} {% if device.status %} - {{ device.name }} : site: {{ device.site }} {% endif %} {% endfor %} ... Once the contributor updates the Git repository via normal Git processes and it is reviewed and merged into the branch that was used, a sync process from Nautobot needs to be completed. This can be done from the default Git view, or within a specific detailed view of a Git repository. From the default Git repositories view: From the detailed view: Tip Once the repository has been synced it's easy to check the history for the templates. Navigate to Git Repositories and select the repository in question. Once you're in the detailed view you can look through the Synchronization Status or Change Log tabs. Now that the Git repository is linked for export templates it can be controlled via the normal Git operations workflow, which allows users or groups of users to perform code reviews using Pull Requests etc.","title":"Modifying a File and Sync Changes"},{"location":"user-guides/git-data-source.html#jobs","text":"Jobs are a way for users to execute custom logic on demand from within the Nautobot UI. Jobs can interact directly with Nautobot data to accomplish various data creation, modification, and validation tasks. For technical details on jobs, please see the documentation on jobs . Jobs allow a user to write scripts in Python. By integrating the scripts with Git, a user can utilize Git workflows to manage source control, versioning, and pipelines. Setting up the repository can be done following the same steps from Export Templates . The only differences is the provides selection changes to jobs . Jobs need to be defined in /jobs/ directory at the root of a Git repository. An example tree for /jobs/ . \u25b6 tree jobs jobs \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 get-device-connection.py 1 directory, 2 files Note As shown in the above example, the /jobs/ directory must contain a file called __init__.py . This may be an empty file, but it must exist. Once the repository is created in Nautobot. Tip The same repository and branch can be used for the different provides methods. Nautobot Git as a data source will look for specific root directory names. Once the scripts have been pushed into the repository, a sync needs to be executed, after which navigating to Jobs via Jobs -> Jobs will show the new jobs loaded from the Git repository. Jobs now shows the job from the Git repository. At this point all changes, and history can be kept using Git. A simple sync operation can be done from Nautobot to pulldown any changes.","title":"Jobs"},{"location":"user-guides/git-data-source.html#config-contexts","text":"Detailed information on config contexts in Git Repositories. Config contexts may be provided as JSON or YAML files located in the /config_contexts/ folder, which must be in the root of the Git repository. Config contexts can be used to provide additional details to different automation tooling. For example Ansible variables, or any other data that you can't natively store in Nautobot. It can also be used in the Golden Configuration Nautobot plugin to provide extra details to generate configuration templates. A few simple examples of Configuration Context data might be: DNS Servers NTP Servers ACL Data Routing Information such as BGP ASNs etc. Similar to the other data sources, the repository can be added by navigating to Extensibility -> Git repositories . Click on Add , and fill out the repository details. Once the repository syncs the details can be found in the Synchronization Status tab. For example, the platform specifics were synced: The repository structure is: \u25b6 tree config_contexts config_contexts \u251c\u2500\u2500 devices \u2502 \u251c\u2500\u2500 site-a-bb-01.yml \u2502 \u251c\u2500\u2500 site-a-rtr-01.yml \u2502 \u251c\u2500\u2500 site-a-rtr-02.yml \u2502 \u251c\u2500\u2500 site-a-spine-01.yml \u2502 \u251c\u2500\u2500 site-a-spine-02.yml \u2502 \u251c\u2500\u2500 site-b-bb-01.yml \u2502 \u251c\u2500\u2500 site-b-leaf-01.yml \u2502 \u251c\u2500\u2500 site-b-leaf-02.yml \u2502 \u251c\u2500\u2500 site-b-rtr-01.yml \u2502 \u251c\u2500\u2500 site-b-rtr-02.yml \u2502 \u251c\u2500\u2500 site-b-spine-01.yml \u2502 \u2514\u2500\u2500 site-b-spine-02.yml \u251c\u2500\u2500 platform_eos.yml \u251c\u2500\u2500 platform_junos.yml \u251c\u2500\u2500 platform_nxos.yml \u2514\u2500\u2500 role_spine.yml 1 directory, 16 files Configuration Context details: Follows an inheritance methodology similar to what Ansible implements. Global contexts can be overwritten by local contexts at both a group level, as well as at a device specific level. Nautobot UI provides a simple view to see merged config contexts. It can be visualized by navigating to a device and clicking on the config contexts tab. Here's an example, with some of the details omitted for brevity. There is a huge benefit to having config contexts managed by a Git workflow. This type of data can be modified often, especially platform specifics, or new device roles. Utilizing a standard Git workflow allows for all the proper reviews and approvals to be accomplished before accepting the changes into Nautobot for use.","title":"Config Contexts"},{"location":"user-guides/git-data-source.html#config-context-schemas","text":"Detailed information on config context schemas in Git Repositories. Config context schemas are used to enforce data validation on config contexts. These schema are managed via the config context schema model and are optionally linked to config context instances, in addition to devices and virtual machines for the purpose of validating their local context data. \u25b6 tree config_contexts config_context_schemas \u251c\u2500\u2500 schema_1.yaml \u251c\u2500\u2500 schema_2.json","title":"Config Context Schemas"},{"location":"user-guides/git-data-source.html#additional-git-data-sources","text":"As seen in Fill out Repository Details , the standard installation of Nautobot will come natively with export templates, jobs, and config contexts. Additional data sources can be incorporated using the Nautobot plugin system. For example, the nautobot-plugin-golden-config plugin implements four additional data sources. Config Contexts Backup Configs Intended Configs Jinja Templates For more information for the Golden Configuration specific data sources, navigate to Nautobot Golden Config Repo .","title":"Additional Git Data Sources"},{"location":"user-guides/git-data-source.html#common-issues-and-troubleshooting","text":"Repository is linked, but data is not properly loaded into Nautobot. Validate the root directory is set to the proper name. Export Templates -> export_templates . Jobs -> jobs . Config Contexts -> config_contexts . Synchronization Status Failures. Validate branch is correct and exists in the remote repository. Validate the remote url is correct and is the http(s) url. ssh urls are not currently supported. Authentication Issues. Check repository permissions. Ensure the password is the Personal Access Token (PAT) for the username supplied. Ensure the PAT permissions are setup properly. At a minimum the repo option should be checked or access.","title":"Common Issues and Troubleshooting"},{"location":"user-guides/graphql.html","text":"GraphQL User Guide \u00b6 Introduction \u00b6 What is GraphQL? \u00b6 GraphQL is a query language for your APIs and a runtime for fulfilling those queries with your existing data. How GraphQL simplifies API Interactions \u00b6 When interacting with APIs, It's often necessary to build relationships between multiple models to achieve the result that is desired. Doing this typically requires multiple API calls to create the relationships. For example, lets assume that there are two devices in Nautobot. Each are assigned a site, region, roles, interfaces, and IP Addresses. Simply querying the /api/dcim/devices/ API route provides: View API Results { \"count\" : 2 , \"next\" : \"https://demo.nautobot.com/api/dcim/devices/?limit=1&offset=2\" , \"previous\" : \"https://demo.nautobot.com/api/dcim/devices/?limit=1\" , \"results\" : [ { \"id\" : \"c8886c88-6eff-4c4f-a079-4ef16b53d4f6\" , \"url\" : \"https://demo.nautobot.com/api/dcim/devices/c8886c88-6eff-4c4f-a079-4ef16b53d4f6/\" , \"name\" : \"ams-edge-02\" , \"display\" : \"ams-edge-02\" , \"device_type\" : { \"id\" : \"244ea351-3c7a-4d23-ba80-5db6b65312cc\" , \"url\" : \"https://demo.nautobot.com/api/dcim/device-types/244ea351-3c7a-4d23-ba80-5db6b65312cc/\" , \"manufacturer\" : { \"id\" : \"687f53d9-2c51-40fd-83aa-875e43d01a05\" , \"url\" : \"https://demo.nautobot.com/api/dcim/manufacturers/687f53d9-2c51-40fd-83aa-875e43d01a05/\" , \"name\" : \"Arista\" , \"slug\" : \"arista\" }, \"model\" : \"DCS-7280CR2-60\" , \"slug\" : \"dcs-7280cr2-60\" , \"display\" : \"Arista DCS-7280CR2-60\" }, \"device_role\" : { \"id\" : \"a3637471-6b4d-4f5a-a249-838d621abe60\" , \"url\" : \"https://demo.nautobot.com/api/dcim/device-roles/a3637471-6b4d-4f5a-a249-838d621abe60/\" , \"name\" : \"edge\" , \"slug\" : \"edge\" }, \"tenant\" : null , \"platform\" : null , \"serial\" : \"\" , \"asset_tag\" : null , \"site\" : { \"id\" : \"4ad439e9-4f1b-41c9-bc8c-dd7c1c921dc3\" , \"url\" : \"https://demo.nautobot.com/api/dcim/sites/4ad439e9-4f1b-41c9-bc8c-dd7c1c921dc3/\" , \"name\" : \"ams\" , \"slug\" : \"ams\" }, \"rack\" : { \"id\" : \"bff3f7af-bd77-49b6-a57a-9c4b8fc7673a\" , \"url\" : \"https://demo.nautobot.com/api/dcim/racks/bff3f7af-bd77-49b6-a57a-9c4b8fc7673a/\" , \"name\" : \"ams-102\" , \"display\" : \"ams-102\" }, \"position\" : 40 , \"face\" : { \"value\" : \"front\" , \"label\" : \"Front\" }, \"parent_device\" : null , \"status\" : { \"value\" : \"active\" , \"label\" : \"Active\" }, \"primary_ip\" : null , \"primary_ip4\" : null , \"primary_ip6\" : null , \"cluster\" : null , \"virtual_chassis\" : null , \"vc_position\" : null , \"vc_priority\" : null , \"comments\" : \"\" , \"local_context_data\" : null , \"tags\" : [], \"custom_fields\" : {}, \"config_context\" : {}, \"created\" : \"2021-02-25\" , \"last_updated\" : \"2021-02-25T14:51:57.609598\" } ] } There is a lot of useful information in that API call, but there is also a lot of information that is missing; such as interfaces and ip addresses associated with the devices. There is also potentially a lot of information that isn't needed for the specific task. To retrieve the missing information, subsequent API calls would need to be performed; and those API results would need to be correlated to the correct device. GraphQL reduces the complexity of performing multiple API calls and correlating results by empowering the user to create their own query that provides the user exactly what they want and nothing that they don't, in a single API call. Exploring GraphQL in Nautobot \u00b6 In Nautobot, there is a link to the GraphQL web interface at the bottom right-hand side of the page. The GraphQL web interface is called GraphiQL. Navigating to the URI ( /graphql ), brings up the GraphiQL tool for creating queries. This interface is useful for exploring the possibilities of GraphQL and validating that written queries execute successfully. Documentation Explorer \u00b6 If you're new to GraphQL, take a little bit of time to explore the Documentation Explorer . This can be accomplished by clicking the < Docs link in the GraphiQL interface. The information within Documentation Explorer is specific to creating queries in Nautobot. In the Documentation Explorer , search for devices . The results are all of the models that utilize the devices model. From the devices query, select devices from Query.devices . This will display all of the potential query fields from devices. First Query \u00b6 Now that you have a basic understanding of how to obtain information to query from the Documentation Explorer , let's craft a query. Earlier in the guide, a sample REST API call was performed to obtain device information. While the query had a lot of important information, it also lacked a lot of information. In this section, lets explore how to craft a GraphQL query that displays all of the information that we want. GraphQL queries are encapsulated in query { } flags (simply { } is also acceptable). With that in mind, let's craft our query from the GraphiQL interface to inspect all devices and display their device names. To do this, let's execute: query { devices { name } } This query will retrieve a list of all devices by their hostname. View GraphQL Query Results Now, let's modify the query to provide interface names for each device. We can do that by modifying the existing query to add interfaces { name } as a sub-query of devices . GraphiQL makes this process a bit easier, because it has syntax completion built in. query { devices { name interfaces { name } } } The result is a list of all the devices by their hostname and associated interfaces by their names. View GraphQL Query Results We can continue iterating on the query until we get exactly what we want from the query. For example, if I wanted to iterate on the previous query to not only display the interfaces of the devices, but also display the interface description, the IP Addresses associated with the interface, and whether or not the interface was a dedicated management interface; I would structure the query like: query { devices { name interfaces { name description mgmt_only ip_addresses { address } } } } The results of the query look like: View GraphQL Query Results Filtering Queries \u00b6 These queries are great, but they are displaying the interface attributes and device names for every device in the Nautobot inventory. Nautobot allows users to filter queries at any level as desired to narrow the scope of the returned data. As an example, we can filter the queried devices by their site location. This is done by adding (site: \"<site name>\") after devices . For example: query { devices(site: \"ams\") { name }} will display all devices in the ams site. As an example. We can query devices by their site location. This is done by adding (site: \"<site name>\") after devices . For example: query { devices(site: \"ams\") { name }} will display all devices in the ams site. View GraphQL Query Results GraphQL also allows you to filter by multiple attributes at once if desired. You can use the Documentation Explorer to assist you in finding criteria attributes to filter on. In this example, I add the role attribute in addition to site . query { devices(site: \"ams\", role: \"edge\") { name } } View GraphQL Query Results You can also filter at deeper levels of the query. On many to one relationships you can filter the results based on an attribute of the field. Any attribute that relates to a GraphQLType can be filtered. query { devices(site: \"ams\", role: \"edge\") { name interfaces(name: \"Ethernet1/1\") { name } } } query { sites(name: \"ams\") { devices(role: \"edge\") { name interfaces(name: \"Ethernet1/1\") { name } } } } View GraphQL Query Results Added in version 1.3.0 You can also paginate the results returned to you when the data set gets larger. To do so, use the keywords \"limit\" and \"offset\". The \"limit\" keyword will limit the count of results returned after the \"offset\". If no \"offset\" is specified, then the default offset is zero. query { devices(site: \"ams01\", , limit: 1, offset: 1) { name } } Using the GraphQL API in Nautobot \u00b6 Now that we've explored how to use the GraphiQL interface to help us create GraphQL queries, let's take our queries and call them with the REST API. This is where the real advantage is going to come in to play, because it will allow us to utilize these queries in a programmatic way. From the Nautobot Swagger documentation , we can see that the API calls to /api/graphql require a HTTP POST method. In the HTTP POST, the query field is required, as it is where we specify the GraphQL query. The variables field is optional; it's where we can assign values to any variables included in the query, if we choose to do so. To simplify the process even more, we'll utilize the PyNautobot SDK . Here is an example Python script using the PyNautobot SDK to query GraphQL: #!/usr/bin/env python3 import pynautobot import json query = \"\"\" query { devices { name interfaces { name description mgmt_only ip_addresses { address } } } } \"\"\" nb = pynautobot . api ( url = \"http://localhost\" , token = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\" ) gql = nb . graphql . query ( query = query ) print ( json . dumps ( gql . json , indent = 2 )) The contents of the query variable was taken directly from the example above where we grabbed all device interfaces and associated attributes. We then take the output and print the contents as a JSON object. Now, let's iterate on the script to filter the contents with the variable flag. Just as we did above, we'll filter by site . #!/usr/bin/env python3 import pynautobot import json variables = { \"site_name\" : \"ams\" } query = \"\"\" query ($site_name: String!) { devices (site: $site_name) { name interfaces { name mgmt_only ip_addresses { address } } } } \"\"\" nb = pynautobot . api ( url = \"http://localhost\" , token = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\" ) gql = nb . graphql . query ( query = query , variables = variables ) print ( json . dumps ( gql . json , indent = 2 )) In the updated script, we add the variables = {\"site_name\": \"ams\"} variable. We then update the query to let GraphQL know that we will be sending parameters to to filter by site . The updated output is still a JSON object. Instead of fetching all devices, we are filtering by devices in the ams site. The PyNautobot SDK has some excellent GraphQL examples . Be sure to check out the documentation. Saving Queries \u00b6 Added in version 1.1.0 Queries can now be stored inside of Nautobot, allowing the user to easily rerun previously defined queries. Inside of Extensibility -> Data Management -> GraphQL Queries , there are views to create and manage GraphQL queries. Saved queries can be executed from the detailed query view or via a REST API request. The queries can also be populated from the detailed query view into GraphiQL by using the \"Open in GraphiQL\" button. Additionally, in the GraphiQL UI, there is now a menu item, \"Queries\", which can be used to populate GraphiQL with any previously saved query. To execute a stored query via the REST API, a POST request can be sent to /api/extras/graphql-queries/[slug]/run/ . Any GraphQL variables required by the query can be passed in as JSON data within the request body. Closing \u00b6 GraphQL is a powerful, yet simple, tool for querying the exact information that is necessary for the task at hand. For further information about GraphQL, be sure to check out the GraphQL Docs !","title":"GraphQL"},{"location":"user-guides/graphql.html#graphql-user-guide","text":"","title":"GraphQL User Guide"},{"location":"user-guides/graphql.html#introduction","text":"","title":"Introduction"},{"location":"user-guides/graphql.html#what-is-graphql","text":"GraphQL is a query language for your APIs and a runtime for fulfilling those queries with your existing data.","title":"What is GraphQL?"},{"location":"user-guides/graphql.html#how-graphql-simplifies-api-interactions","text":"When interacting with APIs, It's often necessary to build relationships between multiple models to achieve the result that is desired. Doing this typically requires multiple API calls to create the relationships. For example, lets assume that there are two devices in Nautobot. Each are assigned a site, region, roles, interfaces, and IP Addresses. Simply querying the /api/dcim/devices/ API route provides: View API Results { \"count\" : 2 , \"next\" : \"https://demo.nautobot.com/api/dcim/devices/?limit=1&offset=2\" , \"previous\" : \"https://demo.nautobot.com/api/dcim/devices/?limit=1\" , \"results\" : [ { \"id\" : \"c8886c88-6eff-4c4f-a079-4ef16b53d4f6\" , \"url\" : \"https://demo.nautobot.com/api/dcim/devices/c8886c88-6eff-4c4f-a079-4ef16b53d4f6/\" , \"name\" : \"ams-edge-02\" , \"display\" : \"ams-edge-02\" , \"device_type\" : { \"id\" : \"244ea351-3c7a-4d23-ba80-5db6b65312cc\" , \"url\" : \"https://demo.nautobot.com/api/dcim/device-types/244ea351-3c7a-4d23-ba80-5db6b65312cc/\" , \"manufacturer\" : { \"id\" : \"687f53d9-2c51-40fd-83aa-875e43d01a05\" , \"url\" : \"https://demo.nautobot.com/api/dcim/manufacturers/687f53d9-2c51-40fd-83aa-875e43d01a05/\" , \"name\" : \"Arista\" , \"slug\" : \"arista\" }, \"model\" : \"DCS-7280CR2-60\" , \"slug\" : \"dcs-7280cr2-60\" , \"display\" : \"Arista DCS-7280CR2-60\" }, \"device_role\" : { \"id\" : \"a3637471-6b4d-4f5a-a249-838d621abe60\" , \"url\" : \"https://demo.nautobot.com/api/dcim/device-roles/a3637471-6b4d-4f5a-a249-838d621abe60/\" , \"name\" : \"edge\" , \"slug\" : \"edge\" }, \"tenant\" : null , \"platform\" : null , \"serial\" : \"\" , \"asset_tag\" : null , \"site\" : { \"id\" : \"4ad439e9-4f1b-41c9-bc8c-dd7c1c921dc3\" , \"url\" : \"https://demo.nautobot.com/api/dcim/sites/4ad439e9-4f1b-41c9-bc8c-dd7c1c921dc3/\" , \"name\" : \"ams\" , \"slug\" : \"ams\" }, \"rack\" : { \"id\" : \"bff3f7af-bd77-49b6-a57a-9c4b8fc7673a\" , \"url\" : \"https://demo.nautobot.com/api/dcim/racks/bff3f7af-bd77-49b6-a57a-9c4b8fc7673a/\" , \"name\" : \"ams-102\" , \"display\" : \"ams-102\" }, \"position\" : 40 , \"face\" : { \"value\" : \"front\" , \"label\" : \"Front\" }, \"parent_device\" : null , \"status\" : { \"value\" : \"active\" , \"label\" : \"Active\" }, \"primary_ip\" : null , \"primary_ip4\" : null , \"primary_ip6\" : null , \"cluster\" : null , \"virtual_chassis\" : null , \"vc_position\" : null , \"vc_priority\" : null , \"comments\" : \"\" , \"local_context_data\" : null , \"tags\" : [], \"custom_fields\" : {}, \"config_context\" : {}, \"created\" : \"2021-02-25\" , \"last_updated\" : \"2021-02-25T14:51:57.609598\" } ] } There is a lot of useful information in that API call, but there is also a lot of information that is missing; such as interfaces and ip addresses associated with the devices. There is also potentially a lot of information that isn't needed for the specific task. To retrieve the missing information, subsequent API calls would need to be performed; and those API results would need to be correlated to the correct device. GraphQL reduces the complexity of performing multiple API calls and correlating results by empowering the user to create their own query that provides the user exactly what they want and nothing that they don't, in a single API call.","title":"How GraphQL simplifies API Interactions"},{"location":"user-guides/graphql.html#exploring-graphql-in-nautobot","text":"In Nautobot, there is a link to the GraphQL web interface at the bottom right-hand side of the page. The GraphQL web interface is called GraphiQL. Navigating to the URI ( /graphql ), brings up the GraphiQL tool for creating queries. This interface is useful for exploring the possibilities of GraphQL and validating that written queries execute successfully.","title":"Exploring GraphQL in Nautobot"},{"location":"user-guides/graphql.html#documentation-explorer","text":"If you're new to GraphQL, take a little bit of time to explore the Documentation Explorer . This can be accomplished by clicking the < Docs link in the GraphiQL interface. The information within Documentation Explorer is specific to creating queries in Nautobot. In the Documentation Explorer , search for devices . The results are all of the models that utilize the devices model. From the devices query, select devices from Query.devices . This will display all of the potential query fields from devices.","title":"Documentation Explorer"},{"location":"user-guides/graphql.html#first-query","text":"Now that you have a basic understanding of how to obtain information to query from the Documentation Explorer , let's craft a query. Earlier in the guide, a sample REST API call was performed to obtain device information. While the query had a lot of important information, it also lacked a lot of information. In this section, lets explore how to craft a GraphQL query that displays all of the information that we want. GraphQL queries are encapsulated in query { } flags (simply { } is also acceptable). With that in mind, let's craft our query from the GraphiQL interface to inspect all devices and display their device names. To do this, let's execute: query { devices { name } } This query will retrieve a list of all devices by their hostname. View GraphQL Query Results Now, let's modify the query to provide interface names for each device. We can do that by modifying the existing query to add interfaces { name } as a sub-query of devices . GraphiQL makes this process a bit easier, because it has syntax completion built in. query { devices { name interfaces { name } } } The result is a list of all the devices by their hostname and associated interfaces by their names. View GraphQL Query Results We can continue iterating on the query until we get exactly what we want from the query. For example, if I wanted to iterate on the previous query to not only display the interfaces of the devices, but also display the interface description, the IP Addresses associated with the interface, and whether or not the interface was a dedicated management interface; I would structure the query like: query { devices { name interfaces { name description mgmt_only ip_addresses { address } } } } The results of the query look like: View GraphQL Query Results","title":"First Query"},{"location":"user-guides/graphql.html#filtering-queries","text":"These queries are great, but they are displaying the interface attributes and device names for every device in the Nautobot inventory. Nautobot allows users to filter queries at any level as desired to narrow the scope of the returned data. As an example, we can filter the queried devices by their site location. This is done by adding (site: \"<site name>\") after devices . For example: query { devices(site: \"ams\") { name }} will display all devices in the ams site. As an example. We can query devices by their site location. This is done by adding (site: \"<site name>\") after devices . For example: query { devices(site: \"ams\") { name }} will display all devices in the ams site. View GraphQL Query Results GraphQL also allows you to filter by multiple attributes at once if desired. You can use the Documentation Explorer to assist you in finding criteria attributes to filter on. In this example, I add the role attribute in addition to site . query { devices(site: \"ams\", role: \"edge\") { name } } View GraphQL Query Results You can also filter at deeper levels of the query. On many to one relationships you can filter the results based on an attribute of the field. Any attribute that relates to a GraphQLType can be filtered. query { devices(site: \"ams\", role: \"edge\") { name interfaces(name: \"Ethernet1/1\") { name } } } query { sites(name: \"ams\") { devices(role: \"edge\") { name interfaces(name: \"Ethernet1/1\") { name } } } } View GraphQL Query Results Added in version 1.3.0 You can also paginate the results returned to you when the data set gets larger. To do so, use the keywords \"limit\" and \"offset\". The \"limit\" keyword will limit the count of results returned after the \"offset\". If no \"offset\" is specified, then the default offset is zero. query { devices(site: \"ams01\", , limit: 1, offset: 1) { name } }","title":"Filtering Queries"},{"location":"user-guides/graphql.html#using-the-graphql-api-in-nautobot","text":"Now that we've explored how to use the GraphiQL interface to help us create GraphQL queries, let's take our queries and call them with the REST API. This is where the real advantage is going to come in to play, because it will allow us to utilize these queries in a programmatic way. From the Nautobot Swagger documentation , we can see that the API calls to /api/graphql require a HTTP POST method. In the HTTP POST, the query field is required, as it is where we specify the GraphQL query. The variables field is optional; it's where we can assign values to any variables included in the query, if we choose to do so. To simplify the process even more, we'll utilize the PyNautobot SDK . Here is an example Python script using the PyNautobot SDK to query GraphQL: #!/usr/bin/env python3 import pynautobot import json query = \"\"\" query { devices { name interfaces { name description mgmt_only ip_addresses { address } } } } \"\"\" nb = pynautobot . api ( url = \"http://localhost\" , token = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\" ) gql = nb . graphql . query ( query = query ) print ( json . dumps ( gql . json , indent = 2 )) The contents of the query variable was taken directly from the example above where we grabbed all device interfaces and associated attributes. We then take the output and print the contents as a JSON object. Now, let's iterate on the script to filter the contents with the variable flag. Just as we did above, we'll filter by site . #!/usr/bin/env python3 import pynautobot import json variables = { \"site_name\" : \"ams\" } query = \"\"\" query ($site_name: String!) { devices (site: $site_name) { name interfaces { name mgmt_only ip_addresses { address } } } } \"\"\" nb = pynautobot . api ( url = \"http://localhost\" , token = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\" ) gql = nb . graphql . query ( query = query , variables = variables ) print ( json . dumps ( gql . json , indent = 2 )) In the updated script, we add the variables = {\"site_name\": \"ams\"} variable. We then update the query to let GraphQL know that we will be sending parameters to to filter by site . The updated output is still a JSON object. Instead of fetching all devices, we are filtering by devices in the ams site. The PyNautobot SDK has some excellent GraphQL examples . Be sure to check out the documentation.","title":"Using the GraphQL API in Nautobot"},{"location":"user-guides/graphql.html#saving-queries","text":"Added in version 1.1.0 Queries can now be stored inside of Nautobot, allowing the user to easily rerun previously defined queries. Inside of Extensibility -> Data Management -> GraphQL Queries , there are views to create and manage GraphQL queries. Saved queries can be executed from the detailed query view or via a REST API request. The queries can also be populated from the detailed query view into GraphiQL by using the \"Open in GraphiQL\" button. Additionally, in the GraphiQL UI, there is now a menu item, \"Queries\", which can be used to populate GraphiQL with any previously saved query. To execute a stored query via the REST API, a POST request can be sent to /api/extras/graphql-queries/[slug]/run/ . Any GraphQL variables required by the query can be passed in as JSON data within the request body.","title":"Saving Queries"},{"location":"user-guides/graphql.html#closing","text":"GraphQL is a powerful, yet simple, tool for querying the exact information that is necessary for the task at hand. For further information about GraphQL, be sure to check out the GraphQL Docs !","title":"Closing"},{"location":"user-guides/getting-started/index.html","text":"Introduction and Scope \u00b6 The audience for this user guide is users new to Nautobot. This guide will demonstrate how to use and perform common operations in Nautobot's Web UI. It is intended to assist the user with: Understanding the prerequisites for adding a new Device Creating the necessary prerequisite objects to add a new Device Adding new Devices Adding and understanding Regions Adding and understanding Platforms Adding and understanding Tenants and Tenant Groups Adding Interfaces to a Device Adding VLANs and understanding VLAN Groups Understanding IP Address Management (IPAM) in Nautobot How to navigate to the object you are interested in How to use the Search Bar to find the object you are interested in This guide does not cover every possible operation in Nautobot's Web UI. However, after reading this guide and working through the included exercises, the user should have a good feel for: Performing basic tasks How to navigate to a specific object and its related objects The Web UI's general structure and organization Extrapolating to use the Web UI for almost any task Requirements \u00b6 A functional Nautobot installation","title":"Getting Started In the Web UI"},{"location":"user-guides/getting-started/index.html#introduction-and-scope","text":"The audience for this user guide is users new to Nautobot. This guide will demonstrate how to use and perform common operations in Nautobot's Web UI. It is intended to assist the user with: Understanding the prerequisites for adding a new Device Creating the necessary prerequisite objects to add a new Device Adding new Devices Adding and understanding Regions Adding and understanding Platforms Adding and understanding Tenants and Tenant Groups Adding Interfaces to a Device Adding VLANs and understanding VLAN Groups Understanding IP Address Management (IPAM) in Nautobot How to navigate to the object you are interested in How to use the Search Bar to find the object you are interested in This guide does not cover every possible operation in Nautobot's Web UI. However, after reading this guide and working through the included exercises, the user should have a good feel for: Performing basic tasks How to navigate to a specific object and its related objects The Web UI's general structure and organization Extrapolating to use the Web UI for almost any task","title":"Introduction and Scope"},{"location":"user-guides/getting-started/index.html#requirements","text":"A functional Nautobot installation","title":"Requirements"},{"location":"user-guides/getting-started/creating-devices.html","text":"Creating Devices in Nautobot \u00b6 Every piece of hardware which is installed within a site or rack exists in Nautobot as a device. More information on Devices can be found in the Devices section of the Nautobot documentation. A network Device in Nautobot has a few required attributes: A Device Role A Device Type A Device Type requires a Manufacturer A Site Looking at the list above, there are four objects in Nautobot that must be present prior to creating a related Device. The following sections will guide you through how to create each object type. Creating a Device Role \u00b6 Devices can be organized by functional roles, which are fully customizable by the user. More information on Device Roles can be found in the Device Roles section of the Nautobot documentation. To create a new Device, you will need an existing Device Role or need to create a new Device Role instance. Note You might create roles for core switches, distribution switches, and access switches within your network. To create a new Device Role: Click on Devices in the top navigation menu Find Device Roles on the drop-down menu Select + In the Add a new device role form, populate the Name The Slug will auto-populate based on the Name field, but can be manually overwritten Click on Create Note A slug is a short label for something, containing only letters, numbers, underscores or hyphens. They\u2019re generally used within URLs. For example: /dcim/device-roles/customer-edge/ Creating a Manufacturer \u00b6 A Manufacturer represents the \"make\" of a device; e.g. Cisco or Dell. Each device type must be assigned to a manufacturer. More information on Manufacturers is in the Manufacturers section of the Nautobot documentation. To create a new Device Type, you will need an existing Manufacturer or need to create a new Manufacturer instance. To create a new Manufacturer: Click on Devices in the top navigation menu Find Manufacturers on the drop-down Select + In the Add a new manufacturer form, populate the Name The Slug will auto-populate based on the entered Name , but can be manually overwritten Click on Create Creating a Device Type \u00b6 A device type represents a particular make and model of hardware that exists in the real world. Device types define the physical attributes of a device (rack height and depth) and its individual components (console, power, network interfaces, and so on). More information on Device Types is in the Device Types section of the Nautobot documentation. To create a new Device, you will need an existing Device Type or need to create a new Device Type instance. To create a new Device Type: Click on Devices in the top navigation menu Find Device Types Select + to go to the Add a new device type form Select the Manufacturer from the drop-down selector Populate the Model (name) Click on Create Creating a Site \u00b6 How you choose to employ sites when modeling your network may vary depending on the nature of your organization, but generally a site will equate to a building or campus. For example, a chain of banks might create a site to represent each of its branches, a site for its corporate headquarters, and two additional sites for its presence in two co-location facilities. More information on Sites is found in the Sites section of the Nautobot documentation. To create a new Device, you will need an existing Site or need to create a new Site instance. To create a new Site: Click on Organization in the top navigation menu Find Sites Select + to go to the Add a new site form Populate the Site's Name The Slug will auto-populate based on the Name field, but can be manually overwritten Set the Status to Active in the drop-down selector Click on Create at the bottom of the form (not shown) Creating a Device \u00b6 To create a new Device: Click on Devices in the top navigation menu Find Devices Select + to go to the Add a new device form Populate the Name Select the Device Role from the drop-down selector Select the Device Type from the down-down selector Select the Site from the drop-down selector Set the Status to the appropriate value in the drop-down selector Click on Create at the bottom of the form (not shown)","title":"Creating Devices"},{"location":"user-guides/getting-started/creating-devices.html#creating-devices-in-nautobot","text":"Every piece of hardware which is installed within a site or rack exists in Nautobot as a device. More information on Devices can be found in the Devices section of the Nautobot documentation. A network Device in Nautobot has a few required attributes: A Device Role A Device Type A Device Type requires a Manufacturer A Site Looking at the list above, there are four objects in Nautobot that must be present prior to creating a related Device. The following sections will guide you through how to create each object type.","title":"Creating Devices in Nautobot"},{"location":"user-guides/getting-started/creating-devices.html#creating-a-device-role","text":"Devices can be organized by functional roles, which are fully customizable by the user. More information on Device Roles can be found in the Device Roles section of the Nautobot documentation. To create a new Device, you will need an existing Device Role or need to create a new Device Role instance. Note You might create roles for core switches, distribution switches, and access switches within your network. To create a new Device Role: Click on Devices in the top navigation menu Find Device Roles on the drop-down menu Select + In the Add a new device role form, populate the Name The Slug will auto-populate based on the Name field, but can be manually overwritten Click on Create Note A slug is a short label for something, containing only letters, numbers, underscores or hyphens. They\u2019re generally used within URLs. For example: /dcim/device-roles/customer-edge/","title":"Creating a Device Role"},{"location":"user-guides/getting-started/creating-devices.html#creating-a-manufacturer","text":"A Manufacturer represents the \"make\" of a device; e.g. Cisco or Dell. Each device type must be assigned to a manufacturer. More information on Manufacturers is in the Manufacturers section of the Nautobot documentation. To create a new Device Type, you will need an existing Manufacturer or need to create a new Manufacturer instance. To create a new Manufacturer: Click on Devices in the top navigation menu Find Manufacturers on the drop-down Select + In the Add a new manufacturer form, populate the Name The Slug will auto-populate based on the entered Name , but can be manually overwritten Click on Create","title":"Creating a Manufacturer"},{"location":"user-guides/getting-started/creating-devices.html#creating-a-device-type","text":"A device type represents a particular make and model of hardware that exists in the real world. Device types define the physical attributes of a device (rack height and depth) and its individual components (console, power, network interfaces, and so on). More information on Device Types is in the Device Types section of the Nautobot documentation. To create a new Device, you will need an existing Device Type or need to create a new Device Type instance. To create a new Device Type: Click on Devices in the top navigation menu Find Device Types Select + to go to the Add a new device type form Select the Manufacturer from the drop-down selector Populate the Model (name) Click on Create","title":"Creating a Device Type"},{"location":"user-guides/getting-started/creating-devices.html#creating-a-site","text":"How you choose to employ sites when modeling your network may vary depending on the nature of your organization, but generally a site will equate to a building or campus. For example, a chain of banks might create a site to represent each of its branches, a site for its corporate headquarters, and two additional sites for its presence in two co-location facilities. More information on Sites is found in the Sites section of the Nautobot documentation. To create a new Device, you will need an existing Site or need to create a new Site instance. To create a new Site: Click on Organization in the top navigation menu Find Sites Select + to go to the Add a new site form Populate the Site's Name The Slug will auto-populate based on the Name field, but can be manually overwritten Set the Status to Active in the drop-down selector Click on Create at the bottom of the form (not shown)","title":"Creating a Site"},{"location":"user-guides/getting-started/creating-devices.html#creating-a-device","text":"To create a new Device: Click on Devices in the top navigation menu Find Devices Select + to go to the Add a new device form Populate the Name Select the Device Role from the drop-down selector Select the Device Type from the down-down selector Select the Site from the drop-down selector Set the Status to the appropriate value in the drop-down selector Click on Create at the bottom of the form (not shown)","title":"Creating a Device"},{"location":"user-guides/getting-started/interfaces.html","text":"Interfaces \u00b6 Interfaces in Nautobot represent network interfaces used to exchange data with connected devices. More information on Interfaces is in the Interfaces section of the Nautobot documentation. Interfaces can be added at the Device or the Device Type level: Interfaces added to an individual Device are for that Device only Interfaces added to the Device Type will be applied to all NEW implementations of that Device Type (not existing implementations) Which one you select depends on your use case; in some instances you will need to use both, as in the example below. Interface Add Example \u00b6 Let\u2019s take an example: We want to define a Device Type of MX240-edge This Device Type will have 20x 10G ( xe-[0-1]/0/[0-9] ) Interfaces and one LAG ( ae0 ) Interface The xe-0/0/9 and xe-1/0/9 Interfaces will be members of the ae0 Interface Creating a Device Type \u00b6 We are going to use the Device Type to achieve part of this goal. Using the Device Type will also provide repeatability because the Device Type object also serves as a template. This templating feature is demonstrated in this example. Device Types can serve as templates for Devices, and as such the two are very similar. Here is a screenshot of a Device Type: Creating a Device Type is very similar to creating a Device . To create a Device Type: Click on Devices in the top navigation menu Click on the Device Types option in the drop-down menu On the Device Types page Add + a new Device Type A Device Type requires a Manufacturer object to be created prior to creating the Device Type Device Type requires Manufacturer , Model , Slug , and Height values at creation In this example, name the Device Type MX240-edge On the home page for the specific Device Type, click on +Add Components and select Interfaces You will now see the Interface Template form: Add the ae0 Interface Template Manufacturer will auto-populate to the Manufacturer of the Device Type you are editing Device Type will auto-populate to the Device Type you are editing Populate a Name Select a Type of Link Aggregation Group (LAG) from the drop-down selector Add a Description and Label (optional) Click Create and Add More Create the xe- Interfaces This example shows bulk creation using a range ( xe-[0-1]/0/[0-9] ) in Name Select the appropriate Type from the drop-down selector Click on Create Clicking the Create button will take you back to the home screen for the Device Type you are editing. There, you will see that the Interfaces tab now has the expected 21 Interfaces listed. Note As of this writing (Nautobot 1.0.3), Interfaces cannot be assigned in to a LAG in the Device Type template; component Interfaces must be designated in the specific instantiation of a Device created from the Device Type. Creating a New Device Using the Device Type \u00b6 Create a new Device with these attributes: Name = edge2.van1 Device role select Customer Edge Device type select Juniper MX240-edge (this will show up as a fusion of the Manufacturer ( Juniper ) for the Device Type and the Device Type ( MX240-edge ) Names) Site select Vancouver 1 On the main screen for the new Device , you will see an Interfaces tab with the expected Interfaces from the Device Type template: Note Device Type properties only apply to new instantiations of Devices from the Type; Devices created prior to a modification of the Device Type will not inherit the changes retroactively Specifying the LAG Components on the Device \u00b6 As of this writing (Nautobot 1.0.3), LAG component Interfaces cannot be assigned in the Device Type template, so we will edit this new Device, specifying the component ae0 Interfaces. On the new Device's main page, select the appropriate Interfaces ( xe-0/0/9 and xe-1/0/9 ) to be added to ae0 and click on the Edit button On the Editing Interfaces form, select ae0 in the Parent LAG drop-down selector Click on Apply ; you will be taken back to the main page for the Device On the Device's main page, notice that xe-0/0/9 and xe-1/0/9 are now assigned to the ae0 LAG:","title":"Interfaces"},{"location":"user-guides/getting-started/interfaces.html#interfaces","text":"Interfaces in Nautobot represent network interfaces used to exchange data with connected devices. More information on Interfaces is in the Interfaces section of the Nautobot documentation. Interfaces can be added at the Device or the Device Type level: Interfaces added to an individual Device are for that Device only Interfaces added to the Device Type will be applied to all NEW implementations of that Device Type (not existing implementations) Which one you select depends on your use case; in some instances you will need to use both, as in the example below.","title":"Interfaces"},{"location":"user-guides/getting-started/interfaces.html#interface-add-example","text":"Let\u2019s take an example: We want to define a Device Type of MX240-edge This Device Type will have 20x 10G ( xe-[0-1]/0/[0-9] ) Interfaces and one LAG ( ae0 ) Interface The xe-0/0/9 and xe-1/0/9 Interfaces will be members of the ae0 Interface","title":"Interface Add Example"},{"location":"user-guides/getting-started/interfaces.html#creating-a-device-type","text":"We are going to use the Device Type to achieve part of this goal. Using the Device Type will also provide repeatability because the Device Type object also serves as a template. This templating feature is demonstrated in this example. Device Types can serve as templates for Devices, and as such the two are very similar. Here is a screenshot of a Device Type: Creating a Device Type is very similar to creating a Device . To create a Device Type: Click on Devices in the top navigation menu Click on the Device Types option in the drop-down menu On the Device Types page Add + a new Device Type A Device Type requires a Manufacturer object to be created prior to creating the Device Type Device Type requires Manufacturer , Model , Slug , and Height values at creation In this example, name the Device Type MX240-edge On the home page for the specific Device Type, click on +Add Components and select Interfaces You will now see the Interface Template form: Add the ae0 Interface Template Manufacturer will auto-populate to the Manufacturer of the Device Type you are editing Device Type will auto-populate to the Device Type you are editing Populate a Name Select a Type of Link Aggregation Group (LAG) from the drop-down selector Add a Description and Label (optional) Click Create and Add More Create the xe- Interfaces This example shows bulk creation using a range ( xe-[0-1]/0/[0-9] ) in Name Select the appropriate Type from the drop-down selector Click on Create Clicking the Create button will take you back to the home screen for the Device Type you are editing. There, you will see that the Interfaces tab now has the expected 21 Interfaces listed. Note As of this writing (Nautobot 1.0.3), Interfaces cannot be assigned in to a LAG in the Device Type template; component Interfaces must be designated in the specific instantiation of a Device created from the Device Type.","title":"Creating a Device Type"},{"location":"user-guides/getting-started/interfaces.html#creating-a-new-device-using-the-device-type","text":"Create a new Device with these attributes: Name = edge2.van1 Device role select Customer Edge Device type select Juniper MX240-edge (this will show up as a fusion of the Manufacturer ( Juniper ) for the Device Type and the Device Type ( MX240-edge ) Names) Site select Vancouver 1 On the main screen for the new Device , you will see an Interfaces tab with the expected Interfaces from the Device Type template: Note Device Type properties only apply to new instantiations of Devices from the Type; Devices created prior to a modification of the Device Type will not inherit the changes retroactively","title":"Creating a New Device Using the Device Type"},{"location":"user-guides/getting-started/interfaces.html#specifying-the-lag-components-on-the-device","text":"As of this writing (Nautobot 1.0.3), LAG component Interfaces cannot be assigned in the Device Type template, so we will edit this new Device, specifying the component ae0 Interfaces. On the new Device's main page, select the appropriate Interfaces ( xe-0/0/9 and xe-1/0/9 ) to be added to ae0 and click on the Edit button On the Editing Interfaces form, select ae0 in the Parent LAG drop-down selector Click on Apply ; you will be taken back to the main page for the Device On the Device's main page, notice that xe-0/0/9 and xe-1/0/9 are now assigned to the ae0 LAG:","title":"Specifying the LAG Components on the Device"},{"location":"user-guides/getting-started/ipam.html","text":"IP Address Management \u00b6 This next section will demonstrate how to: Create a Regional Internet Registry Create an Aggregate Create assignable IP addresses Assign an IP address to an Interface on a Device These type of operations fall under an umbrella called IP Address Management (IPAM). The Nautobot documentation IPAM section has more detail on IPAM and each operation. Creating a Regional Internet Registry (RIR) \u00b6 A RIR allocates globally-routeable IP address space. There are five top-level RIRs, each responsible for a particular section of the globe. Nautobot also considers RFCs 1918 and 6589 to be RIR-like because they allocate private IP space. Nautobot requires any IP allocation be attributed to a RIR. To create a RIR: Click on IPAM in the top-level navigation menu Find RIRs and click on the + ; this takes you to the Add a new RIR form Specify the RIR Name The Slug will auto-populate based on the Name field, but this default can be manually overwritten There is a checkbox to flag Private (internal use) only Click on the Create button Creating an Aggregate \u00b6 An aggregate is a consolidated allocation of IP address space, whether it is public or private. An aggregate must map back to a RIR that has allocated the space. To create an Aggregate: Click on IPAM in the top-level navigation menu Find Aggregates and click on the + ; this takes you to the Add a new aggregate form Specify the Prefix in a prefix/mask format Select a RIR from the drop-down selector Click the Create button You will then be taken to the Aggregates main page, where you will see the Aggregate you just created. Creating a Prefix \u00b6 A Prefix is an IPv4 or IPv6 network and mask expressed in CIDR notation (e.g. 192.0.2.0/24). Prefixes are automatically organized by their parent Aggregates. Additionally, each Prefix can be assigned to a particular Site and virtual routing and forwarding (VRF) instance. To create a prefix: Click on IPAM in the top-level navigation menu Look for Prefixes and click on the + This will take you to the Add a new prefix form Populate the Prefix in CIDR notation Select a Status from the drop-down selector If all addresses in the Prefix are usable, check the Is a pool flag Click on the Create button (not shown) Verifying a Prefix in an Aggregate \u00b6 To view the Prefixes in an Aggregate: Click on IPAM in the top-level navigation menu Click on Aggregates to go to the Aggregates main page Find the Aggregate you are interested in and click on it On the main page for the specific Aggregate, look for a specific Prefix ( 10.10.10.0/24 in this example) Note Nautobot will break an Aggregate into the highest-level child Prefixes to carve out user-defined Prefixes Creating IP Addresses \u00b6 To create an IP address: Click on IPAM in the top-level navigation menu Find IP Addresses and click on the + This will take you to the Add a new IP address form In this example, we are going to create multiple individual addresses, so click on the Bulk Create tab Populate an Address pattern This example uses 10.10.10.[0-1,2-3,6-7]/31 to create 3 non-contiguous /31's The specified mask should be exactly as would be configured on the Device's Interface Select Active for Status from the drop-down selector Click on the Create button Assigning IP Addresses \u00b6 To assign an IP Address to a specific Device and Interface: Click on IPAM in the top-level navigation menu Click on IP Addresses to go to the main IP Addresses page Find the IP address you wish to assign to an Interface and click on it On the main page for the Address, click on the Edit button to go to the Editing IP address page Once on the Editing IP address page: Select a Device from the drop-down selector Select an Interface on the Device Click on the Update button This will take you back to the main page for the IP Address, where you will see the assignment shown as device (interface) Finding an IP Address for an Interface \u00b6 Click on Devices on the top-level navigation menu Click on Devices to go to the main page for Devices Search for the Device you are interested in ( edge2.van1 in this example) and click on the link to go to the main page for the Device Go to the Interfaces tab and look for the row with the Interface you are interested in; find the IP Address(es) in the IP Addresses column in the row Finding IP Addresses in a Prefix \u00b6 To find information on a particular Prefix: Click on IPAM in the top-level drop-down menu Click on Prefixes to get to the Prefixes main page Find the Prefix you are interested in and click on the link To view the available and allocated IP Addresses, click on the IP Addresses tab","title":"IP Address Management"},{"location":"user-guides/getting-started/ipam.html#ip-address-management","text":"This next section will demonstrate how to: Create a Regional Internet Registry Create an Aggregate Create assignable IP addresses Assign an IP address to an Interface on a Device These type of operations fall under an umbrella called IP Address Management (IPAM). The Nautobot documentation IPAM section has more detail on IPAM and each operation.","title":"IP Address Management"},{"location":"user-guides/getting-started/ipam.html#creating-a-regional-internet-registry-rir","text":"A RIR allocates globally-routeable IP address space. There are five top-level RIRs, each responsible for a particular section of the globe. Nautobot also considers RFCs 1918 and 6589 to be RIR-like because they allocate private IP space. Nautobot requires any IP allocation be attributed to a RIR. To create a RIR: Click on IPAM in the top-level navigation menu Find RIRs and click on the + ; this takes you to the Add a new RIR form Specify the RIR Name The Slug will auto-populate based on the Name field, but this default can be manually overwritten There is a checkbox to flag Private (internal use) only Click on the Create button","title":"Creating a Regional Internet Registry (RIR)"},{"location":"user-guides/getting-started/ipam.html#creating-an-aggregate","text":"An aggregate is a consolidated allocation of IP address space, whether it is public or private. An aggregate must map back to a RIR that has allocated the space. To create an Aggregate: Click on IPAM in the top-level navigation menu Find Aggregates and click on the + ; this takes you to the Add a new aggregate form Specify the Prefix in a prefix/mask format Select a RIR from the drop-down selector Click the Create button You will then be taken to the Aggregates main page, where you will see the Aggregate you just created.","title":"Creating an Aggregate"},{"location":"user-guides/getting-started/ipam.html#creating-a-prefix","text":"A Prefix is an IPv4 or IPv6 network and mask expressed in CIDR notation (e.g. 192.0.2.0/24). Prefixes are automatically organized by their parent Aggregates. Additionally, each Prefix can be assigned to a particular Site and virtual routing and forwarding (VRF) instance. To create a prefix: Click on IPAM in the top-level navigation menu Look for Prefixes and click on the + This will take you to the Add a new prefix form Populate the Prefix in CIDR notation Select a Status from the drop-down selector If all addresses in the Prefix are usable, check the Is a pool flag Click on the Create button (not shown)","title":"Creating a Prefix"},{"location":"user-guides/getting-started/ipam.html#verifying-a-prefix-in-an-aggregate","text":"To view the Prefixes in an Aggregate: Click on IPAM in the top-level navigation menu Click on Aggregates to go to the Aggregates main page Find the Aggregate you are interested in and click on it On the main page for the specific Aggregate, look for a specific Prefix ( 10.10.10.0/24 in this example) Note Nautobot will break an Aggregate into the highest-level child Prefixes to carve out user-defined Prefixes","title":"Verifying a Prefix in an Aggregate"},{"location":"user-guides/getting-started/ipam.html#creating-ip-addresses","text":"To create an IP address: Click on IPAM in the top-level navigation menu Find IP Addresses and click on the + This will take you to the Add a new IP address form In this example, we are going to create multiple individual addresses, so click on the Bulk Create tab Populate an Address pattern This example uses 10.10.10.[0-1,2-3,6-7]/31 to create 3 non-contiguous /31's The specified mask should be exactly as would be configured on the Device's Interface Select Active for Status from the drop-down selector Click on the Create button","title":"Creating IP Addresses"},{"location":"user-guides/getting-started/ipam.html#assigning-ip-addresses","text":"To assign an IP Address to a specific Device and Interface: Click on IPAM in the top-level navigation menu Click on IP Addresses to go to the main IP Addresses page Find the IP address you wish to assign to an Interface and click on it On the main page for the Address, click on the Edit button to go to the Editing IP address page Once on the Editing IP address page: Select a Device from the drop-down selector Select an Interface on the Device Click on the Update button This will take you back to the main page for the IP Address, where you will see the assignment shown as device (interface)","title":"Assigning IP Addresses"},{"location":"user-guides/getting-started/ipam.html#finding-an-ip-address-for-an-interface","text":"Click on Devices on the top-level navigation menu Click on Devices to go to the main page for Devices Search for the Device you are interested in ( edge2.van1 in this example) and click on the link to go to the main page for the Device Go to the Interfaces tab and look for the row with the Interface you are interested in; find the IP Address(es) in the IP Addresses column in the row","title":"Finding an IP Address for an Interface"},{"location":"user-guides/getting-started/ipam.html#finding-ip-addresses-in-a-prefix","text":"To find information on a particular Prefix: Click on IPAM in the top-level drop-down menu Click on Prefixes to get to the Prefixes main page Find the Prefix you are interested in and click on the link To view the available and allocated IP Addresses, click on the IP Addresses tab","title":"Finding IP Addresses in a Prefix"},{"location":"user-guides/getting-started/platforms.html","text":"Platforms \u00b6 A Platform object can hold various information about a device, such as the OS, OS version, and NAPALM driver. Further information is available in the Platforms section of the Nautobot docs. While use of Platforms is optional, they provide great value in many use cases. Creating a Platform \u00b6 To create a Platform: Click on Devices in the top navigation menu Find Platforms and click on the + icon in the menu This takes you to the Add a new platform form Provide a Name (required) The Slug will auto-populate based on the Name field; you may override this if necessary Select a Manufacturer from the drop-down selector (optional) Provide the name of the NAPALM driver (optional) (Note: this must be the exact name of the NAPALM driver) Provide NAPALM arguments (optional) Provide Description (optional) Click on the Create button Tip NAPALM Driver Options include: eos (Arista) ios (Cisco) nxos (used with nxapi feature) nxos_ssh (used for ssh login) junos Once completed, you will be sent to the Platforms page, where all the Platform variants are shown. Tip Different use cases for Platforms may require different information. For example, to use a specific Platform with the Device Onboarding Plugin , you may be required to override the default Slug value with that of the Netmiko device_type Adding a Platform to an Existing or New Device \u00b6 The Platform attribute is optional for a Device. It is quite easy to add a Platform to an existing Device or while creating a new Device. A Device's Platform is specified on the Add a new device or Editing device page, depending on whether you are adding or editing a Device. Reaching the Editing device page for an Existing Device \u00b6 The quickest way to reach the Editing device page for a Device is to search for the Device name in the Search Bar. Tip See the Search Bar section of this guide for more information on using the Search Bar Type the device name in the Search Bar in the upper-right of the page Click on the device name on the results page Click on the Edit button on the main page for the Device Reaching the Add a new device Page \u00b6 For a new Device, follow the steps to create a new Device to reach the Add a new device page. Specifying the Device's Platform \u00b6 While on the Add a new device / Editing device page, scroll down to the 'Management' section In the Platform field drop-down selector, select the appropriate Platform Click the 'Update' (edit Device) or Create (add Device) button Tip The only choices that will appear in the Platform drop-down selector will be those Platforms that have the same Manufacturer specified in the Device's Hardware section.","title":"Platforms"},{"location":"user-guides/getting-started/platforms.html#platforms","text":"A Platform object can hold various information about a device, such as the OS, OS version, and NAPALM driver. Further information is available in the Platforms section of the Nautobot docs. While use of Platforms is optional, they provide great value in many use cases.","title":"Platforms"},{"location":"user-guides/getting-started/platforms.html#creating-a-platform","text":"To create a Platform: Click on Devices in the top navigation menu Find Platforms and click on the + icon in the menu This takes you to the Add a new platform form Provide a Name (required) The Slug will auto-populate based on the Name field; you may override this if necessary Select a Manufacturer from the drop-down selector (optional) Provide the name of the NAPALM driver (optional) (Note: this must be the exact name of the NAPALM driver) Provide NAPALM arguments (optional) Provide Description (optional) Click on the Create button Tip NAPALM Driver Options include: eos (Arista) ios (Cisco) nxos (used with nxapi feature) nxos_ssh (used for ssh login) junos Once completed, you will be sent to the Platforms page, where all the Platform variants are shown. Tip Different use cases for Platforms may require different information. For example, to use a specific Platform with the Device Onboarding Plugin , you may be required to override the default Slug value with that of the Netmiko device_type","title":"Creating a Platform"},{"location":"user-guides/getting-started/platforms.html#adding-a-platform-to-an-existing-or-new-device","text":"The Platform attribute is optional for a Device. It is quite easy to add a Platform to an existing Device or while creating a new Device. A Device's Platform is specified on the Add a new device or Editing device page, depending on whether you are adding or editing a Device.","title":"Adding a Platform to an Existing or New Device"},{"location":"user-guides/getting-started/platforms.html#reaching-the-editing-device-page-for-an-existing-device","text":"The quickest way to reach the Editing device page for a Device is to search for the Device name in the Search Bar. Tip See the Search Bar section of this guide for more information on using the Search Bar Type the device name in the Search Bar in the upper-right of the page Click on the device name on the results page Click on the Edit button on the main page for the Device","title":"Reaching the Editing device page for an Existing Device"},{"location":"user-guides/getting-started/platforms.html#reaching-the-add-a-new-device-page","text":"For a new Device, follow the steps to create a new Device to reach the Add a new device page.","title":"Reaching the Add a new device Page"},{"location":"user-guides/getting-started/platforms.html#specifying-the-devices-platform","text":"While on the Add a new device / Editing device page, scroll down to the 'Management' section In the Platform field drop-down selector, select the appropriate Platform Click the 'Update' (edit Device) or Create (add Device) button Tip The only choices that will appear in the Platform drop-down selector will be those Platforms that have the same Manufacturer specified in the Device's Hardware section.","title":"Specifying the Device's Platform"},{"location":"user-guides/getting-started/regions.html","text":"Regions \u00b6 Regions are administrative domains, used to organize Sites and other Regions. They can be nested recursively. A Region might represent a continent, country, city, campus, or other area. A Region can contain Sites and other Regions. Additional information on Regions is in the Regions section of the Nautobot documentation . In the following exercise, we will create three Regions: North America Intended to hold Regions that represent each component country Canada Intended to hold Regions that represent markets in Canada Vancouver Intended to hold Sites within the Vancouver market Creating a Region \u00b6 To create a Region: Click on Organization in the top menu Click on Regions in the Organization drop-down menu From this page you can view any existing Regions Click on the blue + Add button The screenshots below show the creation of each Region. North America: Leave the Parent blank; North America will be a top-tier Region Populate the Name to be North America The Slug will auto-populate based on the Name field, but can be manually overwritten Click on the Create and Add Another button Canada: Select North America as the Parent from the drop-down menu selector Populate the Name to be Canada The Slug will auto-populate based on the Name field, but can be manually overwritten Click on the Create and Add Another button Vancouver: Select Canada as the Parent from the drop-down menu selector Populate the Name to be Vancouver The Slug will auto-populate based on the Name field, but can be manually overwritten Click on the Create button Adding a Site to a Region \u00b6 It's quite easy to add an existing Site to a Region. To access the page where you can modify a Site: Click on Organization in the top navigation menu Click on Sites in the drop-down menu The Sites page will appear; click on the specific Site you want to modify ( Vancouver 1 in this example shown). On the page for the Site, click on the Edit button Once you're on the Editing site form: Look for the Region drop-down selection menu and select the appropriate Region Observe that the correct Region for the Site is selected Click on the Update button The page for the Site you updated will appear again, showing the updated Region hierarchy for the Site. Note In the example, notice that we assigned the Vancouver 1 Site to Vancouver , which is tier-3 Region. Notice that the Regional hierarchy is displayed for the Vancouver 1 Site: North America / Canada / Vancouver","title":"Regions"},{"location":"user-guides/getting-started/regions.html#regions","text":"Regions are administrative domains, used to organize Sites and other Regions. They can be nested recursively. A Region might represent a continent, country, city, campus, or other area. A Region can contain Sites and other Regions. Additional information on Regions is in the Regions section of the Nautobot documentation . In the following exercise, we will create three Regions: North America Intended to hold Regions that represent each component country Canada Intended to hold Regions that represent markets in Canada Vancouver Intended to hold Sites within the Vancouver market","title":"Regions"},{"location":"user-guides/getting-started/regions.html#creating-a-region","text":"To create a Region: Click on Organization in the top menu Click on Regions in the Organization drop-down menu From this page you can view any existing Regions Click on the blue + Add button The screenshots below show the creation of each Region. North America: Leave the Parent blank; North America will be a top-tier Region Populate the Name to be North America The Slug will auto-populate based on the Name field, but can be manually overwritten Click on the Create and Add Another button Canada: Select North America as the Parent from the drop-down menu selector Populate the Name to be Canada The Slug will auto-populate based on the Name field, but can be manually overwritten Click on the Create and Add Another button Vancouver: Select Canada as the Parent from the drop-down menu selector Populate the Name to be Vancouver The Slug will auto-populate based on the Name field, but can be manually overwritten Click on the Create button","title":"Creating a Region"},{"location":"user-guides/getting-started/regions.html#adding-a-site-to-a-region","text":"It's quite easy to add an existing Site to a Region. To access the page where you can modify a Site: Click on Organization in the top navigation menu Click on Sites in the drop-down menu The Sites page will appear; click on the specific Site you want to modify ( Vancouver 1 in this example shown). On the page for the Site, click on the Edit button Once you're on the Editing site form: Look for the Region drop-down selection menu and select the appropriate Region Observe that the correct Region for the Site is selected Click on the Update button The page for the Site you updated will appear again, showing the updated Region hierarchy for the Site. Note In the example, notice that we assigned the Vancouver 1 Site to Vancouver , which is tier-3 Region. Notice that the Regional hierarchy is displayed for the Vancouver 1 Site: North America / Canada / Vancouver","title":"Adding a Site to a Region"},{"location":"user-guides/getting-started/search-bar.html","text":"The Search Bar \u00b6 The exercises in the prior sections in this Getting Started Guide walked you through how to navigate to the proper objects. You can also use the Search Bar to find desired objects with either partial or complete alpha/numeric characters or exact UUID. We will show two quick examples. Example one: Type in 10.10.10.0 in the Search Bar and click on Search This takes you to a search results page Aggregate search result 10.0.0.0/8 (this is the Aggregate for the Prefix 10.10.10.0/24 ) Prefix search result 10.10.10.0/24 IP Address search result 10.10.10.0/31 Interface related to 10.10.10.0/31 Interface Parent (Device) for 10.10.10.0/31 Clicking on any of these objects takes you to the main page for that object. This example shows the result of clicking on the IP Address object (4). Example two shows a Device-specific search: Search for edge This takes you to a search results page In the drop-down selector to the right, select Devices Search results for Devices with edge in the name Tenants for each Device (if applicable) Device Type for each Device Site for each Device Clicking on an of the links for the results takes you to the main page for that object. For example: Clicking on the Vancouver 1 Site takes you to the main page for the Site. Clicking on the edge2.van1 Device takes you to the main page for the Device END OF Getting Started in the Web UI GUIDE","title":"The Search Bar"},{"location":"user-guides/getting-started/search-bar.html#the-search-bar","text":"The exercises in the prior sections in this Getting Started Guide walked you through how to navigate to the proper objects. You can also use the Search Bar to find desired objects with either partial or complete alpha/numeric characters or exact UUID. We will show two quick examples. Example one: Type in 10.10.10.0 in the Search Bar and click on Search This takes you to a search results page Aggregate search result 10.0.0.0/8 (this is the Aggregate for the Prefix 10.10.10.0/24 ) Prefix search result 10.10.10.0/24 IP Address search result 10.10.10.0/31 Interface related to 10.10.10.0/31 Interface Parent (Device) for 10.10.10.0/31 Clicking on any of these objects takes you to the main page for that object. This example shows the result of clicking on the IP Address object (4). Example two shows a Device-specific search: Search for edge This takes you to a search results page In the drop-down selector to the right, select Devices Search results for Devices with edge in the name Tenants for each Device (if applicable) Device Type for each Device Site for each Device Clicking on an of the links for the results takes you to the main page for that object. For example: Clicking on the Vancouver 1 Site takes you to the main page for the Site. Clicking on the edge2.van1 Device takes you to the main page for the Device END OF Getting Started in the Web UI GUIDE","title":"The Search Bar"},{"location":"user-guides/getting-started/tenants.html","text":"Tenants \u00b6 A 'Tenant' signifies ownership of an object in Nautobot and as such, any object may only have a single Tenant assigned. More information on Tenants can be found in the Tenants section of the Nautobot docs. Creating a Tenant \u00b6 To create a Tenant: Click on Organization on the top navigation menu Find Tenants and click on the + Populate the Name field The Slug field will auto-populate based on the Name field, but this can be manually overwritten Click the Create button Assigning a Tenant to an Object \u00b6 It is simple to assign a Tenant to an existing object. This next example will add a Tenant to an existing Device. Click on Devices in the top navigation menu Look for the Devices option and click on it This will take you to the Devices page Click on the specific Device you want to add the Tenant to This will take you to the main page for that Device On the specific Device page, click on the Edit button Once on the page to edit the Device: Make a selection from the Tenant drop-down menu selector Click the Update button This will take you back to the main page for the Device. Notice that the Tenant field is now populated/updated.","title":"Tenants"},{"location":"user-guides/getting-started/tenants.html#tenants","text":"A 'Tenant' signifies ownership of an object in Nautobot and as such, any object may only have a single Tenant assigned. More information on Tenants can be found in the Tenants section of the Nautobot docs.","title":"Tenants"},{"location":"user-guides/getting-started/tenants.html#creating-a-tenant","text":"To create a Tenant: Click on Organization on the top navigation menu Find Tenants and click on the + Populate the Name field The Slug field will auto-populate based on the Name field, but this can be manually overwritten Click the Create button","title":"Creating a Tenant"},{"location":"user-guides/getting-started/tenants.html#assigning-a-tenant-to-an-object","text":"It is simple to assign a Tenant to an existing object. This next example will add a Tenant to an existing Device. Click on Devices in the top navigation menu Look for the Devices option and click on it This will take you to the Devices page Click on the specific Device you want to add the Tenant to This will take you to the main page for that Device On the specific Device page, click on the Edit button Once on the page to edit the Device: Make a selection from the Tenant drop-down menu selector Click the Update button This will take you back to the main page for the Device. Notice that the Tenant field is now populated/updated.","title":"Assigning a Tenant to an Object"},{"location":"user-guides/getting-started/vlans-and-vlan-groups.html","text":"VLANS and VLAN Groups \u00b6 Each VLAN may be assigned to a site, tenant, and/or VLAN group. Each VLAN must be assigned a status. The following statuses are available by default: Active Reserved Deprecated In general, VLANs can have overlapping names and IDs. The exception to this is VLANs within a VLAN Group: each VLAN within a group must have a unique ID and name. A VLAN Group may be assigned to a specific site, but a Group cannot belong to multiple sites. VLANs may be assigned to a specific site as well. The Nautobot documentation has more info about VLANs and VLAN Groups . VLAN Example \u00b6 The following example will show: Creating a vlan 200 without a site assignment ( global scope) Creating two VLANs, each with overlapping Names and IDs: ID = 100 and Name = vlan 100 Neither of the vlan 100 instances will be assigned to a group, but each of the VLANs will be assigned to a different site ( site-specific scope) How the site-specific and global scopes affect which VLANs can be assigned on which Devices Note This example will require a Site ( Ottawa 1 ) within a Region ( Ottawa ) in the North America Region in addition to the Vancouver 1 Site and Vancouver Region created prior. Refer back to the Create a Site and Create a Region sections to do so. Creating the VLANs \u00b6 Click on IPAM in the top navigation menu Look for the VLANS option and click on the + to go to the Add a new VLAN form Populate ID with 200 Populate Name with vlan 200 Select Status as Active Click on Create and Add Another to save; you will then be taken to the Add a new VLAN form Note The required parameters to create a new VLAN are bolded in the Add a new VLAN form: ID , Name , and Status Now we'll create two instances of VLANs, each with ID = 100 and Name = vlan 100 and an Active Status . The differentiator will be that one instance will be assigned to the Vancouver 1 Site and the other to the Ottawa 1 Site. On the Add a new VLAN form: Populate ID with 100 Populate Name with vlan 100 Select Status as Active Select Vancouver from the Region selector drop-down Select Vancouver 1 from the Site selector drop-down Click on the Create and Add Another button Note The Region drop-down selection in step 4 is optional and only meant to narrow down the options presented in the Site drop-down selector in Step 5. A VLAN cannot be assigned to a Region. Populate ID with 100 Populate Name with vlan 100 Select Status as Active Select Ottawa from the Region selector drop-down Select Ottawa 1 from the Site selector drop-down Click on the Create button when complete with the second instance Once you've created the three VLANs and then hit the Create button, you will be taken to the VLANs main page. On that page, you'll see the three VLANs and the Site assignment for each one. Each vlan 100 instance will have a Site assignment, while vlan 200 will not: Assigning VLANs to an Interface \u00b6 To assign a VLAN to an Interface: Click on IPAM on the top-level navigation menu Select Devices to go to the Devices main page Click on the name of the Device you wish to add a VLAN to ( edge2.van1 ) in this example Click on the Edit button for the xe-0/0/0 Interface to go to the Editing interface xe-0/0/0 page On the Editing interface xe-0/0/0 page, set 802.1Q Mode to Access (or whatever mode you need) and then click on the VLAN drop-down selector. Notice that there are two choices: One choice is the vlan 100 instance specifically assigned to the Vancouver 1 Site The other choice is vlan 200 , which was not assigned to a Site, and thus has a global scope Note The vlan 100 instance that is assigned to the Ottawa 1 Site does not show up as an option for the Vancouver1 Site","title":"VLANS and VLAN Groups"},{"location":"user-guides/getting-started/vlans-and-vlan-groups.html#vlans-and-vlan-groups","text":"Each VLAN may be assigned to a site, tenant, and/or VLAN group. Each VLAN must be assigned a status. The following statuses are available by default: Active Reserved Deprecated In general, VLANs can have overlapping names and IDs. The exception to this is VLANs within a VLAN Group: each VLAN within a group must have a unique ID and name. A VLAN Group may be assigned to a specific site, but a Group cannot belong to multiple sites. VLANs may be assigned to a specific site as well. The Nautobot documentation has more info about VLANs and VLAN Groups .","title":"VLANS and VLAN Groups"},{"location":"user-guides/getting-started/vlans-and-vlan-groups.html#vlan-example","text":"The following example will show: Creating a vlan 200 without a site assignment ( global scope) Creating two VLANs, each with overlapping Names and IDs: ID = 100 and Name = vlan 100 Neither of the vlan 100 instances will be assigned to a group, but each of the VLANs will be assigned to a different site ( site-specific scope) How the site-specific and global scopes affect which VLANs can be assigned on which Devices Note This example will require a Site ( Ottawa 1 ) within a Region ( Ottawa ) in the North America Region in addition to the Vancouver 1 Site and Vancouver Region created prior. Refer back to the Create a Site and Create a Region sections to do so.","title":"VLAN Example"},{"location":"user-guides/getting-started/vlans-and-vlan-groups.html#creating-the-vlans","text":"Click on IPAM in the top navigation menu Look for the VLANS option and click on the + to go to the Add a new VLAN form Populate ID with 200 Populate Name with vlan 200 Select Status as Active Click on Create and Add Another to save; you will then be taken to the Add a new VLAN form Note The required parameters to create a new VLAN are bolded in the Add a new VLAN form: ID , Name , and Status Now we'll create two instances of VLANs, each with ID = 100 and Name = vlan 100 and an Active Status . The differentiator will be that one instance will be assigned to the Vancouver 1 Site and the other to the Ottawa 1 Site. On the Add a new VLAN form: Populate ID with 100 Populate Name with vlan 100 Select Status as Active Select Vancouver from the Region selector drop-down Select Vancouver 1 from the Site selector drop-down Click on the Create and Add Another button Note The Region drop-down selection in step 4 is optional and only meant to narrow down the options presented in the Site drop-down selector in Step 5. A VLAN cannot be assigned to a Region. Populate ID with 100 Populate Name with vlan 100 Select Status as Active Select Ottawa from the Region selector drop-down Select Ottawa 1 from the Site selector drop-down Click on the Create button when complete with the second instance Once you've created the three VLANs and then hit the Create button, you will be taken to the VLANs main page. On that page, you'll see the three VLANs and the Site assignment for each one. Each vlan 100 instance will have a Site assignment, while vlan 200 will not:","title":"Creating the VLANs"},{"location":"user-guides/getting-started/vlans-and-vlan-groups.html#assigning-vlans-to-an-interface","text":"To assign a VLAN to an Interface: Click on IPAM on the top-level navigation menu Select Devices to go to the Devices main page Click on the name of the Device you wish to add a VLAN to ( edge2.van1 ) in this example Click on the Edit button for the xe-0/0/0 Interface to go to the Editing interface xe-0/0/0 page On the Editing interface xe-0/0/0 page, set 802.1Q Mode to Access (or whatever mode you need) and then click on the VLAN drop-down selector. Notice that there are two choices: One choice is the vlan 100 instance specifically assigned to the Vancouver 1 Site The other choice is vlan 200 , which was not assigned to a Site, and thus has a global scope Note The vlan 100 instance that is assigned to the Ottawa 1 Site does not show up as an option for the Vancouver1 Site","title":"Assigning VLANs to an Interface"}]}